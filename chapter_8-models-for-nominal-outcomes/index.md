# [Categorical Dependent Var] Chapter_8 ：Models for nominal outcomes


**An outcome is nominal when the categories are assumed to be unordered.** For example, marital status can be grouped nominally into the categories of divorced, never married, married, or widowed. Occupations might be organized as professional, white collar, blue collar, craft, and menial. Other examples include reasons for leaving the parents' home, the organizational context of scientific work such as industry, government, and academia, and the choice of language in a multilingual society. Further, in some cases, you might prefer to treat an outcome as nominal even though it is ordered, ordered on multiple dimensions, or partially ordered. For example, if the response categories are strongly agree, agree, disagree, strongly disagree, and don’t know, the category "don’t know” invalidates models for ordinal outcomes. Or, you might decide to use a nominal regression model when the assumption of parallel regressions is rejected. In general, if you have concerns about the ordinality of the dependent variable, the potential loss of efficiency in using models for nominal outcomes is outweighed by avoiding potential bias.

**In this chapter, we focus on the multinomial logit model (MNL), which is the most frequently used nominal regression model. This model essentially fits separate binary logits for each pair of outcome categories.** 

Next, we consider the stereotype logistic regression model. Although this model is often used for ordinal outcomes, it is closely related to the MNL. These models assume that the data are case-specific, meaning that each independent variable has one value for each individual. Examples of such variables are an individual’s race or education.

After that, we consider several models that include alternative-specific data. Alternative-specific variables vary not only by individual but also by the alternative. For example, if a commuter is selecting one of three modes of travel, an alternative-specific predictor might be her travel time using each alternative.

We use “alternative” to refer to a possible outcome. Sometimes we refer to an alternative as an outcome, a category, or a comparison group to be consistent with the usual terminology for a model or the output generated by Stata. The term “choice” refers to the alternative that is actually observed, which can be thought of as the “most preferred” alternative. For example, if the dependent variable is the party voted for in the last presidential election, the alternatives might be Republican, Democrat, and Independent. If a person voted for the alternative of Democrat, we would say that the choice for this case is Democrat. But you should not infer from the term “choice” that the models we describe can be used only for data where the outcome occurs through a process of choice. For example, if we were modeling the type of injuries that people entering the emergency room of a hospital have, we would use the term “choice” even though the injury sustained is unlikely to be a choice.

**We begin by discussing the MNL, where the biggest challenge is that the model includes many parameters and so it is easy to be overwhelmed by the complexity of the results.** This complexity is compounded by the nonlinearity of the model, which leads to the same difficulties of interpretation found for models in prior chapters. Although fitting the model is straightforward, interpretation involves challenges that are the focus of this chapter. We begin by reviewing the statistical model, followed by a discussion of testing, fit, and finally, methods of interpretation. For a more technical discussion of the model, see Long (1997). As discussed in chapter 1, you can obtain sample do-files and data files by downloading the spostl3_do package.

The outcome for the primary example we have chosen for this chapter is political party affiliation, collected from a survey that used the categories strong Democrat, Democrat, Independent, Republican, and strong Republican. Although this variable may initially appear to be ordinal, our analysis suggests that it is ordered on two dimensions relative to the independent variables we consider. On the attribute of left-right orientation, the categories increase from strong Democrat to strong Republican. In terms of intensity of partisanship, the categories are ordered Independent to either Republican or Democrat and then either strong Republican or strong Democrat. This violates Stevens’ (1946) definition of an ordinal scale as a variable that uses numbers to indicate rank ordering on a single attribute. **Indeed, when you use an ordinal model, we recommend also fitting the model using multinomial logit as a sensitivity analysis.**

## 1 The multinomial logit model

The MNLM can be thought of as simultaneously fitting binary logits for all comparisons among the alternatives. For example, let party3 be a categorical variable with the outcomes D for Democrat, I for Independent, and R for Republican. Assume that there is one independent variable measuring income in $1,000$s. We can examine the effect of income on party3 by fitting three binary logits:

$$\ln\frac{\Pr\left(D\mid\mathbf{x}\right)}{\Pr\left(I\mid\mathbf{x}\right)}=\beta_{0,\mathrm{D}\mid I}+\beta_{1,\mathrm{D}\mid I}\text{income}$$

$$\ln\frac{\Pr\left(R\mid\mathbf{x}\right)}{\Pr\left(I\mid\mathbf{x}\right)}=\beta_{0,\mathrm{R}\mid I}+\beta_{1,\mathrm{R}\mid I}\text{income}$$

$$\ln\frac{\Pr(D\mid\mathbf{x})}{\Pr(R\mid\mathbf{x})}=\beta_{0,\mathrm{D}\mid\mathrm{R}}+\beta_{1,\mathrm{D}\mid\mathrm{R}}\text{income}$$

Where the subscripts to the $\beta ' s$ indicate which comparison is being made. For example, $\beta_{\mathrm{1,D|I}}$ is the coefficient for the first independent variable for the comparison of Democrat and Independent.

The three binary logits include redundant information. Because $\ln a/b=\ln a-\ln b$, the following equality must hold:

$$\ln\frac{\Pr\left(D\mid\mathbf{x}\right)}{\Pr\left(I\mid\mathbf{x}\right)}-\ln\frac{\Pr\left(R\mid\mathbf{x}\right)}{\Pr\left(I\mid\mathbf{x}\right)}=\ln\frac{\Pr\left(D\mid\mathbf{x}\right)}{\Pr\left(R\mid\mathbf{x}\right)}$$

This implies that

$$\beta_{0,\mathrm{D}|1}-\beta_{0,\mathrm{R}|1}=\beta_{0,\mathrm{D}|\mathrm{R}}$$
$$\beta_{\mathrm{1,D|I}}-\beta_{\mathrm{1,R|I}}=\beta_{\mathrm{1,D|R}}$$

In general, with $J$ alternatives, only $J - 1$ binary logits need to be fit. These $J - 1$ logits are referred to as a minimal set. Estimates for the remaining coefficients can be computed using equalities of the sort shown in (8.1).


当你在学习这些概念时，确保理解是至关重要的，我会尽力详细解释。首先，我们来看 MNLM（多项式 Logit 模型）。这个模型的核心思想是同时为所有可比较的选择（例如，民主党、独立派、共和党）拟合二元逻辑回归模型。这意味着我们想要了解一些因素（例如收入）对于一个人选择哪个政党的影响。

首先，模型中使用了一些数学符号和公式，我们会一步步解释。首先，公式中的 $\ln$ 表示自然对数，它可以将一个概率转换为一个连续的值，这样我们可以更容易地进行建模和分析。$\Pr$ 表示概率，它是一个事件发生的可能性。$\beta$ 是我们要估计的参数，它们告诉我们不同因素对于选择的影响程度。

让我们以一个具体的例子来解释。假设我们有一个名为 party3 的变量，它有三种取值：D（民主党）、I（独立派）和 R（共和党）。我们想知道收入对于一个人选择这三个政党的影响。

我们将收入用 $\text{income}$ 表示，单位是千美元。然后，我们可以分别观察收入对于三种政党的影响：

1. 民主党与独立派之间的比较：
   $$\ln\frac{\Pr(D|\mathbf{x})}{\Pr(I|\mathbf{x})}=\beta_{0,\mathrm{D}|I}+\beta_{1,\mathrm{D}|I}\text{income}$$

2. 共和党与独立派之间的比较：
   $$\ln\frac{\Pr(R|\mathbf{x})}{\Pr(I|\mathbf{x})}=\beta_{0,\mathrm{R}|I}+\beta_{1,\mathrm{R}|I}\text{income}$$

3. 民主党与共和党之间的比较：
   $$\ln\frac{\Pr(D|\mathbf{x})}{\Pr(R|\mathbf{x})}=\beta_{0,\mathrm{D}|R}+\beta_{1,\mathrm{D}|R}\text{income}$$

这里的 $\mathbf{x}$ 表示其他可能影响选择的变量，例如教育水平、地理位置等等。

让我们详细解释每个公式：

1. 民主党与独立派之间的比较：
   这个公式告诉我们，我们想知道一个人更倾向于选择民主党还是独立派的可能性。我们用 $\ln\frac{\Pr(D|\mathbf{x})}{\Pr(I|\mathbf{x})}$ 表示这个可能性的对数比率。这里 $\Pr(D|\mathbf{x})$ 是在给定其他因素 $\mathbf{x}$ 的情况下选择民主党的概率，$\Pr(I|\mathbf{x})$ 是选择独立派的概率。$\beta_{0,\mathrm{D}|I}$ 是一个常数项，$\beta_{1,\mathrm{D}|I}$ 是收入对于选择民主党的影响系数。

2. 共和党与独立派之间的比较：
   这个公式告诉我们，我们想知道一个人更倾向于选择共和党还是独立派的可能性。同样地，我们用 $\ln\frac{\Pr(R|\mathbf{x})}{\Pr(I|\mathbf{x})}$ 表示这个可能性的对数比率。其中 $\Pr(R|\mathbf{x})$ 是在给定其他因素 $\mathbf{x}$ 的情况下选择共和党的概率，$\Pr(I|\mathbf{x})$ 是选择独立派的概率。$\beta_{0,\mathrm{R}|I}$ 是一个常数项，$\beta_{1,\mathrm{R}|I}$ 是收入对于选择共和党的影响系数。

3. 民主党与共和党之间的比较：
   这个公式告诉我们，我们想知道一个人更倾向于选择民主党还是共和党的可能性。同样地，我们用 $\ln\frac{\Pr(D|\mathbf{x})}{\Pr(R|\mathbf{x})}$ 表示这个可能性的对数比率。其中 $\Pr(D|\mathbf{x})$ 是在给定其他因素 $\mathbf{x}$ 的情况下选择民主党的概率，$\Pr(R|\mathbf{x})$ 是选择共和党的概率。$\beta_{0,\mathrm{D}|R}$ 是一个常数项，$\beta_{1,\mathrm{D}|R}$ 是收入对于选择民主党的影响系数。

现在，让我们结合一个具体的例子来说明这些公式。

假设我们有两个人，他们的收入分别为 50,000 美元和 100,000 美元。我们想知道他们更倾向于选择哪个政党。我们还知道他们的教育水平、居住地等其他因素。

根据我们的模型，我们可以计算出两个人选择民主党、独立派和共和党的可能性。然后，我们可以使用模型中的系数来计算收入对于他们的影响。

例如，对于第一个人：
- 民主党与独立派之间的可能性对数比率是 0.5
- 共和党与独立派之间的可能性对数比率是 -0.3
- 民主党与共和党之间的可能性对数比率是 0.8

现在，我们可以使用模型中的系数来计算收入对于这些可能性的影响。这样，我们就可以了解收入对于不同政党偏好的影响程度。


首先，我们注意到这些公式之间存在一些冗余，因为对数的性质。作者提到了一个重要的等式，即 $\ln a/b = \ln a - \ln b$。这意味着我们可以用一个公式推导出其他的。在这里，作者使用这个等式来说明我们不需要拟合所有可能的组合，而只需拟合其中一部分，然后通过这些等式来推断其他部分。这样，我们就可以更有效地分析数据，而不会浪费时间和资源。

具体来说，作者指出了一个等式：
$$\ln\frac{\Pr(D|\mathbf{x})}{\Pr(I|\mathbf{x})} - \ln\frac{\Pr(R|\mathbf{x})}{\Pr(I|\mathbf{x})} = \ln\frac{\Pr(D|\mathbf{x})}{\Pr(R|\mathbf{x})}$$

这个等式告诉我们，我们可以通过计算两个比率的对数差来得到第三个比率的对数。这使得我们不必单独拟合每个比较的模型，而只需要拟合其中一部分，然后通过这些等式来计算其他部分。

作者进一步解释了这个等式的含义。对于每个可能的选择，我们只需要拟合一个与其他选择比较的模型。例如，在三个可能的选择中，我们只需要拟合两个模型：一个用来比较民主党和独立派，另一个用来比较民主党和共和党。其他的比较可以通过这些模型和等式来推断出来。

这样做的好处是我们不必为每个可能的选择都拟合一个模型，从而节省了时间和资源。相反，我们只需要拟合几个模型，并使用数学性质来计算其他模型的参数。这使得分析更加高效，同时保持了模型的准确性。


在作者的讨论中，他们提到了更一般的情况，即对于具有 $J$ 个选择的情况，我们只需要拟合 $J-1$ 个二元逻辑回归模型。这些模型构成了一个最小集。其他系数的估计可以使用类似前面提到的等式来计算。

例如，如果有 $J=4$ 个选择，那么我们只需要拟合三个二元逻辑回归模型，用来比较其中三个选择。其他的比较可以通过这些模型和等式来计算。

作者在最后还提到了一些系数之间的关系：
$$\beta_{0,\mathrm{D}|I} - \beta_{0,\mathrm{R}|I} = \beta_{0,\mathrm{D}|R}$$
$$\beta_{1,\mathrm{D}|I} - \beta_{1,\mathrm{R}|I} = \beta_{1,\mathrm{D}|R}$$

这些关系告诉我们，在不同的比较中，一些系数是相关的。例如，第一个等式告诉我们，民主党和独立派之间的常数项减去共和党和独立派之间的常数项等于民主党和共和党之间的常数项。这些关系在理解模型和进行参数估计时非常有用。

综上所述，作者在讨论中详细解释了如何使用二元逻辑回归模型来比较多个选择，并且提出了一些关于参数估计和模型构建的重要概念。这些概念对于理解模型的含义和进行实际分析非常重要。

Fitting the MNLM by fitting a series of binary logits is not optimal because each binary logit is based on a different sample. For example, in the logit comparing $D$ with $I$. those in $R$ are dropped. To see this, we begin by loading the data and examining the distribution of **party3**:

```
use partyid4, clear
tab party3
```

       Party ID |      Freq.     Percent        Cum.
    ------------+-----------------------------------
       Democrat |        693       50.14       50.14
    Independent |        151       10.93       61.07
     Republican |        538       38.93      100.00
    ------------+-----------------------------------
          Total |      1,382      100.00

Next, we fit a binary logit model comparing Democrats with Independents by using the dependent variable **dem_ind**:

```
tab dem_ind,miss
```

    Democrat or |
    Independent |      Freq.     Percent        Cum.
    ------------+-----------------------------------
    Independent |        151       10.93       10.93
       Democrat |        693       50.14       61.07
              . |        538       38.93      100.00
    ------------+-----------------------------------
          Total |      1,382      100.00

```
logit dem_ind income,nolog
```

    Logistic regression                                     Number of obs =    844
                                                            LR chi2(1)    =   0.48
                                                            Prob > chi2   = 0.4873
    Log likelihood = -396.21646                             Pseudo R2     = 0.0006
    
    ------------------------------------------------------------------------------
         dem_ind | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]
    -------------+----------------------------------------------------------------
          income |      -0.00       0.00    -0.70   0.483        -0.01        0.00
           _cons |       1.61       0.15    10.81   0.000         1.31        1.90
    ------------------------------------------------------------------------------

The logit includes only 844 cases out of the sample of 1,382 because those who are Republican are excluded as missing. Next, we fit a binary logit comparing **Republicans** and **Independents**, excluding **Democrats** from the sample:

```
tab rep_ind,miss
```

     Republican |
             or |
    Independent |      Freq.     Percent        Cum.
    ------------+-----------------------------------
    Independent |        151       10.93       10.93
     Republican |        538       38.93       49.86
              . |        693       50.14      100.00
    ------------+-----------------------------------
          Total |      1,382      100.00

```
logit rep_ind income,nolog
```

    Logistic regression                                     Number of obs =    689
                                                            LR chi2(1)    =  20.41
                                                            Prob > chi2   = 0.0000
    Log likelihood = -352.09947                             Pseudo R2     = 0.0282
    
    ------------------------------------------------------------------------------
         rep_ind | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]
    -------------+----------------------------------------------------------------
          income |       0.02       0.00     4.19   0.000         0.01        0.02
           _cons |       0.66       0.16     4.05   0.000         0.34        0.98
    ------------------------------------------------------------------------------

And last, we exclude Independents:

```
tab dem_rep,miss
```

    Democrat or |
     Republican |      Freq.     Percent        Cum.
    ------------+-----------------------------------
     Republican |        538       38.93       38.93
       Democrat |        693       50.14       89.07
              . |        151       10.93      100.00
    ------------+-----------------------------------
          Total |      1,382      100.00

```
logit dem_rep income,nolog
```

    Logistic regression                                     Number of obs =  1,231
                                                            LR chi2(1)    =  71.89
                                                            Prob > chi2   = 0.0000
    Log likelihood = -807.53617                             Pseudo R2     = 0.0426
    
    ------------------------------------------------------------------------------
         dem_rep | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]
    -------------+----------------------------------------------------------------
          income |      -0.02       0.00    -8.00   0.000        -0.02       -0.01
           _cons |       0.95       0.10     9.10   0.000         0.75        1.16
    ------------------------------------------------------------------------------

The results from the binary logits can be compared with those obtained by fitting the MNLM with **``mlogit``**.

```
mlogit party3 income,nolog base(2)
```

    Multinomial logistic regression                         Number of obs =  1,382
                                                            LR chi2(2)    =  73.84
                                                            Prob > chi2   = 0.0000
    Log likelihood = -1283.3075                             Pseudo R2     = 0.0280
    
    ------------------------------------------------------------------------------
          party3 | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]
    -------------+----------------------------------------------------------------
    Democrat     |
          income |      -0.00       0.00    -0.73   0.464        -0.01        0.00
           _cons |       1.61       0.15    10.54   0.000         1.31        1.91
    -------------+----------------------------------------------------------------
    Independent  |  (base outcome)
    -------------+----------------------------------------------------------------
    Republican   |
          income |       0.02       0.00     4.15   0.000         0.01        0.02
           _cons |       0.68       0.16     4.24   0.000         0.36        0.99
    ------------------------------------------------------------------------------


The output is divided into three panels. The top panel is labeled **Democrat**, which is the value label for the first outcome category of the dependent variable; the second panel is labeled **Independent**, which is the base outcome that we discuss shortly; and the third panel corresponds to the third outcome, **Republican**. The coefficients in the first and third panels are for comparisons with the base outcome, **Independent**. Thus the panel **Democrat** shows estimates of coefficients from the comparison of D with the base I, while the panel **Republican** holds the estimates comparing R with I. Accordingly, the top panel should be compared with the coefficients from the binary logit for D and I (outcome variable dem_ind). For example, the estimate for the comparison of D with I from **``mlogit``** is $\beta_{1,D|I} = -0.002724$ with $z = -0.73$, whereas the logit estimate is $\beta_{1 ,D|I} = -0.0024887$ with $z = -0.70$. Overall, the estimates from the binary model are close to those from the MNLM but not exactly the same.

Although theoretically $\beta_{1,D|I} - \beta_{1,R|I} = \beta_{1,D|R}$, the estimates from the binary logits are $\beta_{1,D|I} - \beta_{1,R|I} = (-0.0024887) - (0.0156761) = -0.0181648$, which do not quite equal the binary logit estimate $\beta_{1,D|R} = -0.0183709$. This occurs because a series of binary logits fit with **logit** does not impose the constraints among coefficients that are implicit in the definition of the MNLM. When fitting the model with **mlogit**, these constraints are imposed. Indeed, the output from **mlogit** presents only two of the three comparisons from our example, namely, D versus I and R versus I. The remaining comparison, D versus R, is exactly equal to the difference between the two sets of estimated coefficients. The critical point here is simple:

>The MNLM may be understood as a set of binary logits among all pairs of outcomes.

### 1.1 Formal statement of the model

The MNLM can be written as

$$\ln\Omega_{m|b}\left(\mathbf{x}\right)=\ln\frac{\Pr\left(y=m\mid\mathbf{x}\right)}{\Pr\left(y=b\mid\mathbf{x}\right)}=\text{x}\beta_{m|b}\quad\mathrm{for~}m=1\mathrm{~to~}J$$

Where **b** is the base outcome, sometimes called the reference category. As $\ln\Omega_{b|b}(x) = \ln 1 = 0$, it follows that $\beta_{b|b}= 0$. That is, the log odds of an outcome compared with itself is always 0, and thus the effects of any independent variables must also be 0.

These **J** equations can be solved to compute the probabilities for each outcome:

$$\Pr\left(y=m\mid\mathbf{x}\right)=\frac{\exp\left(\mathbf{x}\beta_{m\mid b}\right)}{\sum_{j=1}^{j}\exp\left(\mathbf{x}\beta_{j\mid b}\right)}$$

The probabilities will be the same regardless of the base outcome **b** that is used. For example, suppose that you have three outcomes and fit the model with alternative 1 as the base, where you would obtain estimates $\hat\beta_{2|1}$ and $\hat\beta_{3|1}$, with $\beta_{1|1} = 0$. The probability equation is

$$\Pr\left(y=m\mid\mathbf{x}\right)=\frac{\exp\left(\mathbf{x}\beta_{m\mid1}\right)}{\sum_{j=1}^{J}\exp\left(\mathbf{x}\beta_{j\mid1}\right)}$$

If someone else set up the model with base outcome 2, they would obtain estimates $\hat\beta_{3|2}$ and $\beta_{1|2} = 0$. Their probability equation would be

$$\Pr\left(y=m\mid\mathbf{x}\right)=\frac{\exp\left(\mathbf{x}\beta_{m\mid2}\right)}{\sum_{j=1}^{J}\exp\left(\mathbf{x}\beta_{j\mid2}\right)}$$

The estimated parameters are different because they are estimating different things, but they produce exactly the same predictions. Confusion arises only if you are not clear about which parameterization you are using. We return to this issue when we discuss how Stata’s **mlogit** parameterizes the model in the next section.

在这段文字中，我们深入了解了多项 Logit 模型（MNLM）的关键概念和公式。

首先，MNLM 可以写成如下形式：
$$\ln\Omega_{m|b}\left(\mathbf{x}\right)=\ln\frac{\Pr\left(y=m\mid\mathbf{x}\right)}{\Pr\left(y=b\mid\mathbf{x}\right)}=\text{x}\beta_{m|b}\quad\mathrm{for~}m=1\mathrm{~to~}J$$
这个方程告诉我们，选择结果 $m$ 相对于基准结果 $b$ 的对数比率 $\ln\Omega_{m|b}$ 可以通过自变量 $\mathbf{x}$ 的线性函数来表示，系数为 $\beta_{m|b}$。作者指出了一个重要的性质，即基准结果相对于自身的对数比率始终为 0，因此 $\beta_{b|b}=0$。这意味着相对于自身的对数几率永远为 0，因此任何自变量的影响也必须为 0。


在这个公式中，$\Omega_{m|b}$ 表示选择结果 $m$ 相对于基准结果 $b$ 的对数比率。$\mathbf{x}$ 是自变量向量，$\beta_{m|b}$ 是相应的系数。选择不同的基准结果 $b$ 会导致不同的参数估计值。

举个例子，假设我们正在研究一个人选择早餐的情况，可能选择吃麦片、吃鸡蛋或吃面包。我们使用年龄和性别作为自变量来预测选择结果。如果我们选择吃麦片作为基准结果，我们将得到一组系数估计值，如 $\beta_{2|1}$ 和 $\beta_{3|1}$，分别对应吃鸡蛋和吃面包相对于吃麦片的对数比率。然而，如果我们选择吃鸡蛋作为基准结果，我们将得到另一组系数估计值，如 $\beta_{1|2}$ 和 $\beta_{3|2}$，分别对应吃麦片和吃面包相对于吃鸡蛋的对数比率。

总之，不同基准结果会导致不同的系数估计值，但模型的预测能力并不会受到影响。换句话说，无论我们选择哪个结果作为基准，模型都会产生相同的预测结果。因此，在实际应用中，我们通常不需要过多地关注选择基准结果的影响，只需专注于模型的预测和解释能力即可。

接着，我们了解了如何使用这些方程来计算每个结果的概率。概率公式为：
$$\Pr\left(y=m\mid\mathbf{x}\right)=\frac{\exp\left(\mathbf{x}\beta_{m\mid b}\right)}{\sum_{j=1}^{J}\exp\left(\mathbf{x}\beta_{j\mid b}\right)}$$
这个公式告诉我们，在给定自变量 $\mathbf{x}$ 的情况下，选择结果 $m$ 的概率是多少。分子是选择结果 $m$ 的对数几率的指数形式，分母是所有可能结果的对数几率的指数形式之和。这种形式确保了所有概率之和为 1。

举个例子，我们考虑了三个可能的选择，并且将第一个选择作为基准结果。我们得到了估计值 $\hat\beta_{2|1}$ 和 $\hat\beta_{3|1}$，其中 $\beta_{1|1}=0$。概率方程为：
$$\Pr\left(y=m\mid\mathbf{x}\right)=\frac{\exp\left(\mathbf{x}\beta_{m\mid1}\right)}{\sum_{j=1}^{J}\exp\left(\mathbf{x}\beta_{j\mid1}\right)}$$
如果另外有人将第二个选择作为基准结果建立模型，他们会得到估计值 $\hat\beta_{3|2}$ 和 $\beta_{1|2}=0$。他们的概率方程与之前的形式相同，但系数向量会有所不同。

这种灵活性使得在实际应用中，我们可以自由选择基准结果，而不会影响到模型的预测能力。这也说明了参数估计值的变化不会影响到模型的预测结果，只有在不清楚使用的参数化方式时才会产生混淆。

接下来，我们可以进一步探讨不同基准结果之间的参数估计值的变化，以及这种变化对模型的解释和应用的影响。

在实际应用中，选择不同的基准结果可能会导致不同的参数估计值。例如，在我们的例子中，选择第一个选择作为基准结果会得到一组估计值，而选择第二个选择作为基准结果会得到另一组估计值。这是因为不同的基准结果会导致对比较的选择组合进行重新参数化。

然而，重要的是要理解，虽然参数估计值可能会因基准结果的选择而有所不同，但模型的预测结果是一样的。这是因为参数估计值的变化只会影响到模型的参数化方式，而不会影响到模型的预测能力。因此，在实际应用中，我们通常不需要过于关注基准结果的选择，只需专注于模型的预测能力和解释能力即可。

举个例子，假设我们想预测一个人的早餐选择：吃麦片、吃鸡蛋还是吃面包。我们用年龄和性别作为特征来预测。假设我们选择吃麦片作为基准结果。然后我们会计算出吃鸡蛋和吃面包相对于吃麦片的对数比率，以及吃鸡蛋和吃面包的概率。即使我们选择了不同的基准结果，我们仍然可以得到相同的预测结果。

假设我们继续使用早餐选择的例子，但这次我们选择吃鸡蛋作为基准结果。那么，我们将重新计算吃麦片和吃面包相对于吃鸡蛋的对数比率，并且重新计算吃麦片和吃面包的概率。尽管参数估计值可能会不同，但我们得到的预测结果仍然是相同的。这意味着我们可以自由地选择基准结果，而不必担心模型的预测能力。

这就是为什么在实际应用中，我们通常不太关心选择哪个结果作为基准结果。重要的是要理解不同参数化方式的影响，并且清楚地表达出模型的参数化方式。这样，我们就可以更好地解释模型的结果，以及做出更准确的预测和决策。

注：当我们说理解不同参数化方式的影响时，我们指的是不同选择基准结果所导致的参数估计值的变化，以及这种变化对模型解释和预测的影响。在多项 Logit 模型中，我们可以选择任何一个结果作为基准结果来建立模型。不同的基准结果会导致模型参数估计值的变化。具体来说，基准结果的选择会影响到模型中的截距项和其他系数的估计值。这是因为基准结果的选择会导致模型中的比较组合重新参数化。然而，重要的是要理解，尽管参数估计值可能会因选择不同的基准结果而有所不同，但模型的预测能力是不变的。换句话说，无论我们选择哪个结果作为基准结果，模型都会产生相同的预测结果。这是因为模型的预测能力不依赖于参数估计值的绝对大小，而是依赖于参数之间的相对关系。因此，理解不同参数化方式的影响意味着我们要意识到选择不同的基准结果会导致参数估计值的变化，但这种变化不会影响到模型的预测能力。在实际应用中，我们通常不需要过多地关注选择基准结果的影响，只需专注于模型的预测和解释能力即可。

总之，MNLM 模型提供了一种强大的工具来比较多个选择，并且通过不同的参数化方式得到相同的预测结果。这种灵活性使得模型在实践中更易于应用，并且不同的参数化方式不会影响到模型的预测能力。因此，在解释和应用模型时，重要的是要理解不同参数化方式的影响，并清楚地表达出模型的参数化方式。

## 2 Estimation using the mlogit command

The MNLM is fit with the following command and its basic options:

**``mlogit depvar [indepvars] [if] [in] [weight], baseoutcome(#) vce(vcetype)``**

For other options, run **``help mlogit``**. In our experience, the model converges quickly, even when there are many outcome categories and independent variables.

* Variable lists

**``depvar``** is the dependent variable. The specific values taken on by the dependent variable are irrelevant as long as they are integers. For example, if you had three outcomes, you could use the values 1, 2, and 3 or -1, 0, and 999. Nevertheless, to avoid confusion, we strongly recommend coding your outcome as consecutive integers beginning with 1.

**``indepvars``** is a list of independent variables. If **``indepvars``** is not included, Stata fits a model with only constants.

* Specifying the estimation sample

The **``if``** and **``in``** qualifiers can be used to restrict the estimation sample. For example, if you want to fit the model with only white respondents, use the command **``mlogit party i.edu income10 if black==0``**.

**``Listwise deletion``**. Stata excludes cases in which there are missing values for any of the variables. Accordingly, if two models are fit using the same dataset but have different sets of independent variables, it is possible to have different samples. We recommend that you use **``mark``** and **``markout``** (discussed in chapter 3) to explicitly remove cases with missing data.

* Weights and complex samples
  
**``mlogit``** can be used with **``fweights``**, **``pweights``**, and **``iweights``**. Survey estimation is supported. See chapter 3 for details.

* Options

**``noconstant``** excludes the constant terms from the model.

**``baseoutcome(#)``** **specifies the value of **``depvar``** that is the base outcome (that is, reference group) for the coefficients that are listed.** This determines how the model is parameterized. If **``baseoutcome()``** is not specified, the most frequent outcome in the estimation sample is used as the base. The base is reported as (base outcome) in the table of estimates.

**``vce(vcetype)``** specifies the type of standard errors to be computed. See section 3.9.1 for details.

**``rrr``** **reports the estimated coefficients transformed to relative-risk ratios, defined as exp(b) rather than b, along with standard errors and confidence intervals for these ratios.** Relative risk ratios are also referred to as odds ratios.

### 2.1 Example of MNLM

The 1992 American National Election Study asked respondents to indicate their political party using one of eight categories. We used these to create **``party``** with five categories: strong Democrat (**``1 = StrDem``**), Democrat (**``2 = Dem``**), independent (**``3 = Indep``**), Republican (**``4 = Rep``**), and strong Republican (**``5 = StrRep``**):

```
use partyid4, clear
tabulate party, miss
```

       Party ID |      Freq.     Percent        Cum.
    ------------+-----------------------------------
         StrDem |        266       19.25       19.25
            Dem |        427       30.90       50.14
          Indep |        151       10.93       61.07
            Rep |        369       26.70       87.77
         StrRep |        169       12.23      100.00
    ------------+-----------------------------------
          Total |      1,382      100.00

To simplify our notation, at times we abbreviate StrDem as **``SD``**, Dem as **``D``**, Indep as **``I``**, Rep as **``R``**, and StrRep as **``SR``**.

Five regressors are included in the model: age, income, race (indicated as black or not), gender, and education (measured as not completing high school, completing high school but not college, and completing college). Descriptive statistics for the continuous and binary variables are:

```
sum age income black female
```

        Variable |        Obs        Mean    Std. dev.       Min        Max
    -------------+---------------------------------------------------------
             age |      1,382    45.94645    16.78311         18         91
          income |      1,382    37.45767    27.78148        1.5     131.25
           black |      1,382    .1374819      .34448          0          1
          female |      1,382    .4934877    .5001386          0          1

The distribution of educational attainment is

```
tabulate educ, miss
```

       Level of |
      education |      Freq.     Percent        Cum.
    ------------+-----------------------------------
    not hs grad |        222       16.06       16.06
        hs only |        802       58.03       74.10
        college |        358       25.90      100.00
    ------------+-----------------------------------
          Total |      1,382      100.00

Using these variables, we fit the model

```
mlogit party age income i.black i.female i.educ
```

Because **``educ``** has three categories, **``i.educ``** is expanded to **``2.educ``** and **``3.educ``**, leading to the following minimal set of equations:




\begin{align*}
\ln\Omega_{\mathrm{SD|SR}}\left(\mathbf{x_i}\right)=\beta_{0,\mathrm{SD|SR}}+\beta_{1,\mathrm{SD|SR}}\mathbf{age}+\beta_{2,\mathrm{SD|SR}}\mathbf{income}+\beta_{3,\mathrm{SD|SR}}\mathbf{black} \\
\end{align*}

\begin{align*}
\qquad +\beta_{4,\mathrm{SD|SR}}\mathbf{female}+\beta_\text{5,SD|SR}2.\mathbf{educ}+\beta_\text{6,SD|SR}3.\mathbf{educ}
\end{align*}


\begin{align*}
\ln\Omega_{\mathrm{D|SR}}\left(\mathbf{x_i}\right)=\beta_{0,\mathrm{D|SR}}+\beta_{1,\mathrm{D|SR}}\mathbf{age}+\beta_{2,\mathrm{D|SR}}\mathbf{incone}+\beta_{3,\mathrm{D|SR}}\mathbf{black}\\
\end{align*}

\begin{align*}
\qquad +\beta_{4,\mathrm{D}|\mathrm{SR}}\mathbf{female}+\beta_{5,\mathrm{D}|\mathrm{SR}}2.\textbf{educ}+\beta_{6,\mathrm{D}|\mathrm{SR}}3.\textbf{educ}
\end{align*}



\begin{align*}
\ln\Omega_{\mathrm{I|SR}}\left(\mathbf{x_i}\right)=\beta_{0,\mathrm{I|SR}}+\beta_{1,\mathrm{I|SR}}\mathbf{age}+\beta_{2,\mathrm{I|SR}}\mathbf{income}+\beta_{3,\mathrm{I|SR}}\mathbf{b}\text{lack} \\
\end{align*}

\begin{align*}
\qquad +\beta_{4,1|\text{SR}}\mathbf{female}+\beta_\text{5,1|SR}2.\mathbf{educ}+\beta_\text{6,1[SR}3.\mathbf{educ}
\end{align*}



\begin{align*}
\ln\Omega_{\mathrm{R|SR}}\left(\mathbf{x_i}\right)=\beta_{0,R|SR}+\beta_{1,R|\mathrm{SR}}\mathbf{age}+\beta_{2,R|\mathrm{SR}}\mathbf{income}+\beta_{3,\mathrm{R|SR}}\mathbf{black} \\
\end{align*}

\begin{align*}
\qquad +\beta_{4,R|\text{SR}}\mathbf{female}+\beta_\text{5,R|SR}2.\mathbf{educ}+\beta_\text{6,R|SR}3.\mathbf{educ}
\end{align*}

where the fifth outcome, StrRep, is the base. The (lengthy) results are

```
mlogit party age income i.black i.female i.educ, base(5) vsquish
```

    Iteration 0:  Log likelihood = -2116.5357  
    Iteration 1:  Log likelihood = -1973.8136  
    Iteration 2:  Log likelihood = -1961.2327  
    Iteration 3:  Log likelihood = -1960.9125  
    Iteration 4:  Log likelihood = -1960.9107  
    Iteration 5:  Log likelihood = -1960.9107  
    
    Multinomial logistic regression                         Number of obs =  1,382
                                                            LR chi2(24)   = 311.25
                                                            Prob > chi2   = 0.0000
    Log likelihood = -1960.9107                             Pseudo R2     = 0.0735
    
    ------------------------------------------------------------------------------
           party | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]
    -------------+----------------------------------------------------------------
    StrDem       |
             age |       0.00       0.01     0.44   0.662        -0.01        0.02
          income |      -0.02       0.00    -3.82   0.000        -0.03       -0.01
           black |
            yes  |       3.08       0.60     5.09   0.000         1.89        4.26
          female |
            yes  |       0.24       0.22     1.10   0.271        -0.18        0.66
            educ |
        hs only  |      -0.55       0.34    -1.62   0.105        -1.23        0.12
        college  |      -1.37       0.40    -3.44   0.001        -2.16       -0.59
           _cons |       1.18       0.51     2.30   0.021         0.18        2.19
    -------------+----------------------------------------------------------------
    Dem          |
             age |      -0.02       0.01    -3.51   0.000        -0.03       -0.01
          income |      -0.01       0.00    -2.87   0.004        -0.02       -0.00
           black |
            yes  |       2.08       0.60     3.45   0.001         0.90        3.26
          female |
            yes  |       0.48       0.19     2.49   0.013         0.10        0.85
            educ |
        hs only  |      -0.21       0.34    -0.62   0.533        -0.87        0.45
        college  |      -0.75       0.37    -2.02   0.043        -1.47       -0.02
           _cons |       2.33       0.47     4.92   0.000         1.40        3.26
    -------------+----------------------------------------------------------------
    Indep        |
             age |      -0.03       0.01    -3.88   0.000        -0.04       -0.01
          income |      -0.01       0.00    -1.88   0.061        -0.02        0.00
           black |
            yes  |       2.29       0.63     3.66   0.000         1.06        3.52
          female |
            yes  |       0.05       0.24     0.20   0.839        -0.42        0.51
            educ |
        hs only  |      -0.60       0.38    -1.59   0.112        -1.34        0.14
        college  |      -1.76       0.45    -3.90   0.000        -2.64       -0.87
           _cons |       2.23       0.55     4.02   0.000         1.14        3.31
    -------------+----------------------------------------------------------------
    Rep          |
             age |      -0.02       0.01    -3.59   0.000        -0.03       -0.01
          income |      -0.00       0.00    -0.38   0.705        -0.01        0.01
           black |
            yes  |       0.11       0.69     0.15   0.877        -1.24        1.45
          female |
            yes  |       0.24       0.19     1.27   0.205        -0.13        0.62
            educ |
        hs only  |      -0.18       0.35    -0.52   0.602        -0.87        0.50
        college  |      -0.70       0.38    -1.83   0.067        -1.44        0.05
           _cons |       2.09       0.48     4.32   0.000         1.14        3.04
    -------------+----------------------------------------------------------------
    StrRep       |  (base outcome)
    ------------------------------------------------------------------------------

Methods of testing coefficients and interpretation of the estimates will be considered after we discuss the effects of selecting different base outcomes.

### 2.2 Selecting different base outcomes

By default, **``mlogit``** sets the base outcome to the alternative with the most observations in the estimation sample. Or, as illustrated in the last example, **you can select the base with the option **``baseoutcome()``**, which can be abbreviated simply as **``b()``**. **``mlogit``** reports the coefficients for each independent variable on each outcome relative to the base outcome.**

Although **``mlogit``** only shows coefficients comparing outcomes with the base outcome, it is important to examine the coefficients for other pairs of outcomes. For example, you might be interested in the effect of being female on being Dem compared with Indep (that is, $\beta_{female,D|I}$), which was not estimated in the output above. Although this coefficient can be estimated by running **``mlogit``** with a different base outcome (for example, **``mlogit party ..., base(3)``**), it is easier to use **``listcoef``**, which presents estimates for all pairs of outcome categories. Because **``listcoef``** can generate lengthy output, we illustrate several options that limit which coefficients are listed. First, if you specify a list of variables, only coefficients for those variables are shown. For example,

```
mlogit party age income i.black i.female i.educ, base(5) vsquish
listcoef female, help
```

    mlogit (N=1382): Factor change in the odds of party 
    
    Variable: 1.female (sd=0.500)
    ------------------------------------------------------------------------------
                                 |         b        z    P>|z|       e^b   e^bStdX
    -----------------------------+------------------------------------------------
    StrDem       vs Dem          |   -0.2408   -1.443    0.149     0.786     0.887
    StrDem       vs Indep        |    0.1889    0.889    0.374     1.208     1.099
    StrDem       vs Rep          |   -0.0079   -0.044    0.965     0.992     0.996
    StrDem       vs StrRep       |    0.2368    1.101    0.271     1.267     1.126
    Dem          vs StrDem       |    0.2408    1.443    0.149     1.272     1.128
    Dem          vs Indep        |    0.4298    2.217    0.027     1.537     1.240
    Dem          vs Rep          |    0.2330    1.587    0.112     1.262     1.124
    Dem          vs StrRep       |    0.4777    2.493    0.013     1.612     1.270
    Indep        vs StrDem       |   -0.1889   -0.889    0.374     0.828     0.910
    Indep        vs Dem          |   -0.4298   -2.217    0.027     0.651     0.807
    Indep        vs Rep          |   -0.1968   -0.983    0.326     0.821     0.906
    Indep        vs StrRep       |    0.0479    0.203    0.839     1.049     1.024
    Rep          vs StrDem       |    0.0079    0.044    0.965     1.008     1.004
    Rep          vs Dem          |   -0.2330   -1.587    0.112     0.792     0.890
    Rep          vs Indep        |    0.1968    0.983    0.326     1.217     1.103
    Rep          vs StrRep       |    0.2447    1.268    0.205     1.277     1.130
    StrRep       vs StrDem       |   -0.2368   -1.101    0.271     0.789     0.888
    StrRep       vs Dem          |   -0.4777   -2.493    0.013     0.620     0.787
    StrRep       vs Indep        |   -0.0479   -0.203    0.839     0.953     0.976
    StrRep       vs Rep          |   -0.2447   -1.268    0.205     0.783     0.885
    ------------------------------------------------------------------------------
           b = raw coefficient
           z = z-score for test of b=0
       P>|z| = p-value for z-test
         e^b = exp(b) = factor change in odds for unit increase in X
     e^bStdX = exp(b*SD of X) = change in odds for SD increase in X
    
Notice that none of the coefficients with the base outcome StrDem are statistically significant, even at the 0.10 level. Although these are the only coefficients that are shown in the output from **``mlogit``**, two of the coefficients relative to outcome Dem are significant at the 0.05 level and two more are almost significant at the 0.10 level. **Before concluding that a variable has no effect, you should examine all the contrasts and compute an omnibus test for the effect of a variable, as discussed in section 8.3.2.**

By default, **``listcoef``** shows coefficients for all contrasts. For example, it even shows you the coefficient comparing StrRep with Rep, which equals -0.2447, and the coefficient comparing Rep with StrRep, which equals 0.2447. **You can limit which contrasts are shown with the **``gt``**, **``lt``**, or **``adjacent``** options. With the **``gt``** option, only coefficients in which the category number of the first alternative is greater than that of the second are shown; **``lt``** shows comparisons when the first alternative is less than the second; and **``adjacent``** limits coefficients to those from adjacent outcomes.** For example,

```
listcoef income age, lt adjacent
```

    mlogit (N=1382): Factor change in the odds of party 
    
    Variable: age (sd=16.783)
    ------------------------------------------------------------------------------
                                 |         b        z    P>|z|       e^b   e^bStdX
    -----------------------------+------------------------------------------------
    StrDem       vs Dem          |    0.0236    4.761    0.000     1.024     1.486
    Dem          vs Indep        |    0.0080    1.287    0.198     1.008     1.144
    Indep        vs Rep          |   -0.0071   -1.099    0.272     0.993     0.888
    Rep          vs StrRep       |   -0.0217   -3.594    0.000     0.979     0.695
    ------------------------------------------------------------------------------
    
    Variable: income (sd=27.781)
    ------------------------------------------------------------------------------
                                 |         b        z    P>|z|       e^b   e^bStdX
    -----------------------------+------------------------------------------------
    StrDem       vs Dem          |   -0.0073   -1.777    0.075     0.993     0.817
    Dem          vs Indep        |   -0.0012   -0.279    0.780     0.999     0.967
    Indep        vs Rep          |   -0.0077   -1.778    0.075     0.992     0.807
    Rep          vs StrRep       |   -0.0013   -0.378    0.705     0.999     0.965
    ------------------------------------------------------------------------------

We will return to the contrasting patterns of coefficients for these two variables when we present methods for plotting coefficients in section 8.11.2.

**You can also restrict the list of contrasts shown to only those with positive coefficients by using the **``positive``** option.** This means you will see all the contrasts that are not simply derived by reversing the sign of another contrast. This can often allow you to see at a glance overall patterns in the direction of the relationships between an independent variable and the outcome, as in this example.

```
listcoef income, pos
```

    mlogit (N=1382): Factor change in the odds of party 
    
    Variable: income (sd=27.781)
    ------------------------------------------------------------------------------
                                 |         b        z    P>|z|       e^b   e^bStdX
    -----------------------------+------------------------------------------------
    Dem          vs StrDem       |    0.0073    1.777    0.075     1.007     1.224
    Indep        vs StrDem       |    0.0085    1.652    0.098     1.009     1.266
    Indep        vs Dem          |    0.0012    0.279    0.780     1.001     1.034
    Rep          vs StrDem       |    0.0162    3.916    0.000     1.016     1.568
    Rep          vs Dem          |    0.0089    3.026    0.002     1.009     1.281
    Rep          vs Indep        |    0.0077    1.778    0.075     1.008     1.239
    StrRep       vs StrDem       |    0.0175    3.816    0.000     1.018     1.625
    StrRep       vs Dem          |    0.0102    2.868    0.004     1.010     1.327
    StrRep       vs Indep        |    0.0090    1.876    0.061     1.009     1.283
    StrRep       vs Rep          |    0.0013    0.378    0.705     1.001     1.036
    ------------------------------------------------------------------------------

From the example, we can see that the positive coefficients for the income variable all correspond to contrasts of a further right outcome to a further left one. In other words, we can see that income is positively related to selecting a partisan identity further to the right. This corresponds to the conclusion that partisan identification behaves like an ordinal variable with respect to income (but, as we will show later, this is not the case for all the variables in our model).

**Finally, a last way to restrict the list of coefficients is with the **``pvalue(#)``** option that restricts coefficients to those that are significant at the specified level.** For example,

```
listcoef female, pvalue(.15)
```

    mlogit (N=1382): Factor change in the odds of party (P<0.15)
    
    Variable: 1.female (sd=0.500)
    ------------------------------------------------------------------------------
                                 |         b        z    P>|z|       e^b   e^bStdX
    -----------------------------+------------------------------------------------
    StrDem       vs Dem          |   -0.2408   -1.443    0.149     0.786     0.887
    Dem          vs StrDem       |    0.2408    1.443    0.149     1.272     1.128
    Dem          vs Indep        |    0.4298    2.217    0.027     1.537     1.240
    Dem          vs Rep          |    0.2330    1.587    0.112     1.262     1.124
    Dem          vs StrRep       |    0.4777    2.493    0.013     1.612     1.270
    Indep        vs Dem          |   -0.4298   -2.217    0.027     0.651     0.807
    Rep          vs Dem          |   -0.2330   -1.587    0.112     0.792     0.890
    StrRep       vs Dem          |   -0.4777   -2.493    0.013     0.620     0.787
    ------------------------------------------------------------------------------

Using these options can reduce the amount of output from **``listcoef``** and focus attention on the most important results.

### 2.3 Predicting perfectly

The **``mlogit``** command handles perfect prediction in the same way as the **``ologit``** and **``oprobit``** commands, but somewhat differently than estimation commands for binary models, **``logit``** and **``probit``**. **``logit``** and **``probit``** automatically remove the observations that imply perfect prediction and compute the estimates accordingly. **``mlogit``** and **``oprobit``** keep these observations in the model, set the χ² statistics for the problem variables to 0, warn that standard errors are questionable, and indicate that a given number of observations are completely determined. You should refit the model after excluding the problem variable and deleting the observations that imply the perfect predictions. Using **``tabulate``** to cross-tabulate the problem variable and the dependent variable should reveal the combination of values that results in perfect prediction.

>**``mlogit``**命令处理完美预测的方式与**``ologit``**和**``oprobit``**命令相同，但与二元模型的估计命令**``logit``**和**``probit``**有些不同。**``logit``**和**``probit``**会自动删除导致完美预测的观测，并相应地计算估计值。**``mlogit``**和**``oprobit``**会保留这些观测值，并将问题变量的χ²统计量设为0，警告标准误差可能存在问题，并指出一定数量的观测完全确定。在排除问题变量并删除导致完美预测的观测后，应重新拟合模型。使用**``tabulate``**对问题变量和因变量进行交叉表分析应该可以显示出导致完美预测的值组合。

## 3 Hypothesis testing

In the MNLM, you can test individual coefficients with the reported z statistics, with a Wald test by using **``test``**, or with an LR test by using **``lrtest``**. As the methods of testing a single coefficient that were discussed in chapters 3, 5, and 7 apply fully, they are not considered further here. **However, in the MNLM, there are new reasons for testing sets of coefficients.** 

First, testing that a variable has no effect requires a test that J - 1 coefficients in a minimal set are simultaneously equal to 0. 

Second, testing whether the independent variables as a group differentiate between two alternatives requires a test of K coefficients, where K is the number of independent variables, including those created by expanding factor-variable notation. In this section, we focus on these two kinds of tests.

>* Caution regarding specification searches: 

>Given the difficulties of interpretation that are associated with the MNLM, it is tempting to search for a more parsimonious model by excluding variables or combining outcome categories based on a sequence of tests. Such a search requires great care. First, these tests involve multiple coefficients. Although the overall test might indicate that as a group the coefficients are not significantly different from 0, an individual coefficient could still be substantively and statistically significant. Accordingly, you should examine the individual coefficients involved in each test before deciding to revise your model. Second, as with all searches that use repeated, sequential tests, there is a danger of overfitting the model to the data. Whenever model specifications are determined based on prior testing using the same data, significance levels should be used only as rough guidelines.

### 3.1 mlogtest for tests of the MNLM

Although the tests in this section can be computed using **``test``** or **``lrtest``**, in practice, this is tedious. The **``mlogtest``** command by Freese and Long (2000) makes the computation of these tests easy. The syntax is

**``mlogtest [varlist] [, lr wald set([setname=] varlist [\ [setname=] varlist [\...]) combine lrcombine iia hausman smhsiao detail base all]``**

**varlist** indicates the variables for which tests of significance should be computed. If no **varlist** is given, tests are run for all independent variables.

* Options

**``lr``** requests a likelihood-ratio (LR) test for each variable in **``varlist``**. If **``varlist``** is not specified, tests for all variables are computed.

**``wald``** requests a Wald test for each variable in **``varlist``**. If **``varlist``** is not specified, tests for all variables are computed.

**``set([setname1=] varlist1 [\ [setname2=] varlist2 ] [\ ...])** specifies that a set of variables be considered together for the LR test or Wald test. \ is used to indicate that a new set of variables is being specified. For example, **``mlogtest, lr set(age income \ 2.educ 3.educ)``** computes one LR test for the hypothesis that the effects of age and income are jointly 0 and a second LR test that the effects of 2.educ and 3.educ are jointly 0. The option **``set()``** is used to label the output.

combine requests Wald tests of whether dependent categories can be combined.

**``lrcombine``** requests LR tests of whether dependent categories can be combined. These tests use constrained estimation and overwrite constraint 999 if it is already defined.

For other options, type help mlogtest.

### 3.2 Testing the effects of the independent variables

With $J$ dependent categories, there are $J-1$ nonredundant coefficients associated with each independent variable $x_k$. For example, in our model of party affiliation, there are four coefficients associated with female: $\beta_{female,SD|SR}$, $\beta_{female,D|SR}$, $\beta_{female,I|SR}$, and $\beta_{female,R|SR}$. The hypothesis that $x_k$ does not affect the dependent variable can be written as:



where $b$ is the base outcome. Because $\beta_{k,b|b}$ is necessarily 0, the hypothesis imposes constraints on $J - 1$ parameters. This hypothesis can be tested with either a Wald or an LR test.

* Likelihood-ratio test

The LR test involves 
1) fitting the full model that includes all the variables, resulting in the LR statistic $LR \chi_{F}^{2};$ .
2) fitting the restricted model th at excludes variable $x_k$,resulting in $\mathrm{LR}\chi_{R}^{2}$; 
3) computing the difference $\mathrm{LR}\chi_{R\mathrm{vs}F}^{2}=\mathrm{LR}\chi_{F}^{2}-\mathrm{LR}\chi_{R}^{2}$, which is distributed as chi-squared with $J-1$ degrees of freedom if the null hypothesis is true. This can be done using **``lrtest``** by first fitting the full model and storing the estimates:

当有 $J$ 个因变量类别时，每个自变量 $x_k$ 都会有 $J-1$ 个与之相关的系数。比如，在我们对党派隶属的模型中，针对女性就有四个系数：$\beta_{female,SD|SR}$、$\beta_{female,D|SR}$、$\beta_{female,I|SR}$ 和 $\beta_{female,R|SR}$。

现在假设 $x_k$ 不影响因变量，这就意味着我们要做一个假设检验，检验 $x_k$ 的系数是否为零。这个假设可以写成：

$$
H_0: \beta_{k1|1} = \beta_{k2|2} = \ldots = \beta_{kJ|J} = 0
$$

其中 $b$ 是基准结果，例如在党派隶属模型中可能是某个特定的党派。

因为基准结果下的系数 $\beta_{k,b|b}$ 必然为0，所以这个假设实际上对 $J - 1$ 个系数进行了约束。我们可以通过似然比检验来验证这个假设。

似然比检验包括以下步骤：

1. 拟合一个包含所有变量的全模型，得到似然比统计量 $LR \chi_{F}^{2}$。
2. 拟合一个不包含变量 $x_k$ 的受限模型，得到 $\mathrm{LR}\chi_{R}^{2}$。
3. 计算两者之间的差异 $\mathrm{LR}\chi_{R\mathrm{vs}F}^{2}=\mathrm{LR}\chi_{F}^{2}-\mathrm{LR}\chi_{R}^{2}$，如果零假设成立，则这个差异服从自由度为 $J-1$ 的卡方分布。

你也可以使用 **``lrtest``** 来完成这个检验，首先拟合全模型并存储参数估计值。

```
use partyid4, clear
mlogit party age income i.black i.female i.educ, base(5) nolog
estimates store full_model
```

Next, we fit a model that drops the variable age, again storing the estimates:

```
mlogit party income i.black i.female i.educ, base(5) nolog
estimates store drop_age
```

Finally, we compute the LR test.

```
lrtest full_model drop_age
```

    Likelihood-ratio test
    Assumption: drop_age nested within full_model
    
     LR chi2(4) =  45.16
    Prob > chi2 = 0.0000

Although using **``lrtest``** is straightforward, the command **``mlogtest, lr``** is even simpler because **it automatically fits the needed models and computes the tests for all variables by making repeated calls to **``lrtest``****:

```
mlogit party age income i.black i.female i.educ, base(5) nolog
mlogtest, lr
```

    LR tests for independent variables (N=1382)
    
      Ho: All coefficients associated with given variable(s) are 0
    
                     |      chi2    df   P>chi2
    -----------------+-------------------------
                 age |    45.165     4    0.000
              income |    24.361     4    0.000
             1.black |   126.467     4    0.000
            1.female |     9.143     4    0.058
              2.educ |     5.567     4    0.234
              3.educ |    21.582     4    0.000

The results of the LR test, regardless of how they are computed, can be interpreted as follows:

>The effect of age on party affiliation is significant at the 0.01 level (LR $\chi^2 = 45.17$, df = 4, $p < 0.01$).

>The effect of being female is significant at the 0.10 level but not at the 0.05 level (LR $\chi^2 = 9.14$, df = 4, $p = 0.06$).

This can also be stated more formally:

>The hypothesis that all the coefficients associated with income are simultaneously equal to 0 can be rejected at the 0.01 level (LR $\chi^2 = 24.36$, df = 4, $p < 0.01$).

* Wald test

**Although we consider the LR test superior, its computational costs can be prohibitive if the model is complex or the sample is very large. Also, LR tests cannot be used if robust standard errors or survey estimation is used. Wald tests are computed using **``test``** and can be used with robust standard errors and survey estimation.** As an example, to compute a Wald test of the null hypothesis that the effect of being female is 0, type:

```
mlogit party age income i.black i.female i.educ, base(5) nolog
test 1.female
```

     ( 1)  [StrDem]1.female = 0
     ( 2)  [Dem]1.female = 0
     ( 3)  [Indep]1.female = 0
     ( 4)  [Rep]1.female = 0
     ( 5)  [StrRep]1o.female = 0
           Constraint 5 dropped
    
               chi2(  4) =    9.09
             Prob > chi2 =    0.0590

The output from **``test``** makes explicit which coefficients are being tested and shows how Stata labels parameters in models with multiple equations. For example, the first line, labeled [StrDem] 1. female, refers to the coefficient for female in the equation comparing the outcome StrDem with the base outcome StrRep; [Dem] 1. female is the coefficient for female in the equation comparing the outcome Dem with the base category StrRep; and so on. The fifth constraint, listed as [StrRep] lo.female = 0, refers to the coefficient comparing StrRep to StrRep, which is automatically constrained to 0 when the model is fit. The lo in lo.female means that the coefficient for outcome 1 was omitted in the model and so the parameter was not estimated. Accordingly, this constraint is dropped. The following command automates this process:

```
mlogtest, wald
```

    Wald tests for independent variables (N=1382)
    
      Ho: All coefficients associated with given variable(s) are 0
    
                     |      chi2    df   P>chi2
    -----------------+-------------------------
                 age |    43.815     4    0.000
              income |    22.985     4    0.000
             1.black |    83.978     4    0.000
            1.female |     9.087     4    0.059
              2.educ |     5.569     4    0.234
              3.educ |    20.613     4    0.000

These tests can be interpreted in the same way as shown for the LR test above.

* Testing multiple independent variables

The logic of the Wald or LR tests can be extended to test that the effects of two or more independent variables are simultaneously 0. For example, the hypothesis to test that $X_k$ and $X_\ell$ have no effects is:

$$H_{0}\colon\beta_{k,1|b}=\cdots=\beta_{k,J|b}=\beta_{\ell,1|b}=\cdots=\beta_{\ell,J|b}=0$$

For example, to test the hypothesis that the effects of age and income are simultaneously equal to 0, we could use **``lrtest``** as follows:

```
mlogit party age income i.black i.female i.educ, base(5) nolog
estimates store full_model
mlogit party i.black i.female i.educ, base(5) nolog
estimates store drop_ageinc
lrtest full_model drop_ageinc
```

    Likelihood-ratio test
    Assumption: drop_ageinc nested within full_model
    
     LR chi2(8) =  71.58
    Prob > chi2 = 0.0000

We can use the **``set``** option in **``mlogtest``** to do the same things. Suppose we use the command:

```
estimates restore full_model
mlogtest , lr set(age&inc=age income \ educ=2.educ 3.educ)
```

    LR tests for independent variables (N=1382)
    
      Ho: All coefficients associated with given variable(s) are 0
    
                     |      chi2    df   P>chi2
    -----------------+-------------------------
                 age |    45.165     4    0.000
              income |    24.361     4    0.000
             1.black |   126.467     4    0.000
            1.female |     9.143     4    0.058
              2.educ |     5.567     4    0.234
              3.educ |    21.582     4    0.000
             age&inc |    71.585     8    0.000
                educ |    26.881     8    0.001
    
       age&inc contains: age income 
       educ contains: 2.educ 3.educ 


### 3.3 Tests for combining alternatives

If none of the independent variables significantly affects the odds of alternative m versus alternative n, we may say that m and n are indistinguishable with respect to the variables in the model (Anderson 1984). To say that alternatives m and n are indistinguishable corresponds to the hypothesis that

$$H_{0}\colon\beta_{1,m|n}=\cdots\beta_{K,m|n}=0$$

which can be tested with either a Wald or an LR test. If alternatives are indistinguishable with respect to the variables in the model, then you can obtain more efficient estimates by combining them. Note, however, that while the **`mlogtest`** command makes it easier to test the hypotheses that each pair of outcomes can be combined, we do not recommend combining categories simply because the null hypothesis is not rejected. This is likely to lead to over-fitting your data and creating outcome variables that do not make substantive sense. Instead, these tests should be used to test a substantively motivated hypothesis that two categories are indistinguishable.

这段话讲的是在统计建模中，当独立变量对于比较两个不同选择（称为m和n）的概率没有显著影响时，可以认为这两个选择在模型中是无法区分的。这意味着对于这些变量，无法区分m和n。在这种情况下，我们可以用统计方法来检验假设，即$$H_{0}\colon\beta_{1,m|n}=\cdots\beta_{K,m|n}=0$$，其中β表示模型中的系数，K表示独立变量的数量。这个假设可以用瓦尔德检验或LR检验来检验。如果根据模型中的变量来说，两个选择是无法区分的，那么可以通过将它们合并来获得更有效的估计值。但是需要注意的是，虽然**`mlogtest`**命令可以更容易地测试每一对结果是否可以合并，但我们不建议仅仅因为零假设没有被拒绝就合并类别。这样做可能会导致过度拟合数据，并创建出不具有实质意义的结果变量。相反，这些测试应该用于测试一个有实质意义的假设，即两个类别是无法区分的。

举例：让我们考虑一个市场调查的情景。假设我们想要了解两种不同的产品（产品m和产品n）在不同年龄段和收入水平下的受欢迎程度。我们使用年龄和收入作为独立变量，并观察它们对于选择产品m或产品n的影响。

如果统计分析显示，无论是年龄还是收入，都对产品m和产品n的选择没有显著影响，那么我们就无法通过这些因素来区分这两种产品。在这种情况下，我们可以考虑将这两种产品合并，以获得更可靠的市场反应估计。然而，需要注意的是，我们不应该仅仅因为统计检验中的零假设未被拒绝而合并这两种产品。相反，我们应该有充分的理由相信这两种产品在这些变量下是无法区分的，比如市场调查的其他结果或者消费者行为理论的支持。

* Wald test for combining alternatives

The command **`mlogtest`**, combine computes Wald tests of the null hypothesis that two alternatives can be combined for all pairs of alternatives. For example,

```
mlogit party age income i.black i.female i.educ, base(5) nolog
mlogtest, combine
```

    Wald tests for combining alternatives (N=1382)
    
      Ho: All coefficients except intercepts associated with a given pair
          of alternatives are 0 (i.e., alternatives can be combined)
    
                     |      chi2    df   P>chi2
    -----------------+-------------------------
        StrDem & Dem |    72.854     6    0.000
      StrDem & Indep |    40.334     6    0.000
        StrDem & Rep |   126.561     6    0.000
     StrDem & StrRep |    83.272     6    0.000
         Dem & Indep |    15.141     6    0.019
           Dem & Rep |    44.862     6    0.000
        Dem & StrRep |    56.580     6    0.000
         Indep & Rep |    49.879     6    0.000
      Indep & StrRep |    60.203     6    0.000
        Rep & StrRep |    22.286     6    0.001

From these results, we can reject the hypothesis that categories StrDem and Dem are indistinguishable. Indeed, our results indicate that all the categories are distinguishable.

* (Advanced) Using test [outcome]

>The **`mlogtest, combine`** command makes its computations by using the **`test`** command for multiple-equation models. **Although most researchers will find that **`mlogtest`** is sufficient for their needs, there might be situations in which you want to conduct tests that are unique to your application.** If so, this section provides some insights into how to use the **`test`** command to test hypotheses involving combining outcomes.

To test that **StrDem** is indistinguishable from the base outcome **StrRep**, type:

```
test [StrDem]
```

     ( 1)  [StrDem]age = 0
     ( 2)  [StrDem]income = 0
     ( 3)  [StrDem]0b.black = 0
     ( 4)  [StrDem]1.black = 0
     ( 5)  [StrDem]0b.female = 0
     ( 6)  [StrDem]1.female = 0
     ( 7)  [StrDem]1b.educ = 0
     ( 8)  [StrDem]2.educ = 0
     ( 9)  [StrDem]3.educ = 0
           Constraint 3 dropped
           Constraint 5 dropped
           Constraint 7 dropped
    
               chi2(  6) =   83.27
             Prob > chi2 =    0.0000

The result matches the results from **`mlogtest`** in row StrDem & StrRep. The command **`test [outcome]`** indicates which equation is being referenced in multiple-equation commands. **`mlogit`** is a multiple-equation command with .7—1 equations that are named by the value label for the outcome categories. In the output above, constraints 3, 5, and 7 were dropped. These constraints correspond to the base categories for factor variables. For example, **0b.black** is the coefficient for the excluded base outcome, which by definition is 0.

The test is more complicated when neither outcome that is being considered is the base. For example, to test that m and n are indistinguishable when the base outcome b is neither m nor n, the hypothesis is:

$$H_0\colon\left(\beta_{1,m|b}-\beta_{1,n|b}\right)=\cdots=\left(\beta_{K,m|b}-\beta_{K,n|b}\right)=0$$

That is, you want to test the difference between two sets of coefficients. This is done with **`test [outcome1 = outcome2]`**. For example, to test whether StrDem and Dem can be combined, type:

```
test [StrDem=Dem]
```

     ( 1)  [StrDem]age - [Dem]age = 0
     ( 2)  [StrDem]income - [Dem]income = 0
     ( 3)  [StrDem]0b.black - [Dem]0b.black = 0
     ( 4)  [StrDem]1.black - [Dem]1.black = 0
     ( 5)  [StrDem]0b.female - [Dem]0b.female = 0
     ( 6)  [StrDem]1.female - [Dem]1.female = 0
     ( 7)  [StrDem]1b.educ - [Dem]1b.educ = 0
     ( 8)  [StrDem]2.educ - [Dem]2.educ = 0
     ( 9)  [StrDem]3.educ - [Dem]3.educ = 0
           Constraint 3 dropped
           Constraint 5 dropped
           Constraint 7 dropped
    
               chi2(  6) =   72.85
             Prob > chi2 =    0.0000
    
The results are identical to those from `mlogtest`.

* LR test for combining alternatives
An LR test of combining m and n can be computed by first fitting the full model with no constraints, with the resulting LR statistic $LR\chi_{F}^{2}$. Then, fit a restricted model $M_r$ in which outcome m is used as the base category and all the coefficients except the constant in the equation for outcome n are constrained to 0, with the resulting test statistic $LR\chi_{F}^{2}$. The test statistic for the test of combining $m$ and $n$ is the difference $\mathrm{LR~}\chi_{R\mathrm{vs}F}^{2}=\mathrm{LR}\chi_{F}^{2}-\mathrm{LR}\chi_{R}^{2}$,which distributed as chi-squared with $K$ degrees of freedom, where $K$ is the number of regressors. The command **`mlogtest, lrcombine`** computes $J x (J — 1)$ tests for all pairs of outcome categories. For example,

```
mlogit party age income i.black i.female i.educ, base(5) nolog
mlogtest, lrcombine
```

    LR tests for combining alternatives (N=1382)
    
      Ho: All coefficients except intercepts associated with a given pair
          of alternatives are 0 (i.e., alternatives can be collapsed)
    
                              |      chi2    df   P>chi2
    --------------------------+-------------------------
                 StrDem & Dem |    80.893     6    0.000
               StrDem & Indep |    44.075     6    0.000
                 StrDem & Rep |   198.758     6    0.000
              StrDem & StrRep |   141.446     6    0.000
                  Dem & Indep |    15.753     6    0.015
                    Dem & Rep |    61.899     6    0.000
                 Dem & StrRep |    73.214     6    0.000
                  Indep & Rep |    60.872     6    0.000
               Indep & StrRep |    78.673     6    0.000
                 Rep & StrRep |    22.894     6    0.001

* (Advanced) Using constraints with Irtest

>The **`mlogtest, lrcombine`** command computes LR tests by using the powerful **`constraint`** command (see [r] constraint). Although most researchers are likely to find **`mlogtest`** sufficient for their needs, some might want to learn more about how constraints are specified when fitting models.

An LR test that categories are indistinguishable can be computed using the command **`constraint`**. To test whether StrDem and StrRep are indistinguishable, we start by fitting the full model and storing the results:

```
mlogit party age income i.black i.female i.educ, base(5) nolog
estimates store full_model
```

Second, we define a constraint by using the command

```
constraint define 999 [StrDem]
```

We arbitrarily chose number 999 to label the constraint. Any integer from 1 to 1,999 inclusive can be used. The expression **`[StrDem]`** indicates that all coefficients should be estimated except for those fixed by the constraint. Third, we refit the model with this constraint. The base category must be StrRep (category 5) so that the coefficients indicated by **`[StrDem]`** are comparisons of StrDem and StrRep:

```
mlogit party age income i.black i.female i.educ, ///
    constraint(999) base(5) nolog
estimates store constraint999
```

The model is fit with the constraint imposed, and results are stored using the name **`constraint999`**. Comparing the full model to the constrained model,

```
lrtest full_model constraint999
```

    Likelihood-ratio test
    Assumption: constraint999 nested within full_model
    
     LR chi2(6) = 141.45
    Prob > chi2 = 0.0000


The result matches that from **`mlogtest, lrcombine`**.

## 4 Independence of irrelevant alternatives

The MNLM, as well as the conditional logit and rank-ordered logit models discussed below, make the assumption known as the independence of irrelevant alternatives (IIA). Here we describe the assumption in terms of the MNLM. In this model,


<figure style="text-align:center;">
  <img src="https://cdn.jsdelivr.net/gh/forest293/Hugo-image/2024.3.2.20.png" style="width:450px;height:100px;" alt="图片描述">
  <figcaption><strong></strong></figcaption>
</figure>

where the odds do not depend on other alternatives that are available. In this sense, those alternatives are “irrelevant”. What this means is that adding or deleting alternatives does not affect the odds among the remaining alternatives.

This point is often made with the red bus blue bus example. Suppose people in a city have three ways of getting to work: by car, by taking a bus operated by a company that uses red buses, or by taking a bus operated by an identical company that uses blue buses. We might expect that many people have a clear preference between taking the car versus taking a bus but are indifferent about whether they take a red bus or a blue bus. Suppose the odds of a person taking a red bus compared with those of taking a car are 1:1. IIA implies the odds will remain 1:1 between these two alternatives even if the blue bus company were to go out of business. The assumption is dubious because we would expect the vast majority of those who take the blue bus to have the red bus as their next preference. Consequently, eliminating the blue bus will increase the probability of traveling by red bus much more than it will increase the probability of someone traveling by car, yielding odds more like 2:1 than 1:1. In other words, because the blue bus and red bus are close substitutes, having the blue bus as an available alternative leads the MNLM to underestimate the preference for red bus versus car.

Tests of IIA involve comparing the estimated coefficients from the full model to those from a restricted model that excludes at least one of the alternatives. If the test statistic is significant, the assumption of IIA is rejected, indicating that the MNLM is inappropriate. In this section, we consider the two most common tests of IIA: the Hausman-McFadden (HM) test (Hausman and McFadden 1984) and the Small-Hsiao (SH) test (Small and Hsiao 1985). For details on other tests, see Fry and Harris (1996, 1998). For a model with .7 alternatives, we consider J ways of computing each test. If you remove the first alternative and refit the model, you get the first restricted model leading to the first variation of the test. If you remove the second alternative, you get the second variation, and so on. Each restricted model will lead to a different test statistic, as we demonstrate below. 

Both the HM test and the SH test are computed by **`mlogtest`**, and for both tests, we compute J variations. As many users of **`mlogtest`** have told us, the HM and SH tests often provide conflicting information on whether IIA has been violated, with some of the tests rejecting the null hypothesis, while others do not. To explore this further, Cheng and Long (2007) ran Monte Carlo experiments to examine the properties of these tests. Their results show that the HM test has poor size properties even with sample sizes of more than 1,000. For some data structures, the SH test has reasonable size properties for samples of 500 or more. But with other data structures, the size properties are extremely poor and do not get better as the sample size increases. Overall, they conclude that these tests are not useful for assessing violations of the IIA property.

It appears that the best advice regarding IIA goes back to an early statement by McFadden (1974), who wrote that the multinomial and conditional logit models should be used only in cases where the alternatives “can plausibly be assumed to be distinct and weighted independently in the eyes of each decision maker”. Similarly, Amemiya (1981) suggests that the MNLM works well when the alternatives are dissimilar. Care in specifying the model to involve distinct alternatives that are not substitutes for one another seems to be reasonable albeit unfortunately ambiguous——advice.

>Caution regarding tests of IIA.
>> **We do not believe that tests of IIA are useful, but we have heard from readers about reviewers or editors who insist that they provide the results of an IIA test. In our experience, you can almost always obtain some tests that accept the null and others that reject the null when using the same model with the same data. We would try to convince those requesting the test that these tests do not provide useful information, perhaps citing our book, along with Fry and Harris (1996, 1998) and Cheng and Long (2007). If this does not work, you may still need to provide the test results.** In this section, we tell you how to compute them and illustrate their limitations.

### 4.1 Hausman-McFadden test of IIA

The HM test of IIA involves the following steps:

1. Fit the full model with all $J$ alternatives included, with estimates in $\widehat{\beta}_{F}$.
2. Fit a restricted model by eliminating one or more alternatives, with estimates in $\widehat{\beta}_{R}$.
3. Let $\hat{\beta_F}^{*}$ be a subset of $\widehat{\beta}_{F}$ after eliminating coefficients not fit in the restricted model. The test statistic is


<figure style="text-align:center;">
  <img src="https://cdn.jsdelivr.net/gh/forest293/Hugo-image/2024.3.2.21.png" style="width:500px;height:80px;" alt="图片描述">
  <figcaption><strong></strong></figcaption>
</figure>

where HM is asymptotically distributed as chi-squared with degrees of freedom equal to the rows in $\widehat{\beta}_{R}$ if IIA is true. Significant values of HM indicate that the IIA assumption has been violated.

The HM test of IIA can be computed with mlogtest:

```
mlogit party age income i.black i.female i.educ, base(5)
mlogtest, hausman
```

    Hausman tests of IIA assumption (N=1382)
    
      Ho: Odds(Outcome-J vs Outcome-K) are independent of other alternatives
    
                     |      chi2    df   P>chi2
    -----------------+-------------------------
              StrDem |     4.622    20    1.000
                 Dem |     0.919    21    1.000
               Indep |    -2.244    19        .
                 Rep |     3.030    21    1.000
              StrRep |    -0.580    21        .
    
      Note: A significant test is evidence against Ho.
      Note: If chi2<0, the estimated model does not meet asymptotic assumptions.

Five tests of IIA are reported. The first four correspond to excluding one of the four nonbase categories. The fifth test, in row StrRep, is computed by refitting the model with the largest remaining outcome as the base category. Three of the tests produce negative chi-squareds, something that is common with this test. Hausman and McFadden (1984, 1226) note this possibility and conclude that a negative result is evidence that IIA has not been violated. Our simulations suggest that negative chi-squareds indicate problems with the test, consistent with the warning that the mlogtest output provides.

### 4.2 Small-Hsiao test of IIA

To compute an SH test, the sample is divided randomly into two subsamples of about equal size. The unrestricted MNLM is fit on both subsamples, where $\widehat{\beta_n}^{S_{1}}$ contains estim ates from th e unrestricted model on the first subsample and $\widehat{\beta_n}^{S_{2}}$ is its counterpart for the second subsample. A weighted average of the coefficients is computed as 


<figure style="text-align:center;">
  <img src="https://cdn.jsdelivr.net/gh/forest293/Hugo-image/2024.3.2.22.png" style="width:450px;height:110px;" alt="图片描述">
  <figcaption><strong></strong></figcaption>
</figure>

Next, a restricted sample is created from the second subsample by eliminating all cases with a chosen value of the dependent variable. The MNLM is fit using the restricted sample, yielding the estimates $\widehat{\beta_r}^{S_{2}}$ and the likelihood $L(\widehat{\beta_r}^{S_{2}})$ The SH statistic is


<figure style="text-align:center;">
  <img src="https://cdn.jsdelivr.net/gh/forest293/Hugo-image/2024.3.2.24.png" style="width:450px;height:80px;" alt="图片描述">
  <figcaption><strong></strong></figcaption>
</figure>

which is asymptotically distributed as chi-squared with degrees of freedom equal to the number of coefficients that are fit in both the full model and the restricted model.

To compute the SH test, use the command **`mlogtest, smhsiao`** (our program uses code from `smhsiao` by Nick Winter [2000] available at the Statistical Software Components archive). Because the SH test requires randomly dividing the data into subsamples, the results will differ with successive calls of the command, because the sample will be randomly divided differently. To obtain test results that can be replicated, you must explicitly set the seed used by the random-number generator. For example,

```
set seed 124386
mlogtest, smhsiao
```

    Small-Hsiao tests of IIA assumption (N=1382)
    
      Ho: Odds(Outcome-J vs Outcome-K) are independent of other alternatives
    
                     |                                    df         
    -----------------+-----------------------------------------------
              StrDem |  -703.527   -693.584     19.886    21    0.528
                 Dem |  -554.226   -547.159     14.134    21    0.864
               Indep |  -759.018   -746.810     24.416    21    0.273
                 Rep |  -594.421   -584.722     19.397    21    0.560
              StrRep |  -747.457   -734.967     24.980    21    0.248
    
      Note: A significant test is evidence against Ho.

These results are consistent with those from the HM test, with none of the tests being significant.

Before taking these results seriously, we tried three other seeds to produce a different random division of the sample. The results varied widely. For example,

```
set seed 254331
mlogtest, smhsiao
```

    Small-Hsiao tests of IIA assumption (N=1382)
    
      Ho: Odds(Outcome-J vs Outcome-K) are independent of other alternatives
    
                     |                                    df         
    -----------------+-----------------------------------------------
              StrDem |  -685.597   -674.385     22.425    21    0.375
                 Dem |  -569.113   -560.231     17.764    21    0.664
               Indep |  -750.556   -740.408     20.296    21    0.503
                 Rep |  -613.921   -603.605     20.631    21    0.482
              StrRep |  -744.059   -732.629     22.860    21    0.352
    
      Note: A significant test is evidence against Ho.

Using the new seed, we reject the null at the 0.001 level in four of the five tests, illustrating a common problem when using the SH test you often get very different results depending on how the sample is randomly divided.

>Tip: Setting the random seed. The random numbers that divide the sample for the SH test are based on the runiform() function, which uses a pseudorandom-number generator to create a sequence of numbers based on a seed number. Although these numbers appear to be random, the same sequence will be generated each time you start with the same seed. In this sense (and some others), these numbers are pseudorandom rather than random. If you specify the seed with **`set seed #`**, you ensure that you can replicate your results later. See [r] set seed for more details.

## 5 Measures of fit

**As with models for binary and ordinal outcomes, many scalar measures of fit for the MNLM model can be computed with the SPost command **`fitstat`**, and information criteria can be computed with **`estat ic`**.** The same caveats against overstating the importance of these scalar measures apply here as to the other models we have considered (see also chapter 3). **To examine the fit of individual observations, you can fit the series of binary logits implied by the MNLM and use the established methods of examining the fit of observations to binary logit estimates.**


## 6 Overview of interpretation

Although the MNLM is a mathematically simple extension of the binary model, interpretation is difficult because of the many possible comparisons. Even in our simple example with five outcomes, we have 10 comparisons: **StrDem** versus **StrRep**, **Dem** versus **StrRep**, **Indep** versus **StrRep**, **Rep** versus **StrRep**, **StrDem** versus **Rep**, **Dem** versus **Rep**, **Indep** versus **Rep**, **StrDem** versus **Indep**, **Dem** versus **Indep**, and **StrDem** versus **Dem**. It is tedious to write all of them, let alone to interpret all of them for each independent variable. **The key to effective interpretation is to avoid overwhelming yourself or your audience with the many comparisons.**

As with models for binary and ordinal outcomes, we prefer methods of interpretation that are based on predicted probabilities. Fortunately, these methods are essentially unchanged from those used for ordinal models in the last chapter, where the predicted probability is now computed with the formula


<figure style="text-align:center;">
  <img src="https://cdn.jsdelivr.net/gh/forest293/Hugo-image/2024.3.2.25.png" style="width:450px;height:140px;" alt="图片描述">
  <figcaption><strong></strong></figcaption>
</figure>

Here w’e assume that the base outcome is J. but any base could be used.

We follow a similar order of presentation to that used in the last chapter,providing new variations in some cases and excluding some topics. In all cases,however,methods from chapter 7 could be used for nominal models, and new ideas shown in this chaptercould be applied to ordinal models. For example, while we do not consider ideal typesin this chapter, they are just as useful for nominal outcomes as they were for the ordinalregression model (ORM ).

We begin by examining the distribution of predictions for each observation in theestimation sample. Then, we consider how marginal effects can be used as an overall assessment of the im pact of each variable, but we also show what can be learned by lookingat the distribution of effects for each observation in the estimation sample. We extendearlier methods for examining tables of predictions and show how to test a differenceof differences. Next, we plot predictions as a continuous independent variable changes,which we use to highlight how results from an ordinal model can be misleading whenan outcome does not behave as if it were ordinal. Finally, we consider interpretationusing odds ratios. Although odds ratios in the MNLM have all the limitations discussedin chapter 6, they are im portant for understanding how independent variables affect thedistribution of observations between pairs of outcomes, something th at cannot be doneusing predicted probabilities alone.

Before beginning, we must also emphasize once again th at, as with other modelsconsidered in this book, **the MNLM is nonlinear in the outcome probabilities, and noapproach can fully describe the relationship between an independent variable and theoutcome probabilities. You should experiment with each of these methods before deciding which approach is most effective in your application.**

## 7 Predicted probabilities with predict

The most basic command for computing probabilities is **`predict`**. After fitting the model with **`mlogit`**, predicted probabilities for all outcomes within the sample can be calculated with

**`predict newvarlist, [if] [in]`**

where you must provide one new variable name for each of the J categories of the dependent variable, ordered from the lowest to the highest numerical values. For example,

```
qui mlogit party age income i.black i.female i.educ, base(5)
predict mnlmSD mnlmD mnlmI mnlmR mnlmSR
```

The variables created by **`predict`** are

```
codebook mnlmSD mnlmD mnlmI mnlmR mnlmSR, compact
```

    Variable    Obs Unique      Mean       Min       Max  Label
    ---------------------------------------------------------------------------------
    mnlmSD     1382   1193  .1924747  .0212015  .7654323  Pr(party==StrDem)
    mnlmD      1382   1193  .3089725   .130991  .5125323  Pr(party==Dem)
    mnlmI      1382   1193  .1092619  .0266534  .2838254  Pr(party==Indep)
    mnlmR      1382   1193  .2670043  .0141873  .5036324  Pr(party==Rep)
    mnlmSR     1382   1193  .1222865  .0047189  .4662779  Pr(party==StrRep)
    ---------------------------------------------------------------------------------

as with the ordinal model, if you specify a single variable name after **`predict`**, you will obtain predicted probabilities for one outcome category, which you can specify using the **`outcome()`** option.

As discussed in section 7.10, examining the distribution of the in-sample predictions can be used to get a general sense of what is going on in your model and can sometimes uncover problems in your data. The distribution of predictions can also be used to informally compare competing models, which we illustrate next.

**We could reasonably argue that the five categories of our dependent variable party are an ordinal scale of party affiliation. Accordingly, it seems reasonable to model these data with an ordinal logit model.** First, we fit the model and compute predictions:

```
qui ologit party age income i.black i.female i.educ
predict olmSD olmD olmI olmR olmSR
codebook olm*, compact
```

    Variable    Obs Unique      Mean       Min       Max  Label
    --------------------------------------------------------------------------
    olmSD      1382   1193  .1934016   .042849  .6546781  Pr(party==1)
    olmD       1382   1193    .30611  .1393038  .3808963  Pr(party==2)
    olmI       1382   1193  .1091385  .0349378  .1221904  Pr(party==3)
    olmR       1382   1193   .269342  .0484485  .3877065  Pr(party==4)
    olmSR      1382   1193  .1220079  .0124719  .3484676  Pr(party==5)
    --------------------------------------------------------------------------

Next, we plot the predicted probability of being a strong Democrat (outcome 1) with the **`dotplot`** command:

```
label var olmSD "ologit"
label var mnlmSD "mlogit"
dotplot olmSD mnlmSD, ylabel(0(.2).8, grid) ytitle(Pr(Strong Democrat))
```

<figure style="text-align:center;">
  <img src="https://cdn.jsdelivr.net/gh/forest293/Hugo-image/2024.3.2.26.png" style="width:650px;height:440px;" alt="图片描述">
  <figcaption><strong></strong></figcaption>
</figure>

```
pwcorr olmSD mnlmSD
```

                 |    olmSD   mnlmSD
    -------------+------------------
           olmSD |   1.0000 
          mnlmSD |   0.9374   1.0000 



The histograms are similar, and the correlation between the predictions for the ordered logit model (**olm**) and the MNLM is 0.94. This suggests that the conclusions from the two models might be similar.

If we look at the middle category of Independent, however, things look quite different, reflecting the abrupt truncation of the distribution of predictions for middle categories that is often found with the OLM:

```
dotplot olmI mnlmI, ylabel(0(.1).3, grid) ytitle(Pr(Independent))
```

<figure style="text-align:center;">
  <img src="https://cdn.jsdelivr.net/gh/forest293/Hugo-image/2024.3.2.28.png" style="width:650px;height:440px;" alt="图片描述">
  <figcaption><strong></strong></figcaption>
</figure>

```
pwcorr olmI mnlmI
```

                 |     olmI    mnlmI
    -------------+------------------
            olmI |   1.0000 
           mnlmI |  -0.1924   1.0000 


Not only are the distributions quite different in shape, but the correlation between the predicted probabilities is negative: -0.19! A scatterplot of the predictions shows striking differences between the two models:

```
way scatter olmI mnlmI, ///
    xtitle("mlogit: Pr(Independent)") ytitle("ologit: Pr(Independent)") ///
    xlabel(0(.1).3, grid gmin gmax)   ylabel(0(.1).3, grid gmin gmax) ///
    sort aspect(1)
```

<figure style="text-align:center;">
  <img src="https://cdn.jsdelivr.net/gh/forest293/Hugo-image/2024.3.2.29.png" style="width:850px;height:440px;" alt="图片描述">
  <figcaption><strong></strong></figcaption>
</figure>

A low correlation between the predictions from the MNLM and the ORM could reflect a lack of ordinality, but this is not necessarily so. For example, in simulations where data were generated to meet the assumptions of the ORM, we found low correlations in predictions between middle categories between the MNLM and the ORM when the size of the assumed error variance in the ORM was large relative to the size of the regression coefficients. Nonetheless, when predictions are very different between an ordinal and nominal regression model, we recommend considering the appropriateness of the ordinal model. This is considered further in section 8.10, where we plot predictions from the MNLM and the OLM.

## 8 Marginal effects

**Average marginal effects are a quick and valuable way to assess the effects of all the independent variables in your model.** Because methods for using marginal effects to interpret the MNLM are identical to those used for the OLM in section 7.11, we only review key points here. We then extend materials from chapter 7 by examining the distribution of effects within the estimation sample. Marginal effects are also used in section 8.11-2 when we plot odds ratios.

The marginal change is the slope of the curve relating $ x_k $ to $ Pr(y = m | x) $, holding all other variables constant, where $ Pr(y = m | x) $ is defined by (8.2). For the MNLM, the marginal change is:


<figure style="text-align:center;">
  <img src="https://cdn.jsdelivr.net/gh/forest293/Hugo-image/2024.3.2.30.png" style="width:850px;height:120px;" alt="图片描述">
  <figcaption><strong></strong></figcaption>
</figure>

Because this equation combines all the $\beta_{k,j|J^{\prime}S}$, the value of the marginal change depends on the levels of all variables in the model and can switch sign at different values of these variables. Also, because the marginal change is the instantaneous rate of change, it can be misleading when the probability curve is changing rapidly. For this reason, we generally prefer using a discrete change and do not discuss the marginal change function in this chapter.

假设我们正在研究一个医疗调查，想要了解一个人是否患有某种疾病（$y$）与他们的年龄（$x_1$）和体重指数（$x_2$）之间的关系。我们使用多项逻辑回归模型来建模这个问题。

1. 平均边际效应公式：
$$
\frac{\partial Pr(y = m | x)}{\partial x_k} = \beta_{k,0} + \sum_{j=1}^{J} \beta_{k,j|J^{\prime}S} x_{j}
$$

让我们假设我们的模型只有两个自变量 $x_1$（年龄）和 $x_2$（体重指数），我们希望了解 $x_1$ 和 $x_2$ 对患病率的影响。这时，$k$ 可以取 $1$ 或 $2$，分别代表年龄和体重指数。

假设我们想了解年龄对患病率的影响，即 $k=1$。在这种情况下，公式可以简化为：

$$
\frac{\partial Pr(y = m | x)}{\partial x_1} = \beta_{1,0} + \beta_{1,1} x_1 + \beta_{1,2} x_2
$$

其中，$\beta_{1,0}$、$\beta_{1,1}$ 和 $\beta_{1,2}$ 是模型的参数。

"The discrete change is the change in the probability of $m$ for a change in $x_k$ from the start value $x_k^\text{start}$ to the end value $x_k^\text{end}$ (for example, a change from $x_k^\text{start} = 0$ to $x_k^\text{end} = 1$), holding other $x$'s constant. Formally,


<figure style="text-align:center;">
  <img src="https://cdn.jsdelivr.net/gh/forest293/Hugo-image/2024.3.2.31.png" style="width:850px;height:120px;" alt="图片描述">
  <figcaption><strong></strong></figcaption>
</figure>

where $Pr(y = m | x, x_k)$ is the probability that $y = m$ given $x$, noting a specific value for $X_k$. The change indicates that when $x_k$ changes from $x_k^\text{start}$ to $x_k^\text{end}$, the probability of outcome $m$ changes by $\Delta Pr(y = m | x) / \Delta x_k$, holding all other variables at $x$. The magnitude of the change depends on the levels of all variables, including $X_k$, and the size of the change in $X_k$ that is being evaluated.

2. 边际变化公式：
$$
\Delta Pr(y = m | x) = Pr(y = m | x, x_k^\text{end}) - Pr(y = m | x, x_k^\text{start})
$$

这个公式表示当一个自变量 $x_k$ 从一个值变化到另一个值时，目标变量的概率会发生多大的变化。我们可以用同样的例子来说明，比如假设我们想了解体重指数对患病率的影响，即 $k=2$。那么公式可以简化为：

$$
\Delta Pr(y = m | x) = Pr(y = m | x, x_2^\text{end}) - Pr(y = m | x, x_2^\text{start})
$$

这里 $x_2^\text{end}$ 是体重指数的一个结束值，$x_2^\text{start}$ 是体重指数的一个起始值。

在实际应用中，我们将模型参数代入这些公式，并对给定的起始值和结束值进行计算，以获得具体的边际效应和边际变化。

**The use of marginal change can be misleading when the probability curve is changing rapidly.** For this reason, we generally prefer using a discrete change and do not discuss the marginal change further in this chapter.

The discrete change is the change in the probability of $m$ for a change in $X_k$ from the start value $x_k^\text{start}$ to the end value $x_k^\text{end}$ (for example, a change from $x_k^\text{start} = 0$ to $x_k^\text{end} = 1$), holding other $x$'s constant. Formally,

$$\frac{\Delta\Pr\left(y=m\mid\mathbf{x}\right)}{\Delta x_k\left(x_k^{\mathrm{start}}\rightarrow x_k^{\mathrm{end}}\right)}=\Pr\left(y=m\mid\mathbf{x},x_k=x_k^{\mathrm{end}}\right)-\Pr\left(y=m\mid\mathbf{x},x_k=x_k^{\mathrm{start}}\right)$$

where $\Pr\left(y=m\mid\mathbf{x},x_k\right)$ is the probability that $y = m$ given $x$,noting a specific value for $x_k$. The change indicates that when $x_k$ changes from $x_k^\text{start}$ to $x_k^\text{end}$, the probability of outcome m changes by $\Delta\Pr\left(y=m\mid\mathbf{x}\right)/\Delta x_k$, holding all other variables at $x$.The magnitude of th e change depends on the levels of all variables, including $x_k$,and the size of the change in $x_k$ that is being evaluated.

Marginal effects are computed by `mchange` as discussed in detail in sections 4.5.4 and 7.11. After fitting the model used as our running example, we compute the average discrete changes for a standard deviation change in continuous variables and a change from 0 to 1 for other variables. The option `amount(sd)` suppresses the default computation of marginal changes and discrete changes of 1 unit, while `width(8)` prevents results wrapping due to the long value labels for educ:

"离散变化"是指当自变量 $X_k$ 的值从一个起始值 $x_k^\text{start}$ 变化到一个结束值 $x_k^\text{end}$ 时，结果 $m$ 的概率发生的变化。具体地说，公式如下：

$$
\frac{\Delta\Pr\left(y=m\mid\mathbf{x}\right)}{\Delta x_k\left(x_k^{\mathrm{start}}\rightarrow x_k^{\mathrm{end}}\right)}=\Pr\left(y=m\mid\mathbf{x},x_k=x_k^{\mathrm{end}}\right)-\Pr\left(y=m\mid\mathbf{x},x_k=x_k^{\mathrm{start}}\right)
$$

其中，$\Pr\left(y=m\mid\mathbf{x},x_k\right)$ 表示给定其他自变量不变的情况下，观察到结果 $m$ 的概率。这个公式告诉我们，当 $x_k$ 从 $x_k^\text{start}$ 变化到 $x_k^\text{end}$ 时，结果为 $m$ 的概率发生的变化。这个变化的大小由 $\Delta\Pr\left(y=m\mid\mathbf{x}\right)/\Delta x_k$ 决定，其中所有其他变量都保持不变。这个变化的大小取决于所有变量的水平，包括 $x_k$，以及 $x_k$ 的变化量。

边际效应是通过 `mchange` 函数计算的，具体讨论可以在第4.5.4节和第7.11节找到。在拟合我们正在运行的模型之后，我们计算了连续变量的标准差变化以及其他变量从0到1的平均离散变化。选项 `amount(sd)` 抑制了默认计算边际变化和离散变化的单位为1的计算，而选项 `width(8)` 则防止由于变量 `educ` 的长标签而导致结果换行。

希望这样解释更容易理解一些！下面是一个简单的例子来说明离散变化的概念：

假设我们正在研究一个医学调查，想要了解一个人是否患有某种疾病（例如心脏病），并且我们想要了解体重指数 $X_k$ 对患病概率的影响。我们用离散变化的概念来解释，如果一个人的体重指数从 $x_k^\text{start} = 20$ 变化到 $x_k^\text{end} = 25$，而其他因素不变，那么这种变化会对患病概率产生多大的影响。

在上述例子中，我们假设体重指数 $X_k$ 是唯一影响患病概率的因素，其他因素保持不变。当体重指数从 $20$ 变化到 $25$ 时，我们可以使用离散变化公式来计算患病概率的变化。

假设在 $x_k^\text{start} = 20$ 时，患病概率为 $0.2$，而在 $x_k^\text{end} = 25$ 时，患病概率为 $0.3$。那么根据离散变化公式：

$$
\Delta Pr(y = m | x) = Pr(y = m | x, x_k^\text{end}) - Pr(y = m | x, x_k^\text{start})
$$

$$
= 0.3 - 0.2 = 0.1
$$

这意味着当体重指数从 $20$ 变化到 $25$ 时，患病概率增加了 $0.1$。

这就是离散变化的概念，它可以帮助我们理解自变量的变化对结果的影响。通过计算患病概率的变化，我们可以了解每个自变量的影响程度，并据此做出相应的决策或解释研究结果。

在实际应用中，我们通常使用统计软件来计算离散变化，并根据具体情况进行解释。比如，在医学研究中，研究人员可能使用多项逻辑回归模型来分析某种疾病的发病率与各种因素之间的关系，包括体重指数、年龄、性别等。

假设在这个模型中，体重指数 $X_k$ 是一个重要的预测因子。通过计算离散变化，研究人员可以确定当体重指数从一个水平变化到另一个水平时，患病概率的变化情况。这对于制定预防措施、制定治疗方案或评估风险因素都是非常有用的。

例如，如果研究发现体重指数每增加 $5$ 点，患病概率就增加 $0.1$，那么医生和公共卫生官员可以根据这个信息来制定针对肥胖患者的预防措施，或者提供更加精准的治疗方案。

因此，离散变化的概念和计算方法为研究人员提供了一种强大的工具，帮助他们理解和解释自变量对结果的影响，从而为实际问题的解决提供了有力支持。


Marginal effects are computed by **`mchange`** as discussed in detail in sections 4.5.4 and 7.11. After fitting the model used as our running example, we compute the average discrete changes for a standard deviation change in continuous variables and a change from 0 to 1 for other variables. The option **`amount(sd)`** suppresses the default computation of marginal changes and discrete changes of 1 unit, while **`width(8)`** prevents results wrapping due to the long value labels for `educ`.

```
mlogit party age income i.black i.female i.educ, base(5)
mchange, amount(sd) brief width(8)
```

    mlogit: Changes in Pr(y) | Number of obs = 1382
    
    Expression: Pr(party), predict(outcome())
    
                            |   StrDem       Dem     Indep       Rep    StrRep 
    ------------------------+-------------------------------------------------
    age                     |                                                 
                        +SD |    0.054    -0.033    -0.024    -0.030     0.032 
                    p-value |    0.000     0.006     0.001     0.009     0.003 
    income                  |                                                 
                        +SD |   -0.039    -0.022    -0.003     0.041     0.023 
                    p-value |    0.001     0.122     0.752     0.002     0.019 
    black                   |                                                 
                  yes vs no |    0.274     0.047     0.041    -0.248    -0.113 
                    p-value |    0.000     0.220     0.142     0.000     0.000 
    female                  |                                                 
                  yes vs no |   -0.006     0.065    -0.024    -0.004    -0.031 
                    p-value |    0.768     0.010     0.153     0.856     0.078 
    educ                    |                                                 
     hs only vs not hs grad |   -0.045     0.031    -0.039     0.027     0.026 
                    p-value |    0.137     0.414     0.208     0.466     0.254 
     college vs not hs grad |   -0.083     0.041    -0.092     0.034     0.100 
                    p-value |    0.025     0.367     0.007     0.441     0.001 
         college vs hs only |   -0.037     0.010    -0.052     0.006     0.073 
                    p-value |    0.142     0.744     0.004     0.825     0.002 

Even for this relatively simple model and looking only at a single amount of change for each variable, there is a lot of information to digest. To make it simpler to interpret these results, we plot the changes with **`mchangeplot`** (see `help mchangeplot` and section 6.2 for additional information about **`mchangeplot`**). We begin by looking at the average discrete changes for standard deviation increases in age and income:

```
mchangeplot age income, symbols(D d i r R) min(-.05) max(.05) gap(.02) ///
    aspect(.2) xtitle(Average Discrete Change) sig(.05)
```

<figure style="text-align:center;">
  <img src="https://cdn.jsdelivr.net/gh/forest293/Hugo-image/2024.3.2.32.png" style="width:750px;height:500px;" alt="图片描述">
  <figcaption><strong></strong></figcaption>
</figure>

By default, **`mchangeplot`** represents each outcome category with the first letter of the value label for that category. In this example, this would be confusing because the categories StrDem and StrRep both begin with S. **The **`symbols()`** option lets you specify one or more letters for each category.** For example, we could use **`symbols(SD D I R SR)`**. Or, as we prefer, we can use **`symbols(D d i r R)`** so that capitals indicate more strongly held affiliations. The resulting graph looks like this, where the *s indicate that an effect is significant at the 0.05 level:

**Before proceeding, you should verify that this graph corresponds to the output above from `mchange`.** Although we probably would not include this graph in a paper, we use it to help describe the effects:

>On average, a standard deviation increase in age, about 17 years, increases the probability of being a strong Republican by 0.03 and of being a strong Democrat by 0.05. The probabilities of other affiliations all decrease by roughly 0.03. All effects are significant at the 0.01 level.

>On average, a standard deviation increase in income, roughly $28,000$, significantly increases the probability of being a Republican by 0.04 and a strong Republican by 0.02, while significantly decreasing the probability of being a strong Democrat by 0.04.

The greater effect of race compared with gender on party affiliation is shown with a plot of their average discrete changes. We use the following command to create the graph:

```
mchangeplot black female, symbols(D d i r R) min(-.3) max(.3) gap(.1) ///
    aspect(.2) xtitle(Average Discrete Change) sig(.05)
```

<figure style="text-align:center;">
  <img src="https://cdn.jsdelivr.net/gh/forest293/Hugo-image/2024.3.2.33.png" style="width:750px;height:500px;" alt="图片描述">
  <figcaption><strong></strong></figcaption>
</figure>

We conclude the following:

>On average, for people similar on other characteristics, being black increases the probability of being a strong Democrat by nearly 0.30 compared with someone who is white. Conversely, being black decreases the probability of being a strong Republican by 0.11 and being a Republican by 0.25. Except for an increase of 0.07 in Democratic affiliation, the effects of gender are not significant.

For the factor variable **educ** with three categories, mchange provides all the pairwise contrasts, comparing those who have a high school diploma with those who do not, those who have a college degree with those who do not have a high school diploma, and those who have a college degree with those who have a high school diploma. One contrast is redundant in the sense that it can be computed from the other two. Still, it is useful to examine all contrasts to find patterns.

```
mchangeplot educ, symbols(D d I r R) min(-.1) max(.1) gap(.02) ///
    aspect(.3) xtitle(Average Discrete Change) sig(.05) leftmargin(5)
```


<figure style="text-align:center;">
  <img src="https://cdn.jsdelivr.net/gh/forest293/Hugo-image/2024.3.2.34.png" style="width:750px;height:500px;" alt="图片描述">
  <figcaption><strong></strong></figcaption>
</figure>

We conclude the following

>Higher education increases the probability of identifying as a strong Republican and decreases the probability of identifying as a strong Democrat. Based on the distribution of other characteristics in the population, if we compare those who do not have a high school diploma with those who have graduated from college, the probability of being a strong Republican increases by 0.10 on average and the probability of being a strong Democrat decreases by 0.08.

Although we do not illustrate these methods here, we could examine other amounts of change and customize the plots with the **`mchangeplot`** options described in **`help mchangeplot`**.

### 8.1 (Advanced) The distribution of marginal effects

>**Because the average marginal effect (AME) is an average, it does not indicate variation in the sample. We find examining the distribution of marginal effects is often very useful, but it requires using loops, macros, and returns and is computationally intensive.** After you are familiar with marginal effects and Stata tools for automation, we encourage you to study this section carefully. Initially, however, we hope you will at least skim it.

The value of a marginal effect depends on the level of all variables in the model (see section 6.2.5). Neither the AME nor the marginal effect at the mean provide information about how much variation there is within the sample in the size of the effects. In this section, we extend the methods from chapter 6 to the MNLM. The same techniques can also be used for the OLM.

The following commands generate the variable incoraedc containing the discrete change for a standard deviation increase in income, where we assume that the estimation results from **`mlogit`** are in memory:

```
mlogit party age income i.black i.female i.educ, base(1)
estimates store mymodel

gen incomedc = .
label var incomedc ///
    "Effect of a one standard deviation change in income on Pr(Dem)"

sum income
local sd = r(sd)
local nobs = _N
forvalues i = 1/`nobs' {
    qui {
        margins in `i', nose predict(outcome(2)) /// Dem
           at(income=gen(income)) at(income=gen(income+`sd'))
        local prstart = el(r(b),1,1)
        local prend = el(r(b),1,2)
        local dc = `prend' - `prstart'
        replace incomedc = `dc' in `i'
    }
}
```

Lines 1 and 2 create and label the variable that will hold the discrete change for each observation. Because we want to compute the effect of a standard deviation change in income, lines 3 and 4 compute the standard deviation and create the local macro sd containing the standard deviation. This is used in the **`margins`** command in line 8. Line 5 creates the macro nobs with the number of observations, which we use in line 6 to begin a **`forvalues`** loop through the 1,382 observations in the estimation sample.

The loop over observations is defined in lines 6 through 14. Because we do not want to see the output from **`margins`**, line 7 uses **`quietly`** to suppress the output. Line 8 uses **`margins`** to compute predictions for observation 'i' for the second outcome category, where **`noSE`** suppresses the computation of the standard error (this speeds up the computations). The first **`at()`** statement specifies the observed value of income, with all other variables held at their observed values; the second **`at()`** specifies the prediction at one standard deviation more than the observed value. **`margins`** returns the predictions to the matrix r(b), and lines 9 and 10 retrieve the starting and ending probabilities. Line 11 computes the discrete change. Line 12 saves the effect for observation 'i' of variable income_dc. The last two lines terminate the **`quietly`** command and the **`forvalues`** loop.

>Computing the distribution of effects is slow because computing effects for individual observations is slow. 
>>Every time **`margins`** is run, it computes predictions for all observations in the sample before it computes predictions at values specified by **`at()`**. In line 8, we need the predictions for a single observation, but **`margins`** computes predictions for all observations. You cannot turn off this behavior. Unfortunately, **`margins`** does not save the predictions it computes for each observation. If it did, we would not need the loop! Accordingly, for each observation in our loop, **`margins`** is computing $2N - 1 - 2$ predictions, for a total of $2(N^2 + N)$ predictions — nearly 4 million in our example! Using Stata/MP for eight cores, our loop took 60 seconds to complete. But we believe it is worth the time.

Although these commands might seem complex at first, the good news is that you can easily modify our code to work for other variables (for example, change income to age), for different outcomes (for example, change outcome(2) to outcome(5)), or for different amounts of change (for example, change 'sd' to 1 for a discrete change of 1). Further, the same commands will work with models for binary, ordinal, and count outcomes or, indeed, for almost any model supported by the **`margins`** command.

To plot the distribution of effects, we first compute the mean of **`incomedc`** and assign it to a local macro named **`ame`**:

```
sum incomedc
local ame = r(mean)
mchange income, amount(sd)
```
    
        Variable |        Obs        Mean    Std. dev.       Min        Max
    -------------+---------------------------------------------------------
        incomedc |      1,382   -.0217109    .0190934  -.0448823   .0305349
    
The mean equals the AME of -0.022 computed by **`mchange`** on page 417. Next, we use **`histogram`** to plot the distribution of effects, with a vertical dashed line showing the value of the AME:

```
histogram incomedc, xlabel(-.05(.01).05) fraction ///
    width(.005) color(gs10) fcolor(gs12) ylab(0(.1).2) xline(0, lpat(dash))
```

<figure style="text-align:center;">
  <img src="https://cdn.jsdelivr.net/gh/forest293/Hugo-image/2024.3.2.35.png" style="width:750px;height:500px;" alt="图片描述">
  <figcaption><strong></strong></figcaption>
</figure>

The histogram shows the distribution of discrete changes in the probability of being Democrat for a standard deviation change in income with the AME represented by a vertical dashed line:

Even the sign of the AME is potentially misleading. The distribution of effects is bimodal with most of the sample having effects around -0.03 and a smaller group with positive effects near 0.025. Suppose the independent variable being considered is an intervention where the spikes corresponded to two groups - say, whites and blacks - with negative effects for the larger group and positive effects for the smaller group. If substantive interest was on how the intervention would affect the smaller group, the AME is misleading because it is dominated by the negative effects for the larger group.

As a second example, we compute (without showing the commands) the distribution of the effects of age on being a strong Republican:

```
* distribution of DC for age
gen agedc = .
label var agedc "Effect of a one standard deviation change in age on Pr(StrRep)"

sum age
local sd = r(sd)
local nobs = _N
forvalues i = 1/`nobs' {
    qui {
        margins in `i', nose predict(outcome(5)) /// StrRep
            at(age=gen(age))  at(age=gen(age+`sd'))
        local prstart = el(r(b),1,1)
        local prend = el(r(b),1,2)
        local dc = `prend' - `prstart'
        replace agedc = `dc' in `i'
    }
}

* test comutations
sum agedc
mchange age, amount(sd)

histogram agedc, xlabel(0(.02).1) fraction width(.005) ///
    color(gs10) fcolor(gs12) ylab(0(.1).2)
```

<figure style="text-align:center;">
  <img src="https://cdn.jsdelivr.net/gh/forest293/Hugo-image/2024.3.2.36.png" style="width:750px;height:500px;" alt="图片描述">
  <figcaption><strong></strong></figcaption>
</figure>

## 9 Tables of predicted probabilities

As with models for binary and ordinal outcomes, tables of predictions can provide useful insights into models when there are substantively important, categorical independent variables. Because exactly the same commands can be used with nominal models as with ordinal models, we do not repeat the types of examples shown before (see section 7.13 for details). Instead, we focus on using tables of probabilities to compare and elaborate discrete changes across groups.

To show the effects of race and gender on party affiliation, we use **`mtable`** to compute probabilities for each combination of race and gender, holding other variables at their means. Although we could use the specification **`at(black = (0 1) female=(0 1))`**, we instead use multiple **`at()`** options to arrange the output in the order we prefer:

```
mlogit party age income i.black i.female i.educ, base(5)
mtable, atmeans noci norownumbers ///
    at(black=0 female=1) at(black=1 female=1) /// white women, black women
    at(black=0 female=0) at(black=1 female=0) /// white men, black men
```

    Expression: Pr(party), predict(outcome())
    
       black    female    StrDem       Dem     Indep       Rep    StrRep
    --------------------------------------------------------------------
           0         0     0.142     0.287     0.115     0.311     0.145
           1         0     0.440     0.328     0.162     0.049     0.021
           0         1     0.138     0.354     0.092     0.304     0.111
           1         1     0.417     0.394     0.127     0.047     0.015
    
    Specified values of covariates
    
               |                            2.        3.
               |      age    income      educ      educ
     ----------+---------------------------------------
       Current |     45.9      37.5       .58      .259

We can interpret this as follows:

>For those who are average on all other characteristics, blacks are far more likely than whites to be strong Democrats and far less likely to be Republicans or strong Republicans. Much smaller differences are found between men and women in party affiliation.

Supposing our substantive interest focuses on race and gender differences in party affiliation, we would want to test the differences in predictions between groups. The easiest way to do this is with **`mchange`**, using the option **`statistics(start end change pvalue)`** to list the predicted probabilities for each group as well as the discrete changes. The effects of race for women (at **`female = 1`**) are

```
mchange black, at(female=1) atmeans brief ///
    statistics(start end change pvalue) title(Effect of race for women)
```

    Effect of race for women | Number of obs = 1382
    
    Expression: Pr(party), predict(outcome())
    
                 |    StrDem        Dem      Indep        Rep     StrRep 
    -------------+------------------------------------------------------
    black        |                                                      
            From |     0.138      0.354      0.092      0.304      0.111 
              To |     0.417      0.394      0.127      0.047      0.015 
       yes vs no |     0.278      0.040      0.035     -0.257     -0.096 
         p-value |     0.000      0.342      0.169      0.000      0.000 

We interpret this as follows:

>Compared with a white woman who is average on all characteristics, an otherwise similar black woman has a 0.28 higher probability of being a strong Democrat, a 0.26 lower probability of being a Republican, and a 0.10 lower probability of being a strong Republican. All differences are significant at the 0.001 level.

Similarly, we compute the effects for men:

```
mchange black, at(female=1) atmeans brief ///
    statistics(start end change pvalue) title(Effect of race for women)
```

    Effect of race for women | Number of obs = 1382
    
    Expression: Pr(party), predict(outcome())
    
                 |    StrDem        Dem      Indep        Rep     StrRep 
    -------------+------------------------------------------------------
    black        |                                                      
            From |     0.138      0.354      0.092      0.304      0.111 
              To |     0.417      0.394      0.127      0.047      0.015 
       yes vs no |     0.278      0.040      0.035     -0.257     -0.096 
         p-value |     0.000      0.342      0.169      0.000      0.000 

Although the effects for race are of roughly the same size for men and women, we would like to test whether they are equal. For example, the discrete change for women is 0.278 and for men is 0.298. Can we say these differences are significantly different from one another? We consider this question in the next section.

### 9.1 (Advanced) Testing second differences

>**Computing and testing second differences is extremely useful, especially in group comparisons.** To make these computations requires more advanced programming and a deeper understanding of how **`margins`** and **`lincom`** work. On first reading, you might want to only skim this section. However, we hope you return to it later.

Earlier, we used **``mchange``** to compute the first difference for race holding **``female``** at either 0 or 1 with other variables held at their means. Now, we want to test the null hypothesis that the discrete change for men is equal to the discrete change for women:

$$H_0{:\frac{\Delta\Pr\left(y=j\mid\overline{\mathrm{x}},\textbf{female}=0\right)}{\Delta\text{b}1\text{ack}\left(0\to1\right)}=\frac{\Delta\Pr\left(y=j\mid\overline{\mathrm{x}},\textbf{female}=1\right)}{\Delta\text{b}1\text{ack}\left(0\to1\right)}}$$

We begin by fitting the model and storing the estimates so that we can restore them after posting estimates from **``margins``**:

```
mlogit party age income i.black i.female i.educ, base(5)
estimates store mymodel
```
Now, we use **``margins``** to compute predictions for all combinations of gender and race for outcome 1. Later, we will use a loop to make computations for all outcomes. Because the **``atlegend``** produced by **``margins``** is in this case quite long, we suppress it with the **``noatlegend``** option and use **``m listat``** to list the values at which the independent variables are held:

```
margins, predict(outcome(1)) post atmeans noatlegend ///
    at(black=0 female=0) at(black=1 female=0) /// white men, black men
    at(black=0 female=1) at(black=1 female=1) // white women, black women
```

    Adjusted predictions                                     Number of obs = 1,382
    Model VCE: OIM
    
    Expression: Pr(party==StrDem), predict(outcome(1))
    
    ------------------------------------------------------------------------------
                 |            Delta-method
                 |     Margin   std. err.      z    P>|z|     [95% conf. interval]
    -------------+----------------------------------------------------------------
             _at |
              1  |       0.14       0.01    10.21   0.000         0.12        0.17
              2  |       0.44       0.04    10.09   0.000         0.35        0.53
              3  |       0.14       0.01     9.79   0.000         0.11        0.17
              4  |       0.42       0.04     9.71   0.000         0.33        0.50
    ------------------------------------------------------------------------------

```
mlistat
```

                               1.        3.
         age    income      educ      educ 
    --------------------------------------
        45.9      37.5       .58      .259 

    at() values vary

       _at |    black    female 
    -------+-------------------
         1 |        0         0 
         2 |        1         0 
         3 |        0         1 
         4 |        1         1 

The **``post``** option saves the predictions so they can be used with **``lincom``** or **``mlincom``** to compute second differences.

Why are we using **``margins``**, which computes predictions for only one outcome, instead of **``m table``**, which computes predictions for all outcomes? To test predictions, those predictions must be saved to the **``e(b)``** and **``e(V)``** matrices. Because **``margins``** computes predictions for only one outcome at a time, we can only post predictions for one outcome. Although **``m table``** can collect predictions for all the outcomes, it can only post predictions for a single outcome, just like **``margins``**. Accordingly, there is no advantage to using **``m table``**.

The second difference is computed by taking the difference between two differences: 
1) the difference between the probability for black men contained in **``_b[2._at]``** and for white men in **``_b[1._at]``**; and 
2) the difference between the probability for black women in **``_b[4._at]``** and for white women in **``_b[3._at]``**:

```
lincom _b[2._at] - _b[1._at] // 1st difference if female
```

     ( 1)  - 1bn._at + 2._at = 0
    
    ------------------------------------------------------------------------------
                 | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]
    -------------+----------------------------------------------------------------
             (1) |       0.30       0.04     7.16   0.000         0.22        0.38
    ------------------------------------------------------------------------------

```
lincom _b[4._at] - _b[3._at] // 1st difference if male
```
     ( 1)  - 3._at + 4._at = 0
    
    ------------------------------------------------------------------------------
                 | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]
    -------------+----------------------------------------------------------------
             (1) |       0.28       0.04     6.82   0.000         0.20        0.36
    ------------------------------------------------------------------------------

```
lincom (_b[2._at] - _b[1._at]) - (_b[4._at] - _b[3._at])
est restore mymodel
```

     ( 1)  - 1bn._at + 2._at + 3._at - 4._at = 0
    
    ------------------------------------------------------------------------------
                 | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]
    -------------+----------------------------------------------------------------
             (1) |       0.02       0.02     0.90   0.366        -0.02        0.06
    ------------------------------------------------------------------------------

The second difference for the first outcome, being a strong Democrat, is less than two points and not significantly different from 0. **``estimates restore mymodel``** restores the mlogit estimates to compute the second difference for other outcomes.

We can automate the process with a forvalues loop over the values of the outcome:

```
forvalues iout = 1/5 {
    margins, predict(outcome(`iout')) post atmeans ///
        at(black=0 female=1) at(black=1 female=1) ///
        at(black=0 female=0) at(black=1 female=0)
    lincom (_b[2._at] - _b[1._at]) - (_b[4._at] - _b[3._at])
    est restore mymodel
}
```

**Line 1 loops through outcome categories 1 through 5, assigning the value to the local `iout`. Line 2 makes four predictions for outcome 'i out' and posts the estimates so they can be used by `lincom`. Line 3 computes the second differences by using `lincom`, and line 4 restores the `mlogit` estimation results. The output is 300 lines long.** It could be shortened by adding the `noatlegend` option along with `mlistat` as shown above. Alternatively, we could use `quietly` to suppress the output.

```
mlincom, clear
forvalues iout = 1/5 {
    qui {
        margins, predict(outcome(`iout')) post atmeans ///
            at(black=0 female=1) at(black=1 female=1) ///
            at(black=0 female=0) at(black=1 female=0)
        mlincom (2-1)-(4-3), save label(Outcome `iout')
        est restore mymodel
    }
}
```

**Line 1** clears the matrix where `mlincom` will accumulate results. Without this line, new results would be attached to results that might have been saved by `mlincom` run earlier. **Lines 3 through 7** use `quietly` to suppress the display of results from `margins` and `mlincom`. **Line 5** replaces `lincom` with `mlincom`, which lets us refer to the predictions by their position in the output of `margins`, rather than requiring the `_b[]` syntax. The `save` option adds the current results to the matrix holding prior results, `label()` adds labels to each set of results, in this case, indicating the outcome being tested. We run `mlincom` to list the results:

```
mlincom
```

                 |   lincom    pvalue        ll        ul 
    -------------+---------------------------------------
       Outcome 1 |   -0.020     0.366    -0.063     0.023 
       Outcome 2 |   -0.001     0.967    -0.039     0.037 
       Outcome 3 |   -0.013     0.238    -0.034     0.008 
       Outcome 4 |    0.004     0.836    -0.037     0.046 
       Outcome 5 |    0.029     0.082    -0.004     0.061 

The second differences are all less than 0.03 in magnitude and none are statistically significant. We conclude the following:

>For those that are average on all characteristics, the marginal effect of race on party affiliation is the same for men and women.

### 9.2 (Advanced) Predictions using local means and subsamples

>Comparing groups by making predictions with local means or by average predictions within subsamples is important for nuanced interpretations of group differences. To do this requires Stata programming and using the `over()` option with `margins`. Although you might want to skip this section on first reading, we encourage you to return to this section when you are comfortable with the commands used in other sections on interpretation with predictions.

To compute the predicted probabilities of party affiliation by race and gender, we used the `atmeans` option to hold all other variables at the means for the estimation sample. Accordingly, the predictions are comparing white men, black men, white women, and black women who have the same values for age, income, and education. Because the four race-gender groups are likely to differ on these variables, the predictions must be viewed as a “what if” experiment: what would happen if these groups had the same distributions of other characteristics?

We could compute predictions by using within-group means, which we refer to as local means. Using methods discussed on page 273, we use `if` conditions to select the sample for each `mtable` command:

```
mlogit party age income i.black i.female i.educ, base(5)
estimates store mymodel
qui mtable if black==0 & female==0, atmeans noci rowname(White Men) clear
qui mtable if black==1 & female==0, atmeans noci rowname(Black Men) below
qui mtable if black==0 & female==1, atmeans noci rowname(White Women) below
mtable if black==1 & female==1, atmeans noci rowname(Black Women) below
```

    Expression: Pr(party), predict(outcome())
    
                  |   StrDem       Dem     Indep       Rep    StrRep
     -------------+-------------------------------------------------
        White Men |    0.128     0.283     0.109     0.325     0.156
        Black Men |    0.461     0.310     0.168     0.044     0.017
      White Women |    0.145     0.354     0.093     0.298     0.109
      Black Women |    0.460     0.364     0.129     0.037     0.011
    
    Specified values of covariates
    
               |                                                2.        3.
               |      age    income     black    female      educ      educ
     ----------+-----------------------------------------------------------
         Set 1 |     45.1      43.5         0         0      .546      .324
         Set 2 |       45      29.8         1         0      .518      .188
         Set 3 |     47.1      35.2         0         1      .624      .222
       Current |     45.4      20.4         1         1       .59      .143
    
The values of the covariates show substantial differences among the groups, especially with respect to income, where white men have more than twice the average income of black women. Consequently, the predicted probabilities with local means differ from those computed with global means. For example, for black women, the probability of being a strong Democrat is 0.417 when global means are used compared with 0.460 when local means are used.

Although we can create the predictions we want by using `mtable` with `if` conditions, this approach does not allow us to compute first and second differences across the groups. Technically, the problem is that at the end of each `mtable` command (or `margins`, if we had used that command instead), only predictions for the current group can be posted to `e(b)` and `e(V)` for use by `lincom` or `mlincom`. **A relatively efficient way to deal with this limitation is to use the `over (over-variables)` option. With the `over()` option, `margins` computes predictions based on the subsample of cases defined by the over-variables.**
**The over-variables can be any categorical variables in the dataset, even if they are not used in the regression model.** For our purposes, we want to use `over(female black)` to compute predictions based on subsamples defined by race and gender:

```
margins, over(female black) atmeans post
estimates restore mymodel
```

    Adjusted predictions                                     Number of obs = 1,382
    Model VCE: OIM
    
    Over: female black
    
    1._predict: Pr(party==StrDem), predict(pr outcome(1))
    2._predict: Pr(party==Dem), predict(pr outcome(2))
    3._predict: Pr(party==Indep), predict(pr outcome(3))
    4._predict: Pr(party==Rep), predict(pr outcome(4))
    5._predict: Pr(party==StrRep), predict(pr outcome(5))
    
    At: 0.female#0.black
            age    = 45.13171 (mean)
            income = 43.54512 (mean)
            black  =        0
            female =        0
            1.educ = .1300813 (mean)
            2.educ = .5463415 (mean)
            3.educ = .3235772 (mean)
        0.female#1.black
            age    = 44.98824 (mean)
            income = 29.84706 (mean)
            black  =        1
            female =        0
            1.educ = .2941176 (mean)
            2.educ = .5176471 (mean)
            3.educ = .1882353 (mean)
        1.female#0.black
            age    = 47.06239 (mean)
            income = 35.18977 (mean)
            black  =        0
            female =        1
            1.educ = .1542461 (mean)
            2.educ = .6239168 (mean)
            3.educ = .2218371 (mean)
        1.female#1.black
            age    =  45.3619 (mean)
            income = 20.42619 (mean)
            black  =        1
            female =        1
            1.educ = .2666667 (mean)
            2.educ = .5904762 (mean)
            3.educ = .1428571 (mean)
  
    ---------------------------------------------------------------------------------------
                          |            Delta-method
                          |     Margin   std. err.      z    P>|z|     [95% conf. interval]
    ----------------------+----------------------------------------------------------------
    _predict#female#black |
                 1#no#no  |       0.13       0.01     9.76   0.000         0.10        0.15
                1#no#yes  |       0.46       0.04    10.70   0.000         0.38        0.55
                1#yes#no  |       0.15       0.01    10.15   0.000         0.12        0.17
               1#yes#yes  |       0.46       0.04    11.18   0.000         0.38        0.54
                 2#no#no  |       0.28       0.02    15.66   0.000         0.25        0.32
                2#no#yes  |       0.31       0.04     8.23   0.000         0.24        0.38
                2#yes#no  |       0.35       0.02    17.90   0.000         0.32        0.39
               2#yes#yes  |       0.36       0.04     9.37   0.000         0.29        0.44
                 3#no#no  |       0.11       0.01     8.70   0.000         0.08        0.13
                3#no#yes  |       0.17       0.03     5.28   0.000         0.11        0.23
                3#yes#no  |       0.09       0.01     7.90   0.000         0.07        0.12
               3#yes#yes  |       0.13       0.03     5.05   0.000         0.08        0.18
                 4#no#no  |       0.32       0.02    16.80   0.000         0.29        0.36
                4#no#yes  |       0.04       0.02     2.82   0.005         0.01        0.07
                4#yes#no  |       0.30       0.02    15.32   0.000         0.26        0.34
               4#yes#yes  |       0.04       0.01     2.81   0.005         0.01        0.06
                 5#no#no  |       0.16       0.02    10.30   0.000         0.13        0.19
                5#no#yes  |       0.02       0.01     1.71   0.088        -0.00        0.04
                5#yes#no  |       0.11       0.01     8.28   0.000         0.08        0.14
               5#yes#yes  |       0.01       0.01     1.69   0.091        -0.00        0.02
    ---------------------------------------------------------------------------------------

```
margins if black==0 & female==0, atmeans post
estimates restore mymodel
margins if black==0 & female==1, atmeans post
estimates restore mymodel
margins if black==1 & female==0, atmeans post
estimates restore mymodel
margins if black==1 & female==1, atmeans post
estimates restore mymodel
```

    Adjusted predictions                                       Number of obs = 105
    Model VCE: OIM
    
    1._predict: Pr(party==StrDem), predict(pr outcome(1))
    2._predict: Pr(party==Dem), predict(pr outcome(2))
    3._predict: Pr(party==Indep), predict(pr outcome(3))
    4._predict: Pr(party==Rep), predict(pr outcome(4))
    5._predict: Pr(party==StrRep), predict(pr outcome(5))
    
    At: age    =  45.3619 (mean)
        income = 20.42619 (mean)
        black  =        1
        female =        1
        1.educ = .2666667 (mean)
        2.educ = .5904762 (mean)
        3.educ = .1428571 (mean)
    
    ------------------------------------------------------------------------------
                 |            Delta-method
                 |     Margin   std. err.      z    P>|z|     [95% conf. interval]
    -------------+----------------------------------------------------------------
        _predict |
              1  |       0.46       0.04    11.18   0.000         0.38        0.54
              2  |       0.36       0.04     9.37   0.000         0.29        0.44
              3  |       0.13       0.03     5.05   0.000         0.08        0.18
              4  |       0.04       0.01     2.81   0.005         0.01        0.06
              5  |       0.01       0.01     1.69   0.091        -0.00        0.02
    ------------------------------------------------------------------------------

Except that **`margins, over() post`** posts the four predictions that allow us to compute and test second differences.

Now, we can use a `forvalues` loop to test whether second differences are equal to 0 for each outcome:

```
mlincom, clear
forvalues iout = 1/5 {
    qui {
        margins, over(female black) predict(outcome(`iout')) post atmeans 
        mlincom (2-1)-(4-3), save label(Outcome `iout') 
        est restore mymodel
      }  //end of quietly
}  //end of forvalues
```

```
mlincom
```

                 |   lincom    pvalue        ll        ul 
    -------------+---------------------------------------
       Outcome 1 |    0.019     0.415    -0.026     0.064 
       Outcome 2 |    0.019     0.345    -0.020     0.057 
       Outcome 3 |    0.024     0.047     0.000     0.048 
       Outcome 4 |   -0.020     0.360    -0.064     0.023 
       Outcome 5 |   -0.041     0.019    -0.075    -0.007 

Gender differences in the effect of race are larger when computed using group-specific means for the other variables. The effect of race on being Independent is significantly larger for men than women, and the effect of race on being strongly Republican is significantly larger for women than men.

A related approach for comparing groups is to compute the average predicted probabilities within each of the subsamples defined by the groups being compared. For example, we can compare the average probability of being Republican for white men with the average probability for black men. To make these computations only requires us to remove the option `atmeans` from the commands used above.

```
mlincom, clear
forvalues iout = 1/5 {
    qui {
        margins, over(female black) predict(outcome(`iout')) post 
        mlincom (2-1)-(4-3), save label(Outcome `iout') 
        est restore mymodel
      }  //end of quietly
}  //end of forvalues
```

```
mlincom
```

                     |   lincom    pvalue        ll        ul 
        -------------+---------------------------------------
           Outcome 1 |    0.021     0.294    -0.018     0.060 
           Outcome 2 |    0.016     0.357    -0.018     0.051 
           Outcome 3 |    0.019     0.080    -0.002     0.041 
           Outcome 4 |   -0.015     0.476    -0.057     0.026 
           Outcome 5 |   -0.041     0.017    -0.075    -0.007 

The results in this example lead to the same substantive conclusions obtained using local means.

## 10 Graphing predicted probabilities

Graphing predicted probabilities for each outcome can also be useful for the MNLM and is done exactly as it was for the ORM. To illustrate this, we create plots to show the effects of age and income on party affiliation. After fitting the model, we compute predictions as income increases from $0$ to $100,000$, holding other variables at their means.

```
qui mlogit party age income i.black i.female i.educ, base(5) vsquish
mgen, atmeans at(income=(0(10)100)) stub(mnlmI) replace
```

    Predictions from: margins, atmeans at(income=(0(10)100)) predict(outcome())
    
    Variable     Obs Unique      Mean       Min       Max  Label
    --------------------------------------------------------------------------------
    mnlmIpr1      11     11  .1604032   .090648  .2445128  pr(y=StrDem) from margins
    mnlmIll1      11     11  .1253144  .0479288  .1908274  95% lower limit
    mnlmIul1      11     11  .1954919  .1333672  .2981981  95% upper limit
    mnlmIincome   11     11        50         0       100  Income in $1,000s
    mnlmICpr1     11     11  .1604032   .090648  .2445128  pr(y<=StrDem)
    mnlmIpr2      11     11  .3325221  .2827661  .3683488  pr(y=Dem) from margins
    mnlmIll2      11     11  .2882002  .2133308  .3252223  95% lower limit
    mnlmIul2      11     11  .3768441  .3522013   .420665  95% upper limit
    mnlmICpr2     11     11  .4929253  .3734141  .6128616  pr(y<=Dem)
    mnlmIpr3      11     11  .1120243  .1014983   .117243  pr(y=Indep) from margins
    mnlmIll3      11     11  .0816545  .0528909  .0967365  95% lower limit
    mnlmIul3      11     11   .142394  .1344071   .152314  95% upper limit
    mnlmICpr3     11     11  .6049496  .4749124   .729903  pr(y<=Indep)
    mnlmIpr4      11     11  .2785857  .1944607  .3642161  pr(y=Rep) from margins
    mnlmIll4      11     11  .2354603  .1558902  .2878424  95% lower limit
    mnlmIul4      11     11  .3217112  .2330312  .4405898  95% upper limit
    mnlmICpr4     11     11  .8835353  .8391285  .9243637  pr(y<=Rep)
    mnlmIpr5      11     11  .1164647  .0756363  .1608715  pr(y=StrRep) from margins
    mnlmIll5      11     11  .0857181  .0519797  .1041749  95% lower limit
    mnlmIul5      11     11  .1472114  .0992929  .2175682  95% upper limit
    mnlmICpr5     11      4         1         1         1  pr(y<=StrRep)
    --------------------------------------------------------------------------------
    
    Specified values of covariates
    
                       1.         1.         2.         3.
          age      black     female       educ       educ 
    -----------------------------------------------------
     45.94645   .1374819   .4934877   .5803184   .2590449 

Using variable labels to assign labels to the lines, we can plot the predictions with these commands:

```
label var mnlmIpr1 "Strong Dem"
label var mnlmIpr2 "Democrat"
label var mnlmIpr3 "Independent"
label var mnlmIpr4 "Republican"
label var mnlmIpr5 "Strong Rep"
```


```
graph twoway connected ///
    mnlmIpr1 mnlmIpr2 mnlmIpr3 mnlmIpr4 mnlmIpr5 mnlmIincome, ///
    title("Multinomial logit model: other variables held at their means", ///
    pos(11) size(medium)) ///
    ytitle(Probability of party affiliation) ylab(0(.1).4, grid gmax gmin) ///
    msym(O Oh dh sh s) mcol(gs1 gs5 gs8 gs5 gs1) ///
    lpat(solid dash shortdash dash solid) lcol(gs1 gs5 gs8 gs5 gs1) ///
    legend(rows(2))
```

<figure style="text-align:center;">
  <img src="https://cdn.jsdelivr.net/gh/forest293/Hugo-image/2024.3.2.37.png" style="width:750px;height:500px;" alt="图片描述">
  <figcaption><strong></strong></figcaption>
</figure>

The probabilities of being a strong Democrat (•) or a Democrat (O) decrease with income, while the probabilities of being a Republican (□) or a strong Republican (■) increase, with little change in the probability of being Independent (O). Using `ologit` to fit the ordinal model, we obtain a nearly identical graph:

```
qui ologit party age income i.black i.female i.educ, vsquish
mgen, atmeans at(income=(0(10)100)) stub(olmI) replace
label var olmIpr1 "Strong Dem"
label var olmIpr2 "Democrat"
label var olmIpr3 "Independent"
label var olmIpr4 "Republican"
label var olmIpr5 "Strong Rep"

graph twoway connected ///
    olmIpr1 olmIpr2 olmIpr3 olmIpr4 olmIpr5 olmIincome, ///
    title("Ordered logit model", pos(11) size(medium)) ///
    ytitle(Probability of party affiliation) ylab(0(.1).4, grid gmax gmin) ///
    msym(O Oh dh sh s) msiz(*1.5 *1.5 *1.7 *1.8 *1.8) mcol(gs1 gs5 gs8 gs5 gs1) ///
    lpat(solid dash shortdash dash solid) lcol(gs1 gs5 gs8 gs5 gs1) ///
    legend(rows(2))
```

<figure style="text-align:center;">
  <img src="https://cdn.jsdelivr.net/gh/forest293/Hugo-image/2024.3.2.39.png" style="width:750px;height:500px;" alt="图片描述">
  <figcaption><strong></strong></figcaption>
</figure>

The results are quite different when we examine the effect of age on party affiliation. For the MNLM, we plot predictions as age increases from 20 to 85, holding other variables at their means.



```
qui mlogit party age income i.black i.female i.educ, base(5) vsquish
mgen, atmeans at(age=(20(5)85)) stub(mnlmA) replace
label var mnlmApr1 "Strong Dem"
label var mnlmApr2 "Democrat"
label var mnlmApr3 "Independent"
label var mnlmApr4 "Republican"
label var mnlmApr5 "Strong Rep"

graph twoway connected ///
    mnlmApr1 mnlmApr2 mnlmApr3 mnlmApr4 mnlmApr5 olmAage, ///
    title("Multinomial logit model", pos(11) size(medium)) ///
    ytitle(Probability of party affiliation) ylab(0(.1).4, grid gmax gmin) ///
    msym(O Oh dh sh s) msiz(*1.5 *1.5 *1.7 *1.8 *1.8) mcol(gs1 gs5 gs8 gs5 gs1) ///
    lpat(solid dash shortdash dash solid) lcol(gs1 gs5 gs8 gs5 gs1) ///
    legend(rows(2))
```

<figure style="text-align:center;">
  <img src="https://cdn.jsdelivr.net/gh/forest293/Hugo-image/2024.3.25.4.png" style="width:750px;height:350px;" alt="图片描述">
  <figcaption><strong></strong></figcaption>
</figure>

```
qui ologit party age income i.black i.female i.educ, vsquish

mgen, atmeans at(income=(0(10)100)) stub(olmI) replace
label var olmIpr1 "Strong Dem"
label var olmIpr2 "Democrat"
label var olmIpr3 "Independent"
label var olmIpr4 "Republican"
label var olmIpr5 "Strong Rep"

graph twoway connected ///
    olmApr1 olmApr2 olmApr3 olmApr4 olmApr5 olmAage, ///
    title("Ordered logit model", pos(11) size(medium)) ///
    ytitle(Probability of party affiliation) ylab(0(.1).4, grid gmax gmin) ///
    msym(O Oh dh sh s) msiz(*1.5 *1.5 *1.7 *1.8 *1.8) mcol(gs1 gs5 gs8 gs5 gs1) ///
    lpat(solid dash shortdash dash solid) lcol(gs1 gs5 gs8 gs5 gs1) ///
    legend(rows(2))
```


<figure style="text-align:center;">
  <img src="https://cdn.jsdelivr.net/gh/forest293/Hugo-image/2024.3.25.2.png" style="width:750px;height:350px;" alt="图片描述">
  <figcaption><strong></strong></figcaption>
</figure>

This example illustrates the risk of assuming an ORM is appropriate simply because the dependent variables can be ordered. Although income affects affiliation as would be expected with a unidimensional, ordinal outcome, age increases the strength of affiliation but does not affect left-right orientation. **When using an ordinal model, we believe it is good practice to examine the sensitivity of the results to the constraints of ordinality by comparing the results from the ordinal model with those from the MNLM or the generalized OLM.**

## 11 Odds ratios

Discrete change coefficients do not show the dynamics among the outcomes. For example, being black increases the probability of being a Democrat or an Independent, but how does it affect the probability of being Democrat relative to being Independent? **To deal with these questions, odds ratios, also referred to as relative-risk ratios and factor change coefficients, can be used to explore how variables affect the choice of one outcome compared with another outcome. Odds ratios do not provide a complete picture of the effects of variables on the outcomes and have the same limitations discussed for the binary logit model in chapter 6; however, in the MNLM, odds ratios complement the information provided by marginal effects and other types of predictions.**

The factor change in the odds of outcome $m$ versus outcome $n$ as $X_k$ increases by $\delta$, holding other variables constant, equals:

$$\frac{\Omega_{m|n}\left(\mathbf{x},x_k+\delta\right)}{\Omega_{m|n}\left(\mathbf{x},x_k\right)}=e^{\beta_{k,m|n}\delta}$$

If the amount of change is $\delta=1$,the odds ratio can be interpreted as follows:

>For a unit increase in $x_k$,the odds of $m$ versus $n$ are expected to change by a factor of $\exp(\beta_{k,m|n})$, holding all other variables constant.

If the amount of change is $$\delta=s_{x_k}$$,then the odds ratio can be interpreted as follows:

>For a standard deviation increase in $X_k$, the odds of $m$ versus $n$ are expected to change by a factor of $\exp(\beta_{\text{fc}}>m|n \times s^*)$, holding all other variables constant.

Other values of $\delta$ can also be used, such as $\delta = 4$  for four years of education or $\delta = 10$ for $10,000$ in income.


当我们研究某个变量对选择一种结果而不是另一种结果的影响时，赔率比是一个有用的概念。赔率比告诉我们，当一个变量增加一定量时，选择一个结果相对于选择另一个结果的赔率会如何改变。让我们用一个例子来解释这个概念。

假设我们对选民进行调查，想知道他们的年龄对于投票给民主党或独立党的影响。我们使用了一个公式来表示年龄变化如何影响投票选择的赔率：

$$ \frac{\Omega_{\text{民主党|独立党}}(\textbf{x},\text{年龄}+\delta)}{\Omega_{\text{民主党|独立党}}(\textbf{x},\text{年龄})} = e^{\beta_{\text{年龄,民主党|独立党}}\delta} $$

现在让我们解释这个公式。假设我们发现，当年龄增加1岁时，投票给民主党相对于独立党的赔率增加了3。这意味着，当年龄增加1岁时，投票给民主党相对于独立党的赔率会乘以$ e^3 $。

换句话说，随着年龄增长，一个人更有可能投票给民主党而不是独立党，而且这种可能性会以指数级别增加。

所以，赔率比告诉我们了解不同变量如何影响结果选择的关键信息。它帮助我们理解特定变量的变化对于结果选择的影响，而且通常可以用具体的倍数来量化这种影响，使我们更好地理解数据中的模式和趋势。

另一个例子是关于肤色对选民投票偏好的影响。假设我们有两个群体，黑人和白人，我们想知道他们投票给民主党相对于独立党的赔率有何不同。

我们可以使用相同的公式来分析这个问题：

$$ \frac{\Omega_{\text{民主党|独立党}}(\textbf{x},\text{肤色}+\delta)}{\Omega_{\text{民主党|独立党}}(\textbf{x},\text{肤色})} = e^{\beta_{\text{肤色,民主党|独立党}}\delta} $$

假设我们发现，当肤色从白变为黑时，投票给民主党相对于独立党的赔率增加了2。这意味着，黑人相对于白人，投票给民主党相对于独立党的赔率是白人的两倍。

通过这个例子，我们可以看到赔率比如何帮助我们理解不同变量对于结果选择的影响，并且通过具体的倍数来量化这种影响，使我们更好地理解数据中的模式和趋势。

### 11.1 Listing odds ratios with listcoef

The difficulty in interpreting odds ratios for the MNLM is that to understand the effect of a variable, you need to examine the coefficients for comparisons among all pairs of outcomes. The standard output from `mlogit` includes only a minimal set of $J - 1$ comparisons with the base outcome. Although you could estimate coefficients for all possible comparisons by rerunning `mlogit` with different bases (for example, `mlogit party female black ..., base(1); mlogit party female black ..., base(2);` etc.), using `listcoef` is simpler. For example, to examine the odds ratios for variable black, type:


```
use partyid4.dta, clear
mlogit party age income i.black i.female i.educ, base(5)
listcoef black, help
```

    mlogit (N=1382): Factor change in the odds of party 
    
    Variable: 1.black (sd=0.344)
    ------------------------------------------------------------------------------
                                 |         b        z    P>|z|       e^b   e^bStdX
    -----------------------------+------------------------------------------------
    StrDem       vs Dem          |    0.9963    5.022    0.000     2.708     1.409
    StrDem       vs Indep        |    0.7845    3.052    0.002     2.191     1.310
    StrDem       vs Rep          |    2.9692    7.666    0.000    19.475     2.781
    StrDem       vs StrRep       |    3.0754    5.091    0.000    21.659     2.885
    Dem          vs StrDem       |   -0.9963   -5.022    0.000     0.369     0.709
    Dem          vs Indep        |   -0.2118   -0.830    0.407     0.809     0.930
    Dem          vs Rep          |    1.9728    5.132    0.000     7.191     1.973
    Dem          vs StrRep       |    2.0791    3.448    0.001     7.997     2.047
    Indep        vs StrDem       |   -0.7845   -3.052    0.002     0.456     0.763
    Indep        vs Dem          |    0.2118    0.830    0.407     1.236     1.076
    Indep        vs Rep          |    2.1846    5.220    0.000     8.887     2.122
    Indep        vs StrRep       |    2.2909    3.658    0.000     9.884     2.202
    Rep          vs StrDem       |   -2.9692   -7.666    0.000     0.051     0.360
    Rep          vs Dem          |   -1.9728   -5.132    0.000     0.139     0.507
    Rep          vs Indep        |   -2.1846   -5.220    0.000     0.113     0.471
    Rep          vs StrRep       |    0.1063    0.155    0.877     1.112     1.037
    StrRep       vs StrDem       |   -3.0754   -5.091    0.000     0.046     0.347
    StrRep       vs Dem          |   -2.0791   -3.448    0.001     0.125     0.489
    StrRep       vs Indep        |   -2.2909   -3.658    0.000     0.101     0.454
    StrRep       vs Rep          |   -0.1063   -0.155    0.877     0.899     0.964
    ------------------------------------------------------------------------------
           b = raw coefficient
           z = z-score for test of b=0
       P>|z| = p-value for z-test
         e^b = exp(b) = factor change in odds for unit increase in X
     e^bStdX = exp(b*SD of X) = change in odds for SD increase in X

The odds ratios of interest are in the column labeled `e(b)`. For example, the odds ratio for black for the outcomes StrDera vs Dera is 2.708, which is significant at the 0.001 level. It can be interpreted as follows:

>Being black increases the odds of having a strong Democratic affiliation compared with a Democratic affiliation by a factor of 2.7, holding other variables constant.

Even for a single variable, there are a lot of coefficients; we often hear people lament that there are “too many” coefficients to interpret. Fortunately, a simple graph makes this task manageable, even for complex models.

### 11.2 Plotting odds ratios

An odds-ratio plot lets you quickly see patterns in coefficients, even for complex models with many outcomes. Methods for plotting odds ratios were developed by Long (1987) while using the MNLM to examine factors that determine the organizational contexts in which scientists work (Long and McGinnis 1981). Although an odds-ratio plot was included in that paper, this is generally not the most effective way to use these graphs. Rather, the graphs provide a quick way to assess all the parameters in the model and to get a general sense of what is going on to help plan further analyses. Experience in teaching suggests that within an hour, students gain a “feel” for these graphs that allows them to evaluate the results of an MNLM in only a few minutes. Building on these insights, more detailed analyses can be planned using other methods of interpretation.

To explain how to interpret an odds-ratio plot, we begin with hypothetical results from an MNLM with three outcomes and three independent variables:


<figure style="text-align:center;">
  <img src="https://cdn.jsdelivr.net/gh/forest293/Hugo-image/2024.3.25.5.png" style="width:550px;height:350px;" alt="图片描述">
  <figcaption><strong></strong></figcaption>
</figure>

These coefficients were constructed to have specific types of relationships among outcomes and variables:

* The $\beta$ coefficients for xi and on $B | A$ (which you can read as $B$ versus $A$) are equal but of opposite sign. The coefficient for $x3$ is half as large. 

* The $\beta$ coefficients for $x1$ and $x2$ on $C | A$ are half as large and in opposite directions
as the coefficients on $B | A$ , whereas the coefficient for $x3$ is in the same direction
but is twice as large.

In an odds-ratio plot, each independent variable is presented on a separate row, with the horizontal axis indicating the magnitude of the $\beta$ coefficients associated with each contrast of outcomes. Here is the plot, where the letters correspond to the outcome categories:


<figure style="text-align:center;">
  <img src="https://cdn.jsdelivr.net/gh/forest293/Hugo-image/2024.3.25.6.png" style="width:750px;height:270px;" alt="图片描述">
  <figcaption><strong></strong></figcaption>
</figure>

We now explain how the graph conveys the information from the table of coefficients.

* **Sign of coefficients.** If a letter is to the right of another letter, increases in the independent variable make the outcome to the right more likely relative to outcomes located to the left, holding all other variables constant. Thus, relative to outcome A, an increase in $X$ increases the odds of C and decreases the odds of D. This corresponds to the positive sign of $\beta1_{\text{C|A}}$ and the negative sign of $\beta_{\text{B|A}}$. The signs of these coefficients are reversed for $X_2$, and accordingly, the plot for $X_2$ is a mirror image of that for $X_1$.

* **Magnitude of coefficients:** The distance between a pair of letters indicates the magnitude of the coefficient. The additive scale on the bottom axis measures the value of the $\beta_{k,m|n}'s$ . The multiplicative scale on the top axis measures the odds ratios $\exp(\beta_{k,m|n})$. For both $X_1$ and $X_2$, the distance between A and B is twice the distance between A and C, which reflects that $\beta1_{\text{B|A}}$ is twice as large as $\beta1_{\text{C|A}}$, and $\beta2_{\text{B|A}}$ is twice as large as $\beta2_{\text{C|A}}$. For $X_3$, the distance between A and B is half the distance between A and C, reflecting that $\beta3_{\text{B|A}}$ is twice as large as $\beta3_{\text{C|A}}$.

* **The additive relationship:** The additive relationships among coefficients in (8.1) are shown in the graph. For all the independent variables,$\beta_{k,C|A}=\beta_{k,B|A}+\beta_{k,C|B}.$Accordingly, the distance from letters $A$ to $C$ in the graph is the sum of the distances from $A$ to $B$ and $B$ to $C$.This is easiest to see in the row for variable $X_3$, where all the coefficients are positive. By plotting the $J - 1$ coefficients from a minimal set, it is possible to visualize the relationships among all pairs of outcomes. 

* **The base outcome:** In the graph above, the $As$ are aligned vertically because the plot uses $A$ as the base outcome when graphing the coefficients. The choice of the base is arbitrary. We could have used alternative $B$ as the base instead, which would shift the rows of the graph to the left or right so that the $Bs$ lined up. Doing this leads to the following graph:


<figure style="text-align:center;">
  <img src="https://cdn.jsdelivr.net/gh/forest293/Hugo-image/2024.3.25.7.png" style="width:750px;height:270px;" alt="图片描述">
  <figcaption><strong></strong></figcaption>
</figure>

* **Creating odds-ratio plots with mlogitplot**

Odds-ratio plots can be easily constructed with the SPost command `mlogitplot`. In this section, we construct a series of odds-ratio plots that illustrate how this command can be used to understand the factors affecting party affiliation. We assume that the results from `mlogit` are in memory. Because `mlogitplot` does not change the estimation results, there is no need to use `estimates restore` and `estimates store`. Full details on the syntax for `mlogitplot` are given in `help mlogitplot`.

As a first step in examining results from an MNLM, it is useful to create an odds-ratio plot for all variables. This is done by typing the command `mlogitplot`. To make our graph more effective, we add a few options:

```
mlogitplot, symbols(D d i r R) base(3) linepvalue(1) leftmargin(2)
```

The option `symbols (D d i r R)` labels the outcomes, and `base(3)` specifies to line up the outcomes on base 3, which is **Indep** shown by `i.` The option `linepvalue(1)` removes lines indicating statistical significance, an important feature that will be discussed soon. Finally, `leftmargin(2)` adds space on the left for the labels associated with factor variables; in practice, you will need to experiment to determine how large this margin needs to be for your plot. The following graph is created:


<figure style="text-align:center;">
  <img src="https://cdn.jsdelivr.net/gh/forest293/Hugo-image/2024.3.25.8.png" style="width:750px;height:470px;" alt="图片描述">
  <figcaption><strong></strong></figcaption>
</figure>

The independent variables are listed on the left, with the vertical distance for a given variable used simply to prevent the symbols from overlapping. The default vertical distance does not always work (for example, notice how “r” is inside the “D” for variable 1 . female), and later we consider options for refining these offsets.

From the plot, we immediately see that the odds ratios for black are the largest, increasing the odds of being a strong Democrat ("D"), a Democrat ("d"), or an Independent ("i") compared with being Republican ("r") or strong Republican ("R"). Having a college education compared with not completing high school (`educ`) increases the odds of being a strong Republican ("R") relative to the other categories. Consistent with our findings when plotting probabilities against age and income, age increases the odds of strongly affiliating with either party relative to affiliations that are less strong, while income increases the odds of more right-leaning affiliations relative to left-leaning.

The current graph has two limitations. **First, while it shows the size of odds ratios, it does not indicate whether they are statistically significant. While it is tempting to assume that larger odds ratios imply smaller values for testing the hypothesis that the odds ratio is 1, that should not be done! Instead, we need to add the significance level to the graph. Second, because a large odds ratio does not necessarily correspond to a large marginal effect, we will add information on marginal effects to the graph.**

* Adding significance levels

The distance between two outcomes indicates the magnitude of the coefficient. **If a coefficient is not significantly different from 0, we add a line connecting the two outcomes, suggesting that those outcomes are “tied together”.** By default, `mlogitplot` connects outcomes where the odds ratio has a p-value greater than 0.10. You can choose other p-values with the `linepvalue(#) ` option, and you can remove all lines by specifying `linepvalue(l)`.

To illustrate how statistical significance is added to the graph, we plot the odds ratios for age and income:

```
mlogitplot age income, symbols(D d i r R) base(3) ///
    ormin(.5) ormax(2) ntics(5) aspect(.3) ///
    offsetlist(2 -2 0 2 -2 2 -2 0 2 -2)
```

<figure style="text-align:center;">
  <img src="https://cdn.jsdelivr.net/gh/forest293/Hugo-image/2024.3.25.9.png" style="width:750px;height:570px;" alt="图片描述">
  <figcaption><strong></strong></figcaption>
</figure>

Based on this graph, we conclude the following:

>Age significantly increases the odds of affiliations that are strong relative to those that are not strong, with no significant odds ratios differentiating outcomes within these two groups.

For income, we conclude the following:

>In terms of the conventional left-right continuum, income increases the odds of affiliations to the right but does not significantly differentiate affiliations that are adjacent, such as strong Democrat compared with Democrat or Republican compared with strong Republican.

Sometimes the lines do not clearly show whether an odds ratio is significant. For example, in the last plot, it is not clear whether there is a line connecting “i" and “d” for income because the symbols are so close to one another. There are several ways to resolve this. 

* First, we can examine the odds ratio in the output from **`listcoef`**, where we see that the effect is not significant. 
* Second, we can reduce the size of the spacing between symbols and the start of the lines by using the **`linegapfactor(#)`** option. If we add **`linegapfactor(0.5)`**, we would see that there is a line connecting “i” and “d”. 
* Finally, we can revise the vertical offsets used for placing letters with the **`offsetlist()`** option. Offsets are determined by specifying one integer in the range from -5 to 5 for each outcome for each variable in the graph. By default, the offsets are **2 -2 0 2 -2 0 2 -2 0 ...** To adjust our prior graph for income, we use **2 -2 1 2 -2** to move “i” up because its offset has been increased from 0 to 1. Remember that these adjustments have no substantive meaning; they simply make the information clearer. Using these offsets, the following command creates a graph that makes it clear that “i” and "D” are linked:

```
mlogitplot age income, symbols(D d i r R) base(3) ///
    ormin(.5) ormax(2) ntics(5) ///
    offsetlist(2 -2 0 2 -2 2 -2 2 2 -2) ysize(2.4) scale(1.1)
```


<figure style="text-align:center;">
  <img src="https://cdn.jsdelivr.net/gh/forest293/Hugo-image/2024.4.1.1.png" style="width:550px;height:370px;" alt="图片描述">
  <figcaption><strong></strong></figcaption>
</figure>

The plot for income illustrates why it is important to examine all contrasts (that is, odds ratios), not just the minimal set. Suppose that we had fit the model with base outcome 3, corresponding to “i” in the graph. At the 0.10 level, none of the odds ratios relative to “i” are significant, as indicated by the lines from “i” to each of the other outcomes. It would be incorrect to assume that income did not significantly affect party affiliation because the contrasts for outcomes “D” versus “r”; “D” versus "R"; “d” versus “r”; and “d” versus “R” are significant. An odds-ratio plot is a quick way to see all the contrasts.

* Adding discrete change

In chapter 6, we emphasized that whereas the factor change in the odds is constant across the levels of all variables, the marginal effect gets larger or smaller at different values of the variables. **For example, if the odds increase by a factor of 10 but the current odds are 1 in 10,000, the change in the probability is small. But if the current odds are 1 in 5, the change in probability is large.** Information on the change in probability can be incorporated into the odds-ratio plot by making the area of a square drawn around a letter proportional to the discrete change in the probability. Sign is indicated by underlining the letter if the marginal effect is negative. **To add this information, we must first run `mchange` to calculate marginal effects for the amount of change we are interested in (for example, 1 unit or a standard deviation). Second, we add the option `mchange` to `mlogitplot`.** To illustrate this, we plot the odds ratios and average discrete changes for `educ` and `black`:

```
mchange black educ, amount(sd)
mlogitplot black educ, mchange ///
    symbols(D d i r R) base(3) ormin(.1) ormax(10) ntics(5) aspect(.5) ///
    offsetlist(2 -2 0 2 -2 2 -2 0 2 -2 2 -2 0 2 -2)
```

<figure style="text-align:center;">
  <img src="https://cdn.jsdelivr.net/gh/forest293/Hugo-image/2024.4.1.2.png" style="width:550px;height:370px;" alt="图片描述">
  <figcaption><strong></strong></figcaption>
</figure>

By far, the largest marginal effect is the increase in the probability of being a strong Democrat ("D”) if you are black. In terms of the odds ratios, race divides affiliations into three groups: 
1) strong Republicans (“R”) and Republicans (“r”); 
2) Democrats (“d”) and Independents (“i”); and 
3) strong Democrats (“D”).

By examining both the odds ratio and the discrete change, it is clear that a positive odds ratio for outcome A compared with outcome D does not indicate the sign of the discrete changes for the two outcomes. For example, with an odds ratio greater than 1, the discrete changes for both outcomes can both be positive, can both be negative, or can differ in sign. An odds ratio indicates the ratio of the change of one category relative to another, not the direction of the change. The graph also illustrates what is found when a variable has no significant effects, as is the case for `educ`. The discrete changes are small and all letters are connected by lines, indicating that none of the odds ratios are significant.

>**The size of symbols.**
>>When the `mchange` option is used, the size of the symbol reflects the size of the effect. Because many letters are more or less square, the size of the area of a square drawn around the symbol is proportional to the absolute magnitude of the marginal effect. This can be misleading in some cases. For example, the letters “r” and “R” both represent the same size effect, but “R” is larger. **As long as you keep this in mind, the size of the letters should give you a rough idea of the magnitude of effects. If you want to be certain, check the output from `mchange`.**

With a little practice, you can quickly see the overall pattern of relationships in your model by using odds-ratio plots. Once the pattern is determined, other methods of interpretation can be effectively used to demonstrate the most important findings.

## 1.12 (Advanced) Additional models for nominal outcomes



