[{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"In this chapter, we include random coefficients or random slopes in addition to random intercepts, thus also allowing the effects of covariates to vary between clusters.","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"1 Introduction In the previous chapter, we considered linear random-intercept models where the overall level of the response was allowed to vary between clusters after controlling for covariates. In this chapter, we include random coefficients or random slopes in addition to random intercepts, thus also allowing the effects of covariates to vary between clusters. Such models involving both random intercepts and random slopes are often called random-coefficient models. In longitudinal settings, where the level-1 units are occasions and the clusters are typically subjects, models with a random-coefficient of time are also referred to as growth-curve models. Such models are the topic of chapter 7. ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:1:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"How effective are different schools? Here we analyze a dataset on inner-London schools that accompanies the MLwiN software (Rasbash et al. 2019) and is part of the data analyzed by Goldstein et al. (1993). At age 16, students took their Graduate Certificate of Secondary Education (GCSE) exams in a number of subjects. A score was derived from the individual exam results. Such scores often form the basis for school comparisons, for instance, to allow parents to choose the best school for their child. However, schools can differ considerably in their intake achievement levels. It may be argued that what should be compared is the “value added”; that is, the difference in mean GCSE score between schools after controlling for the students’ achievement before entering the school. One such measure of prior achievement is the London Reading Test (LRT) taken by these students at age 11. The dataset gcse.dta has the following variables: school: school identifier student: student identifier gcse: Graduate Certificate of Secondary Education (GCSE) score (score, multiplied by 10) lrt: London Reading Test (LRT) score ( score, multiplied by 10) girl: dummy variable for student being a girl (1: girl; 0: boy) schgend: type of school (1: mixed gender; 2: boys only; 3: girls only) One purpose of the analysis is to investigate the relationship between GCSE and LRT and how this relationship varies between schools. The model can then be used to address the question of which schools appear to be most effective, taking prior achievement into account. ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:2:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"3 Separate linear regressions for each school Before developing a model for all 65 schools combined, we consider a separate model for each school. For each school, an obvious model for the relationship between GCSE and LRT is a simple regression model. $$y_{ij} = \\beta_{1j}+\\beta_{2j}x_{ij}+\\epsilon_{ij}$$ where $y_{ij}$ is the GCSE score for the th student in school j, $x{ij}$ is the corresponding LRT score, $\\beta{1 j}$ is the school-specific intercept, $\\beta_{2 j}$ is the school-specific slope, and $\\varepsilon_{i j}$ is a residual error term with school-specific variance $\\theta_j$. For school 1, OLS estimates of the intercept $\\hat{\\beta}{11}$ and the slope $\\hat{\\beta}{21}$ can be obtained using regress , regress gcse lrt if school==1 Source | SS df MS Number of obs = 73\r-------------+---------------------------------- F(1, 71) = 59.44\rModel | 4084.89189 1 4084.89189 Prob \u003e F = 0.0000\rResidual | 4879.35759 71 68.7233463 R-squared = 0.4557\r-------------+---------------------------------- Adj R-squared = 0.4480\rTotal | 8964.24948 72 124.503465 Root MSE = 8.29\r------------------------------------------------------------------------------\rgcse | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\rlrt | 0.71 0.09 7.71 0.000 0.53 0.89\r_cons | 3.83 0.98 3.90 0.000 1.87 5.79\r------------------------------------------------------------------------------\rwhere we have selected school 1 by specifying the condition if school==1. To assess whether this is a reasonable model for school 1, we can obtain the predicted (ordinary least squares) regression line for this school (with $j = 1$ ). $$\\widehat y_{i1}=\\widehat \\beta_{11}+\\widehat \\beta_{21}x_{i1}$$ by using the predict command with the xb option: predict p_gcse, xb We superimpose this line on the scatterplot of the data for the school, as shown in figure 4.1. twoway (scatter gcse lrt) (line p_gcse lrt, sort) if school==1, /// xtitle(LRT) ytitle(GCSE) Figure 4.1: Scatterplot of gcse versus lrt for school 1 with ordinary least-squares regression line\rWe can also produce a trellis graph containing such plots for all 65 schools by using twoway (scatter gcse lrt) (lfit gcse lrt, sort lpatt(solid)), /// by(school, compact legend(off) cols(5)) /// xtitle(LRT) ytitle(GCSE) ysize(3) xsize(2) Figure 4.2: Trellis of scatterplots of gcse versus lrt with fitted regression lines for all 65 schools\rWe will now fit a simple linear regression model for each school, which is easily done using Stata’s prefix command statsby. Then we will examine the variability in the estimated intercepts and slopes. First, calculate the number of students per school by using egen with the count() function to preclude fitting lines to schools with fewer than five students below: egen num = count(gcse), by(school) Then, use statsby to create a new dataset, ols.dta, in the working directory with the variables inter and slope containing OLS estimates of the intercepts (_b[_cons]) and slopes (_b[lrt]) from the command regress gcse lrt if num\u003e4 applied to each school: statsby inter=_b[_cons] slope=_b[lrt], by(school) saving(ols): /// regress gcse lrt if num\u003e4 (running regress on estimation sample)\rCommand: regress gcse lrt if num\u003e4\rinter: _b[_cons]\rslope: _b[lrt]\rBy: school\rStatsby groups:\r.................................................. 50\r..............\rThe new dataset also contains the variable school and is sorted by school, making it easy to merge it into the original dataset (the “master data”) after sorting the latter by school: sort school merge m:1 school using ols drop _merge Result Number of obs\r-----------------------------------------\rNot matched 2\rfrom master 2 (_merge==1)\rfrom using 0 (_merge==2)\rMatched 4,057 (_merge==3)\r-----------------------------------------\rHere we have specified m:1 in the merge command, which stands for “many-to-one merging” (observations for several students per school in the master data, but only on","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:3:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"4 Specification and interpretation of a random-coefficient model ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:4:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"4.1 pecification of a random-coefficient model How can we develop a joint model for the relationships between gcse and lrt in all schools that allows intercepts and slopes to differ between schools? One way would be to use dummy variables for all schools (omitting the overall intercept) and interactions between these dummy variables and lrt (omitting the overall slope of lrt). The school-specific intercepts are then the coefficients of the dummy variables and the school-specific slopes are the interaction coefficients. The only difference between the resulting model and separate regressions is that a common residual error variance $\\theta_j=\\theta$ is assumed. However, this model has 130 regression coefficients! Furthermore, if the schools are viewed as a (random) sample of schools from a population of schools, we are not interested in the individual coefficients characterizing each school’s regression line. Rather, we would like to estimate the mean intercept and slope as well as the (co)variability of the intercepts and slopes in the population of schools. A parsimonious model for the relationships between gcse and lrt can be obtained by specifying a school-specific random intercept $\\zeta_{1j}$ and a school-specific random slope $\\zeta_{2j}$ for lrt $(x_{ij})$: $$\\begin{aligned}y_{ij}\u0026=\\quad\\beta_1+\\beta_2x_{ij}+\\zeta_{1j}+\\zeta_{2j}x_{ij}+\\epsilon_{ij}\\\u0026=\\quad(\\beta_1+\\zeta_{1j})+(\\beta_2+\\zeta_{2j})x_{ij}+\\epsilon_{ij}\\end{aligned}$$ Here $\\zeta_{1j}$ represents the deviation of school $j$ ’s intercept from the mean intercept $\\beta_1$, and $\\zeta_{2j}$ represents the deviation of school $j$ ’s slope from the mean slope $\\beta_2$. Given all covariates $X_j$ in cluster $j$, it is assumed that the random effects $\\zeta_{1j}$ and $\\zeta_{2j}$ have zero expectations: $$E(\\zeta_{1j}|\\mathbf{X}_j)=0$$ $$E(\\zeta_{2j}|\\mathbf{X}_j)=0$$ It is also assumed that the level-1 residual $\\varepsilon_{i j}$ has zero expectation, given the covariates and the random effects: $$E(\\epsilon_{ij}|\\mathbf X_j,\\zeta_{1j},\\zeta_{2j})=0$$ It follows from these mean-independence assumptions that the random terms $\\zeta_{1 j}, \\zeta_{2 j},$ and $\\varepsilon_{i j}$ are all uncorrelated with the covariate $x_{i j}$ and with $\\overline x_{\\cdot j}$ and that $\\varepsilon_{i j}$ is uncorrelated with both $\\zeta_{1 j}$ and $\\zeta_{2 j}$. Both the intercepts $\\zeta_{1 j}$ and slopes $\\zeta_{2 j}$ are assumed to be uncorrelated across schools, and the level-1 residuals $\\varepsilon_{i j}$ are assumed to be uncorrelated across schools and students. An illustration of this random-coefficient model with one covariate $x_{i j}$ for one cluster $j$ is shown in the bottom panel of figure 4.5. A random-intercept model is shown for comparison in the top panel. 问题背景 我们想要研究GCSE（英国普通中等教育证书）成绩和LRT（可能是某种学习资源或时间）之间的关系，并考虑这种关系在不同学校之间可能存在差异。 初步方法 使用哑变量：我们可以为每个学校创建一个哑变量（除了总体截距），并考虑这些哑变量与LRT的交互作用（除了LRT的总体斜率）。这样，每个学校的特定截距就是哑变量的系数，特定斜率就是交互作用的系数。 优点：这种方法可以为每个学校提供一个单独的模型。 缺点：如果学校数量很多（比如130个），那么模型会有130个回归系数，这会导致模型过于复杂。 考虑学校作为样本：如果我们把学校看作是从学校总体中随机抽取的样本，那么我们可能更关心的是学校总体中的平均截距和斜率，以及截距和斜率的变异性。 解决方案：混合效应模型 为了解决上述问题，我们可以使用混合效应模型（也称为多层次模型或随机效应模型）。这种模型允许我们同时估计总体的平均截距和斜率，以及截距和斜率在不同学校之间的变异性。 模型公式 模型可以表示为： $$ y_{ij} = \\beta_1 + \\beta_2x_{ij} + \\zeta_{1j} + \\zeta_{2j}x_{ij} + \\epsilon_{ij} $$ 这里： $ y_{ij} $ 是学生 $ i $ 在学校 $ j $ 的GCSE成绩。 $ x_{ij} $ 是学生 $ i $ 在学校 $ j $ 的LRT。 $ \\beta_1 $ 是所有学校的平均截距。 $ \\beta_2 $ 是所有学校的平均斜率。 $ \\zeta_{1j} $ 是学校 $ j $ 的截距的随机效应，表示学校 $ j $ 的截距与总体平均截距 $ \\beta_1 $ 的偏差。 $ \\zeta_{2j} $ 是学校 $ j $ 的斜率的随机效应，表示学校 $ j $ 的斜率与总体平均斜率 $ \\beta_2 $ 的偏差。 $ \\epsilon_{ij} $ 是随机误差项，表示模型未能解释的变异。 模型的假设 随机效应的期望为零： $$ E(\\zeta_{1j}|\\mathbf X_j) = 0 $$ $$ E(\\zeta_{2j}|\\mathbf X_j) = 0 $$ 这意味着随机效应 $ \\zeta_{1j} $ 和 $ \\zeta_{2j} $ 在给定学校 $ j $ 的所有协变量 $ \\mathbf{X}_j $ 的条件下，期望值为零。 残差的期望为零： $$ E(\\epsilon_{ij}|\\mathbf X_j, \\zeta_{1j}, \\zeta_{2j}) = 0 $$ 这意味着在给定学校 $ j $ 的所有协变量 $ \\mathbf X_j $，以及随机效应 $ \\zeta_{1j} $ 和 $ \\zeta_{2j} $ 的条件下，残差 $ \\epsilon_{ij} $ 的期望值为零。 无相关性： 随机效应 $ \\zeta_{1j} $，$ \\zeta_{2j}","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:4:1","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"4.2 Interpretation of the random-effects variances and covariances Interpreting the covariance matrix $\\Psi$ of the random effects (given the covariates $X_j$) is not completely straightforward. First, the random-slope variance $\\psi_{22}$ and the covariance between random slope and intercept $\\psi_{21}$ depend not just on the scale of the response variable but also on the scale of the covariate, here lrt. Let the units of the response and covariate be denoted as $u_y$ and $u_x$, respectively. For instance, in an application in chapter 7 that considers children’s increase in weight over time, $u_y$ is kilograms and $u_x$ is years. The units of $\\psi_{11}$ are $u_y^2$, the units of $\\psi_{21}$ are $u_y^2/u_x$, and the units of $\\psi_{22}$ are $u_y^2/u_x^2$. It therefore does not make sense to compare the magnitude of random-intercept and random-slope variances. Another issue is that the total residual variance is no longer constant as in random-intercept models. The total residual is now $$\\xi_{ij} \\equiv \\zeta_{1j}+\\zeta_{2j}x_{ij}+\\epsilon_{ij}$$ and the conditional variance of the responses given the covariate, or the conditional variance of the total residual, is $$\\mathrm Var(y_{ij}|\\mathbf X_j) = \\mathrm Var(\\xi_{ij}|\\mathbf X_j) = \\psi_{11}+2\\psi_{21}x_{ij}+\\psi_{22}x_{ij}^2+\\theta\\quad(4.2)$$ This variance is a (quadratic) function of the covariate $x_{ij}$, and the total residual is therefore heteroskedastic. The conditional covariance for two students $i$ and $i’$ with covariate values $x_{ij}$ and $x_{i’j}$ in the same school $j$ is $$\\mathrm Cov(y_{ij},y_{i^{\\prime}j}|\\mathbf X_j)\\quad=\\quad\\mathrm Cov(\\xi_{ij},\\xi_{i^{\\prime}j}|\\mathbf X_j)$$ $$=\\quad\\psi_{11}+\\psi_{21}x_{ij}+\\psi_{21}x_{i’j}+\\psi_{22}x_{ij}x_{i’j}$$ and the conditional intraclass correlation becomes $$\\mathrm Cor(y_{ij},y_{i^{\\prime}j}|\\mathbf X_j) = \\frac{\\mathrm Cov(\\xi_{ij},\\xi_{i^{\\prime}j}|\\mathbf X_j)}{\\sqrt{\\mathrm Var(\\xi_{ij}|\\mathbf X_j)\\mathrm Var(\\xi_{i^{\\prime}j}|\\mathbf X_j)}}$$ where we can plug in the covariance from (4.3) and the variances from (4.2). When $x_{ij}=x_{i’j}=0$, the expression for the intraclass correlation is the same as for the random-intercept model and represents the correlation of the total residuals (from the overall mean regression line) for two students in the same school who both have lrt scores equal to 0 (the mean in this case). However, for pairs of students $i$ and $i’$ in the same school $j$ with other values of lrt, the intraclass correlation is a complicated function of lrt ($x_{ij}$ and $x_{i’j}$). Due to the heteroskedastic total residual variance, it is not straightforward to define coefficients of determination—such as $R^2$, $R^2_2$, and $R^2_Y$, discussed in section 3.5—for random-coefficient models. Snijders and Bosker (2012, 114) suggest removing the random coefficient(s) for the purpose of calculating the coefficient of determination because this will usually yield values that are close to correct (see their section 7.2.2 for how to obtain the correct version). Finally, interpreting the parameters $\\psi_{11}$ and $\\psi_{21}$ can be difficult because their values depend on the translation of the covariate or, in other Figure 4.7: Cluster-specific regression lines for random-coefficient model, illustrating lack of invariance under translation of covariate (Source: Skrondal and Rabe-Hesketh 2004)\rTo make $\\psi_{11}$ and $\\psi_{21}$ interpretable, it makes sense to translate $x_{ij}$ so that the value $x_{ij} = 0$ is a useful reference point in some way. Typical choices are either mean centering (as for lrt) or, if $x_{ij}$ is time, as in growth-curve models, defining 0 to be the initial time in some sense. Because the magnitude and interpretation of $\\psi_{21}$ depend on the location (or translation) of $x_{ij}$, which is often arbitrary, it generally does not make sense to set $\\psi_{21}$ to 0 by specifying uncorrelated intercepts and slopes. A useful way of interpreting the magnitudes of the e","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:4:2","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"5 Estimation using mixed The mixed command can be used to fit linear random-coefficient models by ML or REML. (xtreg can only fit two-level random-intercept models.) ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:5:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"5.1 Random-intercept model We first consider a random-intercept model discussed in the previous chapter: $$\\begin{matrix}y_{ij}\u0026=\u0026(\\beta_1+\\zeta_{1j})+\\beta_2x_{ij}+\\epsilon_{ij}\\end{matrix}$$ This model is a special case of the random-coefficient model in $(4.1)$ with $\\zeta_{2j} =0 $ or, equivalently, with zero random-slope variance and zero random-intercept and random-slope covariance, $\\psi_{22}=\\psi_{21}=0$. ML estimates for the random-intercept model can be obtained using mixed with the mle option (the default), and we also use the vce(robust) option for robust standard errors. mixed gcse lrt || school:, mle stddeviations vce(robust) Performing gradient-based optimization: Iteration 0: Log pseudolikelihood = -14024.799 Iteration 1: Log pseudolikelihood = -14024.799 Computing standard errors ...\rMixed-effects regression Number of obs = 4,059\rGroup variable: school Number of groups = 65\rObs per group:\rmin = 2\ravg = 62.4\rmax = 198\rWald chi2(1) = 852.73\rLog pseudolikelihood = -14024.799 Prob \u003e chi2 = 0.0000\r(Std. err. adjusted for 65 clusters in school)\r------------------------------------------------------------------------------\r| Robust\rgcse | Coefficient std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rlrt | 0.56 0.02 29.20 0.000 0.53 0.60\r_cons | 0.02 0.41 0.06 0.953 -0.77 0.82\r------------------------------------------------------------------------------\r------------------------------------------------------------------------------\r| Robust Random-effects parameters | Estimate std. err. [95% conf. interval]\r-----------------------------+------------------------------------------------\rschool: Identity |\rsd(_cons) | 3.04 0.32 2.48 3.72\r-----------------------------+------------------------------------------------\rsd(Residual) | 7.52 0.13 7.27 7.78\r------------------------------------------------------------------------------\rTo allow later comparison with random-coefficient models via likelihood-ratio tests, we store these estimates by using estimates store ri The random-intercept model assumes that the school-specific regression lines are parallel. The common coefficient or slope $\\beta_2$ of lrt, shared by all schools, is estimated as 0.56 and the mean intercept as 0.02. Schools vary in their intercepts with an estimated standard deviation of 3.04. Within the schools, the estimated residual standard deviation around the school-specific regression lines is 7.52. The within-school correlation, after controlling for lrt, is therefore estimated as We could obtain this within-school correlation by typing estat icc. The ML estimates for the random-intercept model are also given under “Random intercept” in table 4.1. Table 4.1: Maximum likelihood estimates for inner-London-schools data with robust standard errors\r","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:5:1","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"5.2 Random-coefficient model We now relax the assumption that the school-specific regression lines are parallel by introducing random school-specific slopes $β_{j} (j = 1, …, J)$ and $u_{ij}$. $$y_{ij} = (\\beta_1+\\zeta_{1j})+(\\beta_2+\\zeta_{2j})x_{ij}+\\epsilon_{ij}$$ $ y_{ij} $ 是第 $ j $ 所学校的第 $ i $ 个学生的 GCSE（普通中等教育证书）成绩。 $ \\beta_1 $ 和 $ \\beta_2 $ 是固定效应（fixed effects）的截距和斜率。 $ \\zeta_{1j} $ 和 $ \\zeta_{2j} $ 是随机效应的截距和斜率，它们是学校 $ j $ 特有的。 $ x_{ij} $ 是解释变量，比如学生的某种测试成绩。 $ \\epsilon_{ij} $ 是误差项。 To introduce a random slope for lrt using mixed, we simply add that variable name in the specification of the random part, replacing school: with school: lrt. We must also specify the covariance(unstructured) option because mixed will otherwise set the covariance $\\psi_{21}$(and the corresponding correlation) to 0 by default. ML estimates for the random-coefficient model are then obtained using 在Stata中，我们使用 mixed 命令来估计这个模型。如果我们想要为 lrt（可能是某种测试成绩）引入随机斜率，我们需要在随机部分的规范中添加变量名，将 school: 替换为 school: lrt。同时，我们需要指定 covariance(unstructured) 选项，因为默认情况下，mixed 命令会将协方差 $ \\psi_{21} $（以及相应的相关性）设置为0。 mixed gcse lrt || school: lrt, covariance(unstructured) mle stddeviations /// vce(robust) 这个命令做了以下几件事： gcse lrt：指定了固定效应模型，其中 gcse 是因变量，lrt 是解释变量。 || school: lrt：指定了随机效应模型，其中 lrt 的斜率在不同学校之间是随机的。 covariance(unstructured)：允许随机效应的协方差矩阵是无结构的，即每个随机效应都可以有自己的方差和它们之间的协方差。 mle：使用最大似然估计方法。 stddeviations：输出随机效应的标准差而不是方差。 vce(robust)：使用稳健标准误。 Performing EM optimization ...\rPerforming gradient-based optimization: Iteration 0: Log pseudolikelihood = -14004.613 Iteration 1: Log pseudolikelihood = -14004.613 Computing standard errors ...\rMixed-effects regression Number of obs = 4,059\rGroup variable: school Number of groups = 65\rObs per group:\rmin = 2\ravg = 62.4\rmax = 198\rWald chi2(1) = 767.80\rLog pseudolikelihood = -14004.613 Prob \u003e chi2 = 0.0000\r(Std. err. adjusted for 65 clusters in school)\r---------------------------------------------------------------------------------\r| Robust\rgcse | Coefficient std. err. z P\u003e|z| [95% conf. interval]\r----------------+----------------------------------------------------------------\rlrt | 0.56 0.02 27.71 0.000 0.52 0.60\r_cons | -0.12 0.40 -0.29 0.774 -0.90 0.67\r---------------------------------------------------------------------------------\r------------------------------------------------------------------------------\r| Robust Random-effects parameters | Estimate std. err. [95% conf. interval]\r-----------------------------+------------------------------------------------\rschool: Unstructured |\rsd(lrt) | 0.12 0.02 0.08 0.18\rsd(_cons) | 3.01 0.31 2.45 3.69\rcorr(lrt,_cons) | 0.50 0.18 0.09 0.76\r-----------------------------+------------------------------------------------\rsd(Residual) | 7.44 0.13 7.20 7.69\r------------------------------------------------------------------------------\r输出结果提供了固定效应和随机效应的估计值。 固定效应： lrt 的系数是 0.56，意味着在控制其他因素的情况下，lrt 每增加一个单位，gcse 成绩预期增加 0.56 单位。 _cons 是截距项，但在这个模型中它不显著。 随机效应： sd(lrt) 是 lrt 斜率的标准差，即不同学校 lrt 斜率的变异程度。 sd(_cons) 是截距项的标准差，即不同学校截距的变异程度。 corr(lrt,_cons) 是 lrt 斜率和截距之间的相关性。 Because the stddeviations option was used, the output shows the standard deviations, sd(lrt), of the slope and sd(_cons) of the intercept instead of variances. It also shows the correlation between intercepts and slopes, corr(lrt,_cons), instead of the covariance. We can obtain the estimated covariance matrix either by replaying the estimation results without the stddeviations option (or with the variance option). 使用 stddeviations 选项 在混合效应模型中，我们通常对随机效应的方差和协方差感兴趣，因为这些参数描述了随机效应的变异程度和它们之间的关联性。在Stata中，使用 stddeviations 选项可以让我们直接看到随机效应的标准差，而不是方差。 标准差：衡量随机效应的变异程度。例如，sd(lrt) 表示不同学校 lrt 斜率的标准差，即学校之间 lrt 斜率的变异程度。 sd(_cons)：表示不同学校截距的标准差，即学校之间截距的变异程度。 相关性与协方差 相关性：衡量两个随机效应之间的线性关联程度。例如，corr(lrt,_cons) 表示 lrt 斜率和截距之间的相关性。如果这个值接近1或-1，表示它们之间有很强的正相关或负相关关系；如果接近0，则表示它们之间几乎没有线性关系。 协方差：衡量两个随机效应共同变化的程度。协方差和相关性不同，协方差受到变量单位和量级的影响，而相关性是一个无量纲的度量。 获取协方差矩阵 虽然 stddeviations 选项提供了标准差和相关性，但有时我们可能需要原始的协方差矩阵。 我们可以通过以下两种方式之一来获取： 不使用 stddeviations 选项：重新运行模型命令，不使用 stddeviations 选项，","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:5:2","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"6 Testing the slope variance Before interpreting the parameter estimates, we may want to test whether the random slope is needed in addition to the random intercept. Specifically, we test the null hypothesis. $$H_0: \\psi_{22}=0\\quad\\mathrm{against}\\quad H_a:\\psi_{22}\u003e0$$ Note that $H_0$ is equivalent to the hypothesis that the random slopes $\\xi_{2j}$ are all 0. The null hypothesis also implies that $\\psi_{21}=0$, because a variable that does not vary also does not covary with other variables. Setting $\\psi_{22}=0$ and $\\psi_{21}=0$ gives the random-intercept model. A naïve likelihood-ratio test can be performed using the lrtest command. lrtest ri rc, force Likelihood-ratio test\rAssumption: ri nested within rc\rLR chi2(2) = 40.37\rProb \u003e chi2 = 0.0000\rThe force option was used here because without it, Stata will not perform likelihood-ratio tests when robust standard errors have been specified. This is because inferences based on robust standard errors do not require the likelihood to be correct (that is, to correspond to the data-generating mechanism), which is why Stata calls it a pseudolikelihood in the output. Because likelihood-ratio tests require correct likelihoods, Stata will not perform such tests unless forced to do so. Here we accept that, unlike inferences for the regression coefficients based on robust standard errors, likelihood-ratio tests for variance and covariance parameters will not be robust to misspecification of the residual covariance structure. Remember that point estimators of variance and covariance parameters are inconsistent if the residual covariance structure is misspecified (which is also unlike regression coefficients). 在Stata中使用force选项的原因与似然比检验（Likelihood-Ratio Test, LRT）的执行条件有关。似然比检验是一种统计方法，用于比较两个嵌套模型的拟合度，即一个模型是另一个模型的特殊情况。在执行似然比检验时，Stata要求两个模型都必须使用最大似然法（Maximum Likelihood, ML）进行估计，而且通常要求模型的残差协方差结构被正确指定。 当我们在模型中指定vce(robust)选项时，我们告诉Stata我们担心模型的规范可能不正确，因此我们使用稳健标准误（Robust Standard Errors）来得到系数的估计。这些稳健标准误不需要似然函数是正确的，也就是说，它们不需要对应于数据生成机制。因此，当使用稳健标准误时，Stata不会执行似然比检验，因为似然比检验需要正确的似然函数。 force选项在这里的作用是强制Stata执行似然比检验，即使在已经指定了稳健标准误的情况下。使用force选项时，Stata会执行检验，但不会保证检验结果的有效性或可解释性，因为稳健标准误的使用意味着我们可能已经偏离了似然比检验的有效假设。 此外，似然比检验的零假设是模型参数的某些限制是成立的，例如在这种情况下，零假设是随机斜率的方差 $ \\psi_{22}$ 等于0。如果这个零假设是真的，那么模型的拟合度不应该因为这些限制而受到太大影响。似然比检验通过比较有限制和无限制模型的对数似然值来工作，如果两个模型的对数似然值差异显著，那么我们可以拒绝零假设，认为更复杂的模型（无限制模型）提供了更好的拟合度。 在实际操作中，如果模型的残差协方差结构被错误指定，那么基于稳健标准误的回归系数推断将不受影响，但似然比检验的结果可能会受到影响。因此，在执行似然比检验时，我们需要确保模型的规范是正确的，或者使用force选项来强制执行检验，但要意识到这可能会影响检验结果的解释。 总结来说，force选项允许我们在不满足似然比检验所有假设的情况下执行检验，但这样做时要谨慎，因为结果可能不可靠。在实际应用中，如果模型的残差协方差结构被正确指定，那么通常不需要使用force选项。如果使用稳健标准误，那么似然比检验的结果可能不准确，这时可以使用force选项来执行检验，但要清楚这只是一个近似的检验。 This likelihood-ratio test is naive because the variance $\\psi_{22}$ must be nonnegative so that the null hypothesis is on the boundary of the parameter space. As discussed in section 2.6.2 for random-intercept models, the asymptotic null distribution of the likelihood-ratio statistic $L$ is therefore no longer a simple $\\chi^2$ distribution as assumed by the lrtest command. 这段话讨论的是一种统计检验方法，叫做似然比检验（likelihood-ratio test）。这种方法用于比较两个统计模型的拟合优度。 方差 $\\psi_{22}$ 必须是非负的： 方差是衡量数据分散程度的一个指标。在统计学中，方差不能为负数，因为它代表的是平方后的数值。 这里提到的 $\\psi_{22}$ 是一个特定的方差参数，它必须是非负的。 零假设在参数空间的边界上： 零假设（null hypothesis）是指在统计检验中假设没有显著差异或效应。 参数空间是指所有可能的参数值的集合。当零假设在参数空间的边界上时，意味着某些参数（如方差）可能接近于零。 渐近零分布不再是简单的 $\\chi^2$ 分布： 渐近分布是指当样本量趋于无穷大时，统计量的分布。 $\\chi^2$ 分布是一种常见的统计分布，用于许多检验中。 由于零假设在参数空间的边界上，似然比统计量 $L$ 的分布不再是简单的 $\\chi^2$ 分布，而是更复杂的分布。 lrtest 命令的假设： lrtest 是一个用于执行似然比检验的命令。 该命令假设似然比统计量 $L$ 服从 $\\chi^2$ 分布，但在这种情况下，这个假设不成立。 总结一下，这段话的核心意思是：由于方差参数的限制，零假设在参数空间的边界上，导致似然比统计量的分布变得复杂，不再符合常规的 $\\chi^2$ 分布，因此使用 lrtest 命令时需要注意这一点。 In mixed, the default estimation metric (transformation used during estimation) for the covariance matrix of the random effects is the square root or Cholesky decomposition (which is requested by the matsqrt option). This parameterization forces the covariance matrix to be positive semidefinite (estimates on the boundary of parameter space, for example, 0 variance or perf","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:6:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"7 Interpretation of estimates The population-mean intercept and slope are estimated as $-0.12$ and $0.56$, respectively. These estimates are similar to those for the random intercept model (see table 4.1) and are also close to the means of the school-specific intercept and slope estimates given in section 4.3. The estimated random-intercept standard deviation and level-1 residual standard deviation are somewhat lower than for the random intercept model. The latter is because of a better fit of the school-specific regression lines for the random-coefficient model, which relaxes the restriction of parallel regression lines. The estimated covariance matrix of the intercepts and slopes is similar to the sample covariance matrix of the ordinary least-squares estimates reported in section 4.3. As discussed in section 4.4.2, the easiest way to interpret the estimated standard deviations of the random intercept and random slope (conditional on the covariates) is to form intervals within which 95% of the schools’ random intercepts and slopes are expected to lie assuming normality. Remember that these intervals represent ranges within which 95% of the realizations of a random variable are expected to lie, a concept different from confidence intervals, which are ranges within which an unknown parameter is believed to lie. For the intercepts, we obtain $-0.115 \\pm 1.96\\times3.007$, so 95% of schools have their intercept in the range -6.0 to 5.8. In other words, the school mean GCSE scores for children with average LRT scores ($\\text{lrt}=0$) vary between -6.0 and 5.8. For the slopes, we obtain $0.557 \\pm 1.96\\times0.121$, giving an interval from 0.32 to 0.80. Thus, 95% of schools have slopes between 0.32 and 0.80. This exercise of forming intervals is particularly important for slopes because it is useful to know whether the slopes have different signs for different schools (which would be odd in the current example). The range from 0.32 to 0.80 is fairly wide and the regression lines for schools may cross: one school could add more value (produce higher mean GCSE scores for given LRT scores) than another school for students with low LRT scores and add less value than the other school for students with high LRT scores. The estimated correlation $\\widehat{\\rho}_{21}=0.50$ between random intercepts and slopes (given the covariates) means that schools with larger mean GCSE scores for students with average LRT scores than other schools also tend to have larger slopes than those other schools. This correlation, combined with the random-intercept and slope variances and the range of LRT scores, determines how much the lines cross, something that is best explored by plotting the predicted regression lines for the schools, as demonstrated in section 4.8.3. The variance of the total residual $\\xi_{ij}$ (equal to the conditional variance of the responses $y_{ij}$ given the covariates $X_{ij}$ ) was given in (4.2). We can estimate the corresponding standard deviation by plugging in the ML estimates: $$\\sqrt{\\widehat{\\mathrm Var}(\\xi_{ij}|\\mathbf X_{j})} = \\sqrt{\\widehat\\psi_{11}+2\\widehat\\psi_{21}x_{ij}+\\widehat\\psi_{22}x_{ij}^{2}+\\widehat\\theta}$$ $$\\begin{array}{rcl}=\u0026\\sqrt{9.0447+2\\times0.1804\\times x_{ij}+0.0145\\times x_{ij}^2+55.3653}\\end{array}$$ A graph of the estimated standard deviation of the total residual against the covariate lrt ($X_{ij}$) can be obtained using the following twoway function command, which is graphed in figure 4.8: twoway function sqrt(9.0447+2*0.1804*x+0.0145*x^2+55.3653), range(-30 30) /// xtitle(LRT) ytitle(Estimated standard deviation of total residual) Figure 4.8: Heteroskedasticity of total residual $\\xi_{ij}$ as function of lrt\rThe estimated standard deviation of the total residual varies between just under 8 and just under 9.5 for the range of lrt in the data. ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:7:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"1. 固定效应的估计 首先，我们有一个模型，它估计了人口平均截距（intercept）和斜率（slope）分别为 $-0.12$ 和 $0.56$。这些估计值与随机截距模型（random intercept model）的估计值相似，并且也接近于第4.3节给出的学校特定的截距和斜率估计值的平均值。 ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:7:1","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"2. 随机效应的估计 接下来，我们估计了随机截距的标准差和第一层残差（level-1 residual）的标准差。这些估计值比随机截距模型的估计值要低一些。这是因为随机系数模型（random-coefficient model）的拟合效果更好，它放宽了平行回归线的限制。 ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:7:2","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"3. 随机效应的协方差矩阵 我们还估计了截距和斜率的协方差矩阵，这个矩阵与第4.3节报告的普通最小二乘估计的样本协方差矩阵相似。 ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:7:3","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"4. 随机效应的解释 解释随机截距和斜率的标准差的一种方法是形成区间，在这个区间内，假设正态性，95%的学校随机截距和斜率预期会落在这个区间内。这些区间代表了随机变量的实现预期落在的范围内，这与置信区间的概念不同，置信区间是未知参数预期落在的区间。 例子： 对于截距，我们得到 $-0.115 \\pm 1.96 \\times 3.007$，所以95%的学校截距在 $-6.0$ 到 $5.8$ 的范围内。 对于斜率，我们得到 $0.557 \\pm 1.96 \\times 0.121$，给出的区间从 $0.32$ 到 $0.80$。 ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:7:4","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"5. 斜率的解释 形成这些区间对于斜率特别重要，因为知道不同学校的斜率是否有不同的符号（这在当前例子中会很奇怪）是有用的。从 $0.32$ 到 $0.80$ 的范围相当宽，学校的回归线可能会交叉：一所学校可能在低LRT分数的学生中比其他学校增加更多的价值（产生更高的平均GCSE分数），而在高LRT分数的学生中增加的价值比其他学校少。 ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:7:5","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"6. 随机效应之间的相关性 估计的随机截距和斜率之间的相关性（给定协变量）为 $0.50$，这意味着对于平均LRT分数的学生，平均GCSE分数比其他学校大的学校也倾向于比其他学校有更大的斜率。这种相关性，结合随机截距和斜率的方差以及LRT分数的范围，决定了线条交叉的程度，这最好通过绘制学校的预测回归线来探索，如第4.8.3节所示。 ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:7:6","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"7. 总残差的方差估计 总残差 $\\xi_{ij}$（等于给定协变量 $X_{ij}$ 的响应 $y_{ij}$ 的条件方差）的方差在公式 (4.2) 中给出。我们可以通过插入最大似然估计（ML estimates）来估计相应的标准差： $$ \\sqrt{\\widehat{\\mathrm Var}(\\xi_{ij}|\\mathbf X_{j})} = \\sqrt{\\widehat\\psi_{11}+2\\widehat\\psi_{21}x_{ij}+\\widehat\\psi_{22}x_{ij}^{2}+\\widehat\\theta} $$ $$ =\\sqrt{9.0447+2\\times0.1804\\times x_{ij}+0.0145\\times x_{ij}^2+55.3653} $$ ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:7:7","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"8. 残差标准差的图形表示 最后，我们可以使用 twoway function 命令绘制残差标准差与协变量 LRT 的图形，如图4.8所示。 现在，让我们来详细解释一下最后一个公式： $$ \\sqrt{\\widehat{\\mathrm Var}(\\xi_{ij}|\\mathbf X_{j})} = \\sqrt{9.0447+2\\times0.1804\\times x_{ij}+0.0145\\times x_{ij}^2+55.3653} $$ 这个公式是用来估计给定协变量 $X_{ij}$ 的条件下，响应 $y_{ij}$ 的条件方差的平方根。这里的 $\\widehat{\\mathrm Var}(\\xi_{ij}|\\mathbf X_{j})$ 表示的是条件方差，而 $\\sqrt{\\cdot}$ 表示的是取平方根。 $\\widehat\\psi_{11}$、$\\widehat\\psi_{21}$、$\\widehat\\psi_{22}$ 和 $\\widehat\\theta$ 是模型参数的估计值。 $x_{ij}$ 是协变量的值。 公式中的每个部分都是一个平方项或线性项，它们相加得到总的方差估计。然后我们取平方根得到标准差。 现在，让我们计算一下这个公式： 假设 $x_{ij} = 0$（这是一个特定的协变量值），我们可以将这个值代入公式： $$ \\sqrt{9.0447+2\\times0.1804\\times 0+0.0145\\times 0^2+55.3653} = \\sqrt{9.0447 + 0 + 0 + 55.3653} = \\sqrt{64.41} $$ 计算平方根得到： $$ \\sqrt{64.41} \\approx 8.02 $$ 这意味着当 $x_{ij} = 0$ 时，估计的标准差大约是 8.02。 当然可以，让我们更详细地分解一下最后提到的公式： $$ \\sqrt{\\widehat{\\mathrm Var}(\\xi_{ij}|\\mathbf X_{j})} = \\sqrt{9.0447+2\\times0.1804\\times x_{ij}+0.0145\\times x_{ij}^2+55.3653} $$ 这个公式是用来估计给定协变量 $X_{ij}$ 的条件下，响应 $y_{ij}$ 的条件方差的平方根。这里的 $\\widehat{\\mathrm Var}(\\xi_{ij}|\\mathbf X_{j})$ 表示的是条件方差，而 $\\sqrt{\\cdot}$ 表示的是取平方根。 公式分解 $9.0447$: 这是方差估计中的常数项，表示当 $x_{ij} = 0$ 时的方差基础值。 $2 \\times 0.1804 \\times x_{ij}$: 这是方差估计中的线性项，表示 $x_{ij}$ 的值每增加一个单位，方差会增加 $0.1804$ 的两倍。 $0.0145 \\times x_{ij}^2$: 这是方差估计中的二次项，表示 $x_{ij}$ 的值的平方每增加一个单位，方差会增加 $0.0145$。 $55.3653$: 这是方差估计中的另一个常数项，通常与模型的其他部分有关，可能是与模型的误差项有关。 计算步骤 假设 $x_{ij} = 10$（这是一个示例协变量值），我们可以将这个值代入公式： $$ \\sqrt{9.0447 + 2 \\times 0.1804 \\times 10 + 0.0145 \\times 10^2 + 55.3653} $$ 计算线性项: $$ 2 \\times 0.1804 \\times 10 = 3.608 $$ 计算二次项: $$ 0.0145 \\times 10^2 = 1.45 $$ 将所有项相加: $$ 9.0447 + 3.608 + 1.45 + 55.3653 = 69.468 $$ 取平方根: $$ \\sqrt{69.468} \\approx 8.33 $$ 这意味着当 $x_{ij} = 10$ 时，估计的标准差大约是 8.33。 图形表示 公式中的 $x_{ij}$ 可以取不同的值，我们可以通过绘制图形来观察标准差如何随 $x_{ij}$ 的变化而变化。在给定的代码中： twoway function sqrt(9.0447+2*0.1804*x+0.0145*x^2+55.3653), range(-30 30) /// xtitle(LRT) ytitle(Estimated standard deviation of total residual) 这行代码是在统计软件中使用的，用于绘制 $x_{ij}$ 从 -30 到 30 范围内的标准差估计值的图形。图形可以帮助我们直观地看到标准差如何随 $x_{ij}$ 的变化而变化。 ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:7:8","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"8 Assigning values to the random intercepts and slopes Having obtained estimated model parameters $\\hat\\beta_1$, $\\hat\\beta_2$, $\\hat\\psi_{11}$, $\\hat\\psi_{22}$, $\\hat\\psi_{21}$, and $\\hat\\theta$, we now assign values to the random intercepts and slopes (see also section 2.11). This is useful for model visualization, residual diagnostics, and inference for individual clusters, as will be demonstrated. Until section 4.8.5, the estimated parameters will be treated as known. In section 4.8.5, we will use REML estimation to obtain standard errors for empirical Bayes predictions that take uncertainty in estimating $\\hat\\beta_1$ and $\\hat\\beta_2$ into account. ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:8:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"8.1 Maximum “likelihood” estimation Maximum “likelihood” estimates of the random intercepts and slopes can be obtained by first predicting the total residuals $\\xi_{ij} = y_{ij} - (\\hat\\beta_1 + \\hat\\beta_2 x_{ij})$ and then fitting individual regressions of $\\xi_{ij}$ on $x_{ij}$ for each school by OLS. As explained in section 2.11.1, we put “likelihood” in quotes in the section heading because it differs from the marginal likelihood that is used to estimate the model parameters. We can fit the individual regression models by using the statsby prefix command as shown in section 4.3. We first retrieve the mixed estimates stored under rc, estimates restore rc and obtain the predicted total residuals, predict fixed, xb generate totres = gcse - fixed We can then use statsby to produce the variables mli and mls, which contain the ML estimates $\\hat{\\zeta}{1j}$ and $\\hat{\\zeta}{2j}$ of the random intercepts and slopes, respectively: statsby mli=_b[_cons] mls=_b[lrt], by(school) saving(ols, replace): /// regress totres lrt (running regress on estimation sample)\rCommand: regress totres lrt\rmli: _b[_cons]\rmls: _b[lrt]\rBy: school\rStatsby groups:\r.................................................. 50\r...............\rsort school merge m:1 school using ols drop _merge Result Number of obs\r-----------------------------------------\rNot matched 0\rMatched 4,059 (_merge==3)\r-----------------------------------------\rMaximum likelihood estimates will not be available for schools with only one observation or for schools within which $x_{ij}$ does not vary. There are no such schools in the dataset, but school 48 has only two observations, and the ML estimates of the intercept and slope look odd: list lrt gcse mli mls if school==48, clean noobs lrt gcse mli mls -3.7276 -6.9951 -32.607 -7.458484 -4.5541 -1.2908 -32.607 -7.458484 Because there are only two students, the fitted line connects the points perfectly. The school’s intercept and slope are determined by $ϵ_{1j}$ and $ϵ_{2j}$ roughly as much as they are by the true intercept and slope. The intercept and slope estimates are therefore imprecise and can be extreme; the so-called “bouncing beta” phenomenon often encountered when using ML estimation of random effects for clusters that provide little information. In general, we therefore do not recommend using this method and suggest using empirical Bayes prediction instead. ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:8:1","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"8.2 Empirical Bayes prediction As discussed for random-intercept models in section 2.11.2, empirical Bayes (EB) predictions have a smaller prediction error variance (for given model parameters) than ML estimates because of shrinkage toward the mean. Furthermore, EB predictions are available for schools with only one observation or only one unique value of $x_{ij}$, for which ML estimates cannot be obtained. Empirical Bayes predictions $\\tilde{\\zeta}{1j}$ and $\\tilde{\\zeta}{2j}$ of the random intercepts $\\zeta_{1j}$ and slopes $\\zeta_{2j}$, respectively, can be obtained using the predict command with the reffects option after estimation with mixed: estimates restore rc predict ebs ebi, reffects Here we specified the variable names ebs and ebi for the EB predictions $\\tilde{\\zeta}{2j}$ and $\\tilde{\\zeta}{1j}$ of the random slopes and intercepts. The intercept variable comes last because mixed treats the intercept as the last random effect, as reflected by the output. This order is consistent with Stata’s convention of treating the fixed intercept as the last regression parameter in estimation commands. To compare the EB predictions with the ML estimates, we list one observation per school for schools 1–9 and school 48: list school mli ebi mls ebs if pickone==1 \u0026 (school\u003c10 | school==48), noobs Most of the time, the EB predictions are closer to 0 than the ML estimates because of shrinkage, as discussed for random-intercept models in section 2.11.2. However, for models with several random effects, the relationship between EB predictions and ML estimates is somewhat more complex than for random-intercept models. The benefit of shrinkage is apparent for school 48, where the EB predictions appear more reasonable than the ML estimates. We can see shrinkage more clearly by plotting the EB predictions against the ML estimates and superimposing a $y=x$. For the random intercept, the command is twoway (scatter ebi mli if pickone==1 \u0026 school!=48, mlabel(school)) /// (function y=x, range(-10 10)), legend(off) xtitle(ML estimates of intercepts) /// ytitle(EB predictions of intercepts) legend(off) xline(0) and for the random slope, it is twoway (scatter ebs mls if pickone==1 \u0026 school!=48, mlabel(school)) /// (function y=x, range(-0.6 0.6)), xtitle(ML estimates of slopes) /// ytitle(EB predictions of slopes) legend(off) xline(0) Figure 4.9: Scatterplots of empirical Bayes (EB) predictions versus maximum likelihood (ML) estimates of school-specific intercepts (top) and slopes (bottom); equality of EB and ML shown as dashed reference lines and ML estimates of 0 shown as solid reference lines.\rFor ML estimates above 0, the EB prediction tends to be smaller than the ML estimate; the reverse is true for ML estimates below 0. There is more shrinkage for slopes than for intercepts. ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:8:2","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"8.3 Model visualization To better understand the estimates obtained for random-intercept models and random-coefficient models—and in particular, the variability implied by the random part—it is useful to produce graphs of predicted model-implied regression lines for the individual schools. This can be achieved using the predict command with the fitted option to obtain school-specific fitted regression lines, with ML estimates substituted for the regression parameters ($\\beta_1$ and $\\beta_2$) and EB predictions substituted for the random effects ($\\zeta_{1j}$ for the random-intercept model, and $\\zeta_{1j}$ and $\\zeta_{2j}$ for the random-coefficient model). For instance, for the random-coefficient model, the predicted regression line for school $j$ is $$\\widehat{y}{ij} = \\widehat{\\beta}1+\\widehat{\\beta}2x{ij}+\\widetilde{\\zeta}{1j}+\\widetilde{\\zeta}{2j}x_{ij}$$ These predictions are obtained by typing mixed gcse lrt || school: lrt, covariance(unstructured) mle stddeviations /// vce(robust) mixed, variance * mixed, variance estat recovariance estimates store rc predict murc, fitted and a spaghetti plot is produced as follows: sort school lrt twoway (line murc lrt, connect(ascending)), xtitle(LRT) /// ytitle(Empirical Bayes regression lines for model 2) To obtain predictions for the random-intercept model, we must first restore the estimates stored under the name ri: mixed gcse lrt || school:, mle stddeviations vce(robust) estimates restore ri predict muri, fitted sort school lrt twoway (line muri lrt, connect(ascending)), xtitle(LRT) /// ytitle(Empirical Bayes regression lines for model 1) The resulting spaghetti plots of the school-specific regression lines for both the random-intercept model and the random-coefficient model are given in figure 4.10 Figure 4.10: Spaghetti plots of empirical Bayes (EB) predictions of school-specific regression lines for the random-intercept model (top) and the random-coefficient model (bottom)\r","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:8:3","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"8.4 Residual diagnostics If normality is assumed for the random intercepts $ζ_{1j}$, random slopes $ζ_{2j}$, and level-1 residuals $ε_{ij}$, the corresponding EB predictions should also have normal distributions. 如果在多层次模型中，我们假定随机截距（random intercepts）、随机斜率（random slopes）以及第一层残差（level-1 residuals）都服从正态分布，那么由此产生的EB预测也应该服从正态分布。 To plot the distributions of the predicted random effects, we must pick one prediction per school, and we can accomplish this by using the pickone variable created with the command egen pickone = tag(school) in section 4.3. We can now plot the distributions by using histogram ebi if pickone==1, normal xtitle(Predicted random intercepts) histogram ebs if pickone==1, normal xtitle(Predicted random slopes) Figure 4.11: Histograms of predicted random intercepts and slopes\rThe histograms in figure 4.11 look approximately normal although the one for the slopes is perhaps a little positively skewed. It should be noted, however, that moderate nonnormality of random effects can easily be missed because EB predictions tend to be closer to normal than the true random effects. It is also useful to look at the bivariate distribution of the predicted random intercepts and slopes by using a scatterplot, or to display such a scatterplot together with the two histograms: scatter ebs ebi if pickone==1, saving(yx, replace) /// xtitle(\"Random intercept\") ytitle(\"Random slope\") /// ylabel(, nogrid) histogram ebs if pickone==1, freq horizontal saving(hy, replace) normal /// yscale(alt) ytitle(\" \") fxsize(35) ylabel(, nogrid) histogram ebi if pickone==1, freq saving(hx, replace) normal /// xscale(alt) xtitle(\" \") fysize(35) ylabel(, nogrid) graph combine hx.gph yx.gph hy.gph, hole(2) imargin(0 0 0 0) Here the scatterplot and histograms are first plotted separately and then combined using the graph combine command. In the first histogram command, the horizontal option is used to produce a rotated histogram of the random slopes. In the histogram commands, the yscale(alt) and xscale(alt) options are used to put the corresponding axes on the other side, and the normal option is used to overlay normal density curves. The fysize(35) and fxsize(35) options change the aspect ratios of the histograms, making them more flat so that they use up a smaller portion of the combined graph. Finally, in the graph combine command, the graphs are listed in lexicographic order, the hole(2) option denotes that there should be a hole in the second position—that is, the top-right corner—and the imargin(0 0 0 0) option reduces the space between the graphs. The resulting graph is shown in figure 4.12. After estimation with mixed, we obtain the predicted level-1 residuals, $$\\widetilde \\epsilon_{ij} = y_{ij}-(\\widehat\\beta_1+\\widehat\\beta_2x_{ij}+\\widetilde\\zeta_{1j}+\\widetilde\\zeta_{2j}x_{ij})$$ by using predict res1, residuals We plot the residuals by using the following command, which produces the graph in figure 4.13: histogram res1, normal xtitle(Predicted level-1 residuals) Figure 4.13: Histogram of predicted level-1 residuals\rTo obtain standardized level-1 residuals, use the rstandard option in the predict command after estimation using mixed. ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:8:4","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"8.5 Inferences for individual schools Random-intercept predictions $\\tilde{\\gamma}_{1 j}$ are sometimes viewed as measures of institutional performance—in the present context, how much value the schools add for children with LRT scores equal to 0 (the mean). However, we may not have adequately controlled for covariates correlated with achievement that are outside the control of the school, such as student SES. Furthermore, the model assumes that the random intercepts are uncorrelated with the LRT scores, so if schools with higher mean LRT scores tend to add more value, their value added would be underestimated. Nevertheless, predicted random intercepts shed some light on the research question: Which schools are most effective for children with $LRT=0$ ? It does not matter whether we add the predicted fixed part of the model because the ranking of schools is not affected by this. Returning to the question of comparing the schools’ effectiveness for children with LRT scores equal to 0, we can plot the predicted random intercepts with approximate 95% confidence intervals based on comparative standard errors (see section $2.11.3)$ . We recommend fitting the model by REML before using predict in order to obtain estimated comparative standard errors that take uncertainty in the estimated regression coefficients into account: quietly mixed gcse lrt || school: lrt, covariance(unstructured) reml predict slope1 inter1, reffects reses(slope_se inter_se) Here we only need inter_se. We first produce ranks for the schools in ascending order of the random-intercept predictions inter1: gsort + inter1 -pickone generate rank = sum(pickone) Here the gsort command is used to sort in ascending order of inter1 (indicated by + inter1) and, within inter1, in descending order of pickone (indicated by - pickone). The sum() function forms the cumulative sum, so the variable rank increases by 1 every time a new school with a higher value of inter1 is encountered. Before producing the graph, we generate a variable, labpos, for the vertical positions in the graph where the school identifiers should go: generate labpos = inter1 + 1.96*inter_se + .5 We are now ready to produce a so-called caterpillar plot : serrbar inter1 inter_se rank if pickone==1, addplot(scatter labpos rank, /// mlabel(school) msymbol(none) mlabpos(0)) scale(1.96) xtitle(Rank) /// ytitle(Prediction) legend(off) The school labels were added to the graph by superimposing a scatterplot onto the error bar plot with the addplot() option, where the vertical positions of the labels are given by the variable labpos. The resulting caterpillar plot is shown in figure 4.14. 随机截距预测（Random-intercept predictions） 随机截距预测（$\\tilde{\\gamma}_{1j}$）是一种统计模型预测，用来衡量学校对孩子们的“增值”。在这个上下文中，我们关心的是学校对LRT成绩为0（即平均值）的孩子们的增值。 公式分解 随机截距预测的公式通常是： $ \\tilde \\gamma_{1j} = \\beta_0 + b_{0j} $ $\\beta_0$ 是固定效应，表示所有学校的平均增值。 $b_{0j}$ 是随机效应，表示第j所学校相对于平均值的增值。 控制变量 在实际应用中，我们可能需要控制一些与学生成绩相关的变量，比如学生的社会经济地位（SES）。如果不控制这些变量，那么我们的预测可能会受到这些未控制变量的影响。 模型假设 模型假设随机截距与LRT成绩是独立的。如果这个假设不成立，比如如果成绩较高的学校倾向于增加更多的价值，那么这些学校的预测值可能会被低估。 预测随机截距 尽管存在上述问题，预测的随机截距仍然可以为我们提供一些关于哪些学校对LRT=0的学生最有效。 固定部分的影响 在比较学校的有效性时，我们不需要考虑模型的固定部分，因为它不会影响学校的排名。 预测随机截距的可视化 我们可以通过绘制预测的随机截距和95%置信区间来可视化学校的表现。这里使用的是混合效应模型（mixed model），并且使用限制性最大似然估计（REML）来拟合模型。 Stata命令 接下来是Stata命令的解释： mixed gcse lrt || school: lrt, covariance(unstructured) reml：这是一个混合效应模型的命令，其中gcse是因变量，lrt是固定效应，school是随机效应，covariance(unstructured)表示随机效应的协方差结构是无结构的，reml表示使用限制性最大似然估计。 predict slope1 inter1, reffects reses(slope_se inter_se)：这个命令用来预测随机斜率（slope1）和随机截距（inter1），以及它们的标准误差（slope_se和inter_se）。 gsort + inter1 -pickone：这个命令用于根据inter1的值对数据进行排序，+表示升序，-表示降序。 generate rank = sum(pickone)：这个命令生成一个新变量rank，它是根据pickone变量的累计和来计算的。 generate labpos = inter1 + 1.96*inter_se + .5：这个命令生成一个新变量labpos，用于确定学校标识符在图表中的垂直位置。 绘制图表 最后，使用serrbar命令绘制误差棒图，scatter命令绘制散点图，并且使用mlabel(school)来标记学校名称。 计算置信区间 置信区间是通过以下公式计算的： $ \\text{置信区间} = \\text{预测值} \\pm Z \\times \\text{标准误差} $ 其中 Z 是正态分布的临界值，对于95%置信区间，Z 通常是1.96。 在我们的例子中，预测值是 inter1，标准误差是 inter_s","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:8:5","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"9 Two-stage model formulation In this section, we describe an alternative way of specifying random-coefficient models that is popular in some areas such as education (for example, Raudenbush and Bryk 2002). As shown below, models are specified in two stages (for levels 1 and 2), necessitating a distinction between level-1 and level-2 covariates. Many people find this formulation helpful for interpreting and specifying models. Identical models can be formulated using either the approach discussed up to this point or the two-stage formulation. To express the random-coefficient model by using a two-stage formulation, Raudenbush and Bryk (2002) specify a level-1 model: $$y_{ij} = \\beta_{oj} + \\beta_{1j}x_{ij} + r_{ij}$$ where the intercept $\\beta_{oj}$ and slope $\\beta_{1j}$ are school-specific coefficients. Their level-2 models have these coefficients as responses: $$\\beta_{oj} = \\gamma_{00} + u_{oj}$$ $$\\beta_{1j} = \\gamma_{10} + u_{1j}$$ Sometimes the first of these level-2 models is referred to as a “means as outcomes” or “intercepts as outcomes” model, and the second as a “slopes as outcomes” model. It is typically assumed that given the covariate(s), the residuals or disturbances $u_{oj}$ and $u_{1j}$ in the level-2 model have a bivariate normal distribution with 0 mean and covariance matrix $$T = [ \\tau_{00} \\quad \\tau_{01} ] , τ_{10} = τ_{01} $$ The level-2 models cannot be fit on their own because the school-specific coefficients $\\beta_{oj}$ and $\\beta_{1j}$ are not observed. Instead, we must substitute the level-2 models into the level-1 model to obtain the reduced-form model for the observed responses, $y_{ij}$: $$\\begin{array}{rcl}y_{ij}\u0026=\u0026\\underbrace{\\gamma_{00}+u_{0j}}{\\beta{0j}}+\\underbrace{(\\gamma_{10}+u_{1j})}{\\beta{1j}}x_{ij}+r_{ij}\\end{array}$$ $$\\begin{array}{rcl}=\u0026\\underbrace{\\gamma_{00}+\\gamma_{10}x_{ij}}{\\text{fixed}}+\\underbrace{u{0j}+u_{1j}x_{ij}+r_{ij}}_{\\text{random}}\\end{array}$$ $$\\equiv\\quad\\beta_1+\\beta_2x_{ij}\\quad+\\zeta_{1j}+\\zeta_{2j}x_{ij}+\\epsilon_{ij}$$ In the reduced form, the fixed part is usually written first, followed by the random part. As shown in the last line of the equation above, we can return to our previous notation by defining $\\beta_1 = \\gamma_{00},\\beta_2 = \\gamma_{10},\\zeta_{1j} = u_{0j}$, $\\zeta_{2j} = u_{1j}$, and $\\varepsilon_{ij} = r_{ij}$. The above model is thus equivalent to the model in $(4.1)$. The level-1 model contains only level-1 covariates (that vary between units within clusters). Any level-2 covariates (that do not vary within clusters) are included in the level-2 models. For instance, we could include dummy variables for type of school: $w_{1j}$ for boys-only schools and $w_{2j}$ for girls-only schools, with mixed schools as the reference category. If we include these dummy variables in the model for the random intercept, $$\\begin{matrix}\\beta_{0j}\u0026=\u0026\\gamma_{00}+\\gamma_{01}w_{1j}+\\gamma_{02}w_{2j}+u_{0j}\\end{matrix}$$ the reduced form becomes $$\\begin{array}{rcl}y_{ij}\u0026=\u0026\\underbrace{\\gamma_{00}+\\gamma_{01}w_{1j}+\\gamma_{02}w_{2j}+u_{0j}}{\\beta{0j}}+\\underbrace{(\\gamma_{10}+u_{1j})}{\\beta{1j}}x_{ij}+r_{ij}\\\u0026=\u0026\\underbrace{\\gamma_{00}+\\gamma_{01}w_{1j}+\\gamma_{02}w_{2j}+\\gamma_{10}x_{ij}}{\\text{fixed}}+\\underbrace{u{0j}+u_{1j}x_{ij}+r_{ij}}_{\\text{random}}\\end{array}$$ If we also include the dummy variables for type of school in the model for the random slope, $$\\begin{matrix}\\beta_{1j}\u0026=\u0026\\gamma_{10}+\\gamma_{11}w_{1j}+\\gamma_{12}w_{2j}+u_{1j}\\end{matrix}$$ we obtain so-called cross-level interactions between covariates varying at different levels—w_1j by x_ij as well as w_2j by x_ij—in the reduced form The effect of lrt now depends on the type of school, with $\\gamma_{11}$ representing the additional effect of lrt on gcse for boys-only schools compared with mixed schools and representing the additional effect for girls-only schools compared with mixed schools. ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:9:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"第一部分：理解两阶段模型的基本结构 Level-1模型（个体层面） 首先，我们来看Level-1模型，也就是个体层面的模型。这个模型描述的是个体（比如学生）的结果（比如考试成绩）如何受到个体层面变量（比如学习时间）的影响。 公式是这样的： $$ y_{ij} = \\beta_{oj} + \\beta_{1j}x_{ij} + r_{ij} $$ 这里： $ y_{ij} $ 表示第 $ i $ 个学生在第 $ j $ 所学校的考试成绩。 $ \\beta_{oj} $ 是第 $ j $ 所学校的截距，可以理解为如果 $ x_{ij} $ 为0时，学生的平均成绩。 $ \\beta_{1j} $ 是第 $ j $ 所学校的斜率，表示学习时间每增加一个单位，学生成绩的变化量。 $ x_{ij} $ 是学生层面的变量，比如学习时间。 $ r_{ij} $ 是随机误差项，表示除了模型中考虑的因素外，其他因素对学生成绩的影响。 Level-2模型（学校层面） 然后，我们来看Level-2模型，也就是学校层面的模型。这个模型描述的是学校特有的截距和斜率是如何受到学校层面变量（比如学校类型）的影响。 公式是这样的： $$ \\beta_{oj} = \\gamma_{00} + u_{oj} $$ $$ \\beta_{1j} = \\gamma_{10} + u_{1j} $$ 这里： $ \\beta_{oj} $ 和 $ \\beta_{1j} $ 是从Level-1模型中来的，现在被视为响应变量，也就是我们想要解释的变量。 $ \\gamma_{00} $ 和 $ \\gamma_{10} $ 是固定效应，它们是所有学校共有的效应，可以理解为所有学校的平均截距和斜率。 $ u_{oj} $ 和 $ u_{1j} $ 是随机效应，表示每个学校特有的截距和斜率的偏离。 随机效应的分布 我们假设随机效应 $ u_{oj} $ 和 $ u_{1j} $ 服从二元正态分布，均值为0，协方差矩阵为 $ T $： $ T = [ \\tau_{00} \\quad \\tau_{01} ] , \\tau_{10} = \\tau_{01} $ 这意味着 $ u_{oj} $ 和 $ u_{1j} $ 之间的相关性是 $ \\tau_{01} $。 ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:9:1","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"第二部分：将Level-2模型代入Level-1模型 由于我们不能直接观察到学校特有的系数 $ \\beta_{oj} $ 和 $ \\beta_{1j} $，我们需要将Level-2模型代入Level-1模型中，以得到观察到的响应 $ y_{ij} $ 的简化形式模型。 公式是这样的： $$ y_{ij} = \\gamma_{00} + u_{0j} + (\\gamma_{10} + u_{1j})x_{ij} + r_{ij} $$ 这可以进一步分解为固定部分和随机部分： $$ y_{ij} = (\\gamma_{00} + \\gamma_{10}x_{ij}) + (u_{0j} + u_{1j}x_{ij} + r_{ij}) $$ 这里： $ \\gamma_{00} + \\gamma_{10}x_{ij} $ 是固定部分，表示所有学校共有的效应。 $ u_{0j} + u_{1j}x_{ij} + r_{ij} $ 是随机部分，表示每个学校特有的效应和其他随机因素。 引入Level-2协变量 如果我们在模型中引入学校类型的虚拟变量（比如 $ w_{1j} $ 表示男校，$ w_{2j} $ 表示女校），那么Level-2模型会变得更加复杂，但基本思想是相同的。我们会增加一些项来表示不同类型学校的影响。 公式是这样的： $$ \\beta_{0j} = \\gamma_{00} + \\gamma_{01}w_{1j} + \\gamma_{02}w_{2j} + u_{0j} $$ 代入Level-1模型后，我们得到： $$ y_{ij} = (\\gamma_{00} + \\gamma_{01}w_{1j} + \\gamma_{02}w_{2j} + \\gamma_{10}x_{ij}) + (u_{0j} + u_{1j}x_{ij} + r_{ij}) $$ 这里： $ \\gamma_{00} + \\gamma_{01}w_{1j} + \\gamma_{02}w_{2j} + \\gamma_{10}x_{ij} $ 是固定部分，表示不同类型学校的平均效应。 $ u_{0j} + u_{1j}x_{ij} + r_{ij} $ 是随机，表示每个学校特有的效应和其他随机因素。 ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:9:2","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"第三部分：模型的实际应用 模型的含义 我们先来回顾一下，我们的模型现在看起来是这样的： $$ y_{ij} = (\\gamma_{00} + \\gamma_{01}w_{1j} + \\gamma_{02}w_{2j} + \\gamma_{10}x_{ij} + \\gamma_{11}w_{1j}x_{ij} + \\gamma_{12}w_{2j}x_{ij}) + (u_{0j} + u_{1j}x_{ij} + r_{ij}) $$ 这个模型告诉我们，学生的考试成绩 $ y_{ij} $ 不仅受到个体层面变量 $ x_{ij} $（比如学习时间）的影响，还受到学校类型的影响。更具体地说： $ \\gamma_{00} $ 是混合学校（作为参考类别）的平均截距。 $ \\gamma_{01} $ 和 $ \\gamma_{02} $ 分别表示男校和女校相比于混合学校在截距上的差异。 $ \\gamma_{10} $ 是学习时间对所有学校学生成绩的平均影响。 $ \\gamma_{11} $ 和 $ \\gamma_{12} $ 表示男校和女校学生学习时间对成绩影响的额外增加量，与混合学校相比。 随机效应 $ u_{0j} $ 表示第 $ j $ 所学校的截距与平均截距的偏差。 $ u_{1j} $ 表示第 $ j $ 所学校的学习时间对成绩影响的斜率与平均斜率的偏差。 $ r_{ij} $ 是其他未观察到的因素对第 $ i $ 个学生在第 $ j $ 所学校的成绩的影响。 ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:9:3","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"第四部分：模型的解释和应用 解释固定效应 固定效应部分告诉我们，平均来看，不同类型的学校和学习时间如何影响学生的考试成绩。例如，如果 $ \\gamma_{11} $ 是正的，这意味着在控制了其他因素后，男校学生的学习时间对成绩的正向影响大于混合学校。同样，$ \\gamma_{12} $ 告诉我们女校学生的情况。 解释随机效应 随机效应部分告诉我们，即使在考虑了固定效应之后，不同学校的学生成绩仍然会有所差异。这些差异可能由于未观察到的学校特定因素（如学校资源、教师质量等）或随机误差（如测量误差、未包括在模型中的其他变量）造成。 模型的应用 这种模型在教育研究中非常有用，因为它允许我们探索不同学校环境如何影响学生的学习成果，并且可以考虑到学校之间的差异。例如，我们可以用这个模型来比较不同类型学校（男校、女校、混合学校）的教育效果，或者研究学校资源如何影响学生的学习时间与成绩之间的关系。 进一步的分析 在实际应用中，我们可能会对模型进行进一步的细化，比如考虑更多的学校层面或学生层面的变量，或者探索不同变量之间的交互作用。此外，我们还可以对模型的假设进行检验，比如随机效应的分布假设，以及模型是否满足其他统计假设。 希望这个详细的解释能帮助你更好地理解随机系数模型的两阶段表述方法。 The effect of lrt now depends on the type of school, with $\\gamma_{11}$ representing the additional effect of lrt on gcse for boys-only schools compared with mixed schools and representing the additional effect for girls-only schools compared with mixed schools. For estimation in mixed, it is necessary to convert the two-stage formulation to the reduced form because the fixed part of the model is specified first, followed by the random part of the model. Using factor variables in mixed, the command is mixed. mixed gcse i.schgend##c.lrt || school: lrt, covariance(unstructured) mle /// stddeviations vce(robust) Performing EM optimization ...\rPerforming gradient-based optimization: Iteration 0: Log pseudolikelihood = -13998.825 Iteration 1: Log pseudolikelihood = -13998.825 Computing standard errors ...\rMixed-effects regression Number of obs = 4,059\rGroup variable: school Number of groups = 65\rObs per group:\rmin = 2\ravg = 62.4\rmax = 198\rWald chi2(5) = 930.12\rLog pseudolikelihood = -13998.825 Prob \u003e chi2 = 0.0000\r(Std. err. adjusted for 65 clusters in school)\r---------------------------------------------------------------------------------\r| Robust\rgcse | Coefficient std. err. z P\u003e|z| [95% conf. interval]\r----------------+----------------------------------------------------------------\rschgend |\rboys | 0.85 0.96 0.89 0.376 -1.04 2.75\rgirls | 2.43 0.84 2.91 0.004 0.79 4.07\r|\rlrt | 0.57 0.02 24.24 0.000 0.53 0.62\r|\rschgend#c.lrt |\rboys | -0.02 0.05 -0.43 0.671 -0.13 0.08\rgirls | -0.03 0.05 -0.60 0.550 -0.13 0.07\r|\r_cons | -1.00 0.55 -1.80 0.072 -2.08 0.09\r---------------------------------------------------------------------------------\r------------------------------------------------------------------------------\r| Robust Random-effects parameters | Estimate std. err. [95% conf. interval]\r-----------------------------+------------------------------------------------\rschool: Unstructured |\rsd(lrt) | 0.12 0.02 0.08 0.18\rsd(_cons) | 2.80 0.30 2.26 3.46\rcorr(lrt,_cons) | 0.60 0.14 0.26 0.80\r-----------------------------+------------------------------------------------\rsd(Residual) | 7.44 0.13 7.20 7.69\r------------------------------------------------------------------------------\rHere mixed schools are the reference category for schgend to which boys-only schools and girls-only schools are compared. We see that, when lrt is 0, students from girls-only schools perform significantly better at the 5% level than students from mixed schools, whereas students from boys-only schools do not perform significantly better than students from mixed schools. The effect of lrt does not differ significantly between boys-only schools and mixed schools or between girls-only schools and mixed schools. The estimates and the corresponding parameters in the two-stage formulation are given under “Rand. coefficient \u0026 level-2 covariates” in the last three columns of table 4.1. Although equivalent models can be specified using either the reduced form (used by mixed) or the two-stage (used by the HLM software of Raudenbush et al. [2019]) formulation, in practice, model specification to some extent depends on the approach adopted. For instance, cross-level interactions are easily included using the two-stage specification in HLM, whereas same-level interactions must be created outside the program. Papers using HLM therefore tend to include more cross-level interactions and fewer same-level interactions. They also tend to include ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:9:4","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"Some warnings about random-coefficient models ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:10:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"10.1 Meaningful specification It rarely makes sense to include a random slope if there is no random intercept, just like interactions between two covariates usually do not make sense without including the covariates themselves in standard regression models. Similarly, it is seldom sensible to include a random slope without including the corresponding fixed slope because it is usually strange to allow the slope to vary randomly but constrain its population mean to 0. It is generally not a good idea to include a random coefficient for a covariate that does not vary at a lower level than the random coefficient itself. For example, in the inner-London-schools data, it does not make sense to include a school-level random slope for type of school because type of school does not vary within schools. Because we cannot estimate the effect of type of school for individual schools, it also appears impossible to estimate the variability of the effect of type of school between schools. However, level-2 random coefficients of level-2 covariates can be used to construct heteroskedastic random intercepts (see section 7.5.2). 第一部分：为什么需要随机截距 在随机系数模型中，我们经常想要了解个体层面的变量（比如学生的学习时间）是如何影响结果的（比如考试成绩）。但是，这种影响可能在不同的群体（比如不同的学校）之间有所不同。因此，我们引入了随机斜率来捕捉这种差异。 然而，随机斜率通常需要与随机截距一起使用。原因如下： 互动效应：如果我们只包含随机斜率而不包含随机截距，那么我们实际上是在允许斜率随机变化，但假设所有群体的截距都是相同的。这就像我们说，每个学校的学生学习时间对成绩的影响可能不同，但所有学校的平均成绩都是相同的，这通常没有意义。 模型的完整性：包含随机截距可以让模型更完整。随机截距可以捕捉到每个群体（比如学校）的平均效应，而随机斜率则可以捕捉到群体间斜率的变化。这样，我们就可以更全面地了解不同群体的特性。 第二部分：为什么需要固定斜率 同样，当我们在模型中包含随机斜率时，通常也需要包含相应的固定斜率。原因如下： 群体平均效应：固定斜率代表了所有群体的平均效应。如果我们只允许斜率随机变化，而不设定一个固定的平均值，那么我们就没有一个基准来比较不同群体的效应。 模型的可解释性：固定斜率的存在使得模型更容易解释。我们可以清楚地看到，平均来看，个体层面的变量是如何影响结果的。 第三部分：何时不包含随机系数 有时候，我们不应该在模型中包含某个变量的随机系数。这通常发生在以下情况： 变量不变化：如果一个变量在随机系数所定义的层次上不变化，那么包含它的随机系数就没有意义。比如，在学校层面的模型中包含学校的类型作为随机斜率就没有意义，因为学校类型在每个学校内是固定的。 无法估计效应：如果一个变量在个体层面不变化，那么我们无法估计它对个体的影响，因此也就无法估计它在群体间的变异性。 第四部分：如何构建异方差随机截距 尽管我们不能在学校层面为学校类型这样的变量包含随机斜率，但我们可以使用学校层面的变量（比如学校类型）来构建异方差随机截距。这意味着不同学校的平均成绩可以有不同的方差，从而允许模型捕捉到更多学校间的差异。 总结 随机斜率通常需要与随机截距一起使用，以捕捉群体间的平均效应和斜率的变化。 固定斜率提供了一个基准，使得模型更易于解释。 如果一个变量在个体层面不变化，那么包含它的随机系数是没有意义的。 我们可以使用群体层面的变量来构建异方差随机截距，以捕捉更多群体间的差异。 ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:10:1","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"10.2 Many random coefficients It may be tempting to allow many different covariates to have random slopes. However, the number of parameters for the random part of the model increases rapidly with the number of random slopes because there is a variance parameter for each random effect (intercept or slope) and a covariance parameter for each pair of random effects. If there are $k$ random slopes plus one random intercept, then there are $(k+1)(k+2)/2+1$ parameters in the random part (for example, $k=3$ gives 11 parameters). Another problem is that clusters may not provide much information on cluster-specific slopes and hence on the slope variance either if the clusters are small, or if it does not vary much within clusters or varies only in a small number of clusters. Perhaps a useful rule is to consider the random part of the model (ignoring the fixed part) and replace the random effects with fixed regression coefficients. It should be possible (even if not very sensible) to fit the resulting model to a good number of clusters (say, 20 or more), with some error degrees of freedom. Note, however, that it does not matter if some of the clusters have insufficient data as long as there are an adequate number of clusters that do have sufficient data. It is never a good idea to discard clusters merely because they provide little information on some of the parameters of the model. In general, it makes sense to allow for more flexibility in the fixed part of the model than in the random part. For instance, the fixed part of the model may include a dummy variable for each occasion in longitudinal data, but in the random part of the model it may be sufficient to allow for a random intercept and a random slope of time, keeping in mind that in this case it is only assumed that the deviation from the population-average curve is linear in time, not that the relationship itself is linear. See section 7.3 for examples of modeling a nonlinear relationship in the fixed part of the model but not in the random part. The overall message is that random slopes should be included only if strongly suggested by the subject-matter theory related to the application and if the data provide sufficient information. 好的，我们来一步步理解为什么在随机效应模型中不能随意增加太多的随机斜率，以及如何决定是否应该包含随机斜率。我们会分两部分来讲解，首先从参数数量和数据信息量的角度来理解，然后讨论模型的灵活性和随机斜率的适用条件。 第一部分：参数数量和数据信息量 1. 随机斜率的参数数量 当我们在模型中为不同的协变量引入随机斜率时，我们实际上是在允许这些协变量的影响在不同的群体（如不同的学校或班级）之间有所变化。但是，这样做会增加模型中随机部分的参数数量。 假设我们有 $ k $ 个随机斜率和一个随机截距，那么随机部分的参数数量将是 ((k+1)(k+2)/2 + 1) 个。这里的参数包括每个随机效应的方差参数和每对随机效应之间的协方差参数。 例子： 如果我们有3个随机斜率（$ k=3 $），那么随机部分的参数数量将是： $ \\frac{(3+1)(3+2)}{2} + 1 = \\frac{4 \\times 5}{2} + 1 = 11 $ 这意味着我们需要估计11个参数，包括每个随机斜率的方差、每对随机斜率之间的协方差，以及随机截距的方差。 2. 数据对随机斜率的支撑 另一个问题是，如果群体（如学校或班级）数量不多，或者在群体内部协变量的变化不大，或者只在少数群体中变化，那么我们可能没有足够的信息来估计群体特定的斜率，以及斜率的方差。 一个有用的经验法则是，考虑模型的随机部分（忽略固定部分），并将随机效应替换为固定回归系数。即使不是很合理，也应该能够将得到的模型拟合到足够多的群体上（比如说20个或更多），并且有一定的误差自由度。 例子： 假设我们有30所学校的数据，我们想要估计每个学校学生学习时间对成绩的影响。如果大多数学校的学生学习时间对成绩的影响变化不大，那么我们可能没有足够的信息来估计每个学校的随机斜率。在这种情况下，我们可能需要考虑只使用固定斜率，或者使用较少的随机斜率。 第二部分：模型的灵活性和随机斜率的适用条件 1. 模型的灵活性 通常，我们在模型的固定部分比在随机部分允许更多的灵活性。例如，在纵向数据中，固定部分可能包括每个时间点的虚拟变量，但在随机部分，可能只需要允许一个随机截距和一个随时间变化的随机斜率。 例子： 假设我们有学生在不同时间点的考试成绩数据。在固定部分，我们可能包括每个时间点的虚拟变量来捕捉不同时间点的平均成绩变化。但在随机部分，我们可能只需要允许一个随机截距和一个随时间变化的随机斜率，假设从总体平均曲线的偏差随时间线性变化。 2. 随机斜率的适用条件 总的来说，只有在以下情况下才应该包含随机斜率： 理论支持：与应用相关的主题理论强烈建议包含随机斜率。 数据支持：数据提供了足够的信息来估计这些随机斜率。 例子： 如果我们研究的是不同学校的教学方法对学生成绩的影响，并且理论或先前的研究建议教学方法的影响在不同学校之间可能有所不同，那么我们可能会考虑在模型中包含教学方法的随机斜率。同时，如果数据中不同学校的教学方法确实有显著变化，那么我们就有理由包含随机斜率。 总结 在模型中增加随机斜率会增加需要估计的参数数量。 如果群体数量不多或者群体内部协变量变化不大，我们可能没有足够的信息来估计随机斜率。 模型的固定部分可以比随机部分更灵活。 只有在理论和数据都支持的情况下，才应该包含随机斜率。 我们已经讨论了为什么在随机效应模型中不能随意增加太多的随机斜率，以及如何决定是否应该包含随机斜率。现在，我们继续深入探讨这个主题，特别是关于如何在实际中应用这些原则，并提供一些实际的指导。 第三部分：实际应用中的考虑 1. 模型的复杂性与解释性 当我们在模型中引入更多的随机斜率时，模型的复杂性会增加。这不仅使得模型更难以估计，也可能使得结果更难以解释。因此，我们需要在模型的复杂性和解释性之间找到平衡。 例子： 假设我们正在研究工作压力对员工健康的影响，并考虑引入部门作为随机效应。如果我们引入太多的随机斜率（比如每个部门的不同工作压力斜率），模型可能会变得过于复杂，难以向非专业的决策者解释。 2. 数据的质量和数量 在决定是否包含随机斜率时，我们需要考虑数据的质量和数量。如果数据量不足或者数据质量不高，那么估计过多的随机斜率可能会导致模型估计不准确。 例子： 如果我们只有少数几个部门的数据，并且每","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:10:2","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"10.3 Convergence problems Convergence problems can manifest themselves in different ways. Either estimates are never produced, or standard errors are missing, or mixed produces messages such as “nonconcave”, “backed-up”, or “standard error calculation has failed”. Sometimes none of these things happen, but the confidence intervals for some of the correlations cover the full permissible range from to 1 (see sections 7.3 and 8.13.2 for examples). Convergence problems can occur because the estimated covariance matrix “tries” to become negative definite, meaning, for instance, that variances try to become negative or correlations try to be greater than $1$ or less than $-1$ All the commands in Stata force the covariance matrix to be positive (semi)definite, and when parameters approach nonpermissible values, convergence can be slow or even fail. It may help to translate and rescale $x_{ij}$because variances and covariances are not invariant to these transformations. Often a better remedy is to simplify the model by removing some random slopes. Convergence problems can also occur because of lack of identification, and again, a remedy is to simplify the model. However, before giving up on a model, it is worth attempting to achieve convergence by trying both the mle and the reml options, specifying the difficult option, trying the matlog option (which parameterizes the random part differently during maximization), or increasing the number of EM iterations by using either the emiterate() option or even the emonly option. It can also be helpful to monitor the iterations more closely by using trace, which displays the parameter estimates at the end of each iteration (unfortunately, not for the EM iterations). Lack of identification of a parameter might be recognized by that parameter changing wildly between iterations without much of a change in the log likelihood. Problems with a variance approaching 0 can be detected by noticing that the log-standard deviation takes on very large negative values. 收敛问题的详细解释 a. 什么是协方差矩阵？ 协方差矩阵是一个方阵，用于表示多个随机变量之间的协方差。对于两个随机变量 $X$ 和 $Y$，它们的协方差矩阵可能看起来像这样： 其中，$\\text{Var}(X)$ 是 $X$ 的方差，$\\text{Var}(Y)$ 是 $Y$ 的方差，$\\text{Cov}(X, Y)$ 是 $X$ 和 $Y$ 之间的协方差。 b. 为什么协方差矩阵不能是负定的？ 在数学上，协方差矩阵必须是半正定的，这意味着它的所有特征值都必须是非负的。这是因为方差（协方差矩阵的对角元素）总是非负的，而协方差（协方差矩阵的非对角元素）表示变量之间的线性关系，它们的值被限制在 -1 和 1 之间。如果协方差矩阵试图变成负定的，那么它就会违反这些基本的统计属性。 c. 收敛问题的表现 收敛问题可能表现为： 无法产生估计值。 标准误差缺失。 出现错误信息，如“非凹形”、“阻塞”或“标准误差计算失败”。 相关性的置信区间覆盖了从 -1 到 1 的全部范围。 ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:10:3","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"解决收敛问题的方法 a. 转换和重新缩放变量 有时候，变量的尺度和分布会影响模型的收敛。例如，如果一个变量的尺度非常大，它可能会主导模型的估计过程，导致其他变量的估计不稳定。通过转换（如对数转换）和重新缩放（如标准化或归一化）变量，我们可以改善模型的收敛性。 b. 简化模型 如果模型太复杂，可能会导致收敛问题。通过移除一些不必要的随机斜率或固定效应，我们可以简化模型，使其更容易收敛。 c. 使用不同的估计方法 mle（最大似然估计）：这是一种常用的参数估计方法，它试图找到最大化似然函数的参数值。 reml（限制性最大似然估计）：这种方法在估计模型参数时，对某些参数施加了限制，有时可以改善收敛性。 difficult 选项：这个选项通常用于处理难以收敛的问题，它可能会改变算法的一些参数，使其更适合处理困难的问题。 matlog 选项：这个选项在最大化过程中以不同的方式参数化随机部分，有时可以改善收敛性。 emiterate() 和 emonly 选项：这些选项用于增加EM（期望最大化）算法的迭代次数，有时可以改善收敛性。 d. 监控迭代过程 使用 trace 选项可以监控迭代过程，它会在每次迭代结束时显示参数估计值。这有助于我们了解参数是如何随着迭代过程而变化的，从而判断模型是否正在收敛。 ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:10:4","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"识别参数识别问题 参数变化大：如果一个参数在迭代过程中变化很大，而对数似然值变化不大，这可能表明该参数无法被识别。 方差接近0：如果一个方差接近0，可以通过观察对数标准差是否取非常大的负值来检测。 希望这个更详细的解释能帮助你更好地理解收敛问题及其解决方法。如果你还有任何疑问，或者需要进一步的解释，请随时告诉我。 好的，我们继续深入探讨收敛问题及其解决方法。 ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:10:5","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"收敛问题的数学原理 a. 协方差矩阵的正定性 协方差矩阵的正定性是一个重要的数学属性。如果一个矩阵的所有特征值都是正的，那么这个矩阵就是正定的。如果所有特征值都是非负的，那么这个矩阵就是半正定的。在统计学中，协方差矩阵必须是半正定的，因为它代表了数据的方差和协方差，这些都是非负的。 b. 协方差矩阵的非正定性问题 当协方差矩阵试图变成负定的，这意味着模型的参数估计可能导致方差变成负数，或者相关系数超出[-1, 1]的范围。这在数学上是不可能的，因为： 方差是数据分散程度的度量，它必须是非负的。 相关系数是两个变量之间线性关系的度量，它的取值范围是[-1, 1]。 如果模型的参数估计导致协方差矩阵变成非正定的，那么模型的估计就是不稳定的，这就会导致收敛问题。 ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:10:6","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"解决收敛问题的策略 a. 转换和重新缩放变量 转换和重新缩放变量是解决收敛问题的常用方法。例如： 对数转换：如果变量的尺度非常大，对其进行对数转换可以减少尺度的影响。 标准化：将变量转换为均值为0，标准差为1的分布，这有助于稳定模型的估计。 归一化：将变量缩放到[0, 1]的范围内，这有助于处理不同尺度的变量。 b. 简化模型 如果模型太复杂，可能会导致收敛问题。简化模型的方法可能包括： 移除不必要的变量：如果某些变量对模型的贡献不大，可以考虑移除它们。 减少随机效应：在混合效应模型中，减少随机效应的数量可以简化模型。 固定效应：将某些随机效应固定为特定的值，以简化模型。 c. 使用不同的估计方法 不同的估计方法可能对收敛问题有不同的敏感性。例如： mle：最大似然估计是一种常用的估计方法，但它可能对初始值和模型的复杂性敏感。 reml：限制性最大似然估计对某些模型可能更稳定，特别是当模型包含不可估计的参数时。 difficult 选项：这个选项可以改变算法的行为，使其更适合处理难以收敛的问题。 d. 增加迭代次数 增加迭代次数可以给算法更多的时间来寻找最优解。例如： emiterate() 选项：这个选项可以增加EM算法的迭代次数。 emonly 选项：这个选项可以让算法只使用EM算法，而不使用其他方法。 e. 监控迭代过程 使用 trace 选项可以监控迭代过程，这有助于我们了解模型的收敛情况。例如： 参数的变化：如果参数在迭代过程中变化很大，这可能表明模型还没有收敛。 对数似然值的变化：如果对数似然值在迭代过程中变化不大，这可能表明模型已经收敛。 ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:10:7","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"实际操作中的注意事项 在实际操作中，解决收敛问题可能需要尝试多种方法。例如，你可以尝试以下步骤： 检查数据：确保数据没有错误或异常值。 转换和重新缩放变量：尝试不同的转换和缩放方法。 简化模型：尝试移除不必要的变量或随机效应。 尝试不同的估计方法：使用 mle、reml 或其他估计方法。 增加迭代次数：使用 emiterate() 或 emonly 选项。 监控迭代过程：使用 trace 选项来监控模型的收敛情况。 ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:10:8","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"10.4 Lack of identification Sometimes random-coefficient models are simply not identified (or in other words, underidentified). As an important example, consider balanced data with clusters of size $n_{j}=2$ and with a covariate $x_{ij}$ taking the same two values $t_{1}=0$ and $t_{2}=1$ for each cluster (an example would be the peak-expiratory-flow data from chapter 2). A model with a random intercept, a random slope of $x_{ij}$, and a level-1 residual, all of which are normally distributed, is not identified in this case. This can be seen by considering the two distinct variances (for $i=1$ and $i=2$) and one covariance of the total residuals when $t_{1}=0$ and $t_{2}=1$: $$\\begin{matrix}\\mathrm{Var}(\\xi_{1j})\u0026=\u0026\\psi_{11}+\\theta\\end{matrix}$$ $$\\begin{matrix}\\mathrm{Var}(\\xi_{2j})\u0026=\u0026\\psi_{11}+2\\psi_{21}+\\psi_{22}+\\theta\\end{matrix}$$ $$\\begin{matrix}\\mathrm{Cov}(\\xi_{1j},\\xi_{2j})\u0026=\u0026\\psi_{11}+\\psi_{21}\\end{matrix}$$ The marginal distribution of $y_{ij}$ given the covariates is normal and therefore completely characterized by the fixed part of the model and these three model-implied moments (two variances and a covariance). However, the three moments are determined by four parameters of the random part ($\\psi_{11}, \\psi_{22}, \\psi_{21}$, and $\\theta$), so fitting the model-implied moments to the data would effectively involve solving three equations for four unknowns. The model is therefore not identified. We could identify the model by setting $\\theta = 0$, which does not impose any restrictions on the covariance matrix (however, such a constraint is not allowed in mixed). The original model becomes identified if the covariate $x_{ij}$, which has a random slope, varies also between clusters because the model-implied covariance matrix of the total residuals then differs between clusters, yielding more equations to solve for the four parameters. Still assuming that the random effects and level-1 residual are normally distributed, consider now the case of balanced data with clusters of size $n_j=3$ and with a covariate $x_{ij}$ taking the same three values $t_1, t_2,$ and $t_3$ for each cluster. An example would be longitudinal data with three occasions at times $t_1, t_2,$ and $t_3$. Instead of including a random intercept and a random slope of time, it may be tempting to specify a random-coefficient model with a random intercept and two random coefficients for the dummy variables for occasions two and three. In total, such a model would contain seven (co)variance parameters: six for the three random effects and one for the level-1 residual variance. Because the covariance matrix of the responses for the three occasions given the covariates only has six elements, it is impossible to solve for all unknowns. The same problem would occur when attempting to fit this kind of model for more than three occasions. ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:10:9","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"第一部分：模型识别的基本概念和例子 1. 模型识别的含义 模型识别是指我们是否能够通过观测数据来唯一确定模型中的所有参数。如果参数不能被唯一确定，模型就是不可识别的。 2. 模型不可识别的原因 模型不可识别通常是因为模型中的参数数量超过了数据所能提供的信息量。这就像我们有四个未知数，但只有三个方程，所以我们无法解出这四个未知数。 3. 一个简单的例子 假设我们有一个模型，它包含随机截距、随机斜率，以及一级残差，它们都是正态分布的。我们有以下两个方差和一个协方差： $ \\mathrm{Var}(\\xi_{1j}) = \\psi_{11} + \\theta $ $ \\mathrm{Var}(\\xi_{2j}) = \\psi_{11} + 2\\psi_{21} + \\psi_{22} + \\theta $ $ \\mathrm{Cov}(\\xi_{1j}, \\xi_{2j}) = \\psi_{11} + \\psi_{21} $ 这里，$ \\psi_{11} $、$ \\psi_{21} $、$ \\psi_{22} $ 和 $ \\theta $ 是模型的随机部分的参数。我们有四个参数，但只有三个方程（两个方差和一个协方差）。这意味着我们不能唯一地解出这四个参数，因此这个模型是不可识别的。 为了帮助理解，我们可以将这些公式看作是关于这些参数的方程： 假设我们有四个未知数 $ a $、$ b $、$ c $ 和 $ d $，但我们只有三个方程： $ 3a + 2b = 5 $ $ a + b + c = 3 $ $ 2a - b + 3c + d = 1 $ 我们无法解出 $ a $、$ b $、$ c $ 和 $ d $ 的唯一值，因为我们的方程不够。 4. 如何解决模型识别问题 解决模型识别问题的一种方法是减少模型的参数数量。例如，我们可以设定 $ \\theta = 0 $，这样就减少了参数数量，使得模型变得可识别。但这种方法可能会对模型的协方差矩阵施加限制，有时候这是不可接受的。 ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:10:10","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"第二部分：更复杂情况下的模型识别问题 1. 更复杂的情况 现在，我们考虑一个更复杂的情况。假设我们有每个簇大小为3的平衡数据，以及一个在每个簇中取三个值 $ t_1, t_2, t_3 $ 的协变量 $ x_{ij} $。例如，这可以是三个不同时间点的纵向数据。 2. 尝试建模 我们可能会尝试使用一个包含随机截距和两个随机系数（对应于后两个时间点的虚拟变量）的随机系数模型。这样的模型总共包含七个（协）方差参数：六个用于三个随机效应，一个用于一级残差方差。 3. 模型不可识别的原因 问题在于，给定协变量的三个时间点的响应的协方差矩阵只有六个元素。这意味着我们没有足够的信息来解出所有的未知数。因此，这个模型也是不可识别的。 为了帮助理解，我们可以将这七个参数看作是七个未知数，但我们只有六个方程，因此无法解出这七个未知数的唯一值。 4. 解决方案 为了解决这个问题，我们可以尝试改变模型的结构，例如，不包括随机斜率，而是包括随机截距和时间的随机系数。这样，模型的协方差矩阵就会在不同的簇之间有所不同，从而提供了更多的方程来解出参数。 例如，如果我们有四个时间点而不是三个，我们可能需要包括随机截距和三个随机系数（对应于后三个时间点的虚拟变量）。这样，我们就有了更多的方程来解出参数，从而使模型变得可识别。 让我们通过一个具体的数据分析案例来说明模型识别问题是如何影响模型解释的。 ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:10:11","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"案例背景 假设我们有一个关于学生考试成绩的数据集，我们想要研究学生的学习时间（一个协变量 $x_{ij}$）如何影响他们的考试成绩（因变量 $y_{ij}$）。我们收集了多个学校（称为“簇”）的数据，每个学校有多个学生的学习时间和成绩。 ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:10:12","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"模型设定 我们首先设定一个包含随机截距和随机斜率的混合效应模型，这个模型允许每个学校有不同的截距和斜率。模型可以表示为： $$ y_{ij} = (\\beta_0 + u_{0j}) + (\\beta_1 + u_{1j})x_{ij} + e_{ij} $$ 其中： $y_{ij}$ 是第 $i$ 个学生在第 $j$ 所学校的考试成绩。 $x_{ij}$ 是第 $i$ 个学生在第 $j$ 所学校的学习时间。 $\\beta_0$ 和 $\\beta_1$ 是总体的截距和斜率。 $u_{0j}$ 和 $u_{1j}$ 是第 $j$ 所学校的随机截距和随机斜率。 $e_{ij}$ 是误差项。 ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:10:13","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"模型识别问题 在实际分析中，我们可能会遇到模型识别问题。例如，如果我们的数据集中每个学校只有两个学生，且学习时间 $x_{ij}$ 只取两个值（0和1），那么我们的模型可能无法识别。这是因为模型的参数数量超过了数据所能提供的信息量。 具体来说，我们有四个参数（$\\psi_{11}$、$\\psi_{21}$、$\\psi_{22}$ 和 $\\theta$）需要估计，但我们只有三个方程（两个方差和一个协方差）： $\\mathrm{Var}(\\xi_{1j}) = \\psi_{11} + \\theta$ $\\mathrm{Var}(\\xi_{2j}) = \\psi_{11} + 2\\psi_{21} + \\psi_{22} + \\theta$ $\\mathrm{Cov}(\\xi_{1j}, \\xi_{2j}) = \\psi_{11} + \\psi_{21}$ 由于方程数量少于参数数量，我们无法唯一确定这些参数的值，这就是模型识别问题。 ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:10:14","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"模型识别问题的影响 模型识别问题会影响我们对模型的解释。在上述案例中，如果我们不能识别模型，我们就无法准确地估计学习时间对考试成绩的影响，也无法确定不同学校之间的差异。这可能导致我们对教育政策或教学方法的效果做出错误的推断。 ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:10:15","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"解决方案 为了解决这个问题，我们可以采取以下几种方法： 增加数据：增加每个学校的学生数量或学校数量，以提供更多的信息来估计模型参数。 简化模型：减少模型的复杂性，例如，只估计随机截距而不是随机斜率。 使用不同的估计方法：尝试不同的估计方法，如最大似然估计（MLE）或限制性最大似然估计（REML）。 通过这些方法，我们可以提高模型的识别能力，从而更准确地解释数据。在实际应用中，这可能意味着我们需要重新设计数据收集方案，或者在分析时做出一些妥协。 ","date":"2024-10-01","objectID":"/4.chapter-4-random-coefficient-models/:10:16","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 4 - Random-coefficient models","uri":"/4.chapter-4-random-coefficient-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"In this chapter, we extend the variance-components models introduced in the previous chapter by including observed explanatory variables or covariates $x$ .","date":"2024-09-19","objectID":"/chapter-3-random-intercept-models-with-covariates/","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 3 ：Random-intercept models with covariates","uri":"/chapter-3-random-intercept-models-with-covariates/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"2 Does smoking during pregnancy affect birthweight? Abrevaya (2006) investigates the effect of smoking on birth outcomes with the Natality datasets derived from birth certificates by the U.S. National Center for Health Statistics. This is of considerable public health interest because many pregnant women in the U.S. continue to smoke during pregnancy. Indeed, around the time of the study, it was estimated that only 18% to 25% of smokers quit smoking once they become pregnant, according to the 2004 Surgeon General’s Report on The Health Consequences of Smoking. Abrevaya identified multiple births from the same mothers in nine datasets from 1990–1998 by matching mothers across the datasets. Unlike, for instance, the Nordic countries, a unique person identifier such as a social security number is rarely available in U.S. datasets. Perfect matching is thus precluded, and matching must proceed by identifying mothers who have identical values on a set of variables in all datasets. In this study, matching was accomplished by considering mother’s state of birth and child’s state of birth, as well as mother’s county and city of birth; mother’s age, race, education, and marital status; and, if married, husband’s age and race. For the matching on mother’s and child’s states of birth to be useful, the data were restricted to combinations of states that occur rarely. Here we consider the subset of the matches where the observed interval between births was consistent with the interval since the last birth recorded on the birth certificate. The data are restricted to births with complete data for the variables considered by Abrevaya (2006), singleton births (no twins or other multiple births), and births to mothers for whom at least two births between 1990 and 1998 could be matched and whose race was classified as White or Black. The birth outcome we will concentrate on is birthweight. Abrevaya (2006) motivates his study by citing a report from the U.S. Surgeon General: The dataset used by Abrevaya (2006) is available from the Journal of Applied Econometrics Data Archive. Here we took a 10% random sample of the mothers, yielding 8,604 births from 3,978 mothers. We use the following variables from smoking.dta: momid: mother identifier birwt: birthweight (in grams) smoke: dummy variable for mother smoking during pregnancy (1: smoking; 0: not smoking) male: dummy variable for baby being male (1: male; 0: female) mage: mother’s age at the birth of the child (in years) Mother’s education (reference category: did not graduate from high school) hsgrad: dummy variable for having graduated from high school (1: graduated; 0: otherwise) somecoll: dummy variable for having some college education, but no degree (1: some college; 0: otherwise) collgrad: dummy variable for having graduated from college (1: graduated; 0: otherwise) married: dummy variable for mother being married (1: married; 0: unmarried) black: dummy variable for mother being Black (1: Black; 0: White) Kessner index (reference category: Kessner index = 1, or adequate prenatal care) kessner2: dummy variable for Kessner index = 2, or intermediate prenatal care (1: index=2; 0: otherwise) kessner3: dummy variable for Kessner index = 3, or inadequate prenatal care (1: index=3; 0: otherwise) Timing of first prenatal visit (reference category: first trimester) novisit: dummy variable for no prenatal care visit (1: no visit; 0: otherwise) pretri2: dummy variable for first prenatal care visit having occurred in second trimester (1: yes; 0: otherwise) pretri3: dummy variable for first prenatal care visit having occurred in third trimester (1: yes; 0: otherwise) Smoking status was determined from the answer to the question asked on the birth certificate whether there was tobacco use during pregnancy. The dummy variables for mother’s education—hsgrad, somecoll, and collgrad—were derived from the years of education given on the birth certificate. The Kessner index is a measure of the adequacy of prena","date":"2024-09-19","objectID":"/chapter-3-random-intercept-models-with-covariates/:1:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 3 ：Random-intercept models with covariates","uri":"/chapter-3-random-intercept-models-with-covariates/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"2.1 Data structure and descriptive statistics The data have a two-level structure with births (or children or pregnancies) as units at level 1 and mothers as clusters at level 2. In multilevel models, the response variable always varies at the lowest level, taking on different values for different level-1 units within the same level-2 cluster. However, covariates can either vary at level 1 (and therefore usually also at level 2) or vary at level 2 only. For instance, while smoke can change from one pregnancy to the next, black is constant between pregnancies. smoke is therefore said to be a level-1 variable, whereas black is a level-2 variable. Among the variables listed above, black appears to be the only one that cannot in principle change between pregnancies. However, because of the way the matching was done, the education dummy variables (hsgrad, somecoll, and collgrad) and married also remain constant across births for the same mother and are thus level-2 variables. There are a total of 5 level-2 covariates and 8 level-1 covariates here. As we will see in this chapter, the distinction between level-1 and level-2 covariates is important in several ways. Keeping track of the number of level-2 covariates is necessary when the number of level-2 units is not large because the rule of thumb in display 2.1 for choosing between ML and REML, deciding whether robust standard errors can be trusted, and whether asymptotic tests can be used, is $J-q\u003e42$, where $q$ is the number of level-2 covariates. When considering the maximum number of level-2 covariates that can reasonably be included in the model, a rough rule of thumb is that there should be at least about 10 clusters per level-2 covariate; $J/q\\geq10$. It is useful to know not just whether variables vary at levels 1 and 2 but also how much they vary at each of the levels. Variation at the two levels can be explored using the xtsum command (after reading the data and declaring their two-level structure by using xtset): 协变量（Covariate）的层次 Level-1协变量：这些是与个体层次相关的变量，比如学生的性别、年龄等。 Level-2协变量：这些是与群体层次相关的变量，比如学校的资源水平、学校类型等。 为什么Level-2协变量的数量很重要？ 当Level-2单位（如学校）的数量不多时，Level-2协变量的数量会影响模型的估计和检验。这是因为： 模型估计方法：选择最大似然估计（MLE）还是限制最大似然估计（REML）。 标准误的稳健性：判断稳健标准误是否可信。 渐近检验的适用性：判断是否可以使用基于渐近理论的检验。 经验法则公式解析 $ J - q \u003e 42 $ $ J $：Level-2单位的数量，例如学校的数量。 $ q $：Level-2协变量的数量。 这个条件告诉我们，如果Level-2单位的数量减去Level-2协变量的数量大于42，我们可以考虑使用MLE或REML方法，并且可以信赖稳健标准误和使用渐近检验。 例子 假设有45所学校，考虑使用3个Level-2协变量（比如学校的资源水平、师生比例、学校类型）： $ J = 45 $（学校数量） $ q = 3 $（Level-2协变量数量） $ J - q = 45 - 3 = 42 $ 在这个例子中，$ J - q = 42 $，正好等于42，这意味着我们处于经验法则的边界上。如果学校数量再少一些，或者协变量数量再多一些，我们可能就无法使用MLE或REML方法，或者不能信赖稳健标准误和渐近检验了。 $ J/q \\geq 10 $ 这个条件告诉我们，对于每个Level-2协变量，至少应该有10个Level-2单位。这是一个粗略的经验法则，用于确定模型中可以合理包含的Level-2协变量的最大数量。 例子 如果我们有45所学校，根据经验法则，我们可以包含的Level-2协变量的最大数量是： $ J = 45 $（学校数量） $ q $ 应该满足 $ 45/q \\geq 10 $ 解这个不等式，我们得到 $ q \\leq 4.5 $ 因为我们不能有半个协变量，所以我们可以说最多可以有4个Level-2协变量。 变量在不同层次的变异性 了解变量在不同层次的变异性对于模型的构建和解释非常重要。可以使用xtsum命令来探索这种变异性。在使用xtsum之前，需要使用xtset来声明数据的层次结构。 总结 理解Level-1和Level-2协变量的区别。 知道Level-2协变量数量对模型估计和检验的影响。 掌握经验法则公式，并能够应用它们来决定模型中可以包含的协变量数量。 使用xtsum和xtset来探索和分析数据的层次结构。 希望这个更详细的解释能帮助你更好地理解这些概念。如果你还有任何疑问，或者需要进一步的例子，请随时告诉我。 use \"F:\\【3】Electronic books\\0[Applied Statistics]\\（3）第三本\\datasets\\smoking.dta\" quietly xtset momid xtsum birwt smoke black Variable | Mean Std. dev. Min Max | Observations\r-----------------+--------------------------------------------+----------------\rbirwt overall | 3469.931 527.1394 284 5642 | N = 8604\rbetween | 451.1943 1361 5183.5 | n = 3978\rwithin | 276.7966 1528.431 5411.431 | T-bar = 2.1629\r| |\rsmoke overall | .1399349 .3469397 0 1 | N = 8604\rbetween | .3216459 0 1 | n = 3978\rwithin | .1368006 -.5267318 .8066016 | T-bar = 2.1629\r| |\rblack overall | .0717108 .2580235 0 1 | N = 8604\rbetween | .257512 0 1 | n = 3978\rwithin | 0 .0717108 .0717108 | T-bar = 2.1629\rThe total number of observations is $N=8604$; the number of clusters is $J=3978$ (n in the output); and there are on average about 2.2 births per mother (T-bar in the","date":"2024-09-19","objectID":"/chapter-3-random-intercept-models-with-covariates/:1:1","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 3 ：Random-intercept models with covariates","uri":"/chapter-3-random-intercept-models-with-covariates/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"3 The linear random-intercept model with covariates ","date":"2024-09-19","objectID":"/chapter-3-random-intercept-models-with-covariates/:2:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 3 ：Random-intercept models with covariates","uri":"/chapter-3-random-intercept-models-with-covariates/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"3.1 Model specification An obvious model to consider for the continuous response variable, birthweight, is a multiple linear regression model (discussed in chapter 1), including smoking status and various other variables as explanatory variables or covariates. The model for the birthweight $y_ij$ of child $i$ of mother $j$ is specified as $$y_{ij} = \\beta_1+\\beta_2x_{2ij}+\\cdots+\\beta_px_{pij}+\\xi_{ij}$$ where $x_{2ij}$ through $x_{pij}$ are covariates and $\\xi _{ij}$ is a residual or error term. It may be unrealistic to assume that the birthweights of children born to the same mother are uncorrelated given the observed covariates, or in other words that the residuals $\\xi _{ij}$ and $\\xi _{i’j}$ are uncorrelated. We can therefore use the idea introduced in the previous chapter to split the total residual or error into two error components: $\\zeta _{j}$, which is shared between children of the same mother $j$, and $\\epsilon _{ij}$, which is unique for each child $ij$: $$\\xi _{ij} = \\zeta _{j} + \\epsilon _{ij}$$ Substituting for $\\xi _{ij}$ into the multiple regression model (3.1), we obtain a linear random-intercept model with covariates : $$\\begin{aligned}y_{ij}\u0026=\\quad\\beta_1+\\beta_2x_{2ij}+\\cdots+\\beta_px_{pij}+(\\zeta_j+\\epsilon_{ij})\\end{aligned}$$ $$=\\quad(\\beta_1+\\zeta_j)+\\beta_2x_{2ij}+\\cdots+\\beta_px_{pij}+\\epsilon_{ij}$$ This model can be viewed as a regression model with an added level-2 residual $\\xi_j$, or with a mother-specific intercept $\\beta_1 + \\xi_j$. The random intercept $\\xi_j$ can be considered a latent variable that is not estimated along with the fixed parameters $\\beta_1$ through $\\beta_p$, but whose variance $\\psi$ is estimated together with the variance $\\theta$ of the level-1 residuals $\\epsilon_{ij}$. The linear random-intercept model with covariates is the simplest example of a linear mixed (effects) model where there are both fixed and random effects. The random intercept or level-2 residual $\\zeta_j$ is a mother-specific error component, which remains constant across births, whereas the level-1 residual $\\epsilon_{ij}$ is a child-specific error component, which varies between children $i$ as well as mothers $j$. The $\\zeta_j$ are uncorrelated over mothers, the $\\epsilon_{ij}$ are uncorrelated over mothers and children, and the two error components are uncorrelated with each other. The mother-specific error component $\\zeta_j$ represents the combined effects of all omitted mother characteristics or unobserved heterogeneity at the mother level. (Strictly speaking, $\\zeta_j$ represents only the component of this combined effect that is uncorrelated with the covariates because of the level-2 exogeneity assumption discussed below.) If $\\zeta_j$ is positive, the total residuals for mother $j$, $\\xi_{ij}$, will tend to be positive, leading to heavier babies than predicted by the covariates. If $\\zeta_j$ is negative, the total residuals will tend to be negative. Because $\\zeta_j$ is shared by all responses for the same mother, it induces within-mother dependence among the total residuals $\\xi_{ij}$. On first reading, you may want to skip the following more technical material and go to section 3.3.5. 我们来一步一步地理解这个线性混合模型（linear mixed model），特别是它如何应用于分析婴儿出生体重这样的连续响应变量。我会尽量简化解释，让你更容易理解。 1. 第一步：理解多重线性回归模型 首先，我们有一个多重线性回归模型，用来预测婴儿的出生体重 $ y_{ij} $。这个模型包括吸烟状态和其他一些变量作为解释变量或协变量。模型可以写成： $ y_{ij} = \\beta_1 + \\beta_2 x_{2ij} + \\cdots + \\beta_p x_{pij} + \\xi_{ij} $ 这里： $ y_{ij} $ 是第 $ i $ 个婴儿（母亲 $ j $ 的孩子）的出生体重。 $ \\beta_1, \\beta_2, \\ldots, \\beta_p $ 是模型参数。 $ x_{2ij}, \\ldots, x_{pij} $ 是协变量，比如母亲的年龄、体重、是否吸烟等。 $ \\xi_{ij} $ 是残差项，也就是模型未能解释的部分。 2. 第二步：考虑同一母亲的婴儿体重的相关性 在实际中，同一母亲所生的婴儿体重可能存在相关性，因为它们共享相同的遗传和环境因素。因此，我们不能简单地假设 $ \\xi_{ij} $ 和 $ \\xi_{i’j} $（不同婴儿的残差）是不相关的。 3. 第三步：引入随机截距模型 为了解决这个问题，我们可以将残差 $ \\xi_{ij} $ 分解为两部分： $ \\zeta_j $：同一母亲 $ j $ 的所有孩子共享的残差部分。 $ \\epsilon_{ij} $：每个婴儿 $ ij $ 独有的残差部分。 这样，模型可以写成： $ \\xi_{ij} = \\zeta_j + \\epsilon_{ij} $ 将这个分解代入原始的多重线性回归模型，我们得到： $ y_{ij} = (\\beta_1 + \\zeta_j) + \\beta_2 x_{2ij} + \\cdots + \\be","date":"2024-09-19","objectID":"/chapter-3-random-intercept-models-with-covariates/:2:1","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 3 ：Random-intercept models with covariates","uri":"/chapter-3-random-intercept-models-with-covariates/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"3.2 Model assumptions We now explicitly state a set of assumptions that are sufficient for everything we want to do in this chapter but are not always necessary. For this purpose, all observed covariates for unit $i$ in cluster $j$ are placed in the vector $\\mathbf{x}_{ij}$, and the covariates for all the units in cluster $j$ are placed in the matrix $\\mathbf{X}_j$ (with rows $\\mathbf{x_i}_j’$). It is assumed that the level-1 residual $\\epsilon_{ij}$ has zero expectation or mean, given the covariates and the random intercept: $$E(\\epsilon_{ij}|\\mathbf{X}_j,\\zeta_j)=0$$ This mean-independence assumption implies that $E(\\epsilon_{ij}|\\mathbf X_j)=0$ and that Cor($\\epsilon_{ij},\\mathbf x_{ij})=0$ and Cor($\\epsilon_{ij},\\overline{\\mathbf{x}}_{.j})=0$. We call this lack of correlation between covariates and level-1 residual “level-1 exogeneity”. (See also section 1.13 on the exogeneity assumption in standard linear regression.) The random intercept $\\zeta_j$ is assumed to have zero expectation given the covariates, $$E(\\zeta_j|\\mathbf{X}_j) = 0$$ and this mean-independence assumption implies that Cor($\\zeta_j, x_{ij} $)=0 and Cor($\\zeta_j, \\overline{x}_{.j} $)=0. We call the lack of correlation between the covariates and the random intercept “level-2 exogeneity”. Violations of the exogeneity assumptions are called level-1 endogeneity and level-2 endogeneity, respectively. We assume that the level-1 residual is homoskedastic (has constant variance) for given covariates and random intercept, $$Var(\\epsilon_{ij}|X_j,\\zeta_j) = θ$$ which implies that Var($\\epsilon_{ij}$) = θ and that Cor($\\epsilon_{ij},\\zeta_j$) = 0. It is also assumed that the random intercept is homoskedastic given the covariates, $$Var(\\zeta_j|X_j) = ψ$$ which implies that $Var(\\zeta_j) = ψ.$ It is assumed that the level-1 residuals are uncorrelated for two units i and i’ (whether they are nested in the same cluster j or in different clusters j and j’) given the covariates and random intercept(s), $$\\mathrm{Cov}(\\epsilon_{ij},\\epsilon_{i^{\\prime}j^{\\prime}}|\\mathbf X_j,\\mathbf X_{j^{\\prime}},\\zeta_j,\\zeta_{j^{\\prime}}) = 0\\quad\\mathrm{if}\\quad i\\neq i^{\\prime}\\quad\\mathrm{or}\\quad j\\neq j^{\\prime}\\quad(3.7)$$ and that random intercepts are uncorrelated for different clusters j and j’ given the covariates, $$\\mathrm{Cov}(\\zeta_j,\\zeta_{j^{\\prime}}|\\mathbf X_j,\\mathbf X_{j^{\\prime}}) = 0\\quad\\mathrm{~if~} j\\neq j^{\\prime}\\quad(3.8)$$ These assumptions imply the mean and residual covariance structure of the responses described in sections 3.3.3 and 3.3.4. For (restricted) maximum likelihood estimation, normal distributions are specified for $\\epsilon_{ij}| \\mathbf X_j, \\zeta_j$ and $\\zeta_j| \\mathbf X_{-j}.$ (Together with the assumptions (3.3) and (3.5), this implies that $\\zeta_j$ and $\\epsilon_{ij}$ are independent–a stronger property than lack of correlation). Such specification of the distributions is necessary to construct the likelihood and restricted likelihood for ML and REML estimation, respectively. However, it is important to note that the estimators behave well even if the distributions are different from normal. Indeed, consistent estimation of the regression coefficients $\\beta_1,…,\\beta_p$ relies only on correct specification of the mean structure. If, additionally, the total residuals have a symmetric distribution, estimation of the regression coefficients is also unbiased. Correct specification of both the mean and covariance structure is needed only for consistent model-based standard errors, efficient estimation of the regression coefficients, and consistent estimation of $\\psi$ and $\\theta$. If the covariance structure is misspecified, robust standard errors are consistent and perform well if there are enough clusters in the data. How many clusters are needed for reliable robust standard errors? We can use our rule of thumb of 42 discussed in display 2.1, where 42 now refers to the number of clusters $minus$ the number $(q)$ of level-2 covariates, ","date":"2024-09-19","objectID":"/chapter-3-random-intercept-models-with-covariates/:2:2","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 3 ：Random-intercept models with covariates","uri":"/chapter-3-random-intercept-models-with-covariates/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"3.3 Mean structure Assumption (3.3) implies that the cluster-specific or conditional regression (averaged over $\\epsilon_{ij}$ but given $\\zeta_j$ and $X_j$) is linear: $$E(y_{ij}|X_j,\\zeta_j)=E(\\beta_1+\\beta_2x_{2ij}+\\cdots+\\beta_px_{pij})+E(\\zeta_j|X_j,\\zeta_j)+E(\\epsilon_{ij}|X_j,\\zeta_j)$$ $$=\\beta_1+\\beta_2x_{2ij}+\\cdots+\\beta_px_{pij}+\\zeta_j$$ We see that the covariates for other units in the cluster do not affect the mean response for unit $i$ once we control for the covariates $x_{ij}$ for unit $i$ and the random intercept $\\zeta_j$. The covariates $X_j$ for the cluster are then said to be “strictly exogenous given the random intercept”. It follows from $(3.4)$ that the population-averaged or marginal regression (averaged over $\\zeta_j$ and $\\epsilon_{ij}$ but given $X_j$ ) is linear: We sometimes refer to this relationship between the mean and covariates as the mean structure. 公式分解 首先，我们来看一个公式，这个公式描述了在给定随机截距 $\\zeta_j$ 和协变量 $X_j$ 的条件下，单位 $i$ 的响应变量 $y_{ij}$ 的期望值： $$ E(y_{ij}|X_j,\\zeta_j)=\\beta_1+\\beta_2x_{2ij}+\\cdots+\\beta_px_{pij}+\\zeta_j $$ 这个公式实际上是在说，对于群组 $j$ 中的单位 $i$，其响应变量 $y_{ij}$ 的平均值（或期望值）可以通过以下步骤计算得出： 固定效应部分：这部分是模型的主体，其中 $\\beta_1, \\beta_2, \\ldots, \\beta_p$ 是模型参数，$x_{2ij}, \\ldots, x_{pij}$ 是单位 $i$ 的协变量。这部分表示了在不考虑随机截距的情况下，协变量对响应变量的平均影响。 随机截距部分：这是群组 $j$ 的特定效应，反映了该群组中所有单位共享的、未被协变量捕捉到的影响。这个效应是随机的，意味着它在不同的群组之间是变化的。 公式直观理解 这个公式的核心思想是，每个单位的响应变量不仅受到其自身协变量的影响，还受到其所属群组的特定效应的影响。这种效应是“随机”的，因为它是随机分配给每个群组的，而不是由协变量直接决定的。 公式中的“严格外生性” 当我们说协变量 $X_j$ 对于随机截距是“严格外生的”，我们是在说协变量不会影响到随机截距的值。这意味着，尽管随机截距 $\\zeta_j$ 可能会影响到群组内所有单位的响应变量，但它本身并不受这些协变量的直接影响。 人口平均或边际回归 接下来，我们考虑整个人群中的响应变量 $y_{ij}$ 的期望值，这就是所谓的人口平均或边际回归： $$ E(y_{ij}|X_j) = \\beta_1 + \\beta_2x_{2ij} + \\cdots + \\beta_px_{pij} $$ 这个公式告诉我们，当我们不考虑随机截距（即，我们对所有可能的群组取平均）时，响应变量 $y_{ij}$ 的期望值只取决于协变量。这意味着，随机截距 $\\zeta_j$ 对于整个人口的平均响应没有影响。 例子：学校和学生成绩 假设我们想要研究学校环境对学生成绩的影响。我们有以下数据： 群组 $j$：学校 单位 $i$：学生 协变量 $x_{ij}$：包括学生的家庭背景、学习时间等 随机截距 $\\zeta_j$：表示每个学校特有的效应，比如学校的教学质量 在这个模型中，我们假设： 一旦我们控制了学生的家庭背景和学习时间，以及学校的教学质量，其他学生的家庭背景和学习时间就不会影响这个学生的成绩。 学生的成绩期望值只与他们自己的家庭背景、学习时间以及学校的教学质量有关。 总结 通过这些假设和公式，我们可以更好地理解线性混合模型中的固定效应和随机效应如何影响响应变量的期望值。这些假设帮助我们构建一个合理的模型，以便更准确地估计协变量对响应变量的影响。 希望这个更详细的解释能帮助你更好地理解这些概念。如果还有任何疑问，请随时提问。 ","date":"2024-09-19","objectID":"/chapter-3-random-intercept-models-with-covariates/:2:3","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 3 ：Random-intercept models with covariates","uri":"/chapter-3-random-intercept-models-with-covariates/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"3.4 Residual covariance structure It follows from assumptions (3.5) and (3.6) that total residuals or error terms are homoskedastic (having constant variance) given the covariates $X_{j}$, $$\\mathrm{Var}(\\xi_{ij}|\\mathbf{X_j}) = \\mathrm{Var}(\\zeta_j+\\epsilon_{ij}|\\mathbf{X}_j)=\\psi+\\theta $$ or, equivalently, that the responses , given the covariates, are homoskedastic, $$\\mathrm{Var}(y_{ij}|\\mathbf{X}_j)=\\psi+\\theta $$ The conditional correlation between the total residuals for any two children $i$ and $i’$ of the same mother $j$, given the covariates, also called the residual correlation, is $$\\rho\\equiv\\mathrm{Cor}(\\xi_{ij},\\xi_{i^{\\prime}j}|\\mathbf{X}_j) = \\frac\\psi{\\psi+\\theta}$$ where $\\psi $ is the corresponding covariance. Thus, $\\rho$ is also the conditional or residual intraclass correlation of responses $y_{ij} $ and $y_{i’j’}$ given the covariates: $$\\rho\\equiv\\mathrm{Cor}(y_{ij},y_{i’j}|\\mathbf{X}_j) = \\frac{\\psi}{\\psi+\\theta}$$ It is important to distinguish between the intraclass correlation in a model not containing any covariates—sometimes called the unconditional intraclass correlation—and the conditional or residual intraclass correlation in a model containing covariates. The residual covariance structure is shown in matrix form in display 3.2. 让我们深入探讨同质性（Homoscedasticity）和残差相关性（Residual Correlation）的概念，并用更详细的例子来说明。 同质性（Homoscedasticity）的详细解释 同质性是指在给定协变量的条件下，不同观测值的误差项具有相同的方差。这在统计模型中是一个重要的假设，因为它允许我们对模型的误差项进行一致的解释。 公式的详细分解 $$ \\text{Var}(\\xi_{ij}|\\mathbf{X}j) = \\text{Var}(\\zeta_j+\\epsilon{ij}|\\mathbf{X}j) = \\psi + \\theta $$ 这个公式告诉我们，给定协变量 $\\mathbf{X}j$ 后，总残差 $\\xi{ij}$ 的方差等于随机截距 $\\zeta_j$ 的方差 $\\psi$ 加上一级残差 $\\epsilon{ij}$ 的方差 $\\theta$。 $\\text{Var}(\\xi_{ij}|\\mathbf{X}j)$：这是给定协变量后，总残差 $\\xi{ij}$ 的方差。 $\\text{Var}(\\zeta_j+\\epsilon_{ij}|\\mathbf{X}_j)$：这是给定协变量后，随机截距和一级残差之和的方差。 $\\psi + \\theta$：这是两个方差的和，表示总的方差。 残差相关性（Residual Correlation）的详细解释 残差相关性描述了在给定协变量的条件下，同一个群组内不同观测值的残差之间的相关性。 公式的详细分解 $$ \\rho \\equiv \\text{Cor}(\\xi_{ij}, \\xi_{i’j}|\\mathbf{X}_j) = \\frac{\\psi}{\\psi + \\theta} $$ 这个公式告诉我们，同一个母亲 $j$ 的两个孩子 $i$ 和 $i’$ 的总残差之间的相关性是由随机截距的方差 $\\psi$ 与总方差（$\\psi + \\theta$）的比值决定的。 $\\rho$：这是残差之间的条件相关性。 $\\text{Cor}(\\xi_{ij}, \\xi_{i’j}|\\mathbf{X}_j)$：这是给定协变量后，两个孩子的总残差之间的相关性。 $\\frac{\\psi}{\\psi + \\theta}$：这是随机截距方差与总方差的比值，它决定了残差之间的相关性。 无条件与条件内类相关性的区别 无条件内类相关性：这是在不考虑任何协变量的情况下，群组内观测值之间的相关性。它反映了群组内成员之间的天然联系。 条件内类相关性：这是在控制了协变量后，群组内观测值之间的相关性。它反映了在考虑了已知因素后，群组内成员之间仍然存在的相关性。 让我们通过一个具体的例子来深入理解同质性和残差相关性的概念。 例子：学校和学生考试成绩 假设我们正在进行一项研究，目的是了解不同学校环境对学生考试成绩的影响。我们的数据集包括来自多个学校的学生，每个学生都有自己的考试成绩和一些相关的协变量，比如家庭背景、学习时间等。此外，我们还考虑了每个学校可能具有的独特影响，这可能包括学校的教育资源、教师质量等因素。 数据结构 群组 $j$：学校 单位 $i$：学生 协变量 $\\mathbf{X}_j$：包括学生的家庭背景、学习时间等 随机截距 $\\zeta_j$：表示每个学校特有的效应，比如学校的教学质量 一级残差 $\\epsilon_{ij}$：表示每个学生特有的随机波动 模型设定 我们构建了一个线性混合模型来分析数据： $$ y_{ij} = \\beta_0 + \\beta_1 x_{1ij} + \\ldots + \\beta_p x_{pij} + \\zeta_j + \\epsilon_{ij} $$ 其中： $ y_{ij} $ 是第 $ i $ 个学生在第 $ j $ 所学校的考试成绩。 $ x_{1ij}, \\ldots, x_{pij} $ 是第 $ i $ 个学生的协变量。 $ \\beta_0, \\ldots, \\beta_p $ 是模型参数。 $ \\zeta_j $ 是第 $ j $ 所学校的随机截距。 $ \\epsilon_{ij} $ 是第 $ i $ 个学生的成绩误差。 同质性的应用 在模型中，我们假设给定协变量 $ \\mathbf{X}_j $ 后，所有学生的考试成绩残差的方差是恒定的，即同质性。这意味着，无论我们观察哪个学生或哪所学校，只要协变量相同，残差的方差就保持不变。 $$ \\text{Var}(\\xi_{ij}|\\mathbf{X}j) = \\text{Var}(\\zeta_j+\\epsilon{ij}|\\mathbf{X}_j) = \\psi + \\theta $$ 这个假设允许我们对模型的误差项进行一致的解释，并且使得模型更加可靠。 残差相关性的解释 我们还假设同一个学校内不同学生的成绩残差之间存在一定的相关性，这种相关性是由学校的未观测到的特征（如教学质量）所导致的。 $$ \\rho \\equiv \\text{Cor}(\\xi_{ij}, \\xi_{i’j}|\\mathbf{X}_j) = \\frac{\\psi}{\\psi + \\theta} $$ 这里，$ \\rho $ 表示同一个学校内两个学生成绩残差之间的条件相关性。这个相关性是由随机截距 $ \\zeta_j $ 的方差 $ \\psi $ 与总方差（$ \\psi + \\theta $）的比值决定的。 无条件与条件内类相关性的比较 无条件内类相关性：如果我们不考虑任何协变量，只是简单地观察同一个学校内学生的成绩，我们可能会发现学生成绩之间存在一定的天然联系，这就是无条件内类相关性。 条件内类相关性：当我们在模型中控制了学生的协变量（如家庭背景、学习时间）后，我们仍然发现同一个学校内学生的成绩之间存在相关性，这就是条件内类相关性。这种相关性更能反映学校环境对学生成绩的实际影响。 总结 通过这些假设和公式，我们可以更好地理解线性混合模型中的固定效应和随机效应如何影响响应变量的方差和相关性。这些假设帮助我们构建一个合理的模型，以便更准确地估计协变量对响应变量的影响。 希望这个更详细的解释能帮助你更好地理解这些概念。如果还有任何疑问，请随时提问。 ","date":"2024-09-19","objectID":"/chapter-3-random-intercept-models-with-covariates/:2:4","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 3 ：Random-intercept models with covariates","uri":"/chapter-3-random-intercept-models-with-covariates/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"3.5 Graphical illustration of random-intercept model A graphical illustration of the random-intercept model, $y_{ij} = \\beta_1 +\\beta_2 x_{ij} + \\zeta_j + \\epsilon_{ij}$, with a single covariate $x_{ij}$ is given in figure 3.1. Figure 3.1: Illustration of random-intercept model for one mother\rThe bottom line is the population-averaged line, averaged across all mothers, with intercept $\\beta_1$ and slope $\\beta_2$. Mother $j$ ’s regression line has a positive random intercept $\\zeta_j$ and therefore a regression line that is above the population-averaged line and parallel to it. Another graphical illustration of this model with distributional assumptions is given in figure 3.2. Figure 3.2: Illustration of random-intercept model for one mother\rHere the solid line is $E(y_{ij}|x_{ij}) = \\beta_1 + \\beta_2 x_{ij}$, the population-averaged regression line for the population of all mothers j. The normal density curve centered on this line represents the random-intercept distribution with variance $\\psi$ and the hollow circle represents a realization $\\zeta_j$ from this distribution for mother j (this could have been placed anywhere along the line). This negative random intercept $\\zeta_j$ produces the dotted mother-specific regression line $E(y_{ij}|x_{ij},\\zeta_j)=(\\beta_1+\\zeta_j)+\\beta_2 x_{ij}$. This line is parallel to and below the population-averaged regression line. For a mother with a positive $\\zeta_j$, the mother-specific regression line would be parallel to and above the population-averaged regression line. Observed responses $y_{ij}=(\\beta_1+\\zeta_j)+\\beta_2 x_{ij}+\\epsilon_{ij}$ are shown for two values of $x_{ij}$. The responses are sampled from the two normal distributions [with means $(\\beta_1+\\zeta_j)+\\beta_2 x_{ij}$ and variance $\\theta$] shown on the dotted line. 首先，我们从基础的概念开始。 总体平均回归线 (Population-Averaged Regression Line) 首先，我们有一个公式： $$ E(y_{ij}|x_{ij}) = \\beta_1 + \\beta_2 x_{ij} $$ 这个公式表示的是，对于所有的母亲（用 j 表示），在给定她们的某个特征 $x_{ij}$（比如年龄、收入等）的情况下，她们的某个结果 $y_{ij}$（比如孩子的身高、体重等）的期望值（平均值）。 $\\beta_1$ 是截距，即使 $x_{ij}$ 为0时，$y_{ij}$ 的期望值。 $\\beta_2$ 是斜率，表示 $x_{ij}$ 每增加一个单位，$y_{ij}$ 期望值增加的量。 2. 随机截距分布 (Random-Intercept Distribution) 在这个模型中，我们假设每个母亲都有一个随机的截距 $\\zeta_j$，这个截距是从某个分布中随机抽取的，通常假设是从正态分布中抽取，其方差为 $\\psi$。 $\\psi$ 表示随机截距的方差，也就是不同母亲之间截距的变异程度。 3. 个体特定的回归线 (Mother-Specific Regression Line) 对于每个母亲 j，她的个体特定的回归线可以表示为： $$ E(y_{ij}|x_{ij},\\zeta_j) = (\\beta_1 + \\zeta_j) + \\beta_2 x_{ij} $$ 这里，$\\zeta_j$ 是随机截距，它使得每个母亲的回归线与总体平均回归线平行，但位置不同。 如果 $\\zeta_j$ 是负的，那么个体特定的回归线就会低于总体平均回归线。 如果 $\\zeta_j$ 是正的，那么个体特定的回归线就会高于总体平均回归线。 4. 观察到的响应 (Observed Responses) 实际观察到的数据 $y_{ij}$ 不仅仅是由回归线决定的，还受到随机误差 $\\epsilon_{ij}$ 的影响： $$ y_{ij} = (\\beta_1 + \\zeta_j) + \\beta_2 x_{ij} + \\epsilon_{ij} $$ $\\epsilon_{ij}$ 通常假设是从正态分布中抽取，其方差为 $\\theta$。 5. 正态分布和观察值 对于每个 $x_{ij}$ 的值，观察到的 $y_{ij}$ 值是从具有特定均值和方差的正态分布中抽取的。均值由个体特定的回归线决定，方差 $\\theta$ 通常认为是固定的。 例子 假设： $\\beta_1 = 10$ $\\beta_2 = 2$ $\\psi = 4$（随机截距的方差） $\\theta = 1$（误差的方差） 对于一个特定的母亲，假设她的随机截距 $\\zeta_j = -2$，则她的个体特定回归线为： $$ E(y_{ij}|x_{ij},\\zeta_j) = (10 - 2) + 2 x_{ij} = 8 + 2 x_{ij} $$ 如果 $x_{ij} = 3$，则她的预测值是： $$ E(y_{ij}|x_{ij},\\zeta_j) = 8 + 2 \\times 3 = 14 $$ 实际观察到的值 $y_{ij}$ 可能会因为随机误差 $\\epsilon_{ij}$ 而略有不同，比如 $y_{ij} = 14 + \\epsilon_{ij}$。 Figure 3.3: Illustration of different $\\psi$, $\\theta$, and the corresponding residual intraclass correlations $\\rho$\rIn the top left panel where $\\rho = 0.9$, vertical distances $|\\zeta_j|$ of lines from the population average line tend to be greater than distances $|e_{ij}|$ of responses from cluster-specific lines, whereas the opposite is true in the bottom left panel where $\\rho = 0.1$. In the right panels, the scatter of lines around the population-average line is of the same magnitude as the scatter of points around their cluster-specific lines, giving $\\rho = 0.5$. ","date":"2024-09-19","objectID":"/chapter-3-random-intercept-models-with-covariates/:2:5","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 3 ：Random-intercept models with covariates","uri":"/chapter-3-random-intercept-models-with-covariates/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"4 Estimation using Stata ","date":"2024-09-19","objectID":"/chapter-3-random-intercept-models-with-covariates/:3:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 3 ：Random-intercept models with covariates","uri":"/chapter-3-random-intercept-models-with-covariates/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"5 Coefficients of determination or variance explained As discussed in section 1.5 for standard linear regression (without random intercepts), the coefficient of determination, or R-squared, can be viewed as the proportional reduction in prediction error variance, comparing the model of interest with the null model that does not include any covariates. $$R^2 = \\frac{\\mathrm{TSS-SSE}}{\\mathrm{TSS}}$$ Here TSS is the sum of squared prediction errors, or residuals $y_{i}-\\bar{y}$, for the null model and SSE is the sum of squared residuals, $y_{i}-\\hat{y}_{i}$, for the model of interest. Therefore, $R^{2}$ can be written as $$R^2 = \\frac{\\sum_i(y_i-\\overline{y})^2-\\sum_i(y_i-\\widehat{y}_i)^2}{\\sum_i(y_i-\\overline{y})^2} \\approx \\frac{\\frac{\\sum_i(y_i-\\overline{y})^2}{N-1}-\\frac{\\sum_i(y_i-\\widehat{y}_i)^2}{N-p}}{\\frac{\\sum_i(y_i-\\overline{y})^2}{N-1}} = \\frac{\\widehat{\\sigma_0^2}-\\widehat{\\sigma_1^2}}{\\widehat{\\sigma_0^2}}$$ where $\\widehat{\\sigma_{1}^{2}}$ is the estimated residual variance for the model of interest and $\\widehat{\\sigma_{0}^{2}}$ is the estimated residual variance for the null model. The final quantity on the right is also called the adjusted $R$ square . We see that $R^{2}$ is approximately the proportional reduction in the estimated residual variance, where the approximation improves as $N$ increases. This idea can easily be applied to a linear random-intercept model, where the total residual variance is given by $$\\mathrm{Var}(\\zeta_j+\\epsilon_{ij}) = \\psi+\\theta $$ Snijders and Bosker (2012, chap. 7) define a coefficient of determination for this model as the proportional reduction in the estimated total residual variance comparing the model of interest with the null model without covariates. $$R^2~=~\\frac{(\\widehat{\\psi}_0+\\widehat{\\theta}_0)-(\\widehat{\\psi}_1+\\widehat{\\theta}_1)}{\\widehat{\\psi}_0+\\widehat{\\theta}_0}$$ where $\\widehat{\\psi}_0$ and $\\widehat{\\theta}_0$ are the estimates for the null model, and $\\widehat{\\psi}_1$ and $\\widehat{\\theta}_1$ are the estimates for the model of interest. 1. 决定系数 $ R^2 $ 的定义 决定系数 $ R^2 $，也称为系数的确定性，是衡量线性回归模型拟合优度的一个统计量。它表示模型中自变量对因变量变异性的解释程度。$ R^2 $ 的值通常在0到1之间，值越接近1，表示模型解释的变异性越多。 2. $ R^2 $ 的计算 在标准线性回归（没有随机截距）的情况下，$ R^2 $ 可以通过以下步骤计算： 步骤 1：计算总平方和（TSS） 总平方和（TSS）是因变量真实值与其均值之差的平方和。它衡量了如果我们没有任何自变量，只用因变量的均值来预测，会有多大的预测误差。 $$ \\text{TSS} = \\sum_{i} (y_i - \\bar{y})^2 $$ $ y_i $ 是第 $ i $ 个观测值 $ \\bar{y} $ 是所有观测值的均值 步骤 2：计算残差平方和（SSE） 残差平方和（SSE）是因变量真实值与模型预测值之差的平方和。它衡量了我们的模型预测与实际观测值之间的误差。 $$ \\text{SSE} = \\sum_{i} (y_i - \\hat{y}_i)^2 $$ $ \\hat{y}_i $ 是模型预测的第 $ i $ 个观测值 步骤 3：计算 $ R^2 $ $$ R^2 = \\frac{\\text{TSS} - \\text{SSE}}{\\text{TSS}} $$ 这个公式告诉我们，$ R^2 $ 是模型预测误差减少的比例。 3. $ R^2 $ 的另一种表达方式 $$ R^2 = \\frac{\\sum_i(y_i - \\overline{y})^2 - \\sum_i(y_i - \\hat{y}_i)^2}{\\sum_i(y_i - \\overline{y})^2} $$ 这个公式可以进一步近似为： $$ R^2 = \\frac{\\widehat{\\sigma_0^2} - \\widehat{\\sigma_1^2}}{\\widehat{\\sigma_0^2}} $$ $ \\widehat{\\sigma_0^2} $ 是空模型（没有自变量）的残差方差的估计值 $ \\widehat{\\sigma_1^2} $ 是感兴趣模型（包含自变量）的残差方差的估计值 这个公式告诉我们，$ R^2 $ 可以看作是估计的残差方差减少的比例。 4. 随机截距模型中的 $ R^2 $ 在随机截距模型中，我们考虑了不同群体（例如不同学校的学生）之间的变异性。这种模型的残差方差包括了群体内和群体间的变异性。 $$ \\text{Var}(\\zeta_j + \\epsilon_{ij}) = \\psi + \\theta $$ $ \\zeta_j $ 是随机截距，代表了第 $ j $ 个群体的效应 $ \\epsilon_{ij} $ 是随机误差，代表了第 $ j $ 个群体中第 $ i $ 个观测值的误差 5. 随机截距模型的 $ R^2 $ 计算 $$ R^2 = \\frac{(\\widehat{\\psi}_0 + \\widehat{\\theta}_0) - (\\widehat{\\psi}_1 + \\widehat{\\theta}_1)}{\\widehat{\\psi}_0 + \\widehat{\\theta}_0} $$ $ \\widehat{\\psi}_0 $ 和 $ \\widehat{\\theta}_0 $ 是空模型（没有自变量）的群体间和群体内残差方差的估计值 $ \\widehat{\\psi}_1 $ 和 $ \\widehat{\\theta}_1 $ 是感兴趣模型（包含自变量）的群体间和群体内残差方差的估计值 这个公式告诉我们，随机截距模型中的 $ R^2 $ 是模型包含自变量后，估计的总残差方差减少的比例。 6. 例子 假设我们想研究不同学校的学生考试成绩。我们有以下数据： 学校 学生 成绩 (y) A 1 60 A 2 70 A 3 80 B 1 90 B 2 100 步骤 1：计算 TSS 和 SSE 假设成绩的平均值 $ \\bar{y} $ 是 80。 $$ \\text{TSS} = (60-80)^2 + (70-80)^2 + (80-80)^2 + (90-80)^2 + (100-80)^2 $$ $$ \\text{TSS} = 400 + 100 + 0 + 100 + 400 = 1000 $$ 如果我们的随机截距模型预测成绩 $ \\hat{y}_i $ 为： 学校A的预测成绩均值为75 学校B的预测成绩均值为95 那么： $$ \\text{SSE} = (60-75)^2 + (70-75)^2 + (80-75)^2 + (90-95)^2 + (100-95)","date":"2024-09-19","objectID":"/chapter-3-random-intercept-models-with-covariates/:4:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 3 ：Random-intercept models with covariates","uri":"/chapter-3-random-intercept-models-with-covariates/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"6 Hypothesis tests and confidence intervals ","date":"2024-09-19","objectID":"/chapter-3-random-intercept-models-with-covariates/:5:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 3 ：Random-intercept models with covariates","uri":"/chapter-3-random-intercept-models-with-covariates/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"6.1 Hypothesis tests for individual regression coefficients The most commonly used hypothesis test concerns an individual regression parameter, say, $\\beta_2$ , with null hypothesis $$H_{0}:\\beta_{2}=0$$ versus the two-sided alternative $$H_a:\\beta_2\\neq0$$ Stata reports the test statistic $$z~=~\\frac{\\widehat{\\beta_2}}{\\widehat{\\mathrm{SE}}(\\widehat{\\beta}_{2})}$$ which has an asymptotic standard normal null distribution. For instance, in the output from mixed in section 3.4.2, the z statistic for the coefficient of smoke is -11.41, which gives a two-sided p-value of less than 0.001. The square of the z statistic is called a Wald statistic, which has an asymptotic $\\chi^2(1)$ distribution under the null hypothesis. The distribution has 1 degree of freedom because the null hypothesis imposes one restriction, $\\beta_2=0$. 第一步：理解假设检验 假设检验是一种统计方法，用来决定我们是否有足够的证据拒绝一个假设（零假设）。在回归分析中，我们经常测试某个变量（比如 $\\beta_2$）是否对模型有显著影响。 例子：假设你是一名老师，想要知道学生是否因为吸烟（变量 $\\beta_2$）而影响他们的考试成绩。你的零假设 $H_0$ 可能是“吸烟对考试成绩没有影响”，而备择假设 $H_a$ 是“吸烟对考试成绩有影响”。 第二步：检验统计量 Stata 计算了一个叫做 $z$ 的统计量来帮助我们决定是否拒绝零假设。 公式： $ z = \\frac{\\widehat{\\beta_2}}{\\widehat{\\mathrm{SE}}(\\widehat{\\beta}_{2})} $$ $\\widehat{\\beta}_{2}$ 是回归分析中计算出的 $\\beta_2$ 的估计值。 $\\widehat{\\mathrm{SE}}(\\widehat{\\beta_2})$ 是 $\\widehat{\\beta}_{2}$ 的标准误差。 例子：假设你发现吸烟的学生的平均考试成绩比不吸烟的学生低5分。这个5分就是 $\\widehat{\\beta}_{2}$。如果这个估计的标准误差是1分，那么 $z$ 值就是 $5 / 1 = 5$。 第三步：解释 $z$ 统计量 $z$ 值告诉我们 $\\widehat{\\beta}_{2}$ 与其标准误差相比有多大。$z$ 值越大，我们越有理由拒绝零假设。 例子：如果 $z$ 值是5，这意味着吸烟对成绩的影响比标准误差大5倍。这通常被认为是非常显著的，因为正常情况下，我们不会期望看到这么大的偏离。 第四步：Wald 统计量 Wald 统计量是 $z$ 值的平方，它遵循一个自由度为1的卡方分布。 公式： 例子：在我们的例子中，如果 $z$ 值是5，那么 Wald 统计量 $w$ 就是 $5^2 = 25$。 第五步：渐近分布的局限性 渐近分布假设随着样本量的增加，我们的估计会越来越准确。但在样本量小的情况下，这种假设可能不成立。 例子：如果你只调查了10个学生，那么得出的结论可能不太可靠。但如果调查了1000个学生，那么结论就更可信。 第六步：Kenward-Roger 近似 Kenward-Roger 近似是一种更准确的估计自由度的方法，特别是在样本量小的情况下。 例子：假设你在一个小城镇的学校进行调查，样本量不大。使用 Kenward-Roger 近似可以帮助你更准确地估计 $z$ 值或 Wald 统计量的显著性。 第七步：应用到实际数据 在实际应用中，我们通常会使用软件（如 Stata）来计算这些统计量，并根据结果做出决策。 例子：如果你是一名研究人员，你可能会使用 Stata 来分析吸烟和考试成绩之间的关系。Stata 会计算出 $z$ 值和 Wald 统计量，并告诉你是否有足够的证据拒绝零假设。 The asymptotic distributions treat the standard error as known which is a poor approximation when the number of clusters is small. For this reason, a $t$ (or $F$) distribution is sometimes used instead of the standard normal (or chi-square), where various approximations are used for the error degrees of freedom. The Kenward-Roger approximation generally appears to be preferable to the others (Kenward and Roger 1997; Schaalie, McBride, and Fellingham 2002), and can be obtained using mixed with the reml and dfmethod(kroger) options (dfmethod() was introduced in Stata 14). We now demonstrate the use of these options although the default asymptotic distributions should perform well here because there are thousands of clusters in the birthweight data ($J-q ＞＞ 42$): 渐近分布与标准误差 在统计学中，渐近分布是指当样本量变得非常大时，某些统计量的分布。在回归分析中，我们经常使用渐近分布来近似检验统计量，如 $z$ 值和 Wald 统计量。 渐近分布的局限性： 渐近分布假设标准误差（$\\widehat{\\mathrm{SE}}$）是已知的，这在样本量很大时是合理的。但是，当样本量较小，特别是聚类数量较少时，这种假设可能不准确。 例子：假设你在做一项研究，研究吸烟对小城镇中学生考试成绩的影响。如果这个城镇只有几个学校（聚类），那么每个学校的 吸烟学生数量可能很少。在这种情况下，你估计的标准误差可能不够准确。 使用 $t$ 或 $F$ 分布 由于渐近分布在小样本情况下可能不够准确，我们有时会使用 $t$ 分布或 $F$ 分布来代替标准正态分布或卡方分布。这些分布考虑了标准误差的不确定性。 例子：如果你在研究中发现吸烟学生的平均成绩比不吸烟的学生低5分，但这个估计的标准误差是1分，你可能会使用 $t$ 分布来计算 $t$ 值： $$ t = \\frac{5}{1} $$ 这将考虑标准误差的不确定性。 Kenward-Roger 近似 Kenward-Roger 近似是一种用于估计自由度的方法，它在小样本情况下通常比其他方法更准确。 例子：继续上面的例子，如果你使用 Kenward-Roger 近似来估计自由度，你可能会得到一个更准确的 $t$ 值或 $F$ 值，从而更准确地评估吸烟对考试成绩的影响。 在 Stata 中使用 Kenward-Roger 近似 在 Stata 中，你可以使用 mixed 命令和 reml 选项来请求受限最大似然估计，然后使用 dfmethod(kroger) 选项来指定 Kenward-Roger 近似来估计自由度。 例子：假设你在分析一个包含成千上万个聚类（如成千上万个学校）的大数据集。虽然在这种情况下，渐近分布可能已经足够准确，但你仍然可以选择使用 Kenward-Roger 近似来确保准确性： stata mixed y x1 x2, reml dfmethod(kroger) 这里 y 是因变量，x1 和 x2 是自变量。 总结 渐近分布在小样本情况下可能不够准确。 $t$ 分布和 $F$ 分布考虑了标准误差的不确定性，适用于小样本。 Kenward-Roger 近似是一种在小样本情况下更准确的自由度估计方法。 在 Stata 中，你可以使用 mixed 命令和相关选项来实现这些方法。 mixed birwt smoke male mage hsgrad somecoll collgrad /// married black kessner2 kessner3 novisit pretri2 pretri3 /// || momid:, reml dfmet","date":"2024-09-19","objectID":"/chapter-3-random-intercept-models-with-covariates/:5:1","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 3 ：Random-intercept models with covariates","uri":"/chapter-3-random-intercept-models-with-covariates/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"6.2 Joint hypothesis tests for several regression coefficients Consider now the null hypothesis that the regression coefficients of two covariates $x_{2i j}$ and $x_{3 i j}$ are both 0 , $$H_0:\\beta_2=\\beta_3=0$$ versus the alternative hypothesis that at least one of the parameters is nonzero. For example, for the smoking-and-birthweight application, we may want to test the null hypothesis that the quality of prenatal care (as measured by the Kessner index) makes no difference to birthweight (controlling for the other covariates), where the Kessner index is represented by two dummy variables, kessner2 and kessner3. We can also test the simultaneous hypothesis that three or more regression coefficients are all 0, but the expression for the Wald statistic becomes convoluted unless matrix expressions are used. The Wald test for the null hypothesis that the coefficients of the dummy variables kessner2 and kessner3 are both 0 can be performed using the testparm command. To base the test on robust standard errors, we first reestimate the model by using xtreg with the mle and vce(robust) options: ** 3.6.2 Joint hypothesis tests for several regression coefficients quietly xtset momid quietly xtreg birwt smoke male mage hsgrad somecoll collgrad /// married black kessner2 kessner3 novisit pretri2 pretri3, mle vce(robust) testparm kessner2 kessner3 . testparm kessner2 kessner3\r( 1) [birwt]kessner2 = 0\r( 2) [birwt]kessner3 = 0\rchi2( 2) = 23.75\rProb \u003e chi2 = 0.0000\rWe reject the null hypothesis at the 5% level with $w = 23.75$, degrees of freedom $(df)=2, p\u003c0.001$. If the number of clusters is small, with J-q \u003c 42 (see display 2.1), it is better to perform an approximate F test, which can be obtained by estimating the model using mixed with the reml and dfmethod(kroger) options and then using the testparm command with the small option. In addition to providing a finite-sample approximation to the sampling distribution, this approach also has the advantage that the standard errors perform better than ML-based or robust standard errors when the number of clusters is small. The analogous likelihood-ratio test statistic is $$L~=~2(l_1-l_0)$$ where $l_1$ and $l_0$ are now the maximized log likelihoods for the models including and excluding both kessner2 and kessner3, respectively. Under the null hypothesis, the likelihood-ratio statistic also has an asymptotic $\\chi^2(2)$ null distribution (there is no finite-sample approximation). A likelihood-ratio test of the null hypothesis that the coefficients of the dummy variables kessner2 and kessner3 are both 0 can be performed by estimating both models by maximum likelihood and then using the lrtest command: 第一步：理解问题背景 在统计分析中，我们经常需要评估模型中的某些变量是否对结果有显著影响。当我们有多个数据组（或称为“聚类”）时，我们可能会使用多级模型或混合效应模型。如果聚类的数量不多，我们就需要一种更精确的方法来评估这些变量的影响。 第二步：近似F检验 近似F检验是一种在聚类数量较少时使用的统计方法。它通过以下步骤进行： 使用mixed命令：这是一个用于估计混合效应模型的命令。mixed命令可以让我们考虑数据的层次结构或聚类结构。 reml选项：reml代表“限制最大似然”，这是一种估计模型参数的方法，它考虑了缺失数据的情况，通常用于多级模型。 dfmethod(kroger)选项：这是自由度的计算方法。Kroger方法是一种计算自由度的方法，它在小样本情况下表现更好。 testparm命令：这个命令用于测试模型中参数的显著性。当我们加上small选项时，它会提供一个针对小样本的近似F检验。 第三步：似然比检验 (1) 第一步：理解似然比检验 似然比检验是一种统计方法，用于比较两个嵌套模型（一个模型是另一个模型的特殊情况）的拟合优度。这种方法基于两个模型的对数似然值。 (2) 第二步：完全模型和缩减模型 假设我们有两个模型： 完全模型：$ y = \\beta_0 + \\beta_1 x + \\beta_2 kessner2 + \\beta_3 kessner3 + \\epsilon $ 缩减模型：$ y = \\beta_0 + \\beta_1 x + \\epsilon $ 其中： $ y $ 是观测到的结果。 $ x $ 是解释变量。 $ kessner2 $ 和 $ kessner3 $ 是我们感兴趣的两个虚拟变量。 $ \\epsilon $ 是误差项。 (3) 第三步：估计两个模型 完全模型：包含所有感兴趣的变量。 缩减模型：不包含某些感兴趣的变量。 我们需要使用最大似然估计来估计这两个模型。 (4) 第四步：计算对数似然值 对于每个模型，我们计算它的对数似然值： $ l_1 $ 是完全模型的最大对数似然值。 $ l_0 $ 是缩减模型的最大对数似然值。 (5) 第五步：似然比统计量的计算 似然比统计量 $ L $ 由下式给出： $$ L = 2(l_1 - l_0) $$ 这里： $ l_1 $ 是完全模型的对数似然值。 $ l_0 $ 是缩减模型的对数似然值。 (6) 第六步：零假设和备择假设 在似然比检验中，我们通常测试的零假设是： $ H_0 $：两个模型没有显著差异，即 $ \\beta_2 = \\beta_3 = 0 $。 备择假设是： $ H_1 $：两个模型有显著差异，即 $ \\beta_2 \\neq 0 $ 或 $ \\beta_3 \\neq 0 $。 (7) 第七步：卡方分布 在零假设下，似然比统计量 $ L $ 遵循自由度为2的卡方分布（$\\chi^2(2)$）。这是因为我们在比较两个模型时，完全模型比缩减模型多出了两个参数（$ \\beta_2 $ 和 $ \\beta_3 $）。 (8) 第八步：执行检验 计算 $ L $ 值：首先计算 $ L = 2(l_1 ","date":"2024-09-19","objectID":"/chapter-3-random-intercept-models-with-covariates/:5:2","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 3 ：Random-intercept models with covariates","uri":"/chapter-3-random-intercept-models-with-covariates/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"6.3 Predicted means and confidence intervals We can use the margins command to obtain predicted means for mothers and pregnancies with particular covariate values, for example different combinations of education level and smoking status. If we evaluate the other covariates at particular values of our choice, we obtain adjusted means, called adjusted predictions in Stata. Alternatively, we can obtain what Stata calls predictive margins, the mean birthweight we would obtain if the distributions of the other covariates were the same for all combinations of education and smoking status. As mentioned in section 1.7, in linear models, predictive margins can be obtained by evaluating the other covariates at their means. The margins command works only if factor notation is used for the categorical variables for which we want to make predictions (education and smoking status). We therefore define a categorical variable for level of education with appropriate value labels. generate education = hsgrad*1 + somecoll*2 + collgrad*3 label define ed 0 \"No HS Degree\" 1 \"HS Degree\" 2 \"Some Coll\" 3 \"College\", replace label values education ed and refit the model, declaring education and smoke as categorical variables by using i.education and i.smoke: quietly xtset momid quietly xtreg birwt i.smoke male mage i.education married black /// kessner2 kessner3 novisit pretri2 pretri3, mle vce(robust) We then use the margins command to obtain predictive margins for all combinations of smoke and education: margins i.smoke#i.education Predictive margins Number of obs = 8,604\rModel VCE: Robust\rExpression: Linear prediction, predict()\r-----------------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. z P\u003e|z| [95% conf. interval]\r------------------------+----------------------------------------------------------------\rsmoke#education |\rNonsmoker#No HS Degree | 3430.92 24.67 139.07 0.000 3382.56 3479.27\rNonsmoker#HS Degree | 3487.76 13.82 252.32 0.000 3460.67 3514.86\rNonsmoker#Some Coll | 3511.60 13.77 254.99 0.000 3484.61 3538.59\rNonsmoker#College | 3521.75 12.00 293.42 0.000 3498.22 3545.27\rSmoker#No HS Degree | 3212.59 26.55 121.01 0.000 3160.55 3264.62\rSmoker#HS Degree | 3269.43 20.49 159.59 0.000 3229.28 3309.59\rSmoker#Some Coll | 3293.27 21.87 150.59 0.000 3250.41 3336.14\rSmoker#College | 3303.42 22.05 149.83 0.000 3260.21 3346.63\r-----------------------------------------------------------------------------------------\rHere the interaction syntax i.smoke#i.education was used to specify that we want predictions for all combinations of the values of smoke and education. The standard errors and confidence intervals for the predictions are based on the estimated standard errors from the random intercept model. We can plot these predictive margins with confidence intervals by using marginsplot (available from Stata 12). marginsplot, xdimension(education) ","date":"2024-09-19","objectID":"/chapter-3-random-intercept-models-with-covariates/:5:3","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 3 ：Random-intercept models with covariates","uri":"/chapter-3-random-intercept-models-with-covariates/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"6.4 Hypothesis test for random-intercept variance Consider testing the null hypothesis that the between-cluster variance is 0: $$H_0:\\psi=0\\quad\\mathrm{against}\\quad H_a:\\psi\u003e0$$ This null hypothesis is equivalent to the hypothesis that $\\zeta_{j}=0$ or that there is no random intercept in the model. If this is true, a multilevel model is not required. Likelihood-ratio tests are typically used with the test statistic $$L = 2(l_1-l_0)$$ where $l_{1}$ is the maximized log likelihood for the random-intercept model (which includes $\\zeta_j$ ) and $l_0$ is the maximized log likelihood for an ordinary regression model (without $\\zeta_j$ ). This test is also valid if the restricted log likelihood is used. A correct p-value is obtained by dividing the naive p-value based on the $\\chi^2(1)$ by 2, as was discussed in more detail in section 2.6.2 . When ML or REML estimation has been used with model-based standard errors (not the vce(robust) option), the result for the correct test procedure is provided in the last row of output from xtmixed and mixed , where the notation chi2bar2(01) indicates that the p value has been divided by 2. You can see this in section 3.6.1 where we fit the full model by using mixed with the reml option and obtained L = 1120 and p \u003c 0.001. Alternative tests for the random-intercept variance were described in section 2.6.2 . ","date":"2024-09-19","objectID":"/chapter-3-random-intercept-models-with-covariates/:5:4","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 3 ：Random-intercept models with covariates","uri":"/chapter-3-random-intercept-models-with-covariates/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"7 Between and within effects of level-1 covariates We now turn to the estimated regression coefficients for the random-intercept model with covariates. For births where the mother smoked during the pregnancy, the population mean birthweight is estimated to be 218 grams lower than for births where the mother did not smoke, holding all other covariates constant. This estimate represents either a comparison between children of different mothers, one of whom smoked during the pregnancy and one of whom did not (holding all other covariates constant), or a comparison between children of the same mother, where the mother smoked during one pregnancy and not during the other (holding all other covariates constant). This is neither purely a between-mother comparison (because smoking status can change between pregnancies) nor purely a within-mother comparison (because some mothers either smoke or do not smoke during all their pregnancies). ","date":"2024-09-19","objectID":"/chapter-3-random-intercept-models-with-covariates/:6:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 3 ：Random-intercept models with covariates","uri":"/chapter-3-random-intercept-models-with-covariates/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"7.1 Between-mother effects If we wanted to obtain purely between-mother effects of the covariates, we could average the response and covariates for each mother $j$ over children and perform the regression on the resulting cluster means. $$\\frac{1}{n_{j}}\\sum_{i=1}^{n_{j}}y_{ij}\\quad=\\quad\\frac{1}{n_{j}}\\sum_{i=1}^{n_{j}}(\\beta_{1}+\\beta_{2}x_{2ij}+\\cdots+\\beta_{p}x_{pij}+\\zeta_{j}+\\epsilon_{ij})$$ or $$\\begin{array}{rcl}\\overline y_{\\cdot j}\u0026=\u0026\\beta_1+\\beta_2\\overline x_{2\\cdot j}+\\cdots+\\beta_p\\overline x_{p\\cdot j}+\\zeta_j+\\overline{\\epsilon}_{\\cdot j}\\end{array}\\quad(3.11)$$ Here $\\bar y_{j}$ is the mean response for mother $j$, $\\bar x_{2, j}$ is the mean of the first covariate smoke for mother $j$, etc., and $\\bar \\varepsilon_{j}$ is the mean of the level-1 residuals in the original regression model $(3.2)$ . The error term $\\zeta_j+\\bar \\varepsilon_{j}$ has population mean $E\\left(\\zeta_j+\\bar \\varepsilon_{j}\\right)=0$ and is heteroskedastic with variance $\\operatorname{Var}\\left(\\zeta_j+\\bar \\varepsilon_{j}\\right)=\\psi+\\theta / n_{j}$. Any information on the regression coefficients from within-mother variability is eliminated, and the coefficients of covariates that do not vary between mothers are absorbed by the intercept. Ordinary least-squares (OLS) estimates $\\hat{\\beta}^{\\mathrm{B}}$ of the parameters $\\beta$ in the between regression $(3.11)$ whose corresponding covariates vary between mothers (here all covariates) can be obtained using xtmeg with the be (between) option (robust standard errors not available): 随机截距模型 随机截距模型是一种混合效应模型，它考虑了数据的层次结构。在这个例子中，层次结构是母亲和孩子。每个母亲可以看作是一个群体，而她的孩子是这个群体中的个体。 母亲间效应（Between-mother effects） 目标： 我们想要研究不同母亲之间的差异，即不考虑同一个母亲在不同怀孕期的变化。 方法： 为了实现这一点，我们对每个母亲 ( j ) 的所有孩子的响应变量（比如出生体重）和协变量（比如母亲是否吸烟）取平均值。 公式分解： 原始模型： $$ y_{ij} = \\beta_1 + \\beta_2 x_{2ij} + \\cdots + \\beta_p x_{pij} + \\zeta_j + \\epsilon_{ij} $$ $ y_{ij} $ 是第 $ j $ 个母亲第 $ i $ 个孩子的出生体重。 $ x_{2ij}, \\ldots, x_{pij} $ 是第 $ j $ 个母亲第 $ i $ 个孩子的协变量（比如是否吸烟）。 $ \\beta_1, \\beta_2, \\ldots, \\beta_p $ 是回归系数。 $ \\zeta_j $ 是随机截距，表示第 $ j $ 个母亲特有的效应。 $ \\epsilon_{ij} $ 是误差项。 平均值模型： $$ \\overline y_{\\cdot j} = \\beta_1 + \\beta_2 \\overline x_{2\\cdot j} + \\cdots + \\beta_p \\overline x_{p\\cdot j} + \\zeta_j + \\overline \\epsilon_{\\cdot j} $$ $ \\overline y_{\\cdot j} $ 是第 $ j $ 个母亲所有孩子的出生体重的平均值。 $ \\overline x_{2\\cdot j}, \\ldots, \\overline x_{p\\cdot j} $ 是第 $ j $ 个母亲所有孩子的协变量的平均值。 $ \\overline \\epsilon_{\\cdot j} $ 是第 $ j $ 个母亲所有孩子的误差项的平均值。 误差项的方差： $$ \\operatorname{Var}(\\zeta_j + \\overline \\epsilon_{\\cdot j}) = \\psi + \\theta / n_{j} $$ $ \\psi $ 是随机截距 $ \\zeta_j $ 的方差。 $ \\theta $ 是误差项 $ \\epsilon_{ij} $ 的方差。 $ n_{j} $ 是第 $ j $ 个母亲的孩子数量。 解释： 这个模型消除了来自同一个母亲不同怀孕期变化的信息，只考虑了不同母亲之间的差异。 如果某些协变量在不同母亲之间没有变化，那么这些协变量的系数会被截距吸收，因为它们无法在模型中进一步解释变异性。 普通最小二乘（OLS）估计： 使用 xtmeg 命令的 be（between）选项可以估计参数 $ \\beta $ 的值，这里的协变量在不同母亲之间是变化的。 例子： 假设我们有3个母亲，每个母亲有2个孩子，数据如下： 母亲 孩子 出生体重 (g) 母亲吸烟 1 1 3400 否 1 2 3500 否 2 1 3200 是 3 1 3100 是 3 2 3000 否 计算平均值： 母亲1：平均出生体重 = (3400 + 3500) / 2 = 3450g，平均吸烟 = 0（都不吸烟） 母亲2：平均出生体重 = 3200g，平均吸烟 = 1（吸烟） 母亲3：平均出生体重 = (3100 + 3000) / 2 = 3050g，平均吸烟 = 0.5（一个吸烟，一个不吸烟） 回归分析： 使用这些平均值进行回归分析，估计吸烟对出生体重的影响。 quietly xtset momid xtreg birwt smoke male mage hsgrad somecoll collgrad married black kessner2 /// kessner3 novisit pretri2 pretri3, be Between regression (regression on group means) Number of obs = 8,604\rGroup variable: momid Number of groups = 3,978\rR-squared: Obs per group:\rWithin = 0.0299 min = 2\rBetween = 0.1168 avg = 2.2\rOverall = 0.0949 max = 3\rF(13,3964) = 40.31\rsd(u_i + avg(e_i.)) = 424.7306 Prob \u003e F = 0.0000\r------------------------------------------------------------------------------\rbirwt | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\rsmoke | -286.15 23.23 -12.32 0.000 -331.68 -240.61\rmale | 104.94 19.50 5.38 0.000 66.72 143.16\rmage | 4.40 1.51 2.92 0.003 1.45 7.35\rhsgrad | 58.81 25.51 2.30 0.021 8.79 108.83\rsomecoll | 85.07 28.13 3.02 0.003 29.91 140.23\rcollgrad | 99.88 29.3","date":"2024-09-19","objectID":"/chapter-3-random-intercept-models-with-covariates/:6:1","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 3 ：Random-intercept models with covariates","uri":"/chapter-3-random-intercept-models-with-covariates/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"7.2 Within-mother effects If we wanted to obtain purely within-mother effects, we could subtract the between-mother regression (3.11) from the original model (3.2) to obtain the within model. $$y_{ij}-\\overline y_{\\cdot j} = \\beta_2(x_{2ij}-\\overline x_{2\\cdot j})+\\cdots+\\beta_p(x_{pij}-\\overline x_{p\\cdot j})+\\epsilon_{ij}-\\overline{\\epsilon}_{\\cdot j}\\quad (3.12)$$ Here the response and all covariates have simply been centered around their respective cluster means. The error term $\\varepsilon_{i j}-\\bar \\varepsilon_{: j}$ has population mean $E(\\varepsilon_{i j}-\\bar \\varepsilon_{: j})=0$ and is heteroskedastic with variance Var$(\\varepsilon_{i j}-\\bar{\\varepsilon}_{: j})=\\theta(1-1 / n_j)$. Covariates that do not vary within clusters drop out of the equation because the mean-centered covariate is 0. Importantly, the random intercept $\\zeta_j$ drops out for the same reason. OLS can be used to estimate the within effects $\\beta^W$ in $(3.12)$ . The standard errors of the estimated coefficients of covariates that vary little within clusters will be large because estimation is solely based on the within-cluster variability. Identical estimates of within-mother effects can be obtained by replacing the random intercept $\\zeta_j$ for each mother in the original model in $(3.2)$ by a fixed intercept $\\alpha_j$. This could be accomplished using dummy variables for each mother and omitting the intercept $\\beta_1$ so that $\\alpha_j$ represents the total intercept for mother $j$, previously represented by $\\beta_1+\\zeta_j$. Letting $d_{kj}$ be the dummy variable for the kth mother, ($k=1,\\ldots,3,978$), equal to 1 if $j=k$ and 0 otherwise. Then the fixed-effects model can be written as $$\\begin{aligned}y_{ij}\u0026=\\quad\\beta_2x_{2ij}+\\cdots+\\beta_px_{pij}+\\sum_{k=1}^{3,978}d_{kj}\\alpha_k+\\epsilon_{ij}\\\u0026=\\quad\\beta_2x_{2ij}+\\cdots+\\beta_px_{pij}+\\alpha_j+\\epsilon_{ij}\\end{aligned}$$ The text in the image is as follows: and estimated by OLS. In this model, all mother-specific effects are accommodated by $\\alpha_{j}$, leaving only within-mother variation to be explained by covariates. The coefficients of level-2 covariates can therefore not be estimated, which can also be seen by considering that the set of dummy variables is collinear with any such covariates. For example, black is the sum of the dummy variables for all Black mothers. In practice, it is more convenient to eliminate the intercepts by mean-centering all covariates, as in $(3.12)$, instead of estimating 3,978 intercepts. The within estimates $\\hat{\\beta}^{W}$ for the coefficients of covariates that vary within mothers can be obtained using xtmixed with the fe (fixed effects) option: 我们的目标是研究同一个母亲在不同怀孕期的变化，即同一个母亲在不同孩子之间的差异。 方法 为了实现这个目标，我们可以从原始模型中减去母亲间效应的回归模型，得到母亲内部效应的模型。 原始模型（模型 3.2） 原始模型是： $$ y_{ij} = \\beta_1 + \\beta_2 x_{2ij} + \\cdots + \\beta_p x_{pij} + \\zeta_j + \\epsilon_{ij} $$ $ y_{ij} $ 是第 $ j $ 个母亲第 $ i $ 个孩子的出生体重。 $ x_{2ij}, \\ldots, x_{pij} $ 是第 $ j $ 个母亲第 $ i $ 个孩子的协变量（比如是否吸烟）。 $ \\beta_1, \\beta_2, \\ldots, \\beta_p $ 是回归系数。 $ \\zeta_j $ 是随机截距，表示第 $ j $ 个母亲特有的效应。 $ \\epsilon_{ij} $ 是误差项。 母亲间效应的回归模型（模型 3.11） 母亲间效应的回归模型是： $$ \\overline y_{\\cdot j} = \\beta_1 + \\beta_2 \\overline x_{2\\cdot j} + \\cdots + \\beta_p \\overline x_{p\\cdot j} + \\zeta_j + \\overline \\epsilon_{\\cdot j} $$ $ \\overline y_{\\cdot j} $ 是第 $ j $ 个母亲所有孩子的出生体重的平均值。 $ \\overline x_{2\\cdot j}, \\ldots, \\overline{x}_{p\\cdot j} $ 是第 $ j $ 个母亲所有孩子的协变量的平均值。 $ \\overline \\epsilon_{\\cdot j} $ 是第 $ j $ 个母亲所有孩子的误差项的平均值。 母亲内部效应的回归模型（模型 3.12） 通过从原始模型中减去母亲间效应的回归模型，我们得到： $$ y_{ij} - \\overline y_{\\cdot j} = \\beta_2(x_{2ij} - \\overline x_{2\\cdot j}) + \\cdots + \\beta_p(x_{pij} - \\overline x_{p\\cdot j}) + \\epsilon_{ij} - \\overline \\epsilon_{\\cdot j} $$ $ y_{ij} - \\overline y_{\\cdot j} $ 是第 $ j $ 个母亲第 $ i $ 个孩子的出生体重与该母亲所有孩子的出生体重平均值的差。 $ x_{2ij} - \\overline x_{2\\cdot j}, \\ldots, x_{pij} - \\overline x_{p\\cdot j} $ 是第 $ j $ 个母亲第 $ i $ 个孩子的协变量与该母亲所有孩子的协变量平均值的差。 $ \\epsilon_{ij} - \\overline \\epsilon_{\\cdot j} $ 是第 $ j $ 个母亲第 $ i $ 个孩子的误差项与该母亲所","date":"2024-09-19","objectID":"/chapter-3-random-intercept-models-with-covariates/:6:2","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 3 ：Random-intercept models with covariates","uri":"/chapter-3-random-intercept-models-with-covariates/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"8 Fixed versus random effects revisited In section 2.8, we discussed whether the effects of clusters should be treated as random or fixed in models without covariates, and we revisit the issues involved in making this decision in table 3.3. We have previously argued that the decision depends on whether inferences are for the population of clusters or only for the clusters included in the sample. This appears as the first question in table 3.3. To make inferences regarding the population of clusters (or the datagenerating mechanism for the clusters), the random-effects model must be used. Turning to the next question in the table, the random-effects model allows estimation of coefficients of cluster-level covariates, unlike the fixed好的，以下是图片中的英文原文： effects model. However, these inferences require a sufficient number of clusters and assumptions regarding the random-intercept distribution. The most important of these assumptions that is relaxed by the fixed-effects approach is level-2 exogeneity of level-1 covariates (lack of correlation between $\\zeta_j$ and $\\overline{x}_{j}$ or no cluster-level unobserved confounding). Another assumption made by the random-effects approach is that $\\zeta_j$ has a constant variance $\\psi$, although this assumption can be relaxed using robust standard errors if there are enough clusters. The standard assumption of a normal distribution for $\\zeta_j$ is actually not required for consistent estimation of regression coefficients. The main reason for using a fixed-effects approach is to estimate the within-cluster effects of covariates, free from bias due to unobserved cluster-level confounding. Although this endogeneity problem can be addressed using a random-effects model that also includes the cluster means of level-1 covariates, this approach yields inconsistent estimates of parameters pertaining to the between-cluster relationships. Fortunately, there is no endogeneity problem due to omitted covariates when estimating a treatment or intervention effect in a randomized experiment. In section 2.10.3, we stressed that the standard error for the estimator of the mean $\\beta$ in a random-effects model without covariates is for the population of clusters (allowing for random sampling of clusters), whereas the standard error in the corresponding fixed-effects model is for the particular clusters at hand. Generalization to the population of clusters is clearly a necessity for inferences regarding the coefficients of cluster-level covariates (which require a random-effects model). Should we worry about the standard errors from the fixed effects model because they pertain only to the clusters that happen to be in the sample? Not if the coefficients of the level-1 covariates are assumed to be identical across clusters (as they have been in this chapter). In this case, the same relationship is assumed to hold for other clusters not included in the sample and generalizes in that sense. We will see in the next chapter that the random-effect approach allows the assumption of identical within-cluster relationships to be relaxed by including random slopes. While the cluster-specific intercepts are model parameters in the fixed-effects model and can be estimated via the dummy-variable approach, they are just random residuals in the random-effects model, and only their variance is estimated as a model parameter. However, the $c_{j}$ can be predicted by empirical Bayes (EB) after estimating the model parameters. If inference regarding the cluster-specific effects are of interest, cluster sizes should not be too small, and the random-effects approach performs better with small cluster sizes than the fixed-effects approach because of shrinkage or partial pooling as discussed in section 2.11.2. For inferences regarding regression coefficients, there are no particular requirements regarding cluster sizes in either fixed or random-effects models. In fixed-effects models, singletons clusters (of size 1) provide no infor","date":"2024-09-19","objectID":"/chapter-3-random-intercept-models-with-covariates/:7:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 3 ：Random-intercept models with covariates","uri":"/chapter-3-random-intercept-models-with-covariates/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"第一部分：模型选择的决策 是否将群体效应视为随机或固定： 在没有协变量的模型中，决定是将群体效应视为随机还是固定，取决于推断的目标是针对群体总体还是仅针对样本中的群体。 如果目标是针对群体总体（或群体的数据生成机制），则必须使用随机效应模型。 例子：假设你是一位教育研究员，想要研究不同学校的学生数学成绩。每个学校都有其独特的文化和教学方法，这可能会影响学生的成绩。如果你只关心从你研究的几所学校中得到的结果，那么固定效应模型可能更合适。但如果你想从这几所学校推断出所有学校的情况，那么随机效应模型将更适合。 决策依据：选择哪种模型取决于你的目标。如果你的目标是推广到所有可能的学校（即群体总体），那么随机效应模型是必要的。 ","date":"2024-09-19","objectID":"/chapter-3-random-intercept-models-with-covariates/:7:1","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 3 ：Random-intercept models with covariates","uri":"/chapter-3-random-intercept-models-with-covariates/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"第二部分：模型的特点和应用 随机效应模型： 允许估计群体水平协变量的系数，这是固定效应模型所不允许的。 这些推断需要足够数量的群体，并假设随机截距分布的某些条件。最重要的假设是一级协变量的二级外生性（即 $\\zeta_j$ 与 $\\overline{x}_j$ 之间没有相关性，或者没有群体水平的未观测混杂因素）。 随机效应方法的另一个假设是 $\\zeta_j$ 具有恒定的方差 $\\psi$，但如果有足够的群体，即使使用稳健标准误差，这个假设也可以放宽。 例子：继续上面的例子，随机效应模型就像你认为每所学校都有其独特的教学质量，但这些质量是随机分布的。你感兴趣的是学校类型（如公立或私立）如何普遍影响学生的成绩。 特点：这种模型允许你估计学校特征（如学校类型）对学生成绩的影响，但它需要足够的学校数量，并假设这些学校是随机选取的。 固定效应模型： 主要用于估计协变量在群体内部效应，不受未观测群体水平混杂因素的影响。 尽管可以通过包含群体平均数的随机效应模型来解决这种内生性问题，但这种方法会产生不一致的群体间关系的参数估计。 例子：在同一个学校内，如果学校决定实施一种新的教学方法，固定效应模型可以帮助你了解这种变化如何影响学生的成绩，同时控制学校的独特因素。 特点：这种模型专注于学校内部的变化，可以控制那些未观测到的学校特定因素。 实验中的处理效应： 在随机实验中估计处理或干预效应时，由于忽略了协变量，因此不存在由于遗漏协变量而导致的内生性问题。 标准误差的解释： 随机效应模型中的标准误差是针对群体总体的（允许群体的随机抽样），而固定效应模型中的标准误差是针对手头特定群体的。 如果假设一级协变量的系数在群体间是相同的（如本章所讨论的），则不需要担心固定效应模型的标准误差仅针对样本中的群体。在这种情况下，相同的关系被假设适用于未包含在样本中的其他群体，并且在这种意义上具有普遍性。 模型参数： 在固定效应模型中，群体特定的截距是模型参数，可以通过虚拟变量方法估计。 在随机效应模型中，这些截距只是随机残差，只有它们的方差被估计为模型参数。 模型的简洁性： 固定效应模型比随机截距模型更不简洁，因为它为每个群体包含一个参数 $\\alpha_j$，而随机截距模型只有一个参数 $\\psi$ 用于随机截距的方差。 例子：固定效应模型可能会变得复杂，因为它需要为每所学校估计一个特定的效应。相比之下，随机效应模型只需要估计随机效应的方差，这使得模型更简单。 群体大小的影响： 随着群体大小的增加，随机效应模型中一级协变量系数的估计值接近其固定效应对应值，即使在一级协变量的二级内生性下，二级协变量的系数也变得一致。 不幸的是，固定效应和随机效应模型都假设协变量与一级残差 $\\epsilon_{ij}$ 无关（一级外生性）。检查一级内生性并不直接。为了纠正一级内生性，通常需要外部工具变量。 例子：如果你的数据中只有少数几所学校，随机效应模型可能不会很有效，因为它需要足够的群体数量来准确估计随机效应的分布。在这种情况下，固定效应模型可能更合适。 选择依据：如果你的数据集中群体数量较少，或者你只关心这些特定的群体，固定效应模型可能更合适。 ","date":"2024-09-19","objectID":"/chapter-3-random-intercept-models-with-covariates/:7:2","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 3 ：Random-intercept models with covariates","uri":"/chapter-3-random-intercept-models-with-covariates/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"第三部分：模型选择的具体应用 随机效应模型的应用： 当我们想要从样本中的群体推广到群体总体时，随机效应模型是合适的。 如果我们对群体水平的协变量如何影响群体内部的个体感兴趣，随机效应模型可以提供这种估计。 固定效应模型的应用： 如果我们只对样本中的特定群体内的变化感兴趣，并且我们担心存在未观测到的群体水平的混杂因素，那么固定效应模型是更好的选择。 固定效应模型通过为每个群体分配一个固定的截距，可以控制所有时间不变的群体特定效应。 ","date":"2024-09-19","objectID":"/chapter-3-random-intercept-models-with-covariates/:7:3","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 3 ：Random-intercept models with covariates","uri":"/chapter-3-random-intercept-models-with-covariates/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"第四部分：模型的比较和选择 群体大小的影响： 在固定效应模型中，如果群体大小为1（即只包含一个观测值的群体），那么这个群体对于估计回归系数没有提供任何信息，可以丢弃它们而不影响估计结果。 相比之下，在随机效应模型中，即使是大小为1的群体也提供了关于回归系数和总残差方差的一些信息，因此不应丢弃这些群体。 模型的简洁性： 固定效应模型由于为每个群体估计一个参数，因此模型较为复杂。 随机效应模型通常更为简洁，因为它只估计随机截距的方差这一个参数。 模型的效率： 通过均值中心化可以简化固定效应模型的估计问题，但这并不会提高剩余参数的估计效率。 随着群体大小的增加，随机效应模型的估计量会逐渐接近固定效应模型的估计量。 模型的假设： 两种模型都假设协变量与一级残差（$\\epsilon_{ij}$）不相关（一级外生性）。 如果存在一级内生性问题，通常需要使用外部工具变量来纠正。 ","date":"2024-09-19","objectID":"/chapter-3-random-intercept-models-with-covariates/:7:4","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 3 ：Random-intercept models with covariates","uri":"/chapter-3-random-intercept-models-with-covariates/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"第五部分：模型的预测和应用 预测： 固定效应模型：可以通过为每个群体分配一个特定的截距来进行预测。 随机效应模型：可以通过估计随机截距的分布来进行预测。 应用： 固定效应模型：适用于政策评估、处理效应分析等，特别是当我们有理由相信群体特定效应可能会影响结果时。 随机效应模型：适用于多层次模型、群体随机试验等，特别是当我们想要从样本推广到总体时。 预测： 例子：如果你使用固定效应模型发现实施新教学方法可以提高成绩，你可以预测在这所学校实施这种方法将如何影响学生。随机效应模型可以帮助你预测这种效果在其他学校的可能性。 应用：固定效应模型适用于当你想要控制特定群体的所有独特因素时，而随机效应模型适用于当你想要从样本推广到总体时。 ","date":"2024-09-19","objectID":"/chapter-3-random-intercept-models-with-covariates/:7:5","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 3 ：Random-intercept models with covariates","uri":"/chapter-3-random-intercept-models-with-covariates/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"第六部分：模型的选择和实际操作 实际操作： 固定效应模型：可以通过创建虚拟变量或使用特定估计技术来实现。 随机效应模型：可以通过最大似然估计或贝叶斯方法来实现。 选择建议： 考虑研究设计、数据结构、群体大小和研究目标。 考虑模型的假设和限制，以及是否需要对未观测到的混杂因素进行调整。 例子：在实际操作中，固定效应模型可以通过创建虚拟变量（比如为每所学校一个变量）来实现。随机效应模型通常需要更复杂的统计软件来估计。 选择建议：选择哪种模型取决于你的数据结构、研究目标和对模型假设的舒适度。 ","date":"2024-09-19","objectID":"/chapter-3-random-intercept-models-with-covariates/:7:6","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 3 ：Random-intercept models with covariates","uri":"/chapter-3-random-intercept-models-with-covariates/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"实际应用中的考虑 数据结构：如果群体内部的变异性很大，而群体之间的变异性相对较小，固定效应模型可能更合适。如果群体之间的变异性很大，随机效应模型可能更合适。 推断目标：如果目标是进行群体间的比较，随机效应模型可能更合适。如果目标是研究群体内的变化，固定效应模型可能更合适。 群体大小：如果群体很小，随机效应模型可能由于收缩效应（shrinkage）或部分汇总（partial pooling）而表现更好。 模型假设：如果对随机截距的分布有明确假设，随机效应模型可能更合适。如果没有这些假设，或者担心群体特定的效应可能与协变量相关，固定效应模型可能更合适。 ","date":"2024-09-19","objectID":"/chapter-3-random-intercept-models-with-covariates/:7:7","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 3 ：Random-intercept models with covariates","uri":"/chapter-3-random-intercept-models-with-covariates/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"总结 选择固定效应模型还是随机效应模型取决于研究的具体目标、数据结构、以及对模型假设的考虑。在实际应用中，可能需要尝试不同的模型，并比较它们的估计结果和解释能力。 ","date":"2024-09-19","objectID":"/chapter-3-random-intercept-models-with-covariates/:7:8","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 3 ：Random-intercept models with covariates","uri":"/chapter-3-random-intercept-models-with-covariates/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"11 Summary We discussed linear random-intercept models, which are important for investigating the relationship between a continuous response and a set of covariates when the data have a clustered or hierarchical structure. Topics included hypothesis testing, different kinds of coefficients of determination, the choice between fixed- and random-effects approaches, model diagnostics, consequences of using standard regression for clustered data, and power and sample-size determination. ","date":"2024-09-19","objectID":"/chapter-3-random-intercept-models-with-covariates/:7:9","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 3 ：Random-intercept models with covariates","uri":"/chapter-3-random-intercept-models-with-covariates/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"In this chapter, we consider the simple situation of clustered data without covariates or explanatory variables.","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"1 Introduction In this chapter, we consider the simple situation of clustered data without covariates or explanatory variables. For this setting, we introduce a two-level linear model which we refer to as a variance-components model. Other names for this model include one-way random-effects ANOVA or unconditional hierarchical linear model (where “unconditional” refers to not including and therefore not conditioning on covariates). The variance-components model is important in its own right and is also useful for introducing and motivating the notions of random effects and variance components. We also describe basic principles of estimation and prediction in this simple setting. However, this means that some parts of the chapter may be a bit demanding, and you could skip sections 2.10 and 2.11 on first reading. In clustered data, it is usually important to allow for dependence or correlations among the responses observed for units belonging to the same cluster. For example, the adult weights of siblings are likely to be correlated because siblings are genetically related to each other and have usually been raised within the same family. Variance-components models are designed to model and estimate such within-cluster correlations. Figure 2.1: Examples of clustered data\rIn this chapter, we consider the simple situation of clustered data without covariates or explanatory variables. For this setting, we introduce a two-level linear model which we refer to as a variance-components model. Other names for this model include one-way random-effects ANOVA or unconditional hierarchical linear model (where “unconditional” refers to not including and therefore not conditioning on covariates). The variance-components model is important in its own right and is also useful for introducing and motivating the notions of random effects and variance components. We also describe basic principles of estimation and prediction in this simple setting. However, this means that some parts of the chapter may be a bit demanding, and you could skip sections 2.10 and 2.11 on first reading. ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:1:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"2 How reliable are peak-expiratory-flow measurements? The data come from a reliability study conducted by Professor Martin Bland using 17 of his family and colleagues as subjects. The purpose was to illustrate a way of assessing the quality of two instruments for measuring people’s peak-expiratory-flow rate (PEFR). The PEFR, which is roughly speaking how strongly subjects can breathe out, is a central clinical measure in respiratory medicine. The subjects had their PEFR measured twice (in liters per minute) with the standard Wright peak-flow meter and twice with the new Mini Wright peak-flow meter. The methods were used in random order to avoid confounding practice (prior experience) effects with method effects. If the new method agrees sufficiently well with the old, the old method may be replaced with the more convenient Mini meter. Interestingly, the paper reporting this study (Bland and Altman 1986) is the most cited paper in The Lancet, one of the most prestigious medical journals. The data are presented in table 2.1 and are stored in pefr.dta in the same form as in the table, with the following variable names: id: subject identifier wp1: Wright peak-flow meter, occasion 1 wp2: Wright peak-flow meter, occasion 2 wm1: Mini Wright flow meter, occasion 1 wm2: Mini Wright flow meter, occasion 2 Peak-expiratory-flow rate measured on two occasions using both the Wright and the Mini Wright peak-flow meters\r","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:2:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"3 Inspecting within-subject dependence The first and second recordings on the Mini Wright peak-flow meter can be plotted against the subject identifier with a horizontal line representing the overall mean: generate mean_wm = (wm1+wm2)/2 summarize mean_wm Variable | Obs Mean Std. dev. Min Max\r-------------+---------------------------------------------------------\rmean_wm | 17 453.9118 111.2912 243.5 650\rtwoway (scatter wm1 id, msymbol(circle)) /// (scatter wm2 id, msymbol(circle_hollow)), /// xtitle(Subject id) xlabel(1/17) ytitle(Mini Wright measurements) /// legend( order(1 \"Occasion 1\" 2 \"Occasion 2\")) yline(453.9118) Figure 2.2: First and second measurements of peak-expiratory-flow using Mini Wright meter versus subject number (the horizontal line represents the overall mean)\rIt may be tempting to model the response of unit (here measurement occasion) in cluster (here subject) by using a standard regression model without covariates: $$y_{ij}=\\beta+\\xi_{ij}$$ where $\\xi_{ij}$ are residuals, disturbances, or error terms that are uncorrelated over both subjects and occasions (the Greek letter is pronounced “xi”). However, it is clear from the figure that repeated measurements on the same subject tend to be closer to each other than to the measurements on a different subject. Indeed, if this were not the case, the Mini Wright peak-flow meter would be useless as a tool for discriminating between the subjects in this sample. Because there are large differences between subjects (for example, compare subjects 9 and 15) and only small differences within subjects, the responses for occasions 1 and 2 on the same subject tend to lie on the same side of the overall mean, shown as a horizontal line in the figure, and are therefore positively correlated. We expect that the underlying population covariance, defined as the expectation of products of deviations from the expectation, is also positive. See also section 2.4.4 We can also see that there is within-subject dependence by considering prediction of a subject’s response at occasion 2 if we only know all the subjects’ responses at occasion 1. If the response for a given subject at occasion 2 were independent of his or her response at occasion 1, a good prediction would be the mean response at occasion 1 across all subjects. However, it is clear that a much better prediction here is the subject’s own response at occasion 1 because the responses are highly dependent within subject. The within-subject dependence is due to between-subject heterogeneity. If all subjects were more or less alike (for example, pick subjects 2, 4, 10, 11, 14, and 17), there would be much less within-subject dependence. ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:3:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"4 The variance-components model ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:4:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"4.1 Model specification As we saw in the previous section and in figure 2.2, it is unreasonable to assume that the deviations $\\xi_{ij}$ of $y_{ij}$ from the population mean $\\beta$ are uncorrelated within subjects in the regression model. We can model the within-subject dependence by splitting the residual $\\xi_{ij}$ into two uncorrelated components: a permanent component $\\xi_j$ ($\\xi$ is pronounced “zeta”),which is specific to each subject $j$ and constant across occasions $i$; and an idiosyncratic component $\\epsilon_{ij}$ which is specific to each occasion $i$ for each subject $j$. We then obtain a variancecomponents model, $$y_{ij}=\\beta+\\underbrace{\\zeta_j+\\epsilon_{ij}}_{\\xi_ij}$$ 首先，我们来分两部分来理解这个模型。 第一部分：理解基本概念 回归模型中的偏差：在统计学中，回归模型是用来预测一个变量（因变量）基于一个或多个其他变量（自变量）的关系。在这个模型中，我们关注的是因变量 $ y_{ij} $ 与一个总体均值 $ \\beta $ 的偏差，记作 $ \\xi_{ij} $。 相关性问题：当我们假设 $ \\xi_{ij} $ 在同一个受试者（subject）内部是不相关的，这通常是不合理的。因为同一个受试者在不同时间点的测量值很可能是有关联的。 分解偏差：为了解决这个问题，我们可以将偏差 $ \\xi_{ij} $ 分解成两个不相关的部分： 永久性偏差 $ \\zeta_j $：这个偏差是针对每个受试者 $ j $ 特有的，并且在所有时间点 $ i $ 上都是常数。 偶发性偏差 $ \\epsilon_{ij} $：这个偏差是针对每个受试者 $ j $ 在每个时间点 $ i $ 特有的。 第二部分：理解公式 现在我们来看公式： $$y_{ij}=\\beta+\\underbrace{\\zeta_j+\\epsilon_{ij}}_{\\xi_ij}$$ 这个公式实际上是在说，对于每个受试者 $ j $ 在每个时间点 $ i $ 的观测值 $ y_{ij} $，可以由以下几部分组成： 总体均值 $ \\beta $：这是所有观测值的中心点，可以理解为如果没有其他因素影响，所有观测值的平均值。 永久性偏差 $ \\zeta_j $：这是受试者 $ j $ 特有的偏差，反映了这个受试者与总体均值的差异。 偶发性偏差 $ \\epsilon_{ij} $：这是在特定时间点 $ i $ 受试者 $ j $ 的特有偏差，它可能反映了测量误差、受试者当时的状态变化等。 总偏差 $ \\xi_{ij} $：这是将永久性偏差和偶发性偏差合并起来的结果，代表了 $ y_{ij} $ 与总体均值 $ \\beta $ 的总偏差。 例子 假设我们正在研究不同人每天的步数。我们想要建立一个模型来预测步数，但我们知道每个人的步数可能会因为个体差异而有所不同。 $ \\beta $ 可能是所有研究对象平均每天走的步数，比如5000步。 $ \\zeta_j $ 可能是某个特定人（比如张三）平均每天比总体多走的步数，比如200步。 $ \\epsilon_{ij} $ 可能是张三在某一天（比如周一）比他自己的平均水平多走的步数，比如50步。 那么，张三在周一的步数可以表示为： $$ y_{1j} = 5000 + 200 + 50 = 5250 $$ 这样，我们就可以更准确地预测张三在特定日子的步数，并且考虑到了个体差异和日常波动。希望这个例子能帮助你更好地理解这个模型。 as shown for subject in figure 2.3. Figure 2.3: Illustration of variance-components model for a subject $j$\rFigure 2.3: Illustration of variance-components model for a subject $j$\rHere $\\xi_j$ is the random deviation of subject j’s mean measurement (over a hypothetical population of measurement occasions) from the $\\zeta_{j}$,often called a random effect or random intercept of subject $j$,has zero population mean and is uncorrelated across subjects. The $\\zeta_{j}$ can be viewed as representing individual differences due to personal characteristics. The component $\\epsilon_{ij}$ often called the within-subject residual, is the random deviation of $y_{ij}$ from subject j’s mean. This residual has zero population mean and is uncorrelated across occasions and subjects. Note that the subject mean,$\\beta+\\zeta_{j}$ is not the same as the sample mean $(y_{1j}+y_{2j})/2$ for the subject because the sample mean of the within-subject residual, $(\\epsilon_{1j}+\\epsilon_{2j})/2$ is not necessarily 0, although the population mean, $E(\\epsilon_{ij})$ is 0. In the figure, the two within-subject residuals have a negative sample mean—they could also have been both positive or both negative. In classical psychometric test theory, (2.3) represents a measurement model where $\\beta+\\zeta_{j}$ is the true score for subject $j$, defined as the longterm mean measurement, and $\\epsilon_{ij}$ is the measurement error at occasion $i$ for subject . The random intercept $\\zeta_{j}$ has variance $\\psi $ (pronounced “psi”), interpretable as the between-subject variance, and the residual $\\epsilon_{ij}$ has variance $\\theta$ (pronounced “theta”), interpretable as the within-subject variance of $y_{ij}$. The model is a simple example of a two-level model, where occasions are level-1 units and subjects are level-2 units or clusters. The random intercept $\\zeta_{j}$ is then referred to as the level-2 residual with level-2 (between-subject) variance $\\psi $ ; $\\epsilon_{ij}$ is referred to as the level-1 residual with level-1 (between-occasion, within-subject) variance $\\theta$. 受试者均值和样本均值 首先，我们来更深入地理解受试者均值 $ \\beta + \\zeta_j $ 和样本均值 $ \\frac{(y_{1j} + y_{2j})}{","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:4:1","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"4.2 Path diagram We can display the random part of the model (every term except β) by using a path diagram (similar to a directed acyclic graph or DAG), as shown in figure 2.4. Here the rectangles represent the observed responses $y_{1j}$ and $y_{2j}$ for each subject $j$, where the $j$ subscript is implied by the label “subject j” inside the frame surrounding the diagram. The long arrows from $\\zeta_j$ to the responses represent regressions with slopes equal to ： The short arrows pointing at the responses from below represent the additive level-1 residuals $\\epsilon_{1j}$ and $\\epsilon_{2j}$. Figure 2.4: Path diagram of random part of random-intercept model\rThe path diagram makes it clear that the dependence between the two responses is solely due to the shared random intercept. The responses are conditionally independent given $ζ_j$ because they are regressed on $ζ_j$ and there is no arrow directly connecting them. (There is also no two-way arrow connecting the level-1 errors $\\epsilon_{1j}$ and $\\epsilon_{2j}$ to indicate that they are correlated.) It follows that the responses are conditionally uncorrelated given $ζ_j$. $$\\mathrm{Cor}(y_{ij},y_{i^{\\prime}j}|\\zeta_j)=0$$ This can also be seen by imagining that the data in figure 2.2 were generated by the model depicted in figure 2.3, where the dependence is solely due to the measurements being shifted up or down by the shared random intercept $ζ_j$ for each cluster $j$. One way of conditioning on $ζ_j$ is to imagine a dataset consisting of just one cluster (or consisting of a subset of clusters with identical values of $ζ_j$). For that dataset, the responses would be uncorrelated. Another way of understanding conditional independence is to consider predicting the response $y_{2j}$ at occasion 2 for a subject. If we knew $ζ_j$ (and$\\beta$), knowing $y_{1j}$ would not improve our prediction. The (marginal) within-subject correlation is induced by $ζ_j$ because this is shared by all responses for the same subject (see section 2.4.4). As we will see in later chapters, path diagrams can be particularly useful for conveying the structure of complex models involving several random effects. 路径图的组成 观察到的响应：路径图中的矩形代表每个受试者 $ j $ 的观测响应 $ y_{1j} $ 和 $ y_{2j} $。这里的下标 $ j $ 是隐含的，由路径图周围的框架内的“受试者 j”标签表示。 随机截距 $ \\zeta_j $：从 $ \\zeta_j $ 指向响应的长箭头表示回归关系，其斜率等于1。这意味着 $ \\zeta_j $ 对每个响应的直接影响是相等的。 第一级残差 $ \\epsilon_{1j} $ 和 $ \\epsilon_{2j} $：指向响应的短箭头表示第一级残差，这些残差是加性的，意味着它们直接添加到响应上。 路径图的含义 条件独立性：路径图清晰地表明，两个响应之间的依赖性完全是由于共享的随机截距 $ \\zeta_j $。给定 $ \\zeta_j $，响应是条件独立的，因为它们都是对 $ \\zeta_j $ 进行回归，并且没有直接连接它们的箭头。此外，也没有双向箭头连接第一级误差 $ \\epsilon_{1j} $ 和 $ \\epsilon_{2j} $ 来表明它们是相关的。因此，给定 $ \\zeta_j $，响应是条件不相关的。 条件不相关：公式 $\\mathrm{Cor}(y_{ij},y_{i’j}|\\zeta_j)=0$ 表示，在给定随机截距 $ \\zeta_j $ 的条件下，不同时间点 $ i $ 和 $ i’ $ 的响应是不相关的。 条件独立性的解释 条件数据集：通过想象数据集中只包含一个集群（或包含具有相同 $ \\zeta_j $ 值的集群子集），可以更容易地理解条件独立性。对于那个数据集，响应是不相关的。 预测响应：如果我们知道了 $ \\zeta_j $（和 $ \\beta $），知道第一次时间点的响应 $ y_{1j} $ 并不会改善我们对第二次时间点响应 $ y_{2j} $ 的预测。 受试者内相关性的产生 受试者内相关性：边际受试者内相关性是由 $ \\zeta_j $ 引起的，因为 $ \\zeta_j $ 被同一受试者的所有响应共享（见第2.4.4节）。 路径图的用途 复杂模型：在后续章节中，我们将看到路径图在传达涉及多个随机效应的复杂模型的结构时特别有用。 例子 假设我们有一个班级，我们想要了解学生在两次不同考试中的表现。路径图可以帮助我们可视化每个学生（受试者）在两次考试中的得分是如何受到他们个人能力（随机截距 $ \\zeta_j $）的影响。 随机截距 $ \\zeta_j $：假设张三的随机截距是10，这意味着他的平均能力比班级平均水平高出10分。 第一次考试 $ y_{11} $：张三在第一次考试中的表现可能受到他的个人能力的影响，以及一些随机误差 $ \\epsilon_{1j} $。 第二次考试 $ y_{12} $：同样，张三在第二次考试中的表现也受到他的个人能力和另一个随机误差 $ \\epsilon_{2j} $ 的影响。 通过路径图，我们可以清晰地看到张三在两次考试中的表现是如何受到他的个人能力的影响，并且两次考试的表现是条件独立的，因为它们都是基于张三的个人能力来预测的。 希望这个更详细的解释和例子能够帮助你更好地理解路径图和随机截距模型。 ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:4:2","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"4.3 Between-subject heterogeneity Each response differs from the overall mean $\\beta $ by a total residual or error $\\xi _{ij} $, the sum of two error terms or error components:$\\zeta _{j}$ and $\\epsilon _{ij}.$ $$\\xi_{ij}=\\zeta_j+\\epsilon_{ij}$$ The random intercept $ζ_j$ is shared between measurement occasions for the same subject $j$, whereas $\\epsilon_{ij}$ is unique for each occasion $i$ (and subject). The variance of the responses becomes $\\psi +\\theta$. Figure 2.4: Path diagram of random part of random-intercept model\rwhich is the sum of variance components representing between-subject and within-subject variances. The proportion of the total variance that is between subjects is $$\\rho~=~\\frac{\\mathrm{Var}(\\zeta_{j})}{\\mathrm{Var}(y_{ij})}=\\frac{\\psi}{\\psi+\\theta}$$ The coefficient $ρ$ is similar to the coefficient of determination $R^2$ in linear regression discussed in section 1.5, because it expresses how much of the total variability is “explained” by subjects. In the measurement context,$\\psi$is the variance of subjects’ true scores$\\beta +\\zeta_j$,$\\theta$is the measurement-error variance (the squared standard error of measurement), and $\\rho$ is a reliability, here a test-retest reliability. Note that the reliability is not just a characteristic of the measurement instrument; it also depends on the between-subject variance$\\psi$ which can differ between populations. 好的，让我们一步一步来理解这段话中提到的统计概念和公式。 1. 总残差或误差（Total Residual or Error） 首先，我们有一个响应（response）$ y_{ij} $，它与整体平均值 $ \\beta $ 之间的差异被称为总残差或误差 $ \\xi_{ij} $。这个总残差由两个部分组成：随机截距 $ \\zeta_j $ 和随机误差 $ \\epsilon_{ij} $。 公式是这样的： $$ \\xi_{ij} = \\zeta_j + \\epsilon_{ij} $$ $ \\zeta_j $ 是随机截距，它对于同一个受试者 $ j $ 在不同测量时间点是共享的。 $ \\epsilon_{ij} $ 是随机误差，对于每个测量时间点 $ i $（和受试者）都是独特的。 2. 响应的方差（Variance of the Responses） 这个公式是统计学中描述随机效应模型中响应变量 $ y_{ij} $ 方差的一个关键公式。让我们逐步分解这个公式来更好地理解它。 公式分解 响应变量的方差： $ \\text{Var}(y_{ij}) $ 这是第 $ i $ 次测量，第 $ j $ 个受试者的响应变量 $ y_{ij} $ 的方差。 期望的平方： $ E{(y_{ij} - \\beta)^2} $ 这里，$ \\beta $ 是所有响应的平均值，$ E $ 表示期望值。这个表达式计算的是 $ y_{ij} $ 与平均值 $ \\beta $ 之差的平方的期望值，也就是 $ y_{ij} $ 相对于平均值的变异性。 随机误差的平方： $ E{(\\zeta_j + \\epsilon_{ij})^2} $ 展开 $ y_{ij} $ 的表达式，我们得到 $ \\zeta_j + \\epsilon_{ij} $，其中 $ \\zeta_j $ 是随机截距，$ \\epsilon_{ij} $ 是随机误差。这个表达式计算的是随机截距和随机误差之和的平方的期望值。 展开平方： $ E(\\zeta_j^2) + 2E(\\zeta_j\\epsilon_{ij}) + E(\\epsilon_{ij}^2) $ 当我们展开 $ (\\zeta_j + \\epsilon_{ij})^2 $ 时，我们得到三个部分： $ E(\\zeta_j^2) $：随机截距的平方的期望值。 $ 2E(\\zeta_j\\epsilon_{ij}) $：随机截距和随机误差乘积的两倍期望值。这里乘以2是因为在展开过程中，交叉项会出现两次。 $ E(\\epsilon_{ij}^2) $：随机误差的平方的期望值。 方差成分： $ \\psi + \\theta $ 最后，我们将上述三个部分合并，得到响应变量 $ y_{ij} $ 的总方差。这里： $ \\psi $ 代表 $ E(\\zeta_j^2) $，即随机截距的方差。代表受试者之间的方差（between-subject variance）。 $ \\theta $ 代表 $ E(\\epsilon_{ij}^2) $，即随机误差的方差。代表测量误差的方差（measurement-error variance）。 公式解释 这个公式告诉我们，响应变量 $ y_{ij} $ 的方差可以分解为两部分： 一部分是由于受试者之间的差异（由 $ \\psi $ 表示，即随机截距的方差）。 另一部分是由于测量误差（由 $ \\theta $ 表示，即随机误差的方差）。 这个分解非常重要，因为它允许我们理解数据中的变异性是如何由不同的因素（受试者间差异和测量误差）共同贡献的。 例子 假设我们有一个班级的学生，我们想要了解他们的考试成绩的变异性。在这个例子中： $ \\beta $ 是所有学生的平均成绩。 $ \\zeta_j $ 是第 $ j $ 个学生的成绩与其平均成绩的差异。 $ \\epsilon_{ij} $ 是第 $ i $ 次考试，第 $ j $ 个学生的随机误差。 如果我们计算一个学生的考试成绩的方差，我们会考虑这个学生的考试成绩与平均成绩的差异（$ \\zeta_j $ 的方差）以及考试本身的随机误差（$ \\epsilon_{ij} $ 的方差）。这两个部分加起来就给出了学生考试成绩的总方差。 希望这个详细的解释能帮助你更好地理解这个公式。如果你还有任何问题，请随时提问。 3. 方差成分的解释 方差成分 $ \\psi $ 和 $ \\theta $ 的和代表了总方差，即受试者内部和受试者之间的总变异性。 4. 受试者间方差的比例（Proportion of Between-Subject Variance） 受试者间方差的比例 $ \\rho $ 可以用下面的公式表示： $$ \\rho = \\frac{\\text{Var}(\\zeta_j)}{\\text{Var}(y_{ij})} = \\frac{\\psi}{\\psi + \\theta} $$ 这个比例 $ \\rho $ 类似于线性回归中的决定系数 $ R^2 $，因为它表达了总变异性中有多少是由受试者“解释”的。 5. 可靠性（Reliability） 在测量的背景下： $ \\psi $ 是受试者真实分数 $ \\beta + \\zeta_j $ 的方差。 $ \\theta $ 是测量误差的方差（测量误差的标准差的平方）。 $ \\rho $ 是可靠性，这里指的是测试-重测可靠性。需要注意的是，可靠性不仅仅是测量工具的特性；它还依赖于受试者间的方差 $ \\psi $，这在不同人群中可能会有所不同。 6. 例子 假设我们有一个测试，我们想要了解不同学生在多次测试中的得分变化。我们有以下数据： $ \\beta $：所有学生的平均得分。 $ \\zeta_j $：第 $ j $ 个学生的随机截距，表示该学生得分高于或低于平均得分的部分。 $ \\epsilon_{ij} $：第 $ i $ 次测试，第 $ j $ 个学生的随机误差。 如果我们计算一个学生的得分方差，我们会将该学生的随机截距的方差加上该次测试的随机误差的方差。这个总方差就是 $ \\psi + \\theta $。 可靠性 $ \\rho $ 告诉我们，学生得分的总变异性中有多大比例是由于学生之间的差异（而不是由于随机误","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:4:3","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"4.4 Within-subject dependence Intraclass correlation The marginal (not conditional on $S_j$) covariance between the measurements on two occasions $i$ and $i’$ for the same subject $j$ is defined as The corresponding marginal correlation is the above covariance divided by the product of the standard deviations: $$\\mathrm{Cor}(y_{ij},y_{i^{\\prime}j})=\\frac{\\mathrm{Cov}(y_{ij},y_{i^{\\prime}j})}{\\sqrt{\\mathrm{Var}(y_{ij})}\\sqrt{\\mathrm{Var}(y_{i^{\\prime}j})}}$$ It follows from the variance-components model that the population means at both occasions are constrained to be equal to $β$ and the standard deviations are constrained to be equal to $\\sqrt{\\psi+\\theta}$. The marginal (not conditional on $ξ_j$) covariance between the measurements equals $\\psi$. The corresponding correlation, called the intraclass correlation , becomes $$\\mathrm{Cor}(y_{ij},y_{i^{\\prime}j})~=~\\frac{\\mathrm{Cov}(y_{ij},y_{i^{\\prime}j})}{\\sqrt{\\mathrm{Var}(y_{ij})}\\sqrt{\\mathrm{Var}(y_{i^{\\prime}j})}}=\\frac{\\psi}{\\sqrt{\\psi+\\theta}\\sqrt{\\psi+\\theta}}=\\frac{\\psi}{\\psi+\\theta}~=~\\rho $$ Thus,$\\rho$, previously given in (2.4), also represents the within-cluster correlation, which cannot be negative in the variance-components model because $ψ\\geq0$. We see that between-cluster heterogeneity and within-cluster correlations are different ways of describing the same phenomenon; both are 0 when there is no between-cluster variance ($\\psi=0$), and both increase when the between-cluster variance increases relative to the within-cluster variance. The intraclass correlation is estimated by simply plugging in estimates for the unknown parameters: $$\\widehat{\\rho}=\\frac{\\widehat{\\psi}}{\\widehat{\\psi}+\\widehat{\\theta}}$$ Figure 2.5 shows simulated data with an estimated intraclass correlation of $\\hat\\rho = 0.58$ (top panel) and data with an estimated intraclass correlation of $\\hat\\rho = 0.87$ (bottom panel). The intraclass correlation is higher in the right panel because the clusters are more distinguishable, here because the within-cluster variance is considerably lower than in the left panel, whereas the between-cluster variance is similar. Figure 2.5: Illustration of lower intraclass correlation (top) and higher intraclass correlation (bottom)\r类内相关系数（Intraclass Correlation） 类内相关系数是一个衡量同一受试者在不同时间点上测量结果相关性的统计量。 1. 协方差（Covariance） 首先，我们来理解协方差的概念。协方差是衡量两个变量联合变化趋势的统计量。如果两个变量一起增加或一起减少，它们的协方差是正的；如果一个增加而另一个减少，协方差是负的。 公式是： 这个公式是协方差的计算公式，用来衡量两个变量之间的线性关系强度。让我们一步步地分解这个公式，并用一个简单的例子来帮助理解。 公式分解 $ \\text{Cov}(y_{ij},y_{i’j}) $ 是变量 $ y_{ij} $ 和 $ y_{i’j} $ 的协方差。 $ E $ 表示期望值，也就是平均值。 $ y_{ij} $ 是第 $ i $ 次对第 $ j $ 个受试者的测量结果。 $ E(y_{ij}) $ 是 $ y_{ij} $ 的期望值，即所有 $ y_{ij} $ 的平均值。 $ y_{i’j} $ 是第 $ i’ $ 次对第 $ j $ 个受试者的测量结果。 $ E(y_{i’j}) $ 是 $ y_{i’j} $ 的期望值，即所有 $ y_{i’j} $ 的平均值。 公式解释 协方差的计算可以分解为以下几个步骤： 计算每次测量与平均值的差： 对于每次测量 $ y_{ij} $ 和 $ y_{i’j} $，我们计算它们与各自平均值的差。例如，如果 $ y_{ij} $ 是 10，而 $ E(y_{ij}) $ 是 8，那么差就是 $ 10 - 8 = 2 $。 计算差的乘积： 然后，对于每一对测量值（$ y_{ij} $ 和 $ y_{i’j} $），我们计算它们差值的乘积。如果 $ y_{ij} $ 的差是 2，而 $ y_{i’j} $ 的差是 -1（意味着 $ y_{i’j} $ 比平均值低 1），那么乘积就是 $ 2 \\times -1 = -2 $。 计算乘积的平均值： 我们对所有这样的乘积求和，然后除以乘积的总数（如果是两个测量值，总数就是 2），得到平均乘积，这就是协方差的值。 例子 假设我们有3个学生（$ j $），每个学生参加了两次考试（$ i $ 和 $ i’ $），成绩如下： 学生编号 第一次考试分数 ($ y_{ij} $) 第二次考试分数 ($ y_{i’j} $) 学生1 85 88 学生2 75 70 学生3 90 95 步骤1: 计算平均分 首先，我们计算每次考试的平均分： 第一次考试平均分 $ E(y_{ij}) = (85 + 75 + 90) / 3 = 250 / 3 \\approx 83.33 $ 第二次考试平均分 $ E(y_{i’j}) = (88 + 70 + 95) / 3 \\approx 253 / 3 \\approx 84.33 $ 步骤2: 计算差值 然后，我们计算每个学生每次考试分数与平均分的差： 学生编号 第一次考试差值 ($ y_{ij} - E(y_{ij}) $) 第二次考试差值 ($ y_{i’j} - E(y_{i’j}) $) 学生1 $ 85 - 83.33 = 1.67 $ $ 88 - 84.33 = 3.67 $ 学生2 $ 75 - 83.33 = -8.33 $ $ 70 - 84.33 = -14.33 $ 学生3 $ 90 - 83.33 = 6.67 $ $ 95 - 84.33 = 10.67 $ 步骤3: 计算乘积和平均值 接下来，我们计算每对学生考试分数差值的乘积，然后求平均： $$ \\text{Cov}(y_{ij},y_{i’j}) = \\frac{(1.67 \\times 3.67) + (-8.33 \\times -14.33) + (6.67 \\times 10.67)}{3} $$ $$ \\text{Cov}(y_{ij},y_{i’j}) = \\frac{6.11 + 119.08 + 71.34}{3} \\approx \\frac{196.53}{3} \\approx 65.51 $$ 这个值表示学生在第一次考试和第二次考试分数之间的线性关系强度。 结论 通过这个例子，我们可以看到协方差是","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:4:4","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"5 Estimation using Stata In Stata, maximum likelihood estimates for variance-components models can be obtained using xtreg, mle or mixed, mle (the default for mixed). Restricted maximum likelihood (REML) estimates can be obtained using mixed, reml, and feasible generalized least-squares (FGLS) estimates can be obtained using xtreg, re (the default method). See sections 2.10.2 and 3.10.1 for information on these estimation methods. Briefly, the methods produce practically identical results when the number of clusters is large, but REML is preferable when the number of clusters is small, say less than 42. See display 2.1 for the rule of thumb of 42 for choosing between estimation methods. The xtreg command is the most computationally efficient for variance-components models. However, the postestimation command predict for mixed is more useful than predict for xtreg. The mixed command is also preferable when the number of clusters is small because it produces REML estimates when used with the reml option. The community-contributed command gllamm can also be used for maximum likelihood estimation as demonstrated in the gllamm companion for this book that is available from www.gllamm.org. We do not generally recommend using gllamm for the linear models considered in this volume, but the command is useful for more complex models. ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:5:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"5.1 Data preparation: Reshaping from wide form to long form We now set up the data for estimation in Stata. Currently, the responses for occasions 1 and 2 are in wide form as two separate variables, wp1 and wp2 for the Wright peak-flow meter, and wm1 and wm2 for the Mini Wright peak-flow meter. ** 2.5.1 Data preparation: Reshaping to long form list if id\u003c6, clean noobs id wp1 wp2 wm1 wm2 mean_wm 1 494 490 512 525 518.5 2 395 397 430 415 422.5 3 516 512 520 508 514 4 434 401 428 444 436 5 476 470 500 500 500 For model fitting, we need to stack the occasion 1 and 2 measurements using a given meter into one variable. We can use the reshape command to obtain such a long form with one variable, wp, for both Wright peak-flow meter measurements; one variable, wm, for both Mini Wright peak-flow meter measurements; and a variable, occasion (equal to 1 and 2), for the measurement occasion: * stack repeated observations for same method into single variable reshape long wp wm, i(id) j(occasion) Data Wide -\u003e Long\r-----------------------------------------------------------------------------\rNumber of observations 17 -\u003e 34 Number of variables 6 -\u003e 5 j variable (2 values) -\u003e occasion\rxij variables:\rwp1 wp2 -\u003e wp\rwm1 wm2 -\u003e wm\r-----------------------------------------------------------------------------\rlist if id\u003c6, clean noobs id occasion wp wm mean_wm 1 1 494 512 518.5 1 2 490 525 518.5 2 1 395 430 422.5 2 2 397 415 422.5 3 1 516 520 514 3 2 512 508 514 4 1 434 428 436 4 2 401 444 436 5 1 476 500 500 5 2 470 500 500 ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:5:1","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"5.2 Using xtreg We can estimate the parameters of the variance-components model (2.3) by using the xtreg command with the mle option, which stands for maximum likelihood estimation (see section 2.10.2). Before using xtreg and any command starting with xt, the data should be declared as clustered data (referred to as panel data in Stata documentation because longitudinal or panel data are a common example of clustered data) by using the xtset command. Here it is sufficient to declare that id is the cluster identifier $ j=1, \\ldots, 17 $: ** 2.5.2 Using xtreg xtset id Panel variable: id (balanced)\rThe output states that our data are balanced, meaning that the cluster size is constant (here two measurements per subject). We are now ready to use the xtreg command. As in the regress command, the response variable wm and covariates are listed after the command name. In variance-components models, the fixed part is just the intercept $\\beta $, which is included by default, so we do not specify any covariates. The random part includes a random intercept $\\zeta _ {j} $ for the clusters defined in the xtset command. The level-1 residual $\\epsilon _{ij} $ need not be specified because it is always included. Therefore the command is simply xtreg wm, mle Iteration 0: Log likelihood = -187.89003\rIteration 1: Log likelihood = -184.95979\rIteration 2: Log likelihood = -184.76189\rIteration 3: Log likelihood = -184.5855\rIteration 4: Log likelihood = -184.5784\rIteration 5: Log likelihood = -184.57839\rRandom-effects ML regression Number of obs = 34\rGroup variable: id Number of groups = 17\rRandom effects u_i ~ Gaussian Obs per group:\rmin = 2\ravg = 2.0\rmax = 2\rWald chi2(0) = 0.00\rLog likelihood = -184.57839 Prob \u003e chi2 = .\r------------------------------------------------------------------------------\rwm | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_cons | 453.91 26.19 17.33 0.000 402.59 505.24\r-------------+----------------------------------------------------------------\r/sigma_u | 107.05 18.68 76.04 150.69\r/sigma_e | 19.91 3.41 14.23 27.87\rrho | 0.97 0.02 0.92 0.99\r------------------------------------------------------------------------------\rLR test of sigma_u=0: chibar2(01) = 46.27 Prob \u003e= chibar2 = 0.000\rWe see in the output that there are 34 observations belonging to 17 groups (the clusters, here subjects) and that there are 2 observations in each group (the minimum, maximum, and hence average number are all 2). In the Stata output and in the Stata documentation for xtreg, the $i$ subscript is used for clusters (instead of $j$ used in this book), $u_i$ is used for the random intercept (instead of $\\zeta_j$), and the$t$subscript is used for occasions (instead of$i$ used in this book). The estimate of the overall population mean $\\beta$, given next to cons in the output, is 453.91. The estimate of the between-subject standard deviation $\\psi$ of the random intercepts of subjects, referred to as $\\sigma_u$, is 107.05, and the estimate of the within-subject standard deviation $\\theta$, referred to as $\\sigma_e$, is 19.91. It follows that the intraclass correlation is estimated as. $$\\widehat{\\rho}=\\frac{\\widehat{\\psi}}{\\widehat{\\psi}+\\widehat{\\theta}}=\\frac{107.0464^2}{19.91083^2+107.0464^2}=0.97$$ which is referred to as rho in the output. This estimate is close to 1, indicating that the Mini Wright peak-flow meter is very reliable. The parameter estimates are also given under “ML” in table 2.2 below. An alternative estimator for xtreg is feasible generalized least squares (FGLS), obtained using the re option. As discussed in sections 2.10.2 and 3.10.1, FGLS is similar to ML. The vce(robust) option can be used to obtain standard errors that are robust to misspecification of the residual variance and covariance structure (robust standard errors for nonclustered data were used in section 1.6). However, robust standard errors do not perform well when the number of ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:5:2","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"5.3 Using mixed The variance-components model considered here is a simple special case of a linear mixed-effects model that can be fit using the mixed command (which replaced xtmixed in Stata 13). The mixed command can also be used for models with random slopes as well as models with more than one clustering variable (for example, three-level models). The structure of the model is completely specified in the mixed command instead of using the xtset command to define any aspect of the model as for xtreg. The fixed part of the model, here $\\beta$, is specified as in any estimation command in Stata (the response variable followed by a list of covariates). The random part, except the residual $\\epsilon_{ij}$, is specified after two vertical bars (or pipes),||. The cluster identifier, here id, is first given to define the clusters $j$ over which $\\zeta_j$ varies. This is followed by a colon and nothing, because a random intercept $\\zeta_j$ is included by default (it can be excluded using the noconstant option). Finally, we request maximum likelihood estimation by using the mle option (the default), and we use the stddeviations option to obtain standard deviation estimates instead of variances for the error components. mixed wm || id:, mle stddeviations Performing EM optimization ...\rPerforming gradient-based optimization: Iteration 0: Log likelihood = -184.57839 Iteration 1: Log likelihood = -184.57839 Computing standard errors ...\rMixed-effects ML regression Number of obs = 34\rGroup variable: id Number of groups = 17\rObs per group:\rmin = 2\ravg = 2.0\rmax = 2\rWald chi2(0) = .\rLog likelihood = -184.57839 Prob \u003e chi2 = .\r------------------------------------------------------------------------------\rwm | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_cons | 453.91 26.19 17.33 0.000 402.59 505.24\r------------------------------------------------------------------------------\r------------------------------------------------------------------------------\rRandom-effects parameters | Estimate Std. err. [95% conf. interval]\r-----------------------------+------------------------------------------------\rid: Identity |\rsd(_cons) | 107.05 18.68 76.04 150.69\r-----------------------------+------------------------------------------------\rsd(Residual) | 19.91 3.41 14.23 27.87\r------------------------------------------------------------------------------\rLR test vs. linear model: chibar2(01) = 46.27 Prob \u003e= chibar2 = 0.0000\rThe table of estimates for the fixed part of the model has the same form as that for xtreg and all Stata estimation commands. The random part is given under the Random-effects Parameters header. Here sd(_cons) is the estimate of the random-intercept standard deviation $ \\sqrt{\\psi} $, and sd(Residual) is the estimate of the standard deviation $sqrt{\\theta} $ of the level-1 residuals. All of these estimates are identical to the estimates from xtreg given in table 2.2 under “ML”. We could also obtain estimated variances (instead of standard deviations) with their standard errors by omitting the stddeviations option. As will be discussed in section 2.10.2, it is preferable to use REML instead of ML when the number of clusters is small as in this dataset. Simply use the same syntax but now with the reml option: mixed wm || id:, reml stddeviations Performing EM optimization ...\rPerforming gradient-based optimization: Iteration 0: Log restricted-likelihood = -180.37921 Iteration 1: Log restricted-likelihood = -180.37921 Computing standard errors ...\rMixed-effects REML regression Number of obs = 34\rGroup variable: id Number of groups = 17\rObs per group:\rmin = 2\ravg = 2.0\rmax = 2\rWald chi2(0) = .\rLog restricted-likelihood = -180.37921 Prob \u003e chi2 = .\r------------------------------------------------------------------------------\rwm | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+-------------------------------------------------------------","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:5:3","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"6 Hypothesis tests and confidence intervals ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:6:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"6.1 Hypothesis test and confidence interval for the population mean In the regression tables produced by xtreg and mixed, $z$ statistics are reported for instead of the statistics given by the regress command. Like the $t$ statistic in ordinary linear regression, the $z$ statistic for the null hypothesis $$H_{0}:\\beta=0\\quad\\mathrm{against}\\quad H_{a}:\\beta\\neq0$$ is given by $$z~=~\\frac{\\widehat{\\beta}}{\\widehat{\\mathrm{SE}}(\\widehat{\\beta})}$$ Squaring the $z$ statistic gives the Wald statistic, an approximation to the likelihood-ratio statistic, described in section 2.6.2 for a different problem, namely testing the between-cluster variance. An asymptotic 95% confidence interval for $\\beta$ is given by $$\\widehat{\\beta} \\pm z_{0.975}\\widehat{\\mathrm{SE}}(\\widehat{\\beta})$$ where $z_{0.975}$ is the 97.5th percentile of the standard normal distribution, that is, $z_{0.975} = 1.96$. This kind of confidence interval based on assuming a normal sampling distribution is often called a Wald confidence interval. In the Mini Wright application, the 95% Wald confidence interval for the population mean $\\beta$ is from 402.59 to 505.24, as shown, for instance, in the output from xtreg in section 2.5.2. The reason the statistic is called $z$ instead of $t$ and $z_{0.975}$ is used for the confidence interval instead of $t_{0.975, df}$ is that a standard normal sampling distribution is assumed under the null hypothesis instead of a $t$ distribution. The $t$ distribution is a finite-sample distribution whose shape depends on the degrees of freedom. For the variance-components model, the finite-sample distribution does not have a simple form, so Stata’s commands use the asymptotic (large-sample) sampling distribution. As of Stata 14, the mixed command can also provide degrees of freedom (df) For a distribution that approximates the finite-sample distribution of the test statistic. Among several approximations for the degrees of freedom, the Kenward–Roger method generally appears to be preferable (Kenward and Roger 1997; Schaalje, McBride, and Fellingham 2002), and this approximation is obtained using the dfmethod(kroger) option (which works only in combination with the reml option): mixed wm || id:, reml dfmethod(kroger) stddeviations Performing EM optimization ...\rPerforming gradient-based optimization: Iteration 0: Log restricted-likelihood = -180.37921 Iteration 1: Log restricted-likelihood = -180.37921 Computing standard errors ...\rComputing degrees of freedom ...\rMixed-effects REML regression Number of obs = 34\rGroup variable: id Number of groups = 17\rObs per group:\rmin = 2\ravg = 2.0\rmax = 2\rDF method: Kenward–Roger DF: min = 16.00\ravg = 16.00\rmax = 16.00\rF(0, .) = .\rLog restricted-likelihood = -180.37921 Prob \u003e F = .\r------------------------------------------------------------------------------\rwm | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_cons | 453.91 26.99 16.82 0.000 396.69 511.13\r------------------------------------------------------------------------------\r------------------------------------------------------------------------------\rRandom-effects parameters | Estimate Std. err. [95% conf. interval]\r-----------------------------+------------------------------------------------\rid: Identity |\rsd(_cons) | 110.40 19.84 77.63 157.00\r-----------------------------+------------------------------------------------\rsd(Residual) | 19.91 3.41 14.23 27.87\r------------------------------------------------------------------------------\rLR test vs. linear model: chibar2(01) = 46.96 Prob \u003e= chibar2 = 0.0000\rThere are 16 degrees of freedom (see DF: in top-right of output), making the confidence interval broader, spanning from 396.69 to 511.13 (compared with 402.59 to 505.24 based on the asymptotic distribution). Here, the degrees of freedom are just df = J - 1, but there is generally no simple expression and df need not be an integer. For models that include ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:6:1","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"6.2 Hypothesis test and confidence interval for the between-cluster variance We now consider testing hypotheses regarding the between-cluster variance,$\\psi$. In particular, we are often interested in the null hypothesis $$H_0:\\psi=0\\quad\\text{against}\\quad H_a:\\psi\u003e0$$ This null hypothesis is equivalent to the hypothesis that $\\beta_j^*=0$ or that there is no random intercept in the model. If the null hypothesis is true, we can use ordinary regression instead of a variance-components model. The test we will be using most in this book for testing variance components is the likelihood-ratio test. 在统计分析中，我们经常使用似然比检验来测试群组间方差 $ \\psi $ 是否为零的假设。如果零假设 $ H_0: \\psi = 0 $ 成立，意味着不存在群组间的变异，可以采用普通回归模型；如果群组间存在显著差异（即备择假设 $ H_a: \\psi \u003e 0 $ 成立），则需要使用方差分量模型来更准确地描述数据结构。似然比检验通过比较零假设和备择假设下的似然来决定是否拒绝零假设。 Likelihood-ratio test A likelihood-ratio test can be obtained by fitting the model with and without the random intercept. The likelihood-ratio test statistic then is $$L = 2(l_1-l_0)$$ where $l_1$ is the maximized log likelihood for the variance-components model (which includes $\\zeta_j$) and $l_0$ is the maximized log likelihood for the model without $\\zeta_j$. If REML is used, $l_1$ and $l_0$ are the corresponding REML likelihoods. Importantly, the distribution of $L$ under $H_0$ is not $\\chi^2$ with 1 degree of freedom (df) as usual. This is because the null hypothesis is on the boundary of the parameter space since $\\psi\\geq 0$, violating the regularity conditions of standard statistical test theory. For datasets simulated under the null hypothesis, without the random intercept, we would expect positive within-cluster correlations in about half the datasets and negative within-cluster correlations in the other half. Thus,$\\psi$would be estimated as positive half the time and as 0 (because $\\psi$ would have to be negative to produce negative correlations but is constrained to be nonnegative) the other half the time. The correct asymptotic sampling distribution under the null hypothesis hence takes the simple form of a 50:50 mixture of a spike at 0 and $a^2$with 1 df, often written as$0.5\\chi^2(0)+0.5\\chi^2(1)$, where $\\chi^2(0)$ is a spike of height 1 at 0. The correct $p$-value can be obtained by simply dividing the “naive” $p$-value, based on the $\\chi^2$ with 1 df, by 2. These results were derived for ML by Stram and Lee (1994) and others, and Morrell (1998) showed that they also hold for REML. The p-value based on the mixture of chi-square distributions is given at the bottom of the xtreg and mixed output, where the correct sampling distribution is referred to as chibar2(01) (click on chibar2(01), which is shown in blue in the Stata Results window to find an explanation). We can also perform the likelihood-ratio test (based on REML) ourselves by fitting the variance-components model, storing the estimates, then fitting the model without the random intercept, and finally comparing the models by using the lrtest command: 似然比检验的基本概念 似然比检验是一种统计检验方法，用来比较两个模型的拟合优度。这里的两个模型分别是包含随机截距的方差分量模型（模型1）和不包含随机截距的普通回归模型（模型0）。 似然函数和对数似然 似然函数（Likelihood function）是一个关于模型参数的函数，表示给定参数时观测数据出现的概率。 对数似然（Log likelihood）是似然函数的自然对数，通常用于简化计算。 似然比检验的计算 似然比检验统计量 $ L $ 的计算公式是： $$L = 2(l_1 - l_0)$$ 其中，$ l_1 $ 是包含随机截距的方差分量模型的最大对数似然值，$ l_0 $ 是不包含随机截距的模型的最大对数似然值。 似然比检验统计量的分布 在零假设 $ H_0 $ 下，即群组间方差 $ \\psi = 0 $ 时，$ L $ 的分布不是标准的卡方分布（$ \\chi^2 $ 分布）与1自由度。这是因为零假设位于参数空间的边界上，$ \\psi $ 必须非负，这违反了标准统计检验理论的正则性条件。 零假设下的分布特性 在零假设下，我们期望大约一半的数据集显示出群体内的正相关，另一半显示出负相关。因此，$ \\psi $ 有一半的时间被估计为正，另一半时间被估计为0（因为 $ \\psi $ 必须是负的才能产生负相关，但它被限制为非负）。 正确的渐近抽样分布 正确的渐近抽样分布是一个简单的混合分布，形式为： $$ 0.5\\chi^2(0) + 0.5\\chi^2(1) $$ 这里，$ \\chi^2(0) $ 是在0处高度为1的尖峰，表示 $ \\psi = 0 $ 的情况。 p值的计算 基于上述混合分布的p值可以通过将基于 $ \\chi^2 $ 与1自由度的“天真”p值除以2来获得。 软件实现 在某些统计软件（如Stata）中，可以直接使用命令 xtreg 和 mixed 来拟合模型，并使用 lrtest 命令来执行似然比检验。正确的抽样分布在软件输出中可能被称为 chibar2(01)。 总结 似然比检验是一种比较包含和不包含随机截距的两个模型拟合优度的方法。检验统计量的计算基于两个模型的最大对数似然值之差。在零假设下，检验统计量的分布是一个混合分布，需要特别的方法来计算p值。 quietly mixed wm || id:, reml estimates store ri quietly mixed wm, reml lrtest ri . Likeliho","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:6:2","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"7 Model as data-generating mechanism Figure 2.7 shows how the responses $y_{ij}$ can be viewed as resulting from sequential (or hierarchical) sampling, first of $\\zeta_j$ and then of $y_{ij}$ given $\\zeta_j$. For concreteness, we consider normal distributions for $\\zeta_j$ and $\\epsilon_{ij}$, but these distributional assumptions are usually not important for inferences. Figure 2.7: Illustration of hierarchical sampling in variancecomponents model\rAs seen in the top of the figure, the random intercept $ζ_j$ has a normal distribution with mean 0 (and variance $\\psi$). Drawing a realization from this distribution for subject $j$ determines the mean $\\beta + ζ_j$ of the distribution from which responses $y_{ij}$ for this subject are subsequently drawn. At a given measurement occasion $i$, a response $y_{ij}$ is therefore sampled from a normal distribution with mean $\\beta + ζ_j$ (and variance $\\theta$), $y_{ij} \\sim N(\\beta + ζ_j, \\theta)$ (see the bottom distribution in the figure). Equivalently, a residual (or measurement error) $\\epsilon_{ij}$ is drawn from a normal distribution with mean 0 and variance $\\theta$. The hierarchical sampling perspective is the reason why multilevel models are sometimes called hierarchical models. In this description, we have viewed the variance-components model as the data-generating mechanism for $y_{ij}$ for given occasions and subjects. The variance-components model is often motivated in terms of two-stage survey sampling, namely random sampling of clusters, such as schools, followed by random sampling of units (for example, students) from clusters. In this view, the top distribution of figure 2.7 represents the distribution of cluster means $\\beta + \\zeta_j$ in the population of clusters, where each cluster comes with a realized value of $\\zeta_j$. In stage 1, cluster $j$ is sampled, and the bottom distribution represents the population of units in that cluster, where each unit in the cluster comes with a realized value of $\\epsilon_{ij}$ and hence $y_{ij}$. We then randomly draw units from the cluster population, which determines the $y_{ij}$ in our sample. However, when motivating the model through survey sampling, it is important to remember that the sampling itself does not produce the within-cluster dependence. Such dependence must already exist in the population from which the sample was drawn, which we shall refer to as the finite population (because it is not infinite). Two-stage sampling merely guarantees that the sample contains multiple units per cluster, making it possible to separately estimate the between-cluster and within-cluster variance components $ψ$ and $θ$. In contrast, a simple random sample of, say, 1,000 students from all U.S. high school students would be unlikely to contain any two students belonging to the same school, so we could only estimate the total variance, $ψ+θ$, not the separate variance components. Because the dependence preexists in the finite population, it is more useful to think of the variance-components model as the data-generating mechanism that generated the responses $y_{ij}$ for the finite population. Furthermore, it is the model parameters $\\beta$, $\\psi$, and $\\theta$ of this underlying variance-components model that we wish to estimate, not any finite population characteristic. Even if the sample contained the entire finite population, that is, all high school students in the U.S., we would still estimate the parameters with error, because all we have observed is a realization from the model, albeit for a large number of clusters and units. This imprecision of parameter estimates is even more pronounced if the finite population is small, for instance, all high school students in Monaco. When the model is viewed as a data-generating mechanism, randomness comes from drawing the response from a distribution [the $z_j$ from $N(0, \\psi)$ and the $\\epsilon_{ij}$ from $N(0, \\theta)$, resulting in $y_{ij} = \\beta + z_j + \\epsilon_{ij}$], not only from","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:7:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"8 Fixed versus random effects In the peak-expiratory-flow data, each subject $j$ has a different effect $\\zeta_j$ on the measured peak-expiratory-flow rates. In analysis of variance (ANOVA) terminology (see sections 1.4 and 1.9), the subjects can therefore be thought of as the levels of a factor or categorical explanatory variable. Because the effects of subjects are random, the variance-components model is therefore sometimes referred to as a one-way random-effects ANOVA model. The one-way random-effects ANOVA model can be written as $$y_{ij} = \\beta+\\zeta_j+\\epsilon_{ij},\\quad E(\\epsilon_{ij}|\\zeta_j)=0,E(\\zeta_j)=0,\\mathrm{Var}(\\epsilon_{ij}|\\zeta_j)=\\theta,\\mathrm{Var}(\\zeta_j)=\\psi(2.6)$$ where $\\zeta_j$ is a random intercept. In contrast, the one-way fixed-effects ANOVA model is $$y_{ij} = \\beta+\\alpha_j+\\epsilon_{ij},\\quad E(\\epsilon_{ij})=0,\\mathrm{Var}(\\epsilon_{ij})=\\theta,\\quad\\sum_{j=1}^J\\alpha_j=0\\quad(2.7)$$ where $α_j$ are unknown, fixed, cluster-specific parameters. In the random-effects model, the random intercepts are uncorrelated across clusters and uncorrelated with the level-1 residuals. In both models, the level-1 residuals are uncorrelated across units. Both random-effects models and fixed-effects models include cluster-specific intercepts- $ζ_j$ and $α_j$, respectively-to account for unobserved heterogeneity. Thus, a natural question is whether to use a random- or fixed-effects approach. where $alpha_j$; are unknown, fixed, cluster-specific parameters. In the random-effects model, the random intercepts are uncorrelated across clusters and uncorrelated with the level-1 residuals. In both models, the level-1 residuals are uncorrelated across units. Both random-effects models and fixed-effects models include cluster-specific intercepts-$zeta_j$ and $alpha_j$, respectively-to account for unobserved heterogeneity. Thus, a natural question is whether to use a random- or fixed-effects approach. One way of answering this question is by being explicit about the target of inference, namely, whether interest concerns the population of clusters or the particular clusters in the dataset. Here the “population of clusters” refers to the infinite population, or the data-generating mechanism for the clusters. If we are interested in the population of clusters, the random-effects model is appropriate. In that model, $\\beta$ represents the population mean for the population of clusters (and for each cluster, the population of units in the cluster) and $\\psi$ represents the variance for the population of clusters. The model specifies how the cluster-specific means $\\beta + \\zeta_j$ are generated. In the variance-components model, $\\psi$ represents between-cluster variability due to cluster-level random (unexplained) processes that affect the response variable. As we will see in later chapters, the data-generating model for the cluster means can also contain cluster-level covariates to explain between-cluster variability. If we do not wish to generalize beyond the particular clusters in the sample, the fixed-effects model is appropriate. In that model, $\\beta$ represents the mean for the sample of clusters (and for each cluster, the population of units in the cluster). The model allows each cluster to have a different mean $\\beta + \\alpha_j$, but does not specify how the means are generated. If the cluster means were generated by a random process, we merely condition on their realized values and do not learn about the process. It is not possible to include cluster-level covariates in fixed-effects models, so in this approach, no attempt is made to explain between-cluster variability. Standard errors, confidence intervals, and p-values are based on the notion of repeated samples of data from a model. For instance, the standard error for $\\hat{\\beta}$ is the standard deviation of the estimates over repeated samples. In the random-effects approach, the random intercepts $\\zeta_j$ change in repeated samples (in addition to $\\ep","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:8:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"第一部分：随机效应模型与固定效应模型的基本概念 随机效应模型（Random-Effects Model） 随机效应模型假设，不同的组（比如不同的受试者）对测量结果有不同的影响，这种影响是随机的。在方差分析中，我们可以将不同的受试者视为一个因素的不同水平。这里的“随机”意味着每个受试者的影响是从一个共同的分布中随机抽取的。 公式： $y_{ij} = \\beta + \\zeta_j + \\epsilon_{ij}$ $y_{ij}$：第$i$个观测值在第$j$个组中的结果。 $\\beta$：总体均值。 $\\zeta_j$：第$j$个组的随机截距（即受试者效应）。 $\\epsilon_{ij}$：第$i$个观测值在第$j$个组中的随机误差。 期望和方差： $E(\\epsilon_{ij}|\\zeta_j) = 0$：给定受试者效应，误差项的期望为0。 $E(\\zeta_j) = 0$：所有受试者效应的平均值为0。 $\\mathrm{Var}(\\epsilon_{ij}|\\zeta_j) = \\theta$：给定受试者效应，误差项的方差为$\\theta$。 $\\mathrm{Var}(\\zeta_j) = \\psi$：受试者效应的方差为$\\psi$。 固定效应模型（Fixed-Effects Model） 固定效应模型假设，不同的组对测量结果有不同的影响，但这种影响是固定的，不是随机的。这意味着每个组的影响是已知的，并且不会随着样本的不同而变化。 公式： $y_{ij} = \\beta + \\alpha_j + \\epsilon_{ij}$ $\\alpha_j$：第$j$个组的固定效应。 期望和方差： $E(\\epsilon_{ij}) = 0$：误差项的期望为0。 $\\mathrm{Var}(\\epsilon_{ij}) = \\theta$：误差项的方差为$\\theta$。 $\\sum_{j=1}^J \\alpha_j = 0$：所有组的固定效应之和为0。 ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:8:1","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"第二部分：随机效应与固定效应模型的选择 目标的推断：如果你对所有可能的组（比如所有可能的受试者）感兴趣，随机效应模型更合适。如果你只对样本中的特定组感兴趣，固定效应模型更合适。 标准误、置信区间和p值：随机效应模型在重复抽样时，随机截距会变化，导致$\\hat{\\beta}$的标准误更大。固定效应模型中，固定截距在重复抽样时保持不变。 预测和推断：随机效应模型允许对组效应进行预测，这些预测可能比固定效应模型的估计具有更好的属性。 可交换性：随机效应模型要求组效应在交换组标签后具有相同的联合分布，即没有先验的顺序或分组。 固定效应的使用：当样本中的组不能代表所有可能的组时，或者当组间差异是主要兴趣时，应使用固定效应模型。 随机效应的使用：当样本中有足够的组（通常超过10或20个）时，应使用随机效应模型。 组大小：两种方法都不需要大的组大小，除非对组特定的截距进行推断。 模型参数：随机效应模型通常更简洁，因为它只需要估计更少的参数。 ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:8:2","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"第三部分：随机效应与固定效应模型的进一步理解 随机效应模型的应用场景 泛化到更大的群体：如果你的研究目标是泛化到一个更大的群体，比如从研究几个学校的学生泛化到所有学校的学生，随机效应模型是合适的。因为随机效应模型允许我们估计群体间变异性（$\\psi$）。 预测组效应：在随机效应模型中，我们可以预测每个组的效应（$\\zeta_j$）。这种预测通常比固定效应模型中的估计更为精确，因为它们利用了群体间的平均信息，这种现象被称为“收缩”或“部分汇总”。 交换性假设：随机效应模型要求组效应是交换的，这意味着如果我们重新标记组，组效应的联合分布不会改变。这在实际中可能不总是合理的，比如不同国家的效应可能由于文化、经济等因素而有本质的不同。 固定效应模型的应用场景 特定群体的分析：如果你只对研究中的特定群体感兴趣，比如只关心研究中包含的几个学校，固定效应模型是合适的。 不泛化：固定效应模型不涉及泛化到更大的群体，它只关注样本中的组。因此，它不估计群体间变异性，也不会提供关于群体间差异的信息。 组间差异的主要兴趣：如果研究的主要兴趣在于比较不同组之间的差异，固定效应模型可能更合适，因为它允许每个组有一个独特的效应。 模型选择的考虑因素 样本大小：随机效应模型通常需要更多的组来准确估计群体间变异性。如果组的数量较少，固定效应模型可能是更好的选择。 组内和组间变异性：如果组内的变异性远大于组间的变异性，固定效应模型可能更合适。相反，如果组间的变异性是一个重要的研究问题，随机效应模型可能更合适。 模型的简洁性：随机效应模型通常参数更少，更简洁。固定效应模型可能会因为每个组都有一个独特的参数而变得复杂。 数据结构：如果数据结构允许，比如有足够的组和每个组中有足够的观测值，随机效应模型可以提供更丰富的信息。 ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:8:3","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"第四部分：实际例子和应用 让我们通过一个实际的例子来进一步理解这些概念： 假设我们正在研究不同学校学生的数学成绩。我们有来自5个不同学校的学生的成绩数据。 随机效应模型：我们可能会假设每个学校有一个影响学生成绩的随机效应。我们感兴趣的是所有可能学校的学生成绩的总体趋势和学校间成绩的变异性。在这个模型中，我们可以估计所有学校学生的平均成绩（$\\beta$），学校间成绩的变异性（$\\psi$），以及每个学校相对于平均水平的成绩差异（$\\zeta_j$）。 固定效应模型：如果我们只对这5个特定学校的学生成绩感兴趣，并且不打算将结果泛化到其他学校，我们可能会选择固定效应模型。在这个模型中，每个学校都有一个固定效应（$\\alpha_j$），代表了该学校学生成绩的平均差异。 通过这个例子，我们可以看到随机效应模型和固定效应模型在实际研究中的应用和它们的区别。 ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:8:4","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"第五部分：模型估计和模型选择的统计考量 模型估计 参数估计：在随机效应模型中，我们估计的是总体均值（$\\beta$）、群体间变异性（$\\psi$）和误差项的方差（$\\theta$）。在固定效应模型中，我们估计的是总体均值（$\\beta$）和每个组的固定效应（$\\alpha_j$），以及误差项的方差（$\\theta$）。 限制条件：固定效应模型中，为了可识别性，需要有一个限制条件，比如 $\\sum_{j=1}^J \\alpha_j = 0$。这意味着所有组的固定效应之和必须为零。 随机效应的估计：在随机效应模型中，$\\zeta_j$ 是随机变量，它们的估计通常涉及到复杂的统计技术，如限制最大似然估计（REML）或贝叶斯方法。 模型选择 信息准则：可以使用Akaike信息准则（AIC）或贝叶斯信息准则（BIC）来比较不同模型的拟合优度。通常选择信息准则值较低的模型。 假设检验：可以使用F检验或似然比检验（LRT）来比较随机效应模型和固定效应模型的拟合优度。如果随机效应模型的拟合显著优于固定效应模型，可能会倾向于选择随机效应模型。 交叉验证：在某些情况下，可以使用交叉验证来评估模型的预测能力。如果随机效应模型在预测新数据时表现更好，这可能是选择它的一个理由。 理论依据：最终，模型的选择应该基于理论依据和研究问题。如果理论或先前的研究支持群体间存在随机变异性，随机效应模型可能更合适。 ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:8:5","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"第六部分：实际数据分析示例 让我们通过一个简单的数据分析示例来说明这些概念： 假设我们有以下数据集，它包含了三个不同学校（A、B、C）的学生数学成绩： 学生编号 学校 数学成绩 1 A 75 2 A 85 … … … 20 C 80 随机效应模型分析 模型设定：设定 $y_{ij} = \\beta + \\zeta_j + \\epsilon_{ij}$，其中 $i$ 表示学生，$j$ 表示学校。 参数估计：估计 $\\beta$、$\\zeta_j$（对于学校A、B、C）和 $\\theta$。 结果解释：$\\beta$ 表示所有学校学生的平均成绩，$\\zeta_j$ 表示每个学校相对于平均水平的成绩差异。 固定效应模型分析 模型设定：设定 $y_{ij} = \\beta + \\alpha_j + \\epsilon_{ij}$，其中 $\\alpha_j$ 是学校的固定效应。 参数估计：估计 $\\beta$、$\\alpha_A$、$\\alpha_B$、$\\alpha_C$ 和 $\\theta$，并应用限制条件 $\\alpha_A + \\alpha_B + \\alpha_C = 0$。 结果解释：$\\beta$ 表示基准学校（假设为学校A）的平均成绩，$\\alpha_B$ 和 $\\alpha_C$ 表示学校B和C相对于学校A的成绩差异。 ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:8:6","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"第七部分：模型诊断和模型假设检验 模型诊断 残差分析：检查残差是否随机分布，没有模式或趋势。这可以帮助我们确定模型是否适当。 方差齐性检验：检查不同组的残差方差是否一致。如果残差方差不同，可能需要考虑更复杂的模型。 正态性检验：检查残差是否服从正态分布。如果残差不服从正态分布，可能需要转换数据或使用非参数方法。 随机效应的显著性检验：检查随机效应是否显著不同于零。如果随机效应不显著，可能不需要随机效应模型。 模型假设检验 固定效应与随机效应的比较：可以使用LRT（似然比检验）来比较固定效应模型和随机效应模型的拟合优度。 随机效应的显著性：可以使用Wald检验或似然比检验来检验随机效应是否显著。如果不显著，固定效应模型可能是更好的选择。 交叉项的检验：如果模型中包括交互作用，需要检验这些交互作用是否显著。 ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:8:7","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"第八部分：模型的实用性和局限性 实用性 灵活性：随机效应模型和固定效应模型都提供了灵活性，可以根据研究问题和数据特性进行选择。 泛化能力：随机效应模型允许我们泛化到更大的群体，而固定效应模型则专注于特定样本。 控制未观察到的异质性：两种模型都可以控制未观察到的异质性，提高估计的准确性。 局限性 随机效应模型的假设：随机效应模型的假设可能在某些情况下不成立，比如当组效应不满足交换性时。 固定效应模型的泛化能力：固定效应模型不提供关于群体间差异的信息，限制了其泛化能力。 计算复杂性：随机效应模型的参数估计可能涉及复杂的计算，特别是当模型包含多个随机效应时。 数据要求：两种模型都对数据有一定的要求，比如需要足够的组和观测值来估计模型参数。 ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:8:8","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"第九部分：实际应用中的考虑 数据的可用性：在实际应用中，数据的可用性可能限制了模型的选择。例如，如果数据中只有少数几个组，随机效应模型可能不是最佳选择。 研究目的：研究的目的应该指导模型的选择。如果目的是比较不同组之间的差异，固定效应模型可能更合适；如果目的是估计群体间变异性，随机效应模型可能更合适。 模型的解释性：在选择模型时，应该考虑模型结果的解释性。随机效应模型和固定效应模型提供了不同的视角来解释数据。 软件和计算资源：不同的统计软件和计算资源可能影响模型的选择和参数估计的可行性。 ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:8:9","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"结论 随机效应模型和固定效应模型是分析层次数据的强大工具。它们各自有优势和局限性，选择哪种模型取决于研究问题、数据特性和理论依据。通过逐步分析和理解这些模型，我们可以更准确地解释数据并做出合理的推断。 ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:8:10","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"第十部分：具体案例分析 让我们通过一个具体案例来进一步理解随机效应和固定效应模型的应用。 案例背景：假设我们研究的是不同医院的患者康复时间。我们有来自10个不同地区的医院数据。 数据结构： Level-1（个体层面）：每个患者的康复时间。 Level-2（组/医院层面）：不同医院的特定特征，如医院规模、医疗资源等。 研究问题： 不同医院的患者康复时间是否存在显著差异？ 医院特征是否影响患者的康复时间？ 随机效应模型的应用 模型设定：$y_{ij} = \\beta_0 + \\beta_1 x_{ij} + \\zeta_j + \\epsilon_{ij}$，其中 $y_{ij}$ 是第 $i$ 个患者（$i = 1, \\ldots, n_j$）在第 $j$ 个医院的康复时间，$x_{ij}$ 是患者的某个特征，$\\zeta_j$ 是医院 $j$ 的随机效应，$\\epsilon_{ij}$ 是误差项。 参数解释： $\\beta_0$：所有医院患者的平均康复时间。 $\\beta_1$：患者特征对康复时间的影响。 $\\zeta_j$：第 $j$ 个医院的随机效应，反映了医院未观测到的异质性。 模型估计：使用最大似然估计或限制最大似然估计（REML）来估计模型参数。 模型检验：检验医院随机效应的显著性，以确定是否需要随机效应模型。 固定效应模型的应用 模型设定：$y_{ij} = \\beta_0 + \\beta_1 x_{ij} + \\alpha_j + \\epsilon_{ij}$，其中 $\\alpha_j$ 是第 $j$ 个医院的固定效应。 参数解释： $\\beta_0$ 和 $\\beta_1$：同随机效应模型。 $\\alpha_j$：第 $j$ 个医院相对于基准医院（通常为医院1）的固定效应。 模型估计：固定效应模型的参数估计通常较为简单，可以直接使用最小二乘法。 模型检验：检验固定效应是否显著，以确定不同医院是否对康复时间有显著影响。 模型选择 如果我们对泛化到所有可能的医院感兴趣，随机效应模型可能更合适。 如果我们只对样本中的医院感兴趣，固定效应模型可能更合适。 如果医院随机效应显著，表明不同医院之间存在未观测到的异质性，支持随机效应模型的使用。 如果医院数量较少或医院效应不显著，固定效应模型可能是更好的选择。 ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:8:11","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"第十一部分：模型的进一步讨论 模型的稳健性：在实际应用中，可能需要检查模型对异常值或数据分布的敏感性。 模型的敏感性分析：通过改变模型假设或参数，评估模型结果的稳定性。 模型的扩展：根据研究需要，可以扩展模型以包括更多的随机效应、固定效应或交互作用。 模型的实用性：考虑模型在实际研究中的可行性，包括数据的可用性、模型的复杂性和解释性。 ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:8:12","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"9 Crossed versus nested effects So far, we have considered the random or fixed effects of a single cluster identifier or factor, subjects. Another potential factor in the peak-expiratory-flow dataset is the measurement occasion with 2 levels, occasions 1 and 2. In the variance-components model, occasion was allowed to have an effect on the response variable only via the residual term $epsilon_{ij}$, which takes on a different value for each combination of subject and occasion and has mean 0 for each occasion across subjects. We have therefore implicitly treated occasions as nested (and exchangeable) within subjects, meaning that occasion (2 versus 1) does not have a common systematic effect for all subjects. If all subjects had been measured and remeasured in the same sessions and if there were anything specific to the session (for example, time of day, temperature, or calibration of the measurement instrument) that could influence measurements on all subjects in a similar way, then subjects and occasions would be crossed. We would then include an occasion-specific term (“main effect of occasion”) in the model that takes on the same value for all subjects. The distinction between nested and crossed factors is illustrated in figure 2.8. Figure 2.7: Illustration of hierarchical sampling in variancecomponents model\rFigure 2.7: Illustration of hierarchical sampling in variancecomponents model\rIn the nested case, the effect of occasion 1 (or 2) is different for every subject; in the crossed case, there is a main effect of occasion that is the same for each subject (and possibly an occasion by subject interaction). In the crossed case, the model can be described as a two-way ANOVA model. A subject-by-occasion interaction could in this case be included in addition to the main effects of each factor. However, because there are no replications for each subject-occasion combination in the peak-expiratory-flow application, an interaction term would be confounded with the error term $epsilon_{ij}$. If a random effect is specified for subjects and a fixed effect for occasion, we obtain a so-called mixed-effects two-way ANOVA model. Such a model can be fit by introducing a dummy variable for the second occasion in the fixed part of the model: ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:9:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"1. 随机效应与固定效应 在统计学中，我们经常需要考虑数据中的不同来源的变异。随机效应和固定效应是两种不同的处理方式： 随机效应：假设效应是从一个更大的群体中随机抽样的，我们感兴趣的是这个群体的变异性。 固定效应：假设效应是研究中特定的，我们感兴趣的是这些特定效应对结果的影响。 ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:9:1","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"2. 单个簇标识符或因子 这里的“簇”可以理解为数据的一个分组单位。比如在医学研究中，不同的患者可以看作不同的簇。 ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:9:2","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"3. 测量场合（Occasion） 在提到的峰值呼气流量数据集中，测量场合是一个潜在的影响因素，有两个水平：场合1和场合2。这里的“场合”可以理解为数据收集的具体时间点或环境。 ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:9:3","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"4. 方差组分模型 方差组分模型是一种统计模型，它允许我们估计数据中不同来源的方差。在这个模型中，场合被允许通过残差项 $ \\epsilon_{ij} $ 对响应变量产生影响，这个残差项对于每个主体和场合的组合都有不同的值，并且在所有主体中对于每个场合的平均值为0。 ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:9:4","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"5. 嵌套与交叉 嵌套（Nested）：场合是嵌套在主体内部的，意味着每个主体有自己的场合效应，场合1和场合2对每个主体有不同的影响。 交叉（Crossed）：场合效应对所有主体都是相同的，即场合效应是交叉的。 ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:9:5","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"7. 两因素方差分析（Two-way ANOVA） 在交叉的情况下，模型可以被描述为两因素方差分析模型： 主效应：每个因素（如场合）对结果的独立影响。 交互作用：两个因素共同作用对结果的影响。 ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:9:6","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"8. 混合效应模型的构建 混合效应模型（Mixed-Effects Model）是一种统计模型，用于处理具有层次结构的数据。在这种模型中，我们通常有固定效应和随机效应： 固定效应：通常指的是我们感兴趣的主要因素或处理效应，我们假设这些效应在整个研究中是一致的。 随机效应：用来表示数据中的随机变异，例如个体之间的差异或时间点之间的差异。 ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:9:7","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"9. 两因素方差分析（Two-Way ANOVA） 在传统的两因素方差分析中，我们研究两个独立变量（因素）对一个依赖变量的影响。每个因素都有多个水平，我们想要了解这些因素的单个效应以及它们之间可能的交互效应。 ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:9:8","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"10. 混合效应两因素方差分析模型 当我们的数据结构不适合传统的两因素方差分析时，比如数据具有嵌套结构或者我们想要考虑随机效应，我们可以使用混合效应两因素方差分析模型。这种模型允许我们： 考虑固定效应，比如不同场合对峰值呼气流速的影响。 考虑随机效应，比如不同个体之间的变异性。 通过引入虚拟变量来处理固定效应，比如场合的效应。 ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:9:9","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"11. 虚拟变量的引入 在混合效应模型中，我们通过引入虚拟变量来表示固定效应。例如，如果我们有两个场合，我们可以创建一个虚拟变量，当数据点是第一个场合时，虚拟变量取值为0；当数据点是第二个场合时，虚拟变量取值为1。 ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:9:10","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"12. 模型公式 假设我们的数据集中有变量 $ Y $（响应变量，比如峰值呼气流速），$ Subject $（个体），和 $ Occasion $（场合）。我们的混合效应模型可以表示为： $$ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ijk} $$ 其中： $ Y_{ijk} $ 是第 $ i $ 个个体在第 $ j $ 个场合的第 $ k $ 次测量的响应变量。 $ \\mu $ 是总体均值。 $ \\alpha_i $ 是第 $ i $ 个个体的随机效应。 $ \\beta_j $ 是第 $ j $ 个场合的固定效应。 $ \\epsilon_{ijk} $ 是误差项，对于每个个体和场合的组合都有不同的值，且均值为0。 ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:9:11","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"13. 模型的拟合 混合效应模型通常使用最大似然估计或贝叶斯方法来拟合。这些方法可以帮助我们估计模型中的参数，比如固定效应的大小和随机效应的方差。 ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:9:12","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"14. 结果解释 一旦模型拟合完成，我们可以解释固定效应和随机效应对响应变量的影响。例如，我们可以比较不同场合的峰值呼气流速是否有显著差异，或者个体之间的变异性是否显著。 ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:9:13","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"15. 模型假设 在使用混合效应模型时，我们还需要考虑一些基本假设，比如误差项的正态性、方差齐性等。这些假设需要通过数据的探索性分析来检查。 generate occ2 = occasion==2 mixed wm occ2 || id:, reml stddeviations Performing EM optimization ...\rPerforming gradient-based optimization: Iteration 0: Log restricted-likelihood = -177.43977 Iteration 1: Log restricted-likelihood = -177.43977 Computing standard errors ...\rMixed-effects REML regression Number of obs = 34\rGroup variable: id Number of groups = 17\rObs per group:\rmin = 2\ravg = 2.0\rmax = 2\rWald chi2(1) = 0.17\rLog restricted-likelihood = -177.43977 Prob \u003e chi2 = 0.6806\r------------------------------------------------------------------------------\rwm | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rocc2 | 2.88 7.00 0.41 0.681 -10.84 16.61\r_cons | 452.47 27.22 16.62 0.000 399.12 505.82\r------------------------------------------------------------------------------\r------------------------------------------------------------------------------\rRandom-effects parameters | Estimate Std. err. [95% conf. interval]\r-----------------------------+------------------------------------------------\rid: Identity |\rsd(_cons) | 110.35 19.84 77.57 156.98\r-----------------------------+------------------------------------------------\rsd(Residual) | 20.42 3.61 14.44 28.87\r------------------------------------------------------------------------------\rLR test vs. linear model: chibar2(01) = 43.71 Prob \u003e= chibar2 = 0.0000\rInstead of creating the dummy variable occ2, we could have used the factor-variable notation i.occasion within the mixed or xtreg commands. We see that there is no evidence for an effect of occasion, which in this example could only be interpreted as a practice effect because occasion 1 was not at the same time for the subjects. If there had been considerably more than two occasions, we could have specified a random effect for occasion that is crossed with the random effect for subject. Such models with crossed random effects are discussed in chapter 9. ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:9:14","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"10 Parameter estimation ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:10:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"10.1 Model assumptions We now explicitly state a set of assumptions that are sufficient for everything we want to do in this chapter but are not always necessary. We briefly state which assumptions are needed for properties such as consistency and efficiency of the standard estimators for variance components models discussed in section 2.10.2. Mean structure and covariance structure The total residual $\\xi_{ij}=\\zeta_j+\\epsilon_{ij}$ is assumed to have zero expectation: $$E(\\zeta_j+\\epsilon_{ij}) = 0$$ This assumption implies that the expectation of the response (conditional expectation if there were covariates), called the mean structure, is $$E(y_{ij})=\\beta $$ If the mean structure is correctly specified, standard point estimators $\\widehat{\\beta}$ (such as ML, REML, and FGLS) of the parameter $\\beta$ will be consistent, meaning that $\\widehat{\\beta}$ approaches $\\beta$ as the number of clusters $J$ tends to infinity. A consistent estimator need not be unbiased in small samples, meaning that the average of $\\widehat{\\beta}$, over repeated samples, may not equal $\\beta$. For $\\widehat{\\beta}$ to be unbiased, the distribution of $\\zeta_j + \\epsilon_{ij}$ must in general be symmetric (for instance, a normal distribution). For the covariance structure, it is assumed that the random intercept $\\zeta_j$ (with constant variance $\\psi$) and the level-1 residual $\\epsilon_{ij}$ (with constant variance $\\theta$) are uncorrelated, Cor($\\epsilon_{ij}$, $\\zeta_j$) = 0. From this it follows that the variance of the total residual is $$\\mathrm{Var}(\\zeta_j+\\epsilon_{ij}) = \\psi+\\theta $$ The $\\epsilon_{ij}$ are assumed to be uncorrelated across units $i$, from which it follows that the covariance between total residuals for two units $i$ and $i’$ in the same cluster is $$\\mathrm{Cov}(\\zeta_j+\\epsilon_{ij},\\zeta_j+\\epsilon_{i^{\\prime}j}) = \\psi $$ Both $\\zeta_j$ and $\\epsilon_{ij}$ are also uncorrelated across different clusters so that there are no correlations between the total residuals of units in different clusters. These assumptions imply that the covariance structure of the responses is $$\\mathrm{Var}(y_{ij})\\quad=\\quad\\psi+\\theta $$ $$\\mathrm{Cov}(y_{ij},y_{i^{\\prime}j})\\quad=\\quad\\psi\\quad\\mathrm{if} i\\neq i^{\\prime}$$ $$\\mathrm{Cov}(y_{ij},y_{i^{\\prime}j^{\\prime}})\\quad=\\quad0\\quad\\mathrm{if~}j\\neq j^{\\prime}$$ (The corresponding matrix is shown for the case where in $n_j = 3$ display 3.2.) If both the mean and covariance structure are correct, then the estimators of all parameters in the variance-components model are consistent and asymptotically efficient, and the model-based standard errors are consistent. An efficient estimator is one that has a smaller standard error than any other estimator. Asymptotically efficient estimators acquire that property only asymptotically, as the sample size goes to infinity. For many estimators, the asymptotic sampling distribution is normal, making it easy to construct large-sample confidence intervals and tests. In variance-components models and other two-level models, “large sample” and “asymptotics” refer to the number of clusters going to infinity, possibly with fixed cluster size. Consistent estimates for $\\beta $ can be obtained even if the covariance structure is not correct. In this case, model-based standard errors will be inconsistent, but robust standard errors can be used instead if there are enough clusters (for example, $J\\ge42$); see display 2.1. 总残差和期望值 首先，我们有一个总残差 $ \\xi_{ij} $，它由随机截距 $ \\zeta_j $ 和观测误差 $ \\epsilon_{ij} $ 组成： $$ \\xi_{ij} = \\zeta_j + \\epsilon_{ij} $$ 这个总残差的期望值被假定为0： $$ E(\\zeta_j + \\epsilon_{ij}) = 0 $$ 这意味着，如果我们有一个响应变量 $ y_{ij} $，它的期望值（如果有协变量的话就是条件期望）被称为均值结构，并且是： $$ E(y_{ij}) = \\beta $$ 一致性和无偏性 如果均值结构被正确指定，那么标准的点估计量 $ \\widehat{\\beta} $（例如最大似然估计MLE、限制最大似然估计REML、广义最小二乘法FGLS）将会是一致的。这里的“一致”意味着随着簇的数量 $ J $ 趋向于无穷大，估计量 $ \\widehat{\\beta} $ 会趋向于真实的参数 $ \\beta $。 然而，一致的估计量并不一定在小样本中是无偏的。无偏性意味着估计量的平均值，即在重复抽样中，应该等于真实的参数 $ \\beta $。为了使 $ \\widehat{\\beta} $ 无偏，$ \\zeta_j + \\epsilon","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:10:1","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"10.2 Different estimation methods A classical method for estimating the parameters of statistical models is maximum likelihood (ML) as will be explained below. When the number of clusters is not sufficiently large, the ML estimator of $ \\psi $ is downward biased. For this reason, restricted maximum likelihood (REML) estimation has been developed, which would be unbiased for balanced data if negative values of $ \\widehat{\\psi} $ were permitted. For unbalanced data and when negative variances are not permitted (as in mixed, reml), the bias is generally smaller than for ML. While the bias for $ \\psi $ does not lead to bias in the point estimate $ \\widehat{\\beta} $, it does affect the estimated standard error of $ \\widehat{\\beta} $ as will be seen in section 2.10.3. When the number of clusters is small, it is therefore recommended to use REML. See display 2.1 for the rule of thumb to choose REML if $ J \u003c 42 $. Another estimation method, particularly popular in econometrics, is feasible generalized least squares (FGLS). If the variance parameters were known, the ML estimates of $ \\beta $ would be obtained by generalized least squares (GLS). However, this method is not feasible because the variance parameters are not known, so feasible GLS plugs in consistent estimates for the variances. All three methods, ML, REML, and FGLS, are consistent for all parameters if the mean and covariance structure are correctly specified, and hence produce very similar estimates when the number of clusters is large We now give a brief description of ML estimation and relate it to REML and FGLS, as well as ANOVA. The likelihood function is just the joint probability density of all the observed responses $y_{ij}$ , $(i = 1,…,n_j),(j=1,…,J)$, as a function of the model parameters $\\beta$, $\\psi$, and $\\theta$. The likelihood contribution for cluster $j$ can be obtained by integrating the joint distribution of the $y_{ij}$ and $\\zeta_j$ over the random intercept. The product of the likelihood contributions for all clusters is the likelihood, often called the marginal likelihood (averaged over $\\zeta_j$). The idea is to find parameter estimates $\\widehat{\\beta}$, $\\widehat{\\psi}$, and $\\widehat{\\theta}$ that maximize the likelihood function, thus making the responses appear as likely as possible. and the SSE is the sum of squared deviations of the responses from their cluster means, $$\\mathrm{SSE~}=\\sum_{j=1}^J\\sum_{i=1}^n(y_{ij}-\\overline{y}_{\\cdot j})^2$$ The population mean $\\beta$ is estimated by the sample mean, $$\\widehat{\\beta} = \\overline{y}_{..}$$ and the ML estimator of the within-cluster variance $\\theta$ is $$\\widehat{\\theta}~=~\\frac{1}{J(n-1)}\\mathrm{SSE~=~MSE}$$ The ML estimator of the between-cluster variance $\\psi$ is given by where the subtraction of the second term is required because the level-1 residuals contribute to the MSS. With a small number of clusters, boundary estimates of $\\widehat{\\psi} = 0 $ can occur frequently. The ML estimators for $\\beta$ and $\\theta$ are unbiased if the model is true, whereas the estimator for $\\psi$ has downward bias. The REML estimator of $\\psi$ is given by $$\\widehat{\\psi} = \\frac{\\mathrm{MSS}}{(J-1)n}-\\frac{\\widehat{\\theta}}{n} = \\frac{1}{n} \\mathrm{(MMS-MSE)}$$ where MMS is the model mean square from the one-way ANOVA. This estimator is unbiased and corresponds to the moment or ANOVA estimator and the FGLS variance estimators implemented in Stata when cluster-sizes are constant as here. The estimators generally differ for unbalanced data. When negative estimates are set to 0, as in Stata and as shown for the ML estimator above, REML has some positive bias, but the bias is smaller than the negative bias of the ML estimator. The difference between REML and ML is that REML divides the MSS by $n$ times the ANOVA model degrees of freedom $J-1$, whereas ML divides by $n$ times $J$. The difference therefore becomes small when the number of clusters $J$ is large. In the example considered in this chapter, t","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:10:2","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"10.3 Inference for β Estimate and standard error: Balanced case We first consider the balanced case where $n_j=n$. As mentioned in the previous section, the ML and REML estimator $\\widehat{\\beta}$ of $\\beta$ in the variance-components model is simply the overall sample mean $$\\widehat{\\beta}~=~\\frac{1}{Jn}\\sum_{j=1}^{J}\\sum_{i=1}^{n}y_{ij}=\\frac{1}{J}\\sum_{j=1}^{J}\\overline{y}_{\\cdot j}$$ an unweighted mean of the cluster means. The estimated standard error is given by $$\\widehat{\\mathrm{SE}}(\\widehat{\\beta})~=~\\sqrt{\\frac{n\\widehat{\\psi}+\\widehat{\\theta}}{Jn}}=\\sqrt{\\frac{\\widehat{\\psi}+\\widehat{\\theta}/n}J}$$ Remember that $\\beta$ represents the mean $E(\\beta+\\zeta_j)$ of the cluster means for the population of clusters. When the cluster size $n$ is infinite, the cluster means are known with complete precision and uncertainty about $\\beta$ comes only from having a finite sample of $J$ clusters (rather than the infinite population of clusters). The estimated standard error then takes the familiar form $\\sqrt{\\widehat\\psi/J}$–because $\\widehat\\beta$ is the sample mean of $J$ (precisely known) cluster means, its estimated standard error is the standard deviation of the cluster means divided by the square root of the number of clusters. We see that $\\widehat{\\rm SE}(\\widehat\\beta)$ depends on $\\widehat\\psi$ and that the estimated standard error will therefore differ between ML and REML. When the number of clusters is small, the REML standard error is preferable. In the fixed-effects model (with the random $\\zeta_j$ replaced with fixed $\\alpha_j$; see section 2.8), the estimator of $\\beta$ is the same, but now the estimated standard error is $$\\widehat{\\mathrm{SE}}(\\widehat{\\beta}^F) = \\sqrt{\\frac{\\widehat{\\theta}}{Jn}}$$ which is smaller than the standard error $\\widehat{SE}(\\widehat{\\beta})$ in the random-effects model if $\\widehat{\\psi}\u003e0$. Because $\\beta$ now represents the sample mean $1/J\\sum_{j=1}^{J}(\\beta+\\alpha_j)$ of the cluster means for the $J$ clusters in the data, the standard error becomes 0 when the cluster size $n$ is infinite. Now consider the model without cluster-specific random or fixed effects (no $\\zeta_j$ or $\\alpha_j$) that assumes residuals to be independent. Such a single-level model would be used when the nesting of units in clusters is ignored. We refer to the corresponding estimator of $\\beta$ as the ordinary least-squares (OLS) estimator $\\widehat{\\beta}_{\\mathrm{OLS}}$. The estimator is the same as for the random- and fixed-effects models except that the estimated standard error is now approximately $$\\widehat{\\mathrm{SE}}(\\widehat{\\beta}^\\mathrm{OLS}) \\approx \\sqrt{\\frac{\\widehat{\\psi}+\\widehat{\\theta}}{Jn}}$$ where we have approximated the OLS estimate of the residual variance $\\widehat{\\sigma^2}$ by the sum of the estimated variance components $\\widehat{\\psi} + \\widehat{\\theta}$ (the approximation is better for larger $n$). We see that $$\\widehat{\\mathrm{SE}}(\\widehat{\\beta}^{F})\\leq\\widehat{\\mathrm{SE}}(\\widehat{\\beta}^{{\\mathrm{OLS}}})\\leq\\widehat{\\mathrm{SE}}(\\widehat{\\beta})$$ This relationship is best understood by remembering that the standard error is the standard deviation of the estimates over repeated samples (repeated random draws of $y_{ij}$ for all units). In the fixed-effects case, only the $\\epsilon_{ij}$ change from sample to sample (with variance $\\theta$). In the OLS case, the total residuals change (with variance $\\psi + \\theta$), but they are drawn independently (because they are assumed to be independent). In contrast, the total residuals are not drawn independently in the random-effects case; they result from drawing one value $\\zeta_j$ for all units in the cluster and drawing an $\\epsilon_{ij}$ for each unit. As a consequence, the total residuals $\\zeta_j + \\epsilon_{ij}$ for a cluster tend to change in the same direction, leading to larger variability in the resulting $\\widehat{\\beta}$. The difference between the OLS and the random-effects standard error is particularly ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:10:3","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"11 Assigning values to the random intercepts Remember that the cluster-specific intercepts $\\zeta_j$ are treated as random variables and not as model parameters in multilevel models. However, having obtained estimates $\\widehat{\\beta}$, $\\widehat{\\psi}$, and $\\widehat{\\theta}$ of the model parameters $\\beta$, $\\psi$, and $\\theta$ (using ML, REML, or FGLS) we may wish to assign values to the random intercepts $\\zeta_j$ for individual clusters; this would be analogous to obtaining predicted residuals $\\widehat{\\epsilon}_i$ in ordinary linear regression. There are a number of reasons why we may want to obtain values for the random intercepts $\\zeta_j$ for individual clusters. For instance, we will use such assigned values for model diagnostics (see sections 3.9 and 4.8.4), for interpreting and visualizing models (see section 4.8.3), and for inference regarding individual clusters (see section 4.8.5), such as small area estimation (see exercise 3.9) and disease mapping (see section 13.13). An example of inference for individual clusters would be to assign values to subjects’ true expiratory flow $\\beta + \\zeta_j$ based on the fallible measurements $y_{ij}$. It is easy to assign values to the total residuals, $\\widehat{\\xi}{ij} = y{ij} - \\widehat{\\beta}$. However, the total residuals are partitioned as $\\xi_{ij} = \\zeta_j + \\epsilon_{ij}$, and different methods have been proposed for assigning values to its constituent components $\\zeta_j$ and $\\epsilon_{ij}$. A common feature of the methods is that a value is first assigned to the $\\zeta_j$ and then $\\epsilon_{ij}$ is obtained using the relation $\\epsilon_{ij} = \\xi_{ij} - \\zeta_j$. Values are assigned to the random intercepts $\\zeta_j$ by either prediction estimation. We continue treating $\\zeta_j$ as a random variable when prediction is used, whereas the $\\zeta_j$ are instead viewed as unknown fixed parameters when estimation is used. The predominant approaches to assigning values to the $\\zeta_j$ are maximum “likelihood” estimation, described in section 2.11.1, and empirical Bayes prediction, described in section 2.11.2. ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:11:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"11.1 Maximum “likelihood” estimation We first substitute the parameter estimate into the variance-components $\\hat \\beta$ model (2.1), giving It is easy to assign values to the total residuals, $\\widehat \\xi_{ij} = y_{ij} - \\widehat{\\beta}$. However, the total residuals are partitioned as $\\xi_{ij} = \\zeta_j + \\epsilon_{ij}$, and different methods have been proposed for assigning values to its constituent components $\\zeta_j$ and $\\epsilon_{ij}$. A common feature of the methods is that a value is first assigned to the $\\zeta_j$ and then $\\epsilon_{ij}$ is obtained using the relation $\\epsilon_{ij} = \\xi_{ij} - \\zeta_j$. Values are assigned to the random intercepts $\\zeta_j$ by either prediction estimation. We continue treating $\\zeta_j$ as a random variable when prediction is used, whereas the $\\zeta_j$ are instead viewed as unknown fixed parameters when estimation is used. The predominant approaches to assigning values to the $\\zeta_j$ are maximum “likelihood” estimation, described in section 2.11.1, and empirical Bayes prediction, described in section 2.11.2. $$\\text{Likelihood}(y_{1j},y_{2j}|\\zeta_{j})$$ treating the model parameters as known. This approach of treating $\\zeta_j$ as an unknown (and fixed) parameter contradicts the original model specification, where $\\zeta_j$ was treated as a random effect. We put “likelihood” in quotes because it differs from the marginal likelihood (briefly described in section 2.10.2) that is used in ML estimation of the model parameters $(\\beta,\\psi,\\theta)$ in three ways: 1) the model parameters are treated as known, 2) the random effect is treated as an unknown parameter, and 3) the “likelihood” is based on the data for just one cluster. We can rearrange the above model by subtracting $\\widehat{\\beta}$ from $y_{ij}$ to obtain estimated total residuals $\\widehat{\\xi}_{ij}$ and regard these as the responses: $$\\widehat \\xi_{ij} = y_{ij}-\\widehat{\\beta}=\\zeta_j+\\epsilon_{ij}$$ The ML estimator of $\\zeta_j$ is simply the cluster mean of the estimated total residual over the $n_j$ units (here $n_j=2$) for which we have data: $$\\widehat \\zeta_j^{\\mathrm{ML}}=\\frac{1}{n_j}\\sum_{i=1}^{n_j}\\widehat \\xi_{ij}=\\frac{1}{2}(\\widehat \\xi_{1j}+\\widehat \\xi_{2j})$$ 基本概念 参数估计：我们通过数据得到模型参数的估计值。 方差组分模型：这是一个统计模型，用来分析数据中的变异性如何分配到不同的来源。 随机效应：在模型中，我们认为某些效应（如 $ \\zeta_j $）是随机的，意味着它们在不同的组（或称为“簇”）之间是变化的。 公式解释 模型公式： $ y_{ij} = \\widehat{\\beta} + \\zeta_j + \\epsilon_{ij} $ 这里，$ y_{ij} $ 是第 $ j $ 组中的第 $ i $ 个观测值。 $ \\widehat{\\beta} $ 是我们对总体平均效应的估计。 $ \\zeta_j $ 是第 $ j $ 组的随机效应。 $ \\epsilon_{ij} $ 是第 $ j $ 组第 $ i $ 个观测值的随机误差。 总残差： $ \\xi_{ij} = \\zeta_j + \\epsilon_{ij} $ 这是 $ y_{ij} $ 减去 $ \\widehat{\\beta} $ 后的值，表示观测值与总体平均效应的差异。 估计总残差： $ \\widehat \\xi_{ij} = y_{ij} - \\widehat{\\beta} $ 这是我们根据观测值和估计的 $ \\beta $ 计算出的残差。 随机效应的估计： 有两种主要方法来估计随机效应 $ \\zeta_j $：最大似然估计（MLE）和经验贝叶斯预测（EB预测）。 最大似然估计： $ \\widehat{\\zeta_j}^{\\text{ML}} = \\frac{1}{n_j} \\sum_{i=1}^{n_j} \\widehat{\\xi}_{ij} $ 这是通过将每个组的估计总残差平均来得到的 $ \\zeta_j $ 的估计值。 例子 假设我们有一个班级的学生考试成绩数据，我们想要分析班级之间的平均成绩差异。 收集数据：我们有3个班级，每个班级有2个学生的成绩。 计算 $ \\widehat{\\beta} $：我们首先计算所有学生成绩的平均值作为 $ \\widehat{\\beta} $。 计算总残差 $ \\xi_{ij} $：对于每个学生的成绩，我们减去 $ \\widehat{\\beta} $ 来得到总残差。 估计随机效应 $ \\zeta_j $：对于每个班级，我们计算总残差的平均值作为该班级随机效应的估计。 计算 $ \\epsilon_{ij} $：对于每个学生，我们用总残差减去 $ \\zeta_j $ 来得到该学生的随机误差估计。 最大似然估计（MLE） 最大似然估计是一种基于观测数据来估计模型参数的方法，它寻找能够使观测数据出现概率最大的参数值。 公式： $ \\widehat{\\zeta_j}^{\\text{ML}} = \\frac{1}{n_j} \\sum_{i=1}^{n_j} \\widehat{\\xi}_{ij} $ $ \\widehat \\xi_{ij} $ 是估计的总残差，计算方式为 $ y_{ij} - \\widehat{\\beta} $。 $ n_j $ 是第 $ j $ 组的观测数。 解释： MLE 通过计算每个组内所有估计残差的均值来估计随机效应 $ \\zeta_j $。 经验贝叶斯预测（EB预测） 经验贝叶斯方法结合了先验信息和观测数据来估计参数，通常用于处理具有随机效应的复杂模型。 特点： 它使用先验分布来描述随机效应的不确定性。 结合观测数据，计算随机效应的后验分布。 公式： 经验贝叶斯预测通常不直接给出一个封闭形式的公式，因为它涉及到积分计算。不过，基本思想是： $ \\widehat{\\zeta_j}^{\\text{EB}} = \\int \\zeta_j f(\\zeta_j | \\mathbf{y}_j) d\\zeta_j $ 这里，$ f(\\zeta_j | \\mathbf{y}_j) $ 是给定观测数据 $ \\mathbf{y}_j $ 下 $ \\zeta_j $ 的后验分布。 例子 假设我们有3个农场，每个农场有2头牛，我们想要估计每个农场的平均牛奶产量（以升为单位）的差异。 数据： 农场1：牛1产奶量 10L，牛2产奶量 12L 农场2：牛1产奶量 8L，牛2产奶量 9L 农场3：牛1产奶量 15L，牛2产奶量 16L 步骤1","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:11:1","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"11.2 Empirical Bayes prediction Having obtained estimates $\\widehat{\\beta} $, $\\widehat{\\psi} $, and $\\widehat{\\theta}$ of the model parameters by any of the methods described earlier in this chapter, we can treat them as the true parameter values and predict the random intercepts $\\zeta_j$ for individual clusters (subjects in the application). Here we continue to treat $\\zeta_j$ as a random variable, not as a fixed parameter as in ML estimation. ML estimation of $\\zeta_j$ uses the responses $y_{ij}$ for subject $j$ as the only information about $\\zeta_j$ by maximizing the “likelihood” of observing these particular values: $$\\text{Likelihood}(y_{1j},y_{2j}|\\zeta_{j})$$ In contrast, empirical Bayes prediction also uses the prior distribution of $\\zeta_j$, summarizing our knowledge about $\\zeta_j$ before seeing the data for subject $j$: $$\\operatorname{Prior}(\\zeta_j)$$ This prior distribution is just the normal distribution specified for the random intercept with zero mean and estimated variance $\\widehat{\\psi}.$ It represents what we know about $ \\zeta_j $ before we have seen the responses $y_{1j}$ and $y_{2j}$ for subject $ j $. For instance, the most likely value of $ \\zeta_j $ is 0. (Obviously, we have already used all responses to obtain the estimate $ \\widehat{\\psi} $, but we now pretend that $ \\psi $ is known and not estimated.) Once we have observed the responses, we can combine the prior distribution with the “likelihood” to obtain the posterior distribution of $ \\zeta_j $ given the observed responses $ y_{1j} $ and $ y_{2j} $ . According to Bayes theorem, $$\\mathrm{Posterior}(\\zeta_j|y_{1j},y_{2j}) \\propto \\mathrm{Prior}(\\zeta_j)\\times\\mathrm{Likelihood}(y_{1j},y_{2j}|\\zeta_j)$$ where $ \\propto $ means “proportional to”. The posterior of $ \\zeta_j $ represents our updated knowledge regarding $ \\zeta_j $ after seeing the data $y_{1j}$ and $y_{2j}$ for subject $ j $. The empirical Bayes prediction is just the mean of the posterior distribution with parameter estimates $( \\widehat{\\beta} , \\widehat{\\psi} , \\text{and} \\widehat{\\theta} ) $ plugged in. In a linear model with normal error terms, the posterior is normal and the mean is thus equal to the mode. Figure 2.9 shows the prior, “likelihood”, and posterior for a hypothetical example of a subject with $ n_j = 2 $ responses. In both panels, the estimated total residuals $ \\widehat{\\xi}{ij} $ are 3 and 5, and the estimated total variance is $ \\widehat{\\psi} + \\widehat{\\theta} = 5 $ . In the top panel, 80% of this variance is due to within-subject variability, whereas in the bottom panel, 80% is due to between-subject variability. In both cases, the “likelihood” (dotted curve) has its maximum at $ \\zeta_j = 4 $, that is, the mode is 4 (see vertical dotted lines). The ML estimate therefore is $\\widehat{\\zeta}{j}^{\\mathrm{ML}}=4$. In contrast, the mode (and mean) of the posterior depends on the relative sizes of the variance components and is $1.33$ in the top panel and $3.56$ in the bottom panel (see vertical dashed lines). The larger between-subject variance in the bottom panel means that the prior distribution is less informative and will have less influence on the predictions. At the same time the “likelihood” provides more information than in the top panel, giving a prediction much closer to the ML estimate. The mean of the posterior lies between the mean of the prior ($0$, vertical solid lines) and the mode of the “likelihood”. Figure 2.9: Prior distribution, \"likelihood\"(normalized), and posterior distribution for a hypothetical subject with $n_j = 2$ responses with total residuals $\\widehat{\\xi}_{1j} = 3$ and $\\widehat{\\xi}_{2j} = 5$[the vertical lines represent modes (and means) of the distributions]\r参数估计：我们通过某种方法得到了模型参数的估计值，记为 $ \\hat{\\beta} $，$ \\hat{\\psi} $ 和 $ \\hat{\\theta} $。这些估计值被当作真实的参数值来使用。 随机截距的预测：我们用这些参数估计值来预测个体群体（比如研究中的受试者）的随机截距 $ \\zeta_j $。这里，$ \\zeta_j $ 被视为一个随机变量，而不是像在最大似然估计中那样被视为一个固定参数。 最大似然估计：在最大似然估计中，我们只使用受试者 $ j $ 的响应 $ y_{ij} $ 来估计 $ \\zeta_j $，通过最大化观","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:11:2","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"11.3 Empirical Bayes standard errors There are several different kinds of variances (squared standard errors) for empirical Bayes predictions that can be used to express uncertainty regarding the predictions. These variances do not take into account the uncertainty in the parameter estimates because the parameters are typically treated as known in empirical Bayes prediction. The exception is prediction based on REML estimation where uncertainty in the estimation of $\\beta$ is taken into account. Obviously, model-parameter uncertainty matters more in small samples. Posterior and comparative standard errors As defined above, the empirical Bayes prediction of $\\zeta_j$ is the mean of the posterior distribution of $\\zeta_j$, given the responses $y_{1j}$ and $y_{2j}$, and examples of such posterior distributions are shown as dashed curves in figure 2.9. The corresponding variance of the posterior distribution expresses the degree of uncertainty regarding the prediction (if the parameters were known). In linear models, the posterior variance is given by $$\\mathrm{Var}(\\zeta_j|y_{1j},y_{2j}) = (1-R_j) \\psi $$ As expected, this variance is smaller than the prior variance $\\psi$ because of the information gained regarding the random intercept by knowing the responses $y_{1j}$ and $y_{2j}$ for cluster (here subject) $j$. Another way to express uncertainty regarding predictions is via the variance of the prediction errors, $\\widetilde \\zeta_j^{\\mathrm{EB}}-\\zeta_j$, over repeated samples of $\\zeta_j$ and $\\epsilon_{ij}$ (or repeated samples of clusters $j$ and units $i$), also known as the mean squared error of prediction (MSEP). In linear models, the MSEP happens to be equal to the posterior variance, $$\\operatorname{Var}(\\widetilde \\zeta_j^{\\mathrm{EB}}-\\zeta_j)=(1-R_j)\\psi$$ The square root of the MSEP is often called the comparative standard error because it can be used for inferences regarding differences between clusters’ true random intercepts. The comparative standard error can be estimated by plugging in the estimated shrinkage factor, calculated earlier as $\\widehat{R}_j=0.98399821$, and $\\widehat{\\psi}$as follows: 经验贝叶斯预测 经验贝叶斯预测是一种统计方法，它结合了先验信息和观测数据来估计未知参数。这种方法特别适用于参数很多但每个参数的观测数据较少的情况。 后验方差 概念： 后验方差是参数在给定观测数据后的不确定性度量。它是后验分布的方差，表示在考虑了数据之后，我们对参数估计的不确定性。 公式： 公式 $ \\mathrm{Var}(\\zeta_j|y_{1j},y_{2j}) = (1-R_j) \\psi $ 表示后验方差是如何计算的。 这里，$ \\zeta_j $ 是我们要预测的参数，比如一个群体的随机截距。 $ y_{1j} $ 和 $ y_{2j} $ 是这个群体的观测数据。 $ R_j $ 是缩减因子，它衡量了观测数据对预测的影响程度。 $ \\psi $ 是先验方差，表示在没有观测数据时，我们对参数的不确定性。 理解： 缩减因子 $ R_j $ 越接近1，意味着观测数据对预测的贡献越大，后验方差就越小，表示预测的不确定性降低了。 如果 $ R_j $ 接近0，那么后验方差就接近先验方差，表示我们对参数的预测不确定性很高。 比较标准误差 概念： 比较标准误差是预测误差的标准差，用于衡量不同群体或个体之间预测值的差异。 公式： 公式 $ \\operatorname{Var}(\\widetilde \\zeta_j^{\\mathrm{EB}}-\\zeta_j) = (1-R_j)\\psi $ 表示预测误差的方差。 $ \\widetilde \\zeta_j^{\\mathrm{EB}} $ 是根据经验贝叶斯方法得到的预测值。 $ \\zeta_j $ 是真实值。 计算： 比较标准误差的计算是取预测误差方差的平方根： $ \\text{比较标准误差} = \\sqrt{(1 - R_j) \\psi} $ 例子 假设我们有一个简单的线性模型，其中 $ \\zeta_j $ 是某个群体的随机截距，我们有两组数据 $ y_{1j} $ 和 $ y_{2j} $。我们先前估计的缩减因子 $ \\widehat{R}_j $ 是0.98399821，先验方差 $ \\widehat{\\psi} $ 是某个已知的值，比如10。 计算后验方差： $ \\mathrm{Var}(\\zeta_j|y_{1j},y_{2j}) = (1 - 0.98399821) \\times 10 $ 计算得到后验方差大约是0.16。 计算比较标准误差： $ \\text{比较标准误差} = \\sqrt{0.16} $ 计算得到比较标准误差大约是0.4。 结论 通过这个例子，我们可以看到，后验方差和比较标准误差是如何帮助我们理解预测的不确定性的。后验方差越小，表示我们对参数的预测越确定；比较标准误差越小，表示不同群体或个体之间的预测差异越小。 The comparative standard error can be estimated by plugging in the estimated shrinkage factor, calculated earlier as $\\widehat{R}_j=0.98399821$, and $\\widehat{\\psi}$as follows: * comparative standard errors generate comp_se = sqrt((1-.98399821)*exp(_b[lns1_1_1:_cons])^2) display comp_se[1] 13.965028\rWhen predicting the random effects with Stata’s predict command, you can add the reses() (for “random-effects standard errors”) option to produce such comparative standard errors after ML estimation using mixed, so identical results would be obtained as follows: mixed wm || id:, mle predict eb, reffects reses(comp_se) (After REML estimation, the predict command produces standard erro","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:11:3","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"12 Summary and further reading In this chapter, we introduced the idea of decomposing the total variance of the response variable into variance components, specifically the between-cluster variance $ \\psi $ and the within-cluster variance $ \\theta $ . This was accomplished by specifying a model that includes corresponding error components; a level-2 random intercept $ \\xi_j $ for clusters; and a level-1 residual $ \\epsilon_{ij} $ for units within clusters, where $ \\epsilon_{ij} $ is uncorrelated with $ \\xi_j $ . The random intercept induces correlations among responses for units in the same cluster, known as the intraclass correlation. The random intercept is a random variable and not a model parameter. The realizations of $ \\xi_j $ change in repeated samples, either because clusters are sampled or because the random intercept is redrawn from the data-generating model for given clusters. An alternative to random intercepts are fixed intercepts. Some guidelines for choosing between random and fixed intercepts were given, and this issue will be revisited in the next chapter. The concepts discussed in this chapter underlie all multilevel or hierarchical modeling. By considering the simplest case of a multilevel model, we provided some insight into estimation of model parameters and prediction of random effects. We also discussed how to conduct hypothesis testing and construct confidence intervals for variance-components models. Although the expressions for estimators and predictors become more complex for the models discussed in later chapters of this volume, the basic ideas remain the same. For further reading about variance-components models, we recommend Snijders and Bosker (2012, chap. 3), as well as many of the books referred to in later chapters of this volume. Streiner, Norman and Cairney (2015), Shavelson and Webb (1991), and Dunn (2004) are excellent books on linear measurement models. We refer to Skrondal and Rabe-Hesketh (2009) for an overview of methods for prediction of random effects and responses in multilevel models. The exercises cover a range of applications, such as measurement of peak expiratory flow (exercise 2.1), measurement of psychological distress (exercise 2.2), essay grading (exercise 2.5), neuroticism of twins (exercise 2.3), birthweights of siblings (exercise 2.7), head sizes of brothers (exercise 2.6), and achievement of children nested in neighborhoods and schools (exercise 2.4). Exercise 2.8 is about random-effects meta-analysis, a topic not discussed in this chapter. 方差组分模型的概念 总方差的分解： 在统计分析中，我们经常需要理解数据中的变异性。方差组分模型帮助我们将总方差分解为不同的部分，以便更好地理解数据的结构。 组分介绍： 组分之一是组间方差（between-cluster variance），记为 $ \\psi $。它表示不同群体（如不同的学校或家庭）之间的差异。 另一个组分是组内方差（within-cluster variance），记为 $ \\theta $。它表示同一群体内部个体的差异。 模型的组成部分： 为了实现方差的分解，模型中引入了相应的误差组分： Level-2 随机截距 $ \\xi_j $：代表群体级别的随机效应，例如，不同学校的学生平均成绩可能有不同的基线水平。 Level-1 残差 $ \\epsilon_{ij} $：代表个体级别的随机误差，例如，同一学校内不同学生的成绩波动。 随机截距与固定截距： 随机截距是随机变量，它的实现值在不同的样本中会变化，这可能是因为群体本身的变化，或者是因为我们重新从数据生成模型中抽取随机截距。 固定截距则是在整个模型中保持不变的参数。选择随机截距还是固定截距取决于数据的特性和研究目的。 多级或层次模型的基础： 这一章讨论的概念是所有多级或层次模型的基础。通过考虑最简单的多级模型案例，我们可以获得对模型参数估计和随机效应预测的一些见解。 假设检验和置信区间： 我们还讨论了如何对方差组分模型进行假设检验和构建置信区间，这是统计分析中常用的方法来评估模型参数的不确定性。 进一步阅读和练习 为了更深入地了解方差组分模型，推荐了一些文献，如Snijders和Bosker的书籍，以及Streiner, Norman和Cairney关于线性测量模型的书籍。 练习题覆盖了一系列应用，包括测量峰值呼气流速、心理困扰、作文评分、双胞胎的神经质、兄弟姐妹的出生体重、兄弟的头围，以及在社区和学校中的儿童成就等。 练习2.8是关于随机效应元分析的，这是本章没有讨论的一个主题。 结论 通过这段介绍，我们了解到方差组分模型是一种强大的工具，它可以帮助我们理解数据中的变异性，并在多级或层次模型中进行参数估计和预测。希望这个解释有助于你更好地理解方差组分模型的基本概念。$ ","date":"2024-06-03","objectID":"/chapter-2-variance-components-models/:12:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 2 ：Variance-components models","uri":"/chapter-2-variance-components-models/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"In this chapter, we review the statistical models underlying independent samples tests, analysis of variance (ANOVA), analysis of covariance (ANCOVA), simple regression, and multiple regression. We formulate all these models as linear regression models.","date":"0001-01-01","objectID":"/chapter-1-review-of-linear-regression/","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 1 ：Review of linear regression","uri":"/chapter-1-review-of-linear-regression/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"1 Introduction The regression models considered here are essential building blocks for multilevel models. Although linear multilevel or mixed models for continuous responses are sometimes viewed from an ANOVA perspective, the regression perspective is beneficial because it is easily generalizable to binary and other types of responses. Furthermore, the Stata commands for multilevel modeling follow a regression syntax. Even experienced regression modelers are likely to benefit from reading this chapter because it introduces our notation and terminology as well as Stata commands used in later chapters, including factor variables for specifying dummy variables and interactions within estimation commands instead of creating new variables. ","date":"0001-01-01","objectID":"/chapter-1-review-of-linear-regression/:1:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 1 ：Review of linear regression","uri":"/chapter-1-review-of-linear-regression/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"2 Is there gender discrimination in faculty salaries? The primary purpose of these studies was to investigate whether evidence existed for gender inequity in faculty salaries at the university. The data considered here are a subset of the data provided by DeMaris, comprising faculty members, excluding faculty from the Fireland campus, nonprofessors (instructors/lecturers), those not on the graduate faculty, and three professors hired as Ohio Board of Regents Eminent Scholars. We will use the following variables: salary: academic year (9-month) salary in U.S. dollars male: gender (1 male; 0 female) market: marketability of academic discipline, defined as the ratio of the national average salary paid in the discipline to the national average across all disciplines yearsdg: time since degree (in years) rank: academic rank (1 assistant professor; 2 associate professor; 3 full professor) ","date":"0001-01-01","objectID":"/chapter-1-review-of-linear-regression/:2:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 1 ：Review of linear regression","uri":"/chapter-1-review-of-linear-regression/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"3 Independent-samples t test If we have an interest in gender inequity, an obvious first step is to compare mean salaries between male and female professors at the university. We can use the tabstat command to produce a table of means, standard deviations, and sample sizes by gender: tabstat salary, by(male) statistics(mean sd n) male | Mean SD N\r-------+------------------------------\rWomen | 42916.6 9161.61 128\rMen | 53499.24 12583.48 386\r-------+------------------------------\rTotal | 50863.87 12672.77 514\r--------------------------------------\rWe see that the male faculty at the university earn, on average, over $10,000$ more than the female faculty. The standard deviation is also considerably larger for the men than for the women. Due to chance or sampling variation, the large difference between the mean salary ${\\overline{y_{1}}}$ of the $n_1$ men and the mean salary ${\\overline{y}}_{0}$ of the $n_0$ women in the sample does not necessarily imply that the corresponding population means or expectations $\\mu_1$ and $\\mu_0$ for male and female faculty differ (the Greek letter $\\mu$ is pronounced “mew”). Here population refers either to an imagined infinite population from which the data can be viewed as sampled or to the statistical model that is viewed as the data-generating mechanism for the observed data. To define a statistical model, let $y_i$ and $x_i$ denote the salary and gender of the $i$th professor, respectively, where $x_i=1$ for men and $x_i=0$ for women.A standard model for the current problem can then be specified as $$y_i|x_i\\sim N(\\mu_{x_i},\\sigma_{x_i}^2)$$ Here $y_i|x_i\\sim$ means $“y_i,\\text{ for a given value of }x_i,\\text{ is distributed as\"}$ and $N(\\mu_{x_i},\\sigma_{x_i}^2)$ stands for a normal distribution with conditional mean parameter $\\mu_{x_i}$ and conditional variance parameter $\\sigma_{x_i}^2$ (the Greek letter $\\sigma$ is pronounced “sigma”). The term conditional simply means that we are considering only the subset of the population for which some condition is satisfied—in this case, that $x_i$ takes on a particular value. In other words, we are considering the distribution of salaries for men $(x_{i}=1)$ separately from the distribution for women $(x_{i}=0)$ When conditioning on a categorical variable like gender, the expression “conditional on gender” can be replaced by “within gender” or “separately for each gender”. Because $x_i$ gender takes on only two values, we can be more explicit and write the statistical model as: $$y_i|x_i = 0\\sim N(\\mu_{x_i},\\sigma_{x_i}^2)$$ $$y_i|x_i = 1\\sim N(\\mu_{x_i},\\sigma_{x_i}^2)$$ Each conditional distribution has its own conditional expectation, or conditional population mean, $$\\mu_{x_i}\\equiv E(y_i|x_i)$$ denoted $\\mu_1$ for men and $\\mu_0$ for women ($\\equiv$ stands for “defined as”). Each conditional distribution also has its own conditional variance $$\\sigma_{x_i}^2\\equiv\\mathrm{Var}(y_i|x_i)$$ denoted $\\sigma_1^2$ for men and $\\sigma_0^2$ for women. A final assumption is that $y_i$ is conditionally independent of $y_{i’}$ , given the values of $x_i$ and $x_{i’}$ , for different professors ${i}$ and ${i’}$ This means that knowing one professor’s salary does not help us predict another professor’s salary if we already know that other professor’s gender and the corresponding gender-specific mean salary. By modeling $y_i$ conditional on $x_i$, we are treating $y_i$ as a response variable (sometimes also called dependent variable or criterion variable) and $x_i$ as an explanatory variable or covariate (sometimes referred to as independent variable, predictor, or regressor). 第一部分：总体均值和样本均值 总体均值（期望） 符号: $\\mu$ (发音 “mew”) 表示总体均值或期望。 意义: 对于男性教师，总体均值 $\\mu_1$ 表示所有男性教师工资的平均数；对于女性教师，总体均值 $\\mu_0$ 表示所有女性教师工资的平均数。 样本均值 符号: ${\\overline{y_1}}$ 表示男性教师的样本均值，${\\overline{y}}_{0}$ 表示女性教师的样本均值。 计算: 样本均值是通过将样本中所有个体的工资相加，然后除以样本大小来计算的。例如，如果 $n_1$ 是男性教师的人数，那么 ${\\overline{y_1}} = \\frac{1}{n_1} \\sum_{i=1}^{n_1} y_{i}$，其中 $y_{i}$ 是第 $i$ 个男性教师的工资。 第二部分：统计模型的建立 定义变量 $y_i$: 第 $i$ 个教","date":"0001-01-01","objectID":"/chapter-1-review-of-linear-regression/:3:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 1 ：Review of linear regression","uri":"/chapter-1-review-of-linear-regression/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"4 One-way analysis of variance The model underlying the $t$-test with equal variances is also called a oneway analysis-of-variance (ANOVA) model. ANOVA involves partitioning the total sum of squares (TSS), the sum of squared deviations of $y_i$ the from their overall mean, $$\\text{TSS}=\\sum_{i=1}^n(y_i-\\overline{y})^2$$ into the model sum of squares (MSS) and the sum of squared errors (SSE). The MSS, also known as regression sum of squares, is the sum of squared deviations of the sample means from the overall mean, interpretable as the between-group sum of squares (here “ $i,x_i = 0$” and “ $i,x_i = 1$” mean that the sums are taken over females and males, respectively) The SSE, also known as residual sum of squares, is the sum of squared deviations of responses from their respective sample means, interpretable as the within-group sum of squares ( $s_0$ and $s_1$ are the within-group sample standard deviations) The group-specific sample means can be viewed as predictions, y\ri\r^\r=\rμ\r^\rx\ri\r=\ry\r¯\rx\ri\r,representing the best guess of the salary when all that is known about the professor is the gender. These predictions, $\\overline{y}_0$ and $\\overline{y}_1$ minimize the SSE and are therefore referred to as ordinary least-squares OLS estimates of $\\mu_0$ and $\\mu_1$. When evaluating the quality of the predictions $\\widehat{y}_{i}$ , the SSE is interpreted as the sum of the squared prediction errors $y_{i}-\\widehat{y}_i$.\rThe total sum of squares equals the model sum of squares plus the sum of squared errors: $$ TSS~=~MSS+SSE $$ The deviations contributing to each of these sums of squares are shown in figure 1.3 for an observation (shown as ·) in a hypothetical dataset. These deviations add up in the same way as the corresponding sums of squares. For example, for men,$y_{i}-\\overline{y}=(\\overline{y_1}-\\overline{y})+(y_{i}-\\overline{y}_{1}).$ Figure 1.3: Illustration of deviations contributing to total sum of squares (TSS), model sum of squares (MSS), and sum of squared errors (SSE)\rThe model mean square (MMS) and mean squared error (MSE) can be obtained from the corresponding sums of squares by dividing by the appropriate degrees of freedom as shown in table 1.1 for the general case of $g$ groups ( for comparing males and females,$g = 2$). Table 1.1: Sums of squares (SS) and mean squares (MS) for one-way ANOVA\rThe MSE is the pooled within-group sample variance, which is an estimator for the population variance parameter $\\sigma^{2}$: $$\\widehat{\\sigma^2}=\\mathrm{MSE}$$ The $F$statistic for the null hypothesis that the population means are the same (against the two-sided alternative) then is $$F=\\frac{\\mathrm{MMS}}{\\mathrm{MSE}}$$ Under the null hypothesis, this statistic has an $F$ distribution with $g-1$ and $n-g$ degree of freedom.When $g = 2$ as in our example, the statistic is the square of the $t$statistic from the independent-samples test under the equal-variance assumption, and the $p$-values from the two tests are identical. ANOVA的基本概念和计算步骤（详细版） 总平方和（TSS） TSS是观测值与整体平均值之间差异的平方和，它反映了数据集中的总变异性。 计算步骤： 计算所有观测值的平均值 $\\overline{y}$。 对每个观测值 $y_i$，计算其与 $\\overline{y}$ 的差异 $y_i - \\overline{y}$。 将这些差异的平方求和，即 $\\sum_{i=1}^n (y_i - \\overline{y})^2$。 数学表达式： $$ \\text{TSS} = \\sum_{i=1}^n (y_i - \\overline{y})^2 $$ 举例： 假设我们有两组数据，组1有3个观测值（1, 2, 3），组2有2个观测值（4, 5），则 $\\overline{y} = \\frac{1+2+3+4+5}{5} = 3$。 $$ \\text{TSS} = (1-3)^2 + (2-3)^2 + (3-3)^2 + (4-3)^2 + (5-3)^2 = 4 + 1 + 0 + 1 + 4 = 10 $$ 模型平方和（MSS） MSS反映了不同组均值之间的差异，即模型解释的变异性。 计算步骤： 计算每个组的均值。 对每个组，计算其均值与全局均值之间的差异。 将这些差异的平方乘以该组的观测值数量，然后求和。 数学表达式： 在两组的情况下，其数学表达式可以为： $$\\text{MSS} = n_1(\\overline{y}_1 - \\overline{y})^2 + n_2(\\overline{y}_2 - \\overline{y})^2$$ 更一般的情况，如果有k个组，则表达式为： $$ \\text{MSS} = \\sum_{j=1}^{k}n_j(\\overline{y}_j - \\overline{y})^2 $$ 其中，$n_j$是第j组的观测值数量，$\\overline{y}_j$是第j组的均值，而$\\overline{y}$是所有观测值的总体均值。 举例： 假设我们有两组数据，组1有3个观测值（1, 2, 3），组2有2个观测值（4, 5）。 则组1的均值$\\overline{y}_1 = 2$，组2的均值$\\overline{y}_2 = 4.5$，全局均值$\\overline{y} = 3$。 $$ \\text{MSS} = 3 \\times (2 - 3)^2 + 2 \\times (4.5 -","date":"0001-01-01","objectID":"/chapter-1-review-of-linear-regression/:4:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 1 ：Review of linear regression","uri":"/chapter-1-review-of-linear-regression/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"5 Simple linear regression Salaries can vary considerably between academic departments. Some disciplines are more marketable than others, perhaps because there are highly paid jobs in those disciplines outside academia or because there is a low supply of qualified graduates. The dataset contains a variable, market, for the marketability of the discipline, defined as the mean U.S. faculty salary in that discipline divided by the mean salary across all disciplines. Let us now investigate the relationship between salaries and marketability of the discipline. Marketability ranges from 0.71 to 1.33 in this sample, taking on 46 different values. A one-way ANOVA model, allowing for a different mean salary $\\mu_{xi}$for each value of marketability $x_i$ would have a large number of parameters and many groups containing only one individual and would hence be overparameterized. There are two popular ways of dealing with this problem: categorize the continuous explanatory variable into intervals, thus producing fewer and larger groups, or assume a parametric, typically linear, relationship between $\\mu_{xi}$ and $x_i$. Before adopting the latter approach, we inspect the scatterplot in figure 1.4, with a smooth curve (called LOWESS) superimposed: 这段文字讨论的是学术部门之间薪资差异的问题，以及如何研究薪资与学科市场性之间的关系。首先，它定义了市场性（market）这个变量，它是通过计算某个学科在美国的平均教职员工薪资除以所有学科的平均薪资来得出的。接着，作者指出市场性在样本中的范围是从0.71到1.33，并且有46个不同的值。 接下来，作者提到了研究薪资与市场性之间关系时遇到的一个问题：如果使用单因素方差分析（one-way ANOVA）模型，允许每个市场性值 $x_i$ 有一个不同的平均薪资 $mu_{xi}$，那么模型将包含大量的参数，并且许多组可能只包含一个个体，从而导致模型过度参数化（overparameterized）。 为了解决这个问题，作者提出了两种流行的处理方法： 将连续的解释变量（市场性）划分为区间，从而产生更少但更大的组。 假设平均薪资 $mu_{xi}$ 和市场性 $x_i$ 之间存在一个参数化的，通常是线性的关系。 twoway (scatter salary market, msize(small)) /// (lowess salary market, lwidth(medthick) lpatt(solid)) Figure 1.4: Scatterplot with LOWESS curve\rThe LOWESS curve is almost perfectly linear, and we therefore specify a simple linear regression model $$\\begin{matrix}y_i|x_i\u0026\\sim\u0026N(\\mu_{x_i},\\sigma^2)\\end{matrix}$$ where $$\\mu_{x_i}\\equiv E(y_i|x_i)=\\beta_1+\\beta_2x_i$$ Alternatively, the model can be written as $$y_i=\\beta_1+\\beta_2x_i+\\epsilon_i,\\quad\\epsilon_i|x_i\\sim {N}(0,\\sigma^2)$$ where $\\epsilon_{i}$ is the residual or error term for the $i$th professor, assumed to be independent of the residuals for other professors. Here $\\beta_1$(the Greek letter $\\beta$ is pronounced “beta”) is called the intercept (often denoted $\\beta_0$ or $\\alpha$ ) and represents the conditional expectation of $y_i$ when $x_i = 0$ : $$E(y_i|x_i=0)=\\beta_1$$ $\\beta_1$ is called the slope , or regression coefficient , of $x_i$ and represents the increase in conditional expectation when $x_i$ increases by one unit, from some value $\\alpha$ to $\\alpha + 1$: $$E(y_i|x_i=a+1)-E(y_i|x_i=a)=[\\beta_1+\\beta_2(a+1)]-[\\beta_1+\\beta_2a]=\\beta_2$$ We refer to $\\beta_1+\\beta_2{x_i}$ as the fixed part and $\\epsilon_{i}$ as the random part of the model. In addition to assuming that the conditional expectations fall on a straight line, the model assumes that the conditional variances, or residual variances, of the $y_i$ are equal for all $x_i$, $$\\mathrm{Var}(y_i|x_i)=\\mathrm{Var}(\\epsilon_i|x_i)=\\sigma^2$$ which is known as the homoskedasticity assumption (in contrast to heteroskedasticity , where the conditional variances of the $y_i$ are not equal for all $x_i$). A graphical illustration of the simple linear regression model is given in figure 1.5. Here the line represents the conditional expectation $E(y_{i}|x_{i})$ as a function of $x_i$, and the density curves represent the conditional distributions of $y_{i}|x_{i}$ shown only for some values of $x_i$. Figure 1.5: Illustration of simple linear regression model\r第一部分：模型的组成部分 正态分布假设 模型的起点是正态分布的假设： $$ y_i|x_i \\sim N(\\mu_{x_i}, \\sigma^2) $$ 这表示给定 $x_i$，$y_i$ 的条件分布是正态分布，均值为 $\\mu_{x_i}$，方差为 $\\sigma^2$。 均值函数 均值函数 $\\mu_{x_i}$ 描述了 $x_i$ 和 $y_i$ 之间的关系： $$ \\mu_{x_i} \\equiv E(y_i|x_i) = \\beta_1 + \\beta_2x_i $$ 这里： $E(y_i|x_i)$ 是在给定 $x_i$ 的条件下，$y_i$ 的期望值。 $\\beta_1$ 是截距，表示当 $x_i = 0$ 时 $y_i$ 的期望","date":"0001-01-01","objectID":"/chapter-1-review-of-linear-regression/:5:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 1 ：Review of linear regression","uri":"/chapter-1-review-of-linear-regression/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"6 Dummy variables Instead of using a test to compare the population mean salaries between men and women as we did in section 1.4, we can use simple linear regression. This becomes obvious by considering the diagram in figure 1.8, where we have simply used the variable $x_i$, equal to 1 for men and 0 for women, and connected the corresponding conditional expectations of $y_i$ by a straight line. We are not making any assumption regarding the relationship between the conditional means and $x_i$ because any two means can be connected by a straight line. (In contrast, assuming in the previous section that the conditional means for the 46 values of marketability lay on a straight line was a strong but reasonable assumption in view of figure 1.7.) We are, however, assuming equal conditional variances for the two populations because of the homoskedasticity assumption discussed earlier. Figure 1.8: Illustration of simple linear regression with a dummy\rvariable\rThe model can be written as $$\\mu_{x_i}\\equiv E(y_i|x_i)=\\beta_1+\\beta_2x_i,\\quad y_i|x_i\\ ～ N(\\mu_{x_i},\\sigma^2)$$ so that $$\\begin{array}{rcl}\\mu_0\u0026=\u0026\\beta_1+\\beta_2\\times0=\\beta_1\\end{array}$$ $$\\begin{array}{rcl}\\mu_{1}\u0026=\u0026\\beta_{1}+\\beta_{2}\\times1=\\beta_{1}+\\beta_{2}\\end{array}$$ The intercept can now be interpreted as the expectation for the group coded 0, the reference group (here women), whereas the slope $\\beta_2$ represents the difference in expectations $\\beta_2 = \\mu_1 - \\mu_0$ between the group coded 1 (here men) and the reference group. 第一部分：理解背景和概念 简单线性回归：这是一种统计方法，用于分析两个变量之间的关系，其中一个变量（自变量）用来预测另一个变量（因变量）。 虚拟变量（Dummy Variable）：这是一个特殊的变量，通常取值为0或1，用于表示分类数据。在这段文字中，$x_i$ 就是一个虚拟变量，用来区分性别：男性为1，女性为0。 条件期望：$E(y_i|x_i)$ 表示在给定自变量 $x_i$ 的条件下，因变量 $y_i$ 的期望值。 同方差性（Homoskedasticity）：这是一个假设，认为不同组的误差项具有相同的方差。 图1.8：它展示了如何使用虚拟变量来连接不同性别的薪资条件期望，并用一条直线表示。 第二部分：数学模型和解释 模型公式： $$ \\mu_{x_i} \\equiv E(y_i|x_i) = \\beta_1 + \\beta_2 x_i $$ 这里，$\\mu_{x_i}$ 是在给定 $x_i$ 的条件下，$y_i$ 的期望值。$\\beta_1$ 是截距，$\\beta_2$ 是斜率，它们是模型的参数。 正态分布假设： $$ y_i|x_i \\sim N(\\mu_{x_i}, \\sigma^2) $$ 这意味着在给定 $x_i$ 的条件下，$y_i$ 服从均值为 $\\mu_{x_i}$，方差为 $\\sigma^2$ 的正态分布。 计算特定条件的期望： 当 $x_i = 0$（女性）时： $$ \\mu_0 = \\beta_1 + \\beta_2 \\times 0 = \\beta_1 $$ 这表示女性的平均薪资期望是 $\\beta_1$。 当 $x_i = 1$（男性）时： $$ \\mu_1 = \\beta_1 + \\beta_2 \\times 1 = \\beta_1 + \\beta_2 $$ 这表示男性的平均薪资期望是 $\\beta_1 + \\beta_2$。 参数解释： 截距 $\\beta_1$：代表参考组（这里是女性）的平均薪资期望。 斜率 $\\beta_2$：代表男性与女性薪资期望的差异，即 $\\beta_2 = \\mu_1 - \\mu_0$。 例子 假设我们有以下数据： $\\beta_1 = 50000$（女性的平均薪资期望） $\\beta_2 = 10000$（男性比女性的平均薪资期望高出的金额） 那么： 女性的薪资期望 $\\mu_0 = 50000$ 男性的薪资期望 $\\mu_1 = 50000 + 10000 = 60000$ 这意味着在这个模型中，男性的平均薪资期望比女性高出10000。 When a dichotomous variable, coded 0 and 1, is used in a regression model like this, it is referred to as a dummy variable or indicator variable. A useful convention is to give the dummy variable the name of the group for which it is 1 and to describe it as a dummy variable for being in that group; thus, here we have a dummy variable for being male. The null hypothesis that $beta_2 = 0$ is equivalent to the null hypothesis that the population means are the same, $\\mu_1 - \\mu_0 = 0$. We can therefore use simple regression to obtain the same result as previously produced for the two-sided independent-samples test with the equal-variance assumption: regress salary male Source | SS df MS Number of obs = 514\r-------------+---------------------------------- F(1, 512) = 76.96\rModel | 1.0765e+10 1 1.0765e+10 Prob \u003e F = 0.0000\rResidual | 7.1622e+10 512 139887048 R-squared = 0.1307\r-------------+---------------------------------- Adj R-squared = 0.1290\rTotal | 8.2387e+10 513 160599133 Root MSE = 11827\r------------------------------------------------------------------------------\rsalary | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\rmale | 10582.63 1206.35 8.77 0.000 8212.64 12952.63\r_cons | 42916.60 1045.40 41.05 0.000 40862.80 44970.41\r------------------------------------------------------------------------","date":"0001-01-01","objectID":"/chapter-1-review-of-linear-regression/:6:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 1 ：Review of linear regression","uri":"/chapter-1-review-of-linear-regression/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"7 Multiple linear regression An important question when investigating gender discrimination is whether the men and women being compared are similar in the variables that justifiably affect salaries. As we have seen, there is some variability in marketability, and marketability has an effect on salary. Could the lower mean salary for women be due to women tending to work in disciplines with lower marketability? This is a possible explanation only if women tend to work in disciplines with lower marketability than do men, which is indeed the case: tabstat salary, by(male) statistics(mean sd n) male | Mean SD\r-------+--------------------\rWomen | -.0469589 .1314393\rMen | .0155719 .1518486\r-------+--------------------\rTotal | -1.38e-17 .14938\r----------------------------\rA variable like marketability that is associated with both the covariate of interest (here gender) and the response variable is often called a confounder . The impact of ignoring one or more confounders on the estimated gender effect is called confounding. We could render the comparison of salaries more fair by matching each woman to a man with the same value of marketability; however, this would be cumbersome, and we may not find matches for everyone. Instead, we can assume that marketability has the same, linear effect on salary for both genders and check whether gender has any additional effect after allowing for the effect of marketability. This can be accomplished by specifying a multiple linear regression model, $$y_i=\\beta_1+\\beta_2x_{2i}+\\beta_3x_{3i}+\\epsilon_i,\\quad\\epsilon_i|x_{2i},x_{3i} ～ N(0,\\sigma^2)$$ where we have multiple covariates or explanatory variables: a dummy variable $x_2i$ for being a man and a continuous variable $x_3i$ representing mean-centered marketability. (We number covariates beginning with 2 to correspond to the regression coefficients.) For disciplines with mean marketability ($x_3i = 0$), the model specifies that the expected salary is $\\beta_1$ for women ($x_2i = 0$) and $\\beta_1 + \\beta_2$ for men ($x_2i = 1$). Therefore, $\\beta_2$ can be interpreted as the difference in population mean salary between men and women in disciplines with mean marketability. Fortunately, $\\beta_2$ has an even more general interpretation as the difference in population mean salary between men and women in disciplines with any level of marketability, as long as both genders have the same value for marketability, $x_3i = a$. $$E(y_i|x_{2i}=1,x_{3i}=a)-E(y_i|x_{2i}=0,x_{3i}=a)=(\\beta_1+\\beta_2+\\beta_3a)-(\\beta_1+\\beta_3a)=\\beta_2$$ When comparing genders, we are now controlling for, adjusting for, partialing out, or keeping constant marketability. By “controlling” or “keeping constant”, we mean that the comparison between males and females is made for a sub-population of faculty with a given value of marketability. It does not imply any experiment in which some factors are kept constant and others are manipulated. To estimate the gender difference, controlling for marketability, the model makes the strong assumptions that this difference is the same for each value of marketability and that the population mean salary changes linearly with marketability for each gender. Because of the lack of experimental manipulation and the strong model assumptions, it is more cautious to describe coefficients in multiple regression as “adjusted for” other covariates Figure 1.9 shows model-implied regression lines for this model and how the lines depend on the coefficients. We see that determines the difference in means between men and women (vertical distance between the gender-specific regression lines) for any value of marketc. Figure 1.9: Illustration of multiple regression with a dummy variable for male ($x_{2i}$) and a continuous covariate, marketc($x_{3i}$)\rThe gender-specific regression lines both have slope, interpretable as the increase in population mean salary per unit increase in marketability, for a given gender or controlling for gender. The Stata comma","date":"0001-01-01","objectID":"/chapter-1-review-of-linear-regression/:7:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 1 ：Review of linear regression","uri":"/chapter-1-review-of-linear-regression/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"8 Interactions The models considered in the previous section assumed that the effects of different covariates were additive. For instance, if the dummy variable $x_{2i}$ changes from 0 (women) to 1 (men), the mean salary increases by an amount regardless of the values of the other covariates (marketc and yearsdg). However, this is a strong assumption that can be violated. The gender difference may depend on yearsdg if, for instance, starting salaries are similar for men and women but men receive larger or more frequent increases. We can investigate this possibility by including an interaction between gender and time since degree. An interaction between two variables implies that the effect of each variable depends on the value of the other variable: in our case, the effect of gender depends on time since degree and the effect of time since degree depends on gender. We can incorporate the interaction in the regression model (with the usual assumptions) by simply including the product of male ($x_{2i}$) and yearsdg ($x_{4i}$) as a further covariate with regression coefficient $\\beta_5$: From (1.2), we see that the effect of male (also called the gender gap) is given by $\\beta_2+\\beta_5$ yearsdg and hence depends on time since degree if $\\beta_5 \\neq 0$. From (1.3), we see that the effect of yearsdg is given by $\\beta_4+\\beta_5$ male and hence depends on gender if $\\beta_5 \\neq 0$. We can describe time since degree as a moderator or an effect modifier of the effect of gender or vice versa. When including an interaction between two variables, it is usually essential to keep both variables in the model. For instance, dropping male or setting $\\beta_2 = 0$ would force the gender gap to be exactly 0 when time since degree is 0, which is a completely arbitrary constraint unless it corresponds to a specific research question. An illustration of the model is given in figure 1.13 (with marketc set to $0$; $x_{3i} = 0$). If $\\beta_5$ were $0$, we would obtain two parallel regression lines with vertical distance $\\beta_2$. We see that $\\beta_5$ represents the additional slope for men compared with women, or the additional gender gap when $yearsdg$ increases by one unit. We also see that $\\beta_2$ is the gender gap when $yearsdg = 0$ and is the slope of $yearsdg$ when $male = 0$. Figure 1.13: Illustration of interaction between male ($x_{2i}$) and yearsdg ($x_{4i}$) for marketc ($x_{3i}$) equal to $0$ (not to scale)\rTo fit this model in Stata, we can generate the interaction as the product of the dummy variable for being male and years since degree: generate male_years = male*yearsdg We can then include the interaction in the regression: regress salary male marketc yearsdg male_years Source | SS df MS Number of obs = 514\r-------------+---------------------------------- F(4, 509) = 279.95\rModel | 5.6641e+10 4 1.4160e+10 Prob \u003e F = 0.0000\rResidual | 2.5746e+10 509 50581607.4 R-squared = 0.6875\r-------------+---------------------------------- Adj R-squared = 0.6850\rTotal | 8.2387e+10 513 160599133 Root MSE = 7112.1\r------------------------------------------------------------------------------\rsalary | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\rmale | -593.31 1320.91 -0.45 0.654 -3188.42 2001.80\rmarketc | 38436.65 2160.96 17.79 0.000 34191.14 42682.15\ryearsdg | 763.19 83.42 9.15 0.000 599.31 927.07\rmale_years | 227.15 92.00 2.47 0.014 46.41 407.89\r_cons | 36773.64 1072.39 34.29 0.000 34666.78 38880.51\r------------------------------------------------------------------------------\rHere it is natural to interpret the interaction in terms of the effect of gender. When time since degree is $0$ years, the population mean salary for men minus the population mean salary for women (after adjusting for marketability) is estimated as $-593$. For every additional year since completing the degree, we add $227$ to the difference, giving a difference of $0$ after a little o","date":"0001-01-01","objectID":"/chapter-1-review-of-linear-regression/:8:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 1 ：Review of linear regression","uri":"/chapter-1-review-of-linear-regression/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"9 Dummy variables for more than two groups Another important explanatory variable for salary is academic rank, coded in the variable rank as 1 for assistant professor, 2 for associate professor, and 3 for full professor. Using rank as a continuous covariate in a simple regression model would force a constraint on the population mean salaries for the three groups, namely, that the mean salary of associate professors is halfway between the mean salaries of assistant and full professors: Such linearity is a strong assumption for an ordinal variable, such as rank, and a meaningless assumption for unordered categorical covariates, such as ethnicity, where the ordering of the values assigned to categories is arbitrary. It thus makes sense to estimate the mean of each rank freely by treating one of the ranks, for instance, assistant professor, as the reference category and by using dummy variables for the other two ranks. We can create the dummy variables by typing generate associate = rank==2 if !missing(rank) generate full = rank==3 if !missing(rank) Here the logical expression rank==2 evaluates to 1 if it is true and 0 otherwise. This expression yields a 0 when rank is 1, 3, or missing, but we do not want to interpret a missing value; therefore, the if condition is necessary to ensure that missing values in rank translate to missing values in the dummy variables. Our preferred method for producing dummy variables is by using the tabulate command with the generate() option, as follows: drop associate full tabulate rank, generate(r) rename r2 associate rename r3 full rank | Freq. Percent Cum.\r------------+-----------------------------------\rAssistant | 143 27.82 27.82\rAssociate | 160 31.13 58.95\rFull | 211 41.05 100.00\r------------+-----------------------------------\rTotal | 514 100.00\rThe generate(r) option produces dummy variables for each unique value of rank, here named r1, r2, and r3 for the values 1, 2, and 3. (The naming of the dummy variables would have been the same if the unique values had been 0, 1, and 4.) An advantage of using tabulate is that it places missing values into dummy variables whenever the original variable is missing—as shown above, this requires extra caution when using the generate command. Denoting these dummy variables $x_{2i}$ and $x_{3i}$ , respectively, we specify the model Showing that the intercept $\\beta_1$ represents the population mean salary for the reference category (assistant professors), $\\beta_2$ represents the difference in mean salaries between associate and assistant professors, and $\\beta_3$ represents the difference in mean salaries between full and assistant professors. Hence, the coefficient of each dummy variable represents the population mean of the corresponding group minus the population mean of the reference group. Figure 1.15 illustrates how the model-implied means for the three ranks are determined by the regression coefficients. Figure 1.15: Illustration: Interpretations of coefficients of dummy variables $x_{2i}$ and $x_{3i}$ for associate and full professors, with assistant professors as the reference category\rEstimates for the regression model with dummy variables for academic rank are obtained using regress salary associate full Source | SS df MS Number of obs = 514\r-------------+---------------------------------- F(2, 511) = 262.54\rModel | 4.1753e+10 2 2.0877e+10 Prob \u003e F = 0.0000\rResidual | 4.0634e+10 511 79518710.1 R-squared = 0.5068\r-------------+---------------------------------- Adj R-squared = 0.5049\rTotal | 8.2387e+10 513 160599133 Root MSE = 8917.3\r------------------------------------------------------------------------------\rsalary | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\rassociate | 7285.12 1026.19 7.10 0.000 5269.05 9301.19\rfull | 21267.11 965.89 22.02 0.000 19369.51 23164.71\r_cons | 39865.86 745.70 53.46 0.000 38400.84 41330.88\r-----------------------------------------","date":"0001-01-01","objectID":"/chapter-1-review-of-linear-regression/:9:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 1 ：Review of linear regression","uri":"/chapter-1-review-of-linear-regression/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"10 Other types of interactions ","date":"0001-01-01","objectID":"/chapter-1-review-of-linear-regression/:10:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 1 ：Review of linear regression","uri":"/chapter-1-review-of-linear-regression/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"10.1 Interaction between dummy variables Could the salary difference between ranks be gender specific? Equivalently, could the gender gap in salaries depend on academic rank? These questions can be answered by including the two interaction terms $male \\times associate (x_{2i} \\cdot x_{5i})$ and $male \\times full (x_{2i} \\cdot x_{6i})$ in the model (We omit the male by yearsdg interaction here for simplicity). If the other terms denoted as “···” above are omitted, this model becomes a two-way ANOVA model with main effects and an interaction between academic rank and gender. An interaction between dummy variables can be interpreted as a difference of a difference. For instance, we see from (1.4) that $\\beta_7$ represents the difference between men and women of the difference between the mean salaries of associate and assistant professors. We now construct the interactions, generate male_assoc = male*associate generate male_full = male*full and fit the regression model including these interactions: regress salary male marketc yearsdg associate full male_assoc male_full Source | SS df MS Number of obs = 514\r-------------+---------------------------------- F(7, 506) = 205.77\rModel | 6.0969e+10 7 8.7099e+09 Prob \u003e F = 0.0000\rResidual | 2.1418e+10 506 42328437.6 R-squared = 0.7400\r-------------+---------------------------------- Adj R-squared = 0.7364\rTotal | 8.2387e+10 513 160599133 Root MSE = 6506\r------------------------------------------------------------------------------\rsalary | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\rmale | 465.23 1118.95 0.42 0.678 -1733.13 2663.60\rmarketc | 36950.82 1985.14 18.61 0.000 33050.69 40850.95\ryearsdg | 552.34 51.93 10.64 0.000 450.31 654.37\rassociate | 3008.13 1306.74 2.30 0.022 440.81 5575.44\rfull | 9098.93 1894.29 4.80 0.000 5377.28 12820.58\rmale_assoc | 284.04 1574.62 0.18 0.857 -2809.56 3377.64\rmale_full | 2539.39 1919.64 1.32 0.186 -1232.05 6310.83\r_cons | 36397.49 885.95 41.08 0.000 34656.90 38138.09\r------------------------------------------------------------------------------\rFrom (1.5), we see that the estimated difference in population mean salaries between men and women is $\\widehat \\beta_2 + \\widehat \\beta_7 associate + \\widehat \\beta_8 full$. In other words, the estimated coefficient $\\widehat \\beta_7$ of $male_{assoc}$ can be interpreted as the difference in estimated gender gap between associate and assistant professors, and similarly for $male_full$. Neither interaction coefficient is significant at the 5% level. However, it is not considered good practice to include only some of the interaction terms for a group of dummy variables representing one categorical variable; hence, we should test both coefficients simultaneously. testparm male_assoc male_full ( 1) male_assoc = 0\r( 2) male_full = 0\rF( 2, 506) = 0.95\rProb \u003e F = 0.3864\rThere is little evidence for an interaction between gender and academic rank [$F(2,506) = 0.95$, $p=0.39$]. Using factor variables instead of constructing interaction terms ourselves, the regress command becomes regress salary marketc yearsdg i.male##i.rank Source | SS df MS Number of obs = 514\r-------------+---------------------------------- F(7, 506) = 205.77\rModel | 6.0969e+10 7 8.7099e+09 Prob \u003e F = 0.0000\rResidual | 2.1418e+10 506 42328437.6 R-squared = 0.7400\r-------------+---------------------------------- Adj R-squared = 0.7364\rTotal | 8.2387e+10 513 160599133 Root MSE = 6506\r--------------------------------------------------------------------------------\rsalary | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r---------------+----------------------------------------------------------------\rmarketc | 36950.82 1985.14 18.61 0.000 33050.69 40850.95\ryearsdg | 552.34 51.93 10.64 0.000 450.31 654.37\r|\rmale |\rMen | 465.23 1118.95 0.42 0.678 -1733.13 2663.60\r|\rrank |\rAssociate | 3008.13 1306.74 2.30 0.022 440.81 5575.44\rFull | 9098.93 1894.29 4.80 0.000 5377.28 128","date":"0001-01-01","objectID":"/chapter-1-review-of-linear-regression/:10:1","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 1 ：Review of linear regression","uri":"/chapter-1-review-of-linear-regression/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"10.2 Interaction between continuous covariates The effect of marketability, marketc ($x_{3i}$), could increase or decrease with time since degree, yearsdg ($x_{4i}$). We can include an interaction between these two continuous covariates in a regression model: We can then fit this model in Stata by using the commands generate market_yrs = marketc*yearsdg regress salary male marketc yearsdg associate full market_yrs Source | SS df MS Number of obs = 514\r-------------+---------------------------------- F(6, 507) = 247.16\rModel | 6.1397e+10 6 1.0233e+10 Prob \u003e F = 0.0000\rResidual | 2.0990e+10 507 41401072 R-squared = 0.7452\r-------------+---------------------------------- Adj R-squared = 0.7422\rTotal | 8.2387e+10 513 160599133 Root MSE = 6434.4\r------------------------------------------------------------------------------\rsalary | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\rmale | 926.13 712.29 1.30 0.194 -473.27 2325.53\rmarketc | 46905.65 3455.75 13.57 0.000 40116.31 53695.00\ryearsdg | 540.73 51.40 10.52 0.000 439.74 641.72\rassociate | 3303.13 859.65 3.84 0.000 1614.23 4992.04\rfull | 11573.46 1165.94 9.93 0.000 9282.80 13864.12\rmarket_yrs | -750.42 214.13 -3.50 0.000 -1171.10 -329.73\r_cons | 36044.19 711.62 50.65 0.000 34646.10 37442.27\r------------------------------------------------------------------------------\rUsing (1.6), we see that the estimated effect of marketability, $\\widehat \\beta_3 + \\widehat \\beta_7 yearsdg$,decreases from $46,906$ for faculty who have just completed their degree to $46906-750.42×30=24.394$ for faculty who completed their degree 30 years ago. Using factor variables , the model can be fit like this: regress salary i.male c.marketc##c.yearsdg i.rank Source | SS df MS Number of obs = 514\r-------------+---------------------------------- F(6, 507) = 247.16\rModel | 6.1397e+10 6 1.0233e+10 Prob \u003e F = 0.0000\rResidual | 2.0990e+10 507 41401072 R-squared = 0.7452\r-------------+---------------------------------- Adj R-squared = 0.7422\rTotal | 8.2387e+10 513 160599133 Root MSE = 6434.4\r-------------------------------------------------------------------------------------\rsalary | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r--------------------+----------------------------------------------------------------\rmale |\rMen | 926.13 712.29 1.30 0.194 -473.27 2325.53\rmarketc | 46905.65 3455.75 13.57 0.000 40116.31 53695.00\ryearsdg | 540.73 51.40 10.52 0.000 439.74 641.72\r|\rc.marketc#c.yearsdg | -750.42 214.13 -3.50 0.000 -1171.10 -329.73\r|\rrank |\rAssociate | 3303.13 859.65 3.84 0.000 1614.23 4992.04\rFull | 11573.46 1165.94 9.93 0.000 9282.80 13864.12\r|\r_cons | 36044.19 711.62 50.65 0.000 34646.10 37442.27\r-------------------------------------------------------------------------------------\r","date":"0001-01-01","objectID":"/chapter-1-review-of-linear-regression/:10:2","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 1 ：Review of linear regression","uri":"/chapter-1-review-of-linear-regression/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"11 Nonlinear effects We have assumed that the relationship between population mean salary and each of the continuous covariates marketc and yearsdg is linear after controlling for the other variables. However, the increase in population mean salary per year is likely to increase with time since degree (for instance, if percentage increases are constant over time). Such a nonlinear relationship can be modeled by including the square of yearsdg in the model in addition to yearsdg itself (adding the interaction male_years back in): generate yearsdg2 = yearsdg^2 regress salary male marketc yearsdg male_years associate full /// market_yrs yearsdg2 Source | SS df MS Number of obs = 514\r-------------+---------------------------------- F(8, 505) = 192.04\rModel | 6.2005e+10 8 7.7507e+09 Prob \u003e F = 0.0000\rResidual | 2.0382e+10 505 40360475.1 R-squared = 0.7526\r-------------+---------------------------------- Adj R-squared = 0.7487\rTotal | 8.2387e+10 513 160599133 Root MSE = 6353\r------------------------------------------------------------------------------\rsalary | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\rmale | -1181.82 1251.65 -0.94 0.346 -3640.90 1277.25\rmarketc | 46578.20 3544.48 13.14 0.000 39614.45 53541.95\ryearsdg | 39.04 144.88 0.27 0.788 -245.59 323.67\rmale_years | 177.71 89.36 1.99 0.047 2.14 353.28\rassociate | 4811.53 967.94 4.97 0.000 2909.85 6713.21\rfull | 12791.00 1230.26 10.40 0.000 10373.95 15208.05\rmarket_yrs | -726.39 222.43 -3.27 0.001 -1163.38 -289.39\ryearsdg2 | 10.11 3.97 2.55 0.011 2.31 17.91\r_cons | 38837.69 1027.38 37.80 0.000 36819.23 40856.16\r------------------------------------------------------------------------------\rThe estimated coefficient of yearsdg2 is significantly different from 0 at, say, the 5% level ($t=2.55, df=505, p=0.01$), whereas the coefficient of yearsdg is no longer statistically significant. It should nevertheless be retained to form a flexible quadratic curve because the minimum of the curve is otherwise arbitrarily forced to occur when yearsdg=0. We can visualize the relationship between salary and time since degree for male and female assistant professors (associate 0, full 0) in disciplines with mean marketability (marketc 0) by using the twoway function command, which produces figure 1.16: twoway (function Women = _b[_cons] + _b[yearsdg]*x + _b[yearsdg2]*x^2, /// range(0 41) lpatt(dash)) /// (function Men = _b[_cons] + _b[male] + (_b[yearsdg] + _b[male_years])*x /// + _b[yearsdg2]*x^2, range(0 41) lpatt(solid)), /// xtitle(Time since degree (years)) ytitle(Mean salary) The quadratic term for yearsdg can also be included in the regression by using factor variables, simply by using the expression c.yearsdg#c.yearsdg. We can therefore use the following command to fit the same model as before: regress salary i.male##c.yearsdg c.marketc c.marketc#c.yearsdg i.rank c.yearsdg#c.yearsdg Here we use a double-hash, ##, in i.male##c.yearsdg to include i.male, c.yearsdg, and i.male#c.yearsdg. To avoid duplicating the term c.yearsdg, we use a single hash for c.marketc#c.yearsdg and then include the missing term c.marketc. In fact, it is not necessary to avoid duplication because Stata will drop any redundant terms. However, the terms are then listed in the output together with the label omitted. A preferred approach therefore is to factorize out all terms that interact with c.yearsdg by typing regress salary (i.male c.marketc c.yearsdg)##c.yearsdg i.rank Source | SS df MS Number of obs = 514\r-------------+---------------------------------- F(8, 505) = 192.04\rModel | 6.2005e+10 8 7.7507e+09 Prob \u003e F = 0.0000\rResidual | 2.0382e+10 505 40360475.1 R-squared = 0.7526\r-------------+---------------------------------- Adj R-squared = 0.7487\rTotal | 8.2387e+10 513 160599133 Root MSE = 6353\r-------------------------------------------------------------------------------------\rsalary | Coefficient Std. err. t P\u003e|t| [95% co","date":"0001-01-01","objectID":"/chapter-1-review-of-linear-regression/:11:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 1 ：Review of linear regression","uri":"/chapter-1-review-of-linear-regression/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"12 Residual diagnostics Predicted residuals are defined as the differences between the observed responses $y_i$ and the predicted responses $\\widehat \\y_i$: $$\\widehat{\\epsilon_i}=y_i-\\underbrace{(\\widehat{\\beta}1+\\widehat{\\beta}2x{2i}+\\cdots+\\widehat{\\beta}px{pi})}{\\widehat{y}_i}$$ Predicted standardized residuals are obtained as $$r_i=\\frac{\\widehat{\\epsilon_i}}{s_{r_i}}$$ where $s_{r_i}$ is the estimated standard error of the residual. Predicted residuals or standardized residuals can be used to investigate whether model assumptions, such as homoskedasticity and normally distributed errors, are violated. Predicted standardized residuals have the advantage that they have an approximate standard normal distribution if the model assumptions are true. For instance, a value greater than 3 should occur only about 0.1% of the time and may therefore be an outlier. The postestimation command predict with the residuals option provides predicted residuals for the last regression model that was fit. predict res, residuals Standardized residuals can be obtained by using the rstandard option of predict. A histogram of the predicted residuals with an overlaid normal distribution is produced by the histogram command with the normal option. histogram res, normal Figure 1.17: Predicted residuals with overlaid normal distribution\rThe distribution is somewhat skewed, suggesting that salary should perhaps be log-transformed as discussed in section 1.3. Again, it may be advisable to use the vce(robust) option, which provides estimated standard errors for regression coefficients that are not just robust to heteroskedasticity but also to other violations of the distributional assumptions. ","date":"0001-01-01","objectID":"/chapter-1-review-of-linear-regression/:12:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Chapter 1 ：Review of linear regression","uri":"/chapter-1-review-of-linear-regression/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"Multilevel \u0026 Longitudinal Models : When and why?","date":"2024-04-30","objectID":"/chapter-0-introduction/","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Introduction","uri":"/chapter-0-introduction/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"Level-1 units within the same level-2 unit or cluster tend to be more similar to each other than to units in other clusters. One reason for this is that units do not end up in the same cluster by chance but through some mechanism that may be related to their characteristics. For instance, which schools children go to is influenced by their family background through place of residence or parental choice, so children within a school already have something in common from the first day of school. Perhaps more importantly, children within a school are subsequently affected by their peers, teachers, and school policies, making them even more similar. Such within-school similarity or dependence will be particularly apparent if there are large between-school differences in terms of children’s backgrounds, school environments, and school policies. In the same vein, siblings are similar at birth because they have the same parents and therefore share genes, and they subsequently become even more similar by being raised in the same family, where they share a common environment and experiences. Within-cluster dependence violates the assumption of ordinary regression models that responses are conditionally independent given the covariates (the residuals are independent). Consequently, ordinary regression produces incorrect standard errors, a problem that can be overcome by using multilevel models. More importantly, multilevel modeling allows us to disentangle processes operating at different levels, both by including explanatory variables at the different levels and by attributing unexplained variability to the different levels. One important challenge in multilevel modeling is to distinguish within- and between-cluster effects of lower-level covariates. For instance, for students nested in schools, an important student-level explanatory variable for achievement is socioeconomic status (SES). In addition to the effect of own SES, the school-average SES can be strongly associated with achievement, both through peer effects and because low-SES children tend to end up in worse schools. Policy interventions often occur at the level of institutions, and it is important to understand how such higher-level variables affect the response variable. In the school setting, typical examples would be new curricula or different kinds of teacher professional development. The effect of such interventions on achievement is usually the primary concern, but it is also important to investigate the differential effects for different student subpopulations. For instance, a new science curriculum may be particularly beneficial for girls and hence reduce the gender gap in science achievement. If it does, then there is a cross-level interaction between the level-1 variable gender and the level-2 variable curriculum. It will generally not be possible to explain all within-school and between-school variability in achievement. Quantifying the amount of unexplained variability at the different levels can be of interest in its own right, and sometimes multilevel models without explanatory variables are used to see how much the higher-level membership matters. For instance, achievement may vary less between schools in Scandinavia than in the U.S., and this tells us something about the societies and schooling systems. Repeated observations on the same units are also clustered data, for instance, longitudinal or panel data on children’s weight, mothers’ postnatal depression, taxpayers’ tax liability, employees’ wages and union membership, and the investments or number of patents of firms. Although longitudinal data are quite different from clustered cross-sectional data, the same kinds of questions arise and the same kinds of models can be used to address them. An example is data on children’s weight, where interest is in growth, the relationship between weight and the level-1 variable time. Level-2 explanatory variables include gender and ethnicity. Cross-level interaction","date":"2024-04-30","objectID":"/chapter-0-introduction/:0:0","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Introduction","uri":"/chapter-0-introduction/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"第一步：理解集群数据和多层次结构 集群数据：指的是数据点按照某种自然方式被分成不同的组或集群。例如，学生（level-1 units）被分在不同的学校（level-2 units）中。 多层次结构：数据存在嵌套关系，即较低层级单位（如学生）位于较高层级单位（如学校）之内。 ","date":"2024-04-30","objectID":"/chapter-0-introduction/:0:1","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Introduction","uri":"/chapter-0-introduction/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"第二步：集群内部相似性的原因 非随机分配：单元（如学生）不是随机分配到集群（如学校）的，而是通过与他们特征相关的机制。 家庭背景和选择：学生的居住地或家长选择影响他们上哪所学校，导致同一学校的学生从一开始就有共同点。 学校环境的影响：学生在学校内受同学、老师和学校政策的影响，进一步增加了他们的相似性。 ","date":"2024-04-30","objectID":"/chapter-0-introduction/:0:2","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Introduction","uri":"/chapter-0-introduction/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"第三步：违反普通回归模型的假设 条件独立性：普通回归模型假设给定协变量后，响应是条件独立的，即残差是独立的。 集群内部依赖性：集群内部的相似性或依赖性违反了这一假设，导致普通回归模型产生不正确的标准误。 ","date":"2024-04-30","objectID":"/chapter-0-introduction/:0:3","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Introduction","uri":"/chapter-0-introduction/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"第四步：多层次模型的应用 解决依赖性问题：多层次模型可以克服普通回归模型的局限性，适用于处理集群数据。 不同层级的过程分离：多层次模型允许我们通过在不同层级包含解释变量，并将未解释的变异归因于不同层级，来分离在不同层级上起作用的过程。 ","date":"2024-04-30","objectID":"/chapter-0-introduction/:0:4","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Introduction","uri":"/chapter-0-introduction/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"第五步：区分集群内外效应 学生成就的例子：学生的经济社会地位（SES）是影响其成就的重要学生层级解释变量。除了自身的SES，学校平均SES也与成就强烈相关，这是通过同伴效应以及低SES儿童倾向于进入较差学校实现的。 ","date":"2024-04-30","objectID":"/chapter-0-introduction/:0:5","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Introduction","uri":"/chapter-0-introduction/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"第六步：政策干预和高阶层级变量的影响 机构层面的干预：政策干预通常发生在机构层面，理解这些高阶层级变量如何影响响应变量（如学生成就）是很重要的。 不同学生亚群体的效应差异：例如，新的科学课程可能对女孩特别有益，从而减少科学成就中的性别差距。 ","date":"2024-04-30","objectID":"/chapter-0-introduction/:0:6","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Introduction","uri":"/chapter-0-introduction/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"第七步：量化未解释的变异 多层次模型的应用：即使没有解释变量，也可以使用多层次模型来量化不同层级上未解释变异的数量，这有助于我们了解社会和教育系统的特点。 ","date":"2024-04-30","objectID":"/chapter-0-introduction/:0:7","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Introduction","uri":"/chapter-0-introduction/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"第八步：重复观测和纵向数据 纵向数据的集群特性：例如，对儿童体重、母亲的产后抑郁、纳税人的税收责任等的重复观测也是集群数据。 多层次模型的应用：尽管纵向数据与集群横截面数据非常不同，但可以应用相同类型的模型来解决类似问题。 ","date":"2024-04-30","objectID":"/chapter-0-introduction/:0:8","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Introduction","uri":"/chapter-0-introduction/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"第九步：因果推断和政策评估 政策效果的评估：在评估如公共场所禁烟政策这类通常依赖于观察数据的政策时，重要的是估计集群内部效应，并控制影响立法和香烟消费的集群层级变量。 ","date":"2024-04-30","objectID":"/chapter-0-introduction/:0:9","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Introduction","uri":"/chapter-0-introduction/"},{"categories":["Multilevel \u0026 Longitudinal Models Using stata"],"content":"总结 多层次模型为我们提供了一种强大的工具，用于分析和理解嵌套数据结构中的复杂关系，允许我们考虑和解释不同层级上的变量对响应变量的影响，以及这些变量之间可能存在的交互作用。通过这种方式，我们可以更准确地评估政策的影响，理解不同群体间的差异，并为决策提供更深入的洞见。 ","date":"2024-04-30","objectID":"/chapter-0-introduction/:0:10","tags":["Multilevel \u0026 Longitudinal Models","stata"],"title":"[Multilevel  Longitudinal Models] Introduction","uri":"/chapter-0-introduction/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"In this chapter, we focus on the multinomial logit model (MNL), which is the most frequently used nominal regression model.","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"Count variables record how many times something has happened. Examples include the number of patients, hospitalizations, daily homicides, theater visits, international conflicts, beverages consumed, industrial injuries, soccer goals scored, new companies, and arrests by police. Although the linear regression model has often been applied to count outcomes, these estimates can be inconsistent or inefficient. In some cases, the linear regression model can provide reasonable results; however, it is much safer to use models specifically designed for count outcomes. In this chapter, we consider seven regression models for count outcomes, all based on the Poisson distribution. We begin with the Poisson regression model (PRM), which is the foundation for other count models. We then consider the negative binomial regression model (NBRM), which adds unobserved, continuous heterogeneity to the PRM and often provides a much better fit to the data. To deal with outcomes where observations with zero counts are missing, we consider the zero-truncated Poisson and zero-truncated negative binomial models. By combining a zero-truncated model with a binary model, we develop the hurdle regression model, which models zero and nonzero counts in separate equations. Finally, we consider the zero-inflated Poisson and the zero-inflated negative binomial models, which assume that there are two sources of zero counts. As with earlier chapters, we review the statistical models, consider issues of testing and fit, and then discuss methods of interpretation. These discussions are intended as a review for those who are familiar with the models. See Long (1997) for a more technical introduction to count models and Cameron and Trivedi (2013) for a definitive review. You can obtain sample do-files and datasets as explained in chapter 2. ","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:0:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1 The Poisson distribution Because the univariate Poisson distribution is fundamental to understanding regression models for counts, we start by exploring this distribution. Let $\\mu$ be the rate of occurrence or the expected number of times an event will occur during a given period of time. Let $y$ be a random variable indicating the actual number of times an event did occur. Sometimes the event will occur fewer times than expected, even not at all, and other times it will occur more often. The relationship between the expected count $\\mu$ and the probability of observing a given count $y$ is specified by the Poisson distribution $$\\Pr(y\\mid\\mu)=\\frac{e^{-\\mu}\\mu^{y}}{y!}\\quad\\mathrm{~for~}y=0,1,2,\\ldots $$ where $ \\mu \u003e 0 $ is the sole parameter defining the distribution. $ y! $ is the factorial operator: for example, $ 4! = 4 \\times 3 \\times 2 \\times 1 $. The easiest way to get a sense of the Poisson distribution is to compare plots of predicted probabilities for different values of the rate $ \\mu $, as shown in Figure 9.1. The Poisson probability density function (PDF) for different rates\rThe figure illustrates four characteristics of the Poisson distribution that are important for understanding regression models for counts: As the mean of the distribution $\\mu$ increases, the mass of the distribution shifts to the right. The mean ($ \\mu $) is also the variance. Thus $ \\text{Var}(y) = \\mu $, which is known as equidispersion. In real data, count variables often have a variance greater than the mean, which is called overdispersion. It is possible for counts to be underdispersed, but this is rarer. As $ \\mu $ increases, the probability of a zero count decreases rapidly. For many count variables, there are more observed 0s than predicted by the Poisson distribution. as $\\mu$ increases, the Poisson distribution approximates a normal distribution. This is shown by the distribution for $ \\mu = 10.5 $. 泊松分布的定义： 假设你经营一家呼叫中心，你想知道在一天中接到特定数量电话的概率。你预计平均每天会接到10个电话（$\\mu = 10$）。 泊松分布可以帮你计算实际接到5个、10个或15个电话的概率。它不关心具体哪个电话是在哪个时间接到的，只关心一天结束时总共接到了多少电话。 概率计算公式： 泊松分布的概率公式是 $\\Pr(y\\mid\\mu)=\\frac{e^{-\\mu}\\mu^{y}}{y!}$。以 $\\mu = 10$ 为例，计算接到恰好5个电话的概率是 $\\Pr(y=5\\mid\\mu=10)=\\frac{e^{-10} \\cdot 10^5}{5!}$。 $e^{-10}$ 是一个小于1的数，$10^5$ 是10的5次方，$5!$ 是5的阶乘，即 $5 \\times 4 \\times 3 \\times 2 \\times 1$。这个公式的结果会给出接到5个电话的概率。 泊松分布的特性： 分布的移动：如果平均电话数量从10增加到20，那么接到更多电话的概率会增加。比如，接到15个电话的概率在 $\\mu = 10$ 时可能很低，但在 $\\mu = 20$ 时会显著增加。 方差与平均数的关系：在泊松分布中，方差（即数据波动的程度）等于平均数。这意味着如果事件平均发生次数是10，那么它的波动程度也是10。但在实际数据中，我们经常看到方差大于平均数，这被称为过度分散。比如，虽然平均每天接到10个电话，但实际的电话数量可能会在很大的范围内波动。 零事件概率的降低：随着平均数的增加，事件完全没有发生的概率迅速降低。例如，如果预计每天接到10个电话，那么完全没有接到电话的概率会很低。 泊松分布向正态分布的近似：当平均数 $\\mu$ 很大时，泊松分布看起来越来越像正态分布，即钟形曲线。这意味着事件发生次数的分布变得更加对称和集中。 These ideas are used as we develop regression models for count outcomes in the rest of the chapter. Aside: Plotting the Poisson PDF. The commands below were used to create Figure 9.1. The first generate creates variable $ k $, which contains the values 0 to 20 that are the counts for which we want to compute probabilities. This is done by subtracting 1 from _n, where _n is how Stata refers to the row number of an observation. The probability of outcome $ k $ from a Poisson distribution with mean $ \\mu $ is computed with the function poissonp(mu, k) for each of four values of $ \\mu $. clear all set obs 21 gen k = _n - 1 label var k \"y = # of events\" gen psn1 = poissonp(0.8, k) label var psn1 \"\u0026mu = 0.8\" gen psn2 = poissonp(1.5, k) label var psn2 \"\u0026mu = 1.5\" gen psn3 = poissonp(2.9, k) label var psn3 \"\u0026mu = 2.9\" gen psn4 = poissonp(10.5, k) label var psn4 \"\u0026mu = 10.5\" graph twoway connected psn1 psn2 psn3 psn4 k, /// ytitle(\"Probability\") ylabel(0(.1).5) xlabel(0(2)20) /// lwidth(thin thin thin thin) msymbol(O D S T) The Poisson probability density function (PDF) for different rates\r","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:1:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.1 Fitting the Poisson distribution with the poisson command To illustrate count models, we use data from Long (1990) on the number of articles written by biochemists in the 3 years prior to receiving their doctorate. The variables considered are: use couart4, clear codebook art female married kid5 mentor phd, compact Variable Obs Unique Mean Min Max Label\r-------------------------------------------------------------------------------\rart 915 15 1.692896 0 19 Articles in last 3 yrs of PhD\rfemale 915 2 .4601093 0 1 Gender: 1=female 0=male\rmarried 915 2 .6622951 0 1 Married: 1=yes 0=no\rkid5 915 4 .495082 0 3 # of kids \u003c 6\rmentor 915 49 8.767213 0 77 Mentor's # of articles\rphd 915 83 3.103109 .755 4.62 PhD prestige\r-------------------------------------------------------------------------------\rtabulate art, missing Articles in |\rlast 3 yrs |\rof PhD | Freq. Percent Cum.\r------------+-----------------------------------\r0 | 275 30.05 30.05\r1 | 246 26.89 56.94\r2 | 178 19.45 76.39\r3 | 84 9.18 85.57\r4 | 67 7.32 92.90\r5 | 27 2.95 95.85\r6 | 17 1.86 97.70\r7 | 12 1.31 99.02\r8 | 1 0.11 99.13\r9 | 2 0.22 99.34\r10 | 1 0.11 99.45\r11 | 1 0.11 99.56\r12 | 2 0.22 99.78\r16 | 1 0.11 99.89\r19 | 1 0.11 100.00\r------------+-----------------------------------\rTotal | 915 100.00\rOften, the first step in analyzing a count variable is to compare the mean with the variance to determine whether there is overdispersion. By default, summarize does not report the variance, so we use the detail option: sum art, detail Articles in last 3 yrs of PhD\r-------------------------------------------------------------\rPercentiles Smallest\r1% 0 0\r5% 0 0\r10% 0 0 Obs 915\r25% 0 0 Sum of wgt. 915\r50% 1 Mean 1.692896\rLargest Std. dev. 1.926069\r75% 2 12\r90% 4 12 Variance 3.709742\r95% 5 16 Skewness 2.51892\r99% 7 19 Kurtosis 15.66293\rThe variance is more than twice as large as the mean, providing clear evidence of overdispersion. ","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:1:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.2 Comparing observed and predicted counts with mgen We can visually inspect the overdispersion in a rt by comparing the observed probabilities with those predicted from the Poisson distribution. Later, we will use the same method as a first assessment of the specification of count regression models (section 9.2.5). We begin by using the poisson command to fit a model with a constant but no independent variables. When there are no independent variables, poisson fits a univariate Poisson distribution, where $ \\exp(\\beta_0) $ equals the mean $ \\mu $. Using art as the outcome, poisson art, nolog Poisson regression Number of obs = 915\rLR chi2(0) = 0.00\rProb \u003e chi2 = .\rLog likelihood = -1742.5735 Pseudo R2 = 0.0000\r------------------------------------------------------------------------------\rart | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_cons | 0.53 0.03 20.72 0.000 0.48 0.58\r------------------------------------------------------------------------------\rBecause $ \\hat{\\beta}_0 = 0.526 $, the estimated rate is $ \\hat{\\mu} = \\exp(0.526) = 1.693 $, which matches the mean of art obtained with summarize earlier. In earlier chapters, we used mgen to compute predictions as an independent variable changed, holding other variables constant. Although this can be done with count models, as illustrated below, the mgen option meanpred creates variables with observed and average predicted probabilities, where the rows correspond to values of the outcome. The syntax for mgen used in this way is: mgen, meanpred stub(stub) pr(min/max) [options] where option pr (min/max) specifies that we want to create variables with predictions for each of the counts from min to max. The new variables are: In a regression model with no independent variables, the predicted probability of $ y = k $ is the same for all observations. Accordingly, mgen is simply computing $ Pr(y = k) $ from a Poisson distribution with a mean equal to the mean of the outcome. In our example, poisson art, nolog mgen, pr(0/9) meanpred stub(psn) Predictions from: Variable Obs Unique Mean Min Max Label\r---------------------------------------------------------------------------------------\rpsnval 10 10 4.5 0 9 Articles in last 3 yrs of PhD\rpsnobeq 10 10 .0993443 .0010929 .3005464 Observed proportion\rpsnoble 10 10 .8328962 .3005464 .9934426 Observed cum. proportion\rpsnpreq 10 10 .0999988 .0000579 .311469 Avg predicted Pr(y=#)\rpsnprle 10 10 .8307106 .1839859 .9999884 Avg predicted cum. Pr(y=#)\rpsnob_pr 10 10 -.0006546 -.0691068 .1165606 Observed - Avg Pr(y=#)\r---------------------------------------------------------------------------------------\rmgen created six variables with 10 observations that correspond to the counts 0-9. In the list below, $ p_sn_val $ contains the count values, $ psnobeq $ contains observed proportions or probabilities, and $ psnpreq $ has predicted probabilities from a Poisson distribution with mean 1.69: mgen 创建了六个变量，每个变量有10个观测值，分别对应计数0-9。在下面的列表中，$ psnval $ 包含计数值，$ psnobeq $ 包含观测到的比例或概率，而 $ psnpreq $ 则有来自均值为1.69的泊松分布的预测概率。 这段话是在描述一个统计模拟过程，其中 mgen 可能是一个用于生成模拟数据的软件或程序。这里的关键点解释如下： 六个变量：这意味着 mgen 生成了六个不同的变量。这些变量可能代表不同的实验组或条件。 每个变量有10个观测值：每个变量都包含10个观测值，这些观测值可能代表某种事件发生的次数。 计数0-9：观测值代表事件发生的次数，范围从0（事件没有发生）到9（事件发生了9次）。 psnval：这个变量包含了具体的计数值。例如，如果事件发生了3次，那么 $ psnval $ 中的相应条目就是3。 psnobeq：这个变量包含了观测到的比例或概率。例如，如果有10个观测值中有3个是3，那么 $ psnobeq $ 中对应的概率就是 $ \\frac{3}{10} $。 psnpreq：这个变量包含了根据泊松分布预测的概率。泊松分布是一种统计学上用来描述特定时间内发生某事件的次数的概率分布。在这里，泊松分布的均值（即事件的平均发生次数）被设定为1.69。因此，$ psnpreq $ 中的每个条目都是根据泊松公式计算得出的，对应于每个计数值的预测概率。 list psnval psnobeq psnpreq in 1/10 +--------------------------------+\r| psnval psnobeq psnpreq |\r|--------------------------------|\r1. | 0 .30054645 .1839859 |\r2. | 1 .26885246 .31146902 |\r3. | 2 .19453552 .26364236 |\r4. | 3 .09180328 .14877305 |\r5. | 4 .07322404 .06296433 |\r|--------------------------------|\r6. | 5 .0295082 .02131841 |\r7. | 6 .01857923 .00601498 |\r8. | 7 .01311475 .00145","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:1:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2 The Poisson regression model The PRM extends the Poisson distribution by allowing each observation i to have a different rate $ \\mu_i $. More formally, the PRM assumes that the observed count for observation i is drawn from a Poisson distribution with mean $ \\mu_i $ where $ \\mu_i $ is estimated from the independent variables in the model. This is sometimes referred to as incorporating observed heterogeneity and leads to the structural equation $$\\mu_{i}=E\\left(y_{i}\\mid\\mathbf{x_i}\\right)=\\exp\\left(\\mathbf{x}_{i}\\boldsymbol{\\beta}\\right)$$ Taking the exponential of $x_i\\beta$ forces $\\mu_i$ to be positive, which is necessary because counts can be only 0 or positive. To see how this works, consider the PRM with one independent variable: the mean $\\begin{array}{rcl}\\mu\u0026=\u0026\\exp\\left(\\alpha+\\beta x\\right)\\end{array}$ shown by the solid, curved line that increases as x increases. For each value of $\\mu$,the Poisson distribution around the mean is shown by the dots, which should be thought of as coming out of the page to represent the probability of each count. Interpretation of the model involves assessing how changes in the independent variables affect the conditional mean and the probabilities of each count. Details on interpretation are given after we consider estimation. 首先，我们知道泊松分布是用来描述在一段时间或空间内发生某事件的次数。在基本的泊松分布中，我们假设这个事件发生的平均速率（mean rate）是固定的，我们用符号 $ \\mu $ 来表示它。但是，在现实生活中，这个平均速率可能会因为不同的条件或因素而变化。 这就是泊松回归模型（PRM）的作用。它扩展了基本的泊松分布，允许每个观测值有自己的平均速率 $ \\mu_i $。这意味着，对于不同的观测值，事件发生的平均次数可以是不同的。 在泊松回归模型中，我们假设每个观测值的计数（比如事件发生的次数）是从一个平均速率为 $ \\mu_i $ 的泊松分布中抽取的。这个 $ \\mu_i $ 是通过模型中的自变量来估计的。这就是说，我们可以根据不同的条件或因素来预测事件发生的平均次数。 模型的数学方程是 $ \\mu_{i} = E(y_{i} | \\mathbf{x_i}) = \\exp(\\mathbf{x}_{i} \\boldsymbol{\\beta}) $。这里的 $ \\exp $ 表示自然指数函数，它的作用是保证 $ \\mu_i $ 是正数。这是必要的，因为事件的次数只能是0或正数。 当我们只有一个自变量时，模型可以表示为 $ \\mu = \\exp(\\alpha + \\beta x) $。这个方程告诉我们，事件发生的平均次数是随着自变量 $ x $ 的增加而增加的。 图表展示了这个模型是如何工作的。实线表示事件发生的平均次数 $ \\mu $，它随着 $ x $ 的增加而上升。围绕着这个平均值的点表示泊松分布，这些点可以被想象成从页面跳出来，代表每个计数的概率。 解释这个模型涉及到评估自变量的变化如何影响事件的平均次数和每个计数的概率。 ","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:2:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.1 Estimation using poisson The PRM is fit with the command poisson depvar [ indepvars ] [if] [ m ] [ weight ] i , noconstant exposure(.varnarne) vce(vcetype) irr ] In our experience, poisson converges quickly and difficulties are rare. Variable lists depvar is the dependent variable, poisson does not require this to be an integer; however, if you have noninteger values, you obtain the following warning: note: you are responsible for interpretation of noncount dep. variable. indepvars is a list of independent variables. If indepvars is not included, a model with only an intercept is estimated that fits a univariate Poisson distribution, as shown in the previous section. Specifying the estimation sample if and in qualifiers. if and in qualifiers can be used to restrict the estimation sample. For example, if you want to fit a model for only women, you could specify poisson depvar indepvars if female == 1. Listwise deletion: Stata excludes observations with missing values for any of the variables in the model. Accordingly, if two models are estimated using the same data but have different independent variables, it is possible to have different samples. As discussed in chapter 3, we recommend that you explicitly remove observations with missing data. Weights and complex samples poisson can be used with fweights, pweights, and iweights. Survey estimation for complex samples is possible using svy. See chapter 3 for details. Option noconstant suppresses the constant term or intercept in the model exposure(varname) specifies a variable indicating the amount of time during which an observation was “at risk” of the event occurring. Details are given in section 9.2.6. vce(vcetype) specifies the type of standard errors to be computed, vce(robust) requests that robust variance estimates be used. See sections 3.1.9 and 9.3.5 for details. irr reports estimated coefficients that are transformed to incidence-rate ratios defined as $\\exp(\\beta_k)$. These are discussed in section 9.2.2. Example of the PRM If scientists who differ in their rates of productivity are combined, the univariate distribution of articles will be overdispersed, with the variance greater than the mean. Differences among scientists in their rates of productivity could be due to factors such as quality of their graduate program, gender, marital status, number of young children, and the number of articles written by a scientist’s mentor. To account for these differences, we add these variables as independent variables: use couart4, clear poisson art i.female i.married kid5 phd mentor, nolog Poisson regression Number of obs = 915\rLR chi2(5) = 183.03\rProb \u003e chi2 = 0.0000\rLog likelihood = -1651.0563 Pseudo R2 = 0.0525\r------------------------------------------------------------------------------\rart | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rfemale |\rFemale | -0.22 0.05 -4.11 0.000 -0.33 -0.12\r|\rmarried |\rMarried | 0.16 0.06 2.53 0.011 0.03 0.28\rkid5 | -0.18 0.04 -4.61 0.000 -0.26 -0.11\rphd | 0.01 0.03 0.49 0.627 -0.04 0.06\rmentor | 0.03 0.00 12.73 0.000 0.02 0.03\r_cons | 0.30 0.10 2.96 0.003 0.10 0.51\r------------------------------------------------------------------------------\rHow you interpret a count model depends on whether you are interested in the expected value or rate of the count outcome the distribution of counts. If your interest is in the rate of occurrence, several methods can be used to compute the change in the rate for a change in an independent variable, holding other variables constant. If your interest is in the distribution of counts or perhaps the probability of a specific count, such as not publishing, the probability of a count for a given level of the independent variables can be computed. We begin with interpretation using rates. ","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:2:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.2 Factor and percentage changes in E(y|x) In th e PRM, $$\\mu=E\\left(y\\mid\\mathbf{x}\\right)=\\exp\\left(\\mathbf{x}\\beta\\right)$$ The changes in mu as an independent variable changes can be presented in several ways. Factor change and percentage change coefficients are counterparts to odds ratios that were discussed in previous chapters. In those chapters, we expressed reservations about the usefulness of interpreting coefficients that indicated changes in the odds. As we will explain shortly, the interpretation of factor and percentage change coefficients in count models is much clearer and more useful. Perhaps the most common method of interpretation is the factor change in the rate.Let $E\\left(y\\mid\\mathbf{x},x_{k}\\right)$ be the expected count for a given x, where we explicitly note the value of $x_k$,and let $E\\left(y\\mid\\mathbf{x},x_{k}+1\\right)$ be the expected count after increasing $x_k$ by 1.Simple algebra shows that the ratio is $$\\frac{E\\left(y\\mid\\mathbf{x},x_k+1\\right)}{E\\left(y\\mid\\mathbf{x},x_k\\right)}=e^{\\beta_k}$$ Therefore, Factor change: For a unit change in $x_k$, the expected count changes by a factor of $\\exp(\\beta_k)$, holding other variables constant. In some discussions of count models, $\\mu$ is referred to as the incidence rate and Equation (9.1) is called the incidence-rate ratio. These coefficients are shown by poisson by adding the option irr or by using the listcoef command, which is illustrated below. We can easily generalize Equation (9.1) to changes in $X_k$ of any amount $\\delta$: $$\\frac{E\\left(y\\mid\\mathbf{x},x_k+\\delta\\right)}{E\\left(y\\mid\\mathbf{x},x_k\\right)}=e^{\\beta_k\\delta}$$ This leads to interpretations such as the following: Factor change for $\\delta$: For a change of $\\delta$ in $X_k$, the expected count changes by a factor of $\\exp(\\beta_k \\delta)$, holding other variables constant. Standardized factor change: For a standard deviation change in $x_k$, the expected rate changes by a factor of $\\exp(\\beta_k s_k)$, holding other variables constant. Alternatively, we can compute the percentage change in the expected count for a $\\delta$-unit change in $x_k$, holding other variables constant: $$100\\times\\frac{E\\left(y\\mid\\mathbf{x},x_k+\\delta\\right)-E\\left(y\\mid\\mathbf{x},x_k\\right)}{E\\left(y\\mid\\mathbf{x},x_k\\right)}=100\\times\\left(e^{\\beta_k\\delta}-1\\right)$$ which can be interpreted as follows: Percentage change for $\\delta$: For a change of $\\delta$ in $x_k$, the expected count changes by $100 \\times \\left(e^{\\beta_k \\delta} - 1\\right)%$, holding other variables constant. Whether you use percentage or factor change is a matter of taste and convention in your area of research. 首先，原文提到： 在泊松回归模型（PRM）中， $$\\mu=E\\left(y\\mid\\mathbf{x}\\right)=\\exp\\left(\\mathbf{x}\\beta\\right)$$ 这里，$\\mu$ 表示给定自变量 $\\mathbf{x}$ 时因变量 $y$ 的期望值，它通过向量 $\\mathbf{x}$ 和参数向量 $\\beta$ 的乘积的指数函数来计算。 接下来，文中讨论了期望计数随独立变量的变化而变化的方式，提到了因子变化和百分比变化系数，这与之前章节讨论过的几率比相对应。在泊松回归模型中，解释因子和百分比变化系数要比解释表示概率变化的优势比系数更加清晰和有用。 现在让我们分解一个关键公式： $$\\frac{E\\left(y\\mid\\mathbf{x},x_k+1\\right)}{E\\left(y\\mid\\mathbf{x},x_k\\right)}=e^{\\beta_k}$$ 这个公式表示，当我们把变量 $x_k$ 的值增加1时，期望计数 $E(y|\\mathbf{x},x_k)$ 的变化比率是 $e^{\\beta_k}$。因此，如果 $\\beta_k$ 是2，那么 $e^{\\beta_k}$ 就是 $e^2$，大约是7.39。这意味着，保持其他变量不变的情况下，$x_k$ 增加1个单位，期望计数会增加大约7.39倍。 文中还提到了一个更一般的情况，即变量 $x_k$ 的变化量为 $\\delta$： $$\\frac{E\\left(y\\mid\\mathbf{x},x_k+\\delta\\right)}{E\\left(y\\mid\\mathbf{x},x_k\\right)}=e^{\\beta_k\\delta}$$ 这个公式告诉我们，对于 $x_k$ 的任意变化量 $\\delta$，期望计数的变化比率是 $e^{\\beta_k\\delta}$。 接下来，文中提到了如何计算 $x_k$ 变化 $\\delta$ 单位时的期望计数的百分比变化： $$100\\times\\frac{E\\left(y\\mid\\mathbf{x},x_k+\\delta\\right)-E\\left(y\\mid\\mathbf{x},x_k\\right)}{E\\left(y\\mid\\mathbf{x},x_k\\right)}=100\\times\\left(e^{\\beta_k\\delta}-1\\right)$$ 这个公式表示，对于 $x_k$ 的 $\\delta$ 单位变化，期望计数的百分比变化是 $100 \\times (e^{\\beta_k\\delta} - 1)%$。 最后，文中指出，使用百分比变化还是因子变化取决于你所在研究领域内的习惯和偏好。 让我们通过一个具体的例子来解释这些概念。假设你正在研究某个城市中餐馆的日客流量，你想了解不同因素如何影响客流量。你收集了数据，包括餐馆的位置（城市中心或郊区）、餐馆的大小（小型、中型或大型）以及是否提供外卖服务。你的泊松回归模型可能如下所示： $$\\log(\\mu) = \\beta_0 + \\beta_1 \\times \\text{城市中心} + \\beta_2 \\times \\text{餐馆大小}","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:2:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.3 Marginal effects on E(y | x) Marginal effects indicate the change in the rate for a given change in one independent variable, holding all other variables constant. As with the models in earlier chapters, we can compute the marginal change that indicates the rate of change in $ E(y | x) $ for an infinitely small change in $ x_k $ or a discrete change that indicates the amount of change in $ E(y | x) $ for a discrete change in $ x_k $. For the PRM, the marginal change in $ E(y | x) $ = $\\mu$ equals $$\\frac{\\partial E\\left(y\\mid\\mathbf{x}\\right)}{\\partial x_{k}}=E\\left(y\\mid\\mathbf{x}\\right)\\beta_{k}$$ The marginal change depends on both $\\beta_k$ and $E(y|x)$. For $\\beta_k \u003e 0$, the larger the current value of $E(y | x)$, the larger the rate of change; for $\\beta_k \u003c 0$, the smaller the rate of change.Because $E(y | x)$ depends on the levels of all variables in the model, the size of the marginal change also depends on the levels of all variables. The marginal change can be computed at the mean or at other representative values. Alternatively, the average marginal change over all the observations in the sample can be computed. A discrete change is the change in the rate as $ x_k $ changes from $ x_k^{\\text{start}} $ to $ x_k^{\\text{end}} $, while holding other variables constant: $$\\frac{\\Delta E\\left(y\\mid\\mathbf{x}\\right)}{\\Delta x_k\\left(x_k^{\\mathrm{start}}\\to x_k^{\\mathrm{end}}\\right)}=E\\left(y\\mid\\mathbf{x},x_k=x_k^{\\mathrm{end}}\\right)-E\\left(y=1\\mid\\mathbf{x},x_k=x_k^{\\mathrm{start}}\\right)$$ Different am ounts of change can be computed depending on your purpose: The effect of a binary variable $ x_k $ is computed by letting $ x_k $ change from 0 to 1. The effect of an uncentered change of 1 in $ x_k $ is computed by changing from the observed $ x_{ik} $ to $ x_{ik} + 1 $. A centered change is computed by changing from $ x_{ik} - \\left(1/2\\right) $ to $ x_{ik} + \\left(1/2\\right) $. Change can also be computed from other values of $ x_k $, such as the mean. Then, the uncentered unit change in $ x_k $ is from $\\overline{x}_k$ to $\\overline{x}_k + 1$ The centered discrete change is the result of the change from $\\overline{x}_k-\\left(1/2\\right)$ to $\\overline{x}_k+\\left(1/2\\right)$ The effect of an uncentered change of $ \\delta $, where $ \\delta $ might be the standard deviation of $ x_k $, is computed by changing from $ x_{ik} $ to $ x_{ik} + \\delta $. The centered change is computed by changing from $ x_{ik} - \\left(\\frac{\\delta}{2}\\right) $ to $ x_{ik} + \\left(\\frac{\\delta}{2}\\right) $. Change can also be computed from values of $ x_k $ other than the observed value, such as the mean. Then, the uncentered change of $\\delta$ in $ x_k $ is from $ \\overline{x}_k $ to $ \\overline{x}_k + \\delta $ and the centered change is from $ \\overline{x}_k - \\left(\\frac{\\delta}{2}\\right) $ to $ \\overline{x}_k + \\left(\\frac{\\delta}{2}\\right) $. Changes of a standard deviation or some other value may be particularly useful when the scale of $ x_k $ is very large or very small. For example, when the independent variable is a proportion, a unit change would be at least as large as the entire range of the variable. When the range of $ x_k $ is large, the effect of a unit change can be quite small. The total possible effect of $ x_k $ is found by letting $ x_k $ change from its minimum to its maximum. Trimmed ranges can also be used and may be particularly useful when the distribution of $ x_k $ is highly skewed or contains outliers. 在统计学中，特别是在回归分析中，边际效应是指一个自变量（解释变量）发生单位变化时，因变量（响应变量）期望值的平均变化量。在泊松回归模型中，我们关注的是计数数据，比如一天内发生的事件次数。 边际效应的连续变化 在泊松回归模型中，期望计数 $\\mu$ 的表达式是： $$\\mu=E\\left(y\\mid\\mathbf{x}\\right)=\\exp\\left(\\mathbf{x}\\beta\\right)$$ 这里的 $\\mathbf{x}$ 是自变量的向量，$\\beta$ 是相应的参数向量。如果我们想知道 $\\mathbf{x}$ 中某个特定变量 $x_k$ 的变化对 $\\mu$ 的影响，我们可以计算 $\\mu$ 对 $x_k$ 的导数，这就是边际效应： $$\\frac{\\partial E\\left(y\\mid\\mathbf{x}\\right)}{\\partial x_{k}}=E\\left(y\\mid\\mathbf{x}\\right)\\beta_{k}$$ 这个公式的意思是，当 $x_k$ 发生微小变化时，$\\mu$ 的变化量是 $E(y|\\mathbf{x})$ 乘以 $\\beta_k$。这里的 $\\beta_k$ 是","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:2:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.4 Interpretation using predicted probabilities The estimated parameters can also be used to compute predicted probabilities with the formula $$\\widehat{\\Pr}(y=k\\mid\\mathbf{x})=\\frac{e^{-\\mathbf{x}\\widehat{\\beta}}\\left(\\mathbf{x}\\widehat{\\beta}\\right)^k}{k!}$$ Predictions at the observed values for all observations can be made using predict. Probabilities at specified values or average predicted probabilities can be computed using margins or mtable. Changes in the probabilities can be computed with mchange, and plots can be made using mgen. Predicted probabilities using mtable and mchange mtable, at(married=(0 1) female=1 kid5=0) atmeans pr(0/5) width(7) Expression: Pr(art), predict(pr())\r| married 0 1 2 3 4 5\r----------+--------------------------------------------------------------\r1 | 0 0.244 0.344 0.243 0.114 0.040 0.011\r2 | 1 0.193 0.317 0.261 0.143 0.059 0.019\rSpecified values of covariates\r| female kid5 phd mentor\r----------+-----------------------------------\rCurrent | 1 0 3.1 8.77\rRow 1 has predictions for those who are not married (that is, married=0), while row 2 has predictions for those who are married. Individuals who are not married have higher probabilities for zero and one publications, while married women have higher probabilities for two or more publications. The values of variables that are held constant are shown below the predictions. We can use mchange to compute and test differences between women who are single and those who are married, using the option stat(from to change p) to request the starting prediction, the ending prediction, the change, and the p-value for the test that the effect is 0: mchange married, at(female=1 kid5=0) atmeans pr(0/5) /// stat(from to change p) width(7) brief poisson: Changes in Pr(y) | Number of obs = 915\rExpression: Pr(art), predict(pr())\r| 0 1 2 3 4 5 -------------------+-----------------------------------------------------\rmarried | From | 0.244 0.344 0.243 0.114 0.040 0.011 To | 0.193 0.317 0.261 0.143 0.059 0.019 Married vs Single | -0.051 -0.027 0.019 0.029 0.019 0.008 p-value | 0.011 0.014 0.014 0.011 0.013 0.017 The rows From and To correspond exactly to the predictions for unmarried and married women in the earlier mtable output, while the row Married vs Single contains the discrete changes. The results show that the differences in the predicted probabilities are all statistically significant at the 0.05 level but not the 0.01 level. In the prior example, we specified the values of all the independent variables and examined how predictions differed when the variable married was changed from 0 to 1. The result was a discrete change at a representative value. In our next example, we consider female scientists who are married, and we compute the average rate of productivity and average predicted probabilities if we assume these women all have no children, all have one child, all have two children, and so on. Because we are focusing on married women, we average the predictions over the subsample of married women rather than the full estimation sample. We do this by adding the condition if married==1 \u0026 female==1 to mtable. We begin by computing average rates assuming different numbers of children: quietly mtable if married==1 \u0026 female==1, at(kid5=(0/3)) long By default, mtable uses the wide format when reporting rates. Here we use the long option so that we can combine the rates with predicted probabilities that are by default shown in long format. The norownumbers option suppresses row numbers for the predictions. mtable if married==1 \u0026 female==1, pr(0/5) /// at(kid5=(0/3)) atvars(_none) right norownumbers brief Expression: Pr(art), predict(pr())\rkid5 mu 0 1 2 3 4 5\r------------------------------------------------------------------------------\r0 1.656 0.205 0.315 0.249 0.136 0.059 0.022\r1 1.376 0.266 0.343 0.227 0.105 0.039 0.013\r2 1.144 0.331 0.357 0.199 0.078 0.025 0.007\r3 0.951 0.398 0.358 0.168 0.056 0.015 0.004\rThe column mu shows that if all married women were ","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:2:4","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.5 Comparing observed and predicted counts to evaluate model specification Does the PRM fit the data in our example well? In this section, we present an informal way to evaluate the fit of the PRM by comparing the predicted distribution of counts with the observed distribution, extending the ideas from section 9.1.2. After fitting our model, we can compute the predicted probabilities for specific counts by using predict. For example, for counts of 0, 1, and 2, we type: poisson art i.female i.married kid5 phd mentor, nolog predict prob0, pr(0) predict prob1, pr(1) predict prob2, pr(2) sum prob0 prob1 prob2 Variable | Obs Mean Std. dev. Min Max\r-------------+---------------------------------------------------------\rprob0 | 915 .2092071 .0794247 .0000659 .4113403\rprob1 | 915 .3098447 .0634931 .0006345 .3678775\rprob2 | 915 .242096 .0311473 .0030544 .2706704\rThe means are the average predicted probabilities defined as $$\\overline{\\mathrm{Pr}}(y=k)=\\frac{1}{N}\\sum_{i=1}^{N}\\widehat{\\mathrm{Pr}}(y_{i}=k\\mid\\mathbf{x}_{i})$$ The average predictions from predict are identical to the average predictions computed by mtable: mtable, pr(0/2) brief Expression: Pr(art), predict(pr())\r0 1 2\r----------------------------\r0.209 0.310 0.242\rThe mgen, meanpred command computes the same average predicted probabilities but saves the predictions in variables that can be graphed: mgen, stub(PR) pr(0/9) meanpred Predictions from: Variable Obs Unique Mean Min Max Label\r----------------------------------------------------------------------------------------\rPRval 10 10 4.5 0 9 Articles in last 3 yrs of PhD\rPRobeq 10 10 .0993443 .0010929 .3005464 Observed proportion\rPRoble 10 10 .8328962 .3005464 .9934426 Observed cum. proportion\rPRpreq 10 10 .0998819 .0009304 .3098447 Avg predicted Pr(y=#)\rPRprle 10 10 .8308733 .2092071 .9988187 Avg predicted cum. Pr(y=#)\rPRob_pr 10 10 -.0005376 -.0475604 .0913393 Observed - Avg Pr(y=#)\r----------------------------------------------------------------------------------------\rlist PRval PRpreq PRobeq in 1/3, clean PRval PRpreq PRobeq 1. 0 .2092071 .30054645 2. 1 .30984468 .26885246 3. 2 .24209596 .19453552 The listed values of PRpreq match those from mtable, and the values of PRobeq are the observed proportions for each value of art. To show how mgen, meanpred is used to compare predictions from different models, we begin by fitting a model with no independent variables, which is simply fitting the Poisson PDF: poisson art, nolog Poisson regression Number of obs = 915\rLR chi2(0) = 0.00\rProb \u003e chi2 = .\rLog likelihood = -1742.5735 Pseudo R2 = 0.0000\r------------------------------------------------------------------------------\rart | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_cons | 0.53 0.03 20.72 0.000 0.48 0.58\r------------------------------------------------------------------------------\rNext, we compute predictions for counts 0-9: mgen, stub(PDF) pr(0/9) meanpred Variable Obs Unique Mean Min Max Label\r--------------------------------------------------------------------------------------\rPDFval 10 10 4.5 0 9 Articles in last 3 yrs of PhD\rPDFobeq 10 10 .0993443 .0010929 .3005464 Observed proportion\rPDFoble 10 10 .8328962 .3005464 .9934426 Observed cum. proportion\rPDFpreq 10 10 .0999988 .0000579 .311469 Avg predicted Pr(y=#)\rPDFprle 10 10 .8307106 .1839859 .9999884 Avg predicted cum. Pr(y=#)\rPDFob_pr 10 10 -.0006546 -.0691068 .1165606 Observed - Avg Pr(y=#)\r--------------------------------------------------------------------------------------\rBecause we specified the stub PDF, mgen created a new variable called PDFpreq with the average predicted probabilities for counts 0-9 from a univariate Poisson distribution. Variable PDFobeq contains the corresponding observed probability, while PDFval contains the values of the count itself (for example, 1 for the row that contains information about the observed and predicted counts of y = 1). Nex","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:2:5","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.6 (Advanced) Exposure time This section is marked as advanced because it may be more useful to read if you are working with an application in which observations vary in terms of exposure time—that is, how long they have been at risk of the event being counted. If you have previous experience with survival models or event history models, this section may also help you better understand how modeling event counts outcomes is related to modeling whether events have happened. So far, we have implicitly assumed that each observation was at risk of an event occurring for the same amount of time. For our example, for each person in the sample, we counted their articles over a 3-year period. Often, when collecting data, however, different observations have different exposure times. For example, the sample of scientists might have received their degrees in different years, and our outcome could have been the total publications from the PhD to the date of the survey. The amount of time in the career would clearly affect the number of publications, and scientists vary in the length of their careers. The same issue arises if we use data in which the counts are collected from regions that have different sizes or populations. For example, if the outcome variable was the number of homicides reported in a city, the counts would be affected by the population size of the city. The methods we explain in this section could be applied in the same way. To illustrate how to adjust for exposure time and the problems that occur if you do not make the appropriate adjustment, we have artificially constructed a variable named profage measuring a scientist’s professional age. This is the amount of time that a scientist has been “exposed” to the possibility of publishing. The variable totalarts is the total number of articles during the scientist’s career. Fitting a PRM, we obtain the following results: use couexposure4, clear poisson totalarts kid5 mentor, nolog irr Poisson regression Number of obs = 915\rLR chi2(2) = 277.18\rProb \u003e chi2 = 0.0000\rLog likelihood = -2551.3379 Pseudo R2 = 0.0515\r------------------------------------------------------------------------------\rtotalarts | IRR Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rkid5 | 1.16 0.03 6.42 0.000 1.11 1.22\rmentor | 1.02 0.00 16.65 0.000 1.02 1.03\r_cons | 2.13 0.06 26.33 0.000 2.02 2.26\r------------------------------------------------------------------------------\rNote: _cons estimates baseline incidence rate.\rAs expected, the mentor’s productivity has a strong and significant effect on the student’s productivity. Surprisingly, having young children increases scientific productivity; most research in this area finds that having young children decreases productivity. One paper, however, found that having young children increases productivity. This result was an artifact from a sample where scientists with more children were older and the dependent variable was the total number of publications. Essentially, the number of children was a proxy for professional age, which has a positive effect on total publications. The same is true in our simulated data. Now, let’s consider how to adjust for the artifact. 导师的指导质量、研究水平等对学生的学术成就有重要影响。 然而，更令人惊讶的发现是有年幼孩子的科学家其生产力更高。这与大多数研究结果相悖，后者认为有年幼孩子会分散科学家的精力，从而降低其生产力。但有一篇论文却发现有年幼孩子实际上能增加科学家的生产力。这一看似矛盾的结果实际上是由于研究所选取的样本和数据导致的：样本中拥有更多孩子的科学家年龄较大，而论文总数作为因变量，实际上与科学家的专业年龄正相关。也就是说，随着科学家专业年龄的增长，其论文总数也会增加，因此孩子的数量成为了专业年龄的代理变量。 同样的情况也出现在模拟数据中。这说明在进行数据分析时，需要考虑到各种可能的因素，以避免出现误导性的结果。因此，作者提出需要考虑如何调整这一人为现象，以获得更准确的研究结果。 听明白了，请看下文： Exposure time can be easily incorporated into count models. Let $t_i$ be the amount of time that observation $i$ is at risk. If $\\mu_i$ is the expected number of observations for one unit of time for case $i$, then $t_i \\cdot \\mu_i$ is the rate over a period of length $t_i$. Assuming only two independent variables for simplicity, our count equation becomes: $$\\mu_{i}t_{i}={\\exp\\left(\\beta","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:2:6","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3 The negative binomial regression model The PRM accounts for observed heterogeneity by stating that the rate $ \\mu_i $ depends on the observed $ x_k $’s. In practice, the PRM often fails due to overdispersion. In other words, the model does not adequately capture the level of variability in the outcome. The negative binomial regression model (NBRM) addresses this issue by introducing the parameter $ \\alpha $, which represents unobserved heterogeneity among observations3. For instance, when considering three independent variables, the PRM is $$\\mu_{i}=\\exp\\left(\\beta_{0}+\\beta_{1}x_{i1}+\\beta_{2}x_{i2}+\\beta_{3}x_{i3}\\right)$$ 3.The NBRM can also be derived through a process of contagion where the occurrence of an event changes the probability of further events—an approach not considered further here. The NBRM adds the error $\\varepsilon $ that is assumed to be uncorrelated with the $ x’s $. $$\\widetilde\\mu_{i}=\\exp\\left(\\beta_{0}+\\beta_{1}x_{i1}+\\beta_{2}x_{i2}+\\beta_{3}x_{i3}+\\varepsilon_{i}\\right)$$ With basic algebra and defining $\\delta$ as $ \\exp(\\varepsilon) $, the model becomes \\begin{align*} \\quad \\quad\\widetilde\\mu_{i}=\\exp\\left(\\beta_{0}+\\beta_{1}x_{i1}+\\beta_{2}x_{i2}+\\beta_{3}x_{i3}\\right)\\exp\\left(\\varepsilon_{i}\\right)\\ \\end{align*} \\begin{align*} =\\exp\\left(\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\beta_3x_{i3}\\right)\\delta_i \\end{align*} To identify the model, we assume that $E\\left(\\delta\\right)=1$,which corresponds to the assumption $E\\left(\\varepsilon\\right)=0$ in the linear regression model. With this assumption, it follows that $E\\left(\\widetilde{\\mu}\\right)=\\mu E\\left(\\delta\\right)=\\mu.$ Thus the PRM and the NBRM have the same mean structure. Accordingly, if the assumptions of the NBRM are correct, the expected rate for a given level of the independent variables will be the same in both models. However, the standard errors in the PRM are biased downward, resulting in spuriously large $ z-values $and spuriously small $p-values$ (Cameron and Trivedi 2013)4. As we discuss in section 9.3.5, robust standard errors would be consistent and would produce p-values that yield nominal coverage, as discussed by Cameron and Trivedi (2013). Instead of following the robust approach, we focus on using a more efficient estimator whose standard errors are consistently estimated. To better understand the link between the PRM and the NBRM, as well as their differences, suppose that the error term $ \\varepsilon $ in (9.3) is an observed variable with a regression coefficient constrained to equal 1: $$\\widetilde\\mu_{i}=\\exp\\left(\\beta_{0}+\\beta_{1}x_{i1}+\\beta_{2}x_{i2}+\\beta_{3}x_{i3}+1\\varepsilon_{i}\\right)$$ The distribution of observations conditional on the values of both the $ x_i $ and the $ \\varepsilon_i $ has a Poisson distribution, just as it did for the PRM. Accordingly $$\\mathrm{Pr}(y_{i}\\mid\\mathbf{x_i},\\varepsilon_{i})=\\frac{e^{-\\widetilde\\mu_{i}}\\widetilde\\mu_{i}^{y_{i}}}{y_{i}!}$$ However, because e is unknown, we cannot compute $\\operatorname{Pr}\\left(y\\mid\\mathbf{x},\\varepsilon\\right)$ This limitation is resolved by assuming that exp$(\\varepsilon)$ has a gamma distribution (see Long [1997, 231-232] or Cameron and Trivedi [2013, 80-89]). The probability of $ y $ conditional on $ x $, but not conditional on $ \\varepsilon $, is computed as a weighted combination of $ \\text{Pr} (y | x , \\varepsilon) $ for all values of $ \\varepsilon $, where the weights are determined by $ \\text{Pr}(\\exp(\\varepsilon)) $ from the gamma distribution. The mathematics for this mixing are complex and not particularly helpful for understanding the interpretation of the model, leading to the negative binomial distribution for $ y $ given $ x $. $$\\Pr\\left(y\\mid\\mathbf{x}\\right)=\\frac{\\Gamma(y+\\alpha^{-1})}{y!\\Gamma(\\alpha^{-1})}\\left(\\frac{\\alpha^{-1}}{\\alpha^{-1}+\\mu}\\right)^{\\alpha-1}\\left(\\frac{\\mu}{\\alpha^{-1}+\\mu}\\right)^{y}$$ where $\\Gamma(\\cdot)$ The gamma function is the gamma function. The parameter $ a $ determines the degree of d","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:3:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3.1 Estimation using nbreg The NBRM is fit with the following command nbreg depvar[indepvars] [if] [in] [weight] [, noconstant dispersion([mean | constant]) exposure(vamame) vce(vcetype) irr] Most options are the same as those for poisson with the notable exception of the option dispersion(), which is discussed in the next section. Because of differences in how poisson and nbreg are implemented in Stata, models fit with nbreg take longer to converge. NB1 and NB2 variance functions The dispersion() option specifies the function for the variance of $y$ given $x$, referred to as the dispersion function. To understand what this means, consider the dispersion function from the PRM. In that model, $$\\mathrm{Var}\\left(y_i\\mid\\mathbf{x}_i\\right)=E\\left(y_i\\mid\\mathbf{x}_i\\right)=\\mu_i$$ which is referred to as equidispersion. In real-world data, counts are usually overdispersed, meaning that the conditional variance is larger than the conditional mean. The NBRM addresses this problem by allowing overdispersion through the $\\alpha$ parameter.Cameron and Trivedi (2013) show that a variety of variance functions are possible. The most commonly used function, the default in Stata, is $$\\mathrm{Var}\\left(y_i\\mid\\mathbf{x}\\right)=\\mu_i+\\alpha\\mu_i^2$$ which Cameron and Trivedi refer to as the NB2 model, in reference to the squared term $\\mu^2$. The dispersion(constant) option specifies the NB1 model in which $$\\mathrm{Var}\\left(y_i\\mid\\mathbf{x}\\right)=\\mu_i+\\alpha\\mu_i$$ NBin refers to the power of 1 in the $\\alpha \\mu$ term. Because the NB2 model is used most often in applied research, we use this form of the model throughout the book. 在 NBRM 中，我们使用一个额外的参数 $\\alpha$ 来处理过度分散问题。过度分散是指数据的真实方差大于模型预测的方差，这会导致泊松回归模型（PRM）中的标准误差估计偏低，从而导致 $ p $ 值偏小，这在统计学中称为过度分散。 NBRM 通过引入参数 $\\alpha$ 来解决这个问题，这个参数代表观测之间未观察到的异质性。在 NBRM 中，我们假设 $\\exp(\\varepsilon)$ 服从伽马分布。这样，我们可以计算出条件概率 $\\Pr\\left(y\\mid\\mathbf{x}\\right)$，它遵循负二项分布。 负二项分布的公式是： $$\\Pr\\left(y\\mid\\mathbf{x}\\right)=\\frac{\\Gamma(y+\\alpha^{-1})}{y!\\Gamma(\\alpha^{-1})}\\left(\\frac{\\alpha^{-1}}{\\alpha^{-1}+\\mu}\\right)^{\\alpha-1}\\left(\\frac{\\mu}{\\alpha^{-1}+\\mu}\\right)^{y}$$ 其中，$\\Gamma(\\cdot)$ 是伽马函数，$\\mu$ 是期望事件发生次数，$\\alpha$ 是模型参数，它决定了预测的分散程度。 现在，让我们回到提到的 dispersion() 选项。这个选项指定的是 $y$ 给定 $x$ 的方差函数，也就是我们之前提到的负二项分布中的 $\\alpha$ 参数。 在泊松回归模型（PRM）中，假设数据的方差等于其期望值，即 $\\mathrm{Var}\\left(y_i\\mid\\mathbf{x}_i\\right)=E\\left(y_i\\mid\\mathbf{x}_i\\right)=\\mu_i$，这被称为等方差性。 但在实际数据中，事件发生的次数通常表现出过度分散，这意味着条件方差大于条件均值。 为了处理过度分散，NBRM 允许使用不同的方差函数。Cameron 和 Trivedi (2013) 展示了多种可能的方差函数。最常用的函数，也是 Stata 的默认设置，是： $$\\mathrm{Var}\\left(y_i\\mid\\mathbf{x}\\right)=\\mu_i+\\alpha\\mu_i^2$$ 这个函数被称为 NB2 模型，因为它包含了 $\\mu_i^2$ 的平方项。NB2 模型在实际研究中使用得最为频繁。 另一种可能的方差函数是： $$\\mathrm{Var}\\left(y_i\\mid\\mathbf{x}\\right)=\\mu_i+\\alpha\\mu_i$$ 这个函数被称为 NB1 模型。NBin 指的是 $\\alpha \\mu$ 项的指数。 让我们分别举例子来解释 dispersion() 选项中的两种模型：NB1 和 NB2。 NB1 模型 NB1 模型是 dispersion(constant) 选项指定的模型，其方差函数为： $$\\mathrm{Var}\\left(y_i\\mid\\mathbf{x}\\right)=\\mu_i+\\alpha\\mu_i$$ 这个模型假设条件方差是条件均值的一个常数倍。让我们通过一个例子来解释这个模型。 假设我们研究的是一个城市中每天发生的交通事故次数。我们的数据包括一天中的时间、天气情况和是否是周末。如果我们假设 $\\alpha=1$，那么方差函数变为： $$\\mathrm{Var}\\left(y_i\\mid\\mathbf{x}\\right)=\\mu_i+1\\mu_i$$ 这意味着无论时间、天气或周末如何变化，每天发生的交通事故次数的方差始终是均值的1倍。这显然是一个简化的模型，它忽略了实际数据中可能存在的过度分散。 NB2 模型 NB2 模型是最常用的模型，其方差函数为： $$\\mathrm{Var}\\left(y_i\\mid\\mathbf{x}\\right)=\\mu_i+\\alpha\\mu_i^2$$ 这个模型假设条件方差是条件均值的线性函数。让我们通过一个例子来解释这个模型。 假设我们研究的是一个城市中每天发生的交通事故次数。我们的数据包括一天中的时间、天气情况和是否是周末。如果我们假设 $\\alpha=1$，那么方差函数变为： $$\\mathrm{Var}\\left(y_i\\mid\\mathbf{x}\\right)=\\mu_i+1\\mu_i^2$$ 这意味着如果每天发生的交通事故次数增加，其方差也会增加，但增加的幅度会小于均值的增加幅度。这更符合实际情况，因为我们通常观察到数据中的过度分散。 通过这两个例子，我们可以看到，NB1 模型假设条件方差是条件均值的一个常数倍，而 NB2 模型则假设条件方差是条件均值的线性函数。在实际应用中，NB2 模型更常用于处理过度分散的数据。 ","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:3:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3.2 Example of NBRM Here we use the same example as for the PRM above: use couart4, clear nbreg art i.female i.married kid5 phd mentor Fitting Poisson model:\rIteration 0: Log likelihood = -1651.4574 Iteration 1: Log likelihood = -1651.0567 Iteration 2: Log likelihood = -1651.0563 Iteration 3: Log likelihood = -1651.0563 Fitting constant-only model:\rIteration 0: Log likelihood = -1625.4242 Iteration 1: Log likelihood = -1609.9746 Iteration 2: Log likelihood = -1609.9368 Iteration 3: Log likelihood = -1609.9367 Fitting full model:\rIteration 0: Log likelihood = -1565.6652 Iteration 1: Log likelihood = -1561.0095 Iteration 2: Log likelihood = -1560.9583 Iteration 3: Log likelihood = -1560.9583 Negative binomial regression Number of obs = 915\rLR chi2(5) = 97.96\rDispersion: mean Prob \u003e chi2 = 0.0000\rLog likelihood = -1560.9583 Pseudo R2 = 0.0304\r------------------------------------------------------------------------------\rart | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rfemale |\rFemale | -0.22 0.07 -2.98 0.003 -0.36 -0.07\r|\rmarried |\rMarried | 0.15 0.08 1.83 0.067 -0.01 0.31\rkid5 | -0.18 0.05 -3.32 0.001 -0.28 -0.07\rphd | 0.02 0.04 0.42 0.672 -0.06 0.09\rmentor | 0.03 0.00 8.38 0.000 0.02 0.04\r_cons | 0.26 0.14 1.85 0.065 -0.02 0.53\r-------------+----------------------------------------------------------------\r/lnalpha | -0.82 0.12 -1.05 -0.58\r-------------+----------------------------------------------------------------\ralpha | 0.44 0.05 0.35 0.56\r------------------------------------------------------------------------------\rLR test of alpha=0: chibar2(01) = 180.20 Prob \u003e= chibar2 = 0.000\rThe output is similar to that of poisson except for the results at the bottom of the output, which initially can be confusing. Although the model is defined in terms of the parameter $\\alpha$, nbreg estimates $\\ln(\\alpha)$, which forces the estimate of $a$ to be positive as required for the gamma distribution.The estimate $\\widehat{\\ln(\\alpha)}$ is reported as /lnalpha, with the value of $a$ shown on the next line. Test statistics are not given for ln($\\alpha$) or $\\alpha$ because they require special treatment, as discussed next. 这个输出与泊松回归模型的输出相似，除了输出底部的一些结果，这些结果最初可能会让人感到困惑。尽管模型是通过参数 $\\alpha$ 来定义的，但是 nbreg 估计的是 $\\ln(\\alpha)$，这使得 $\\alpha$ 的估计值必须为正，这是伽马分布所要求的。估计值 $\\widehat{\\ln(\\alpha)}$ 被报告为 /lnalpha，而 $\\alpha$ 的值则在下一行显示。对于 $\\ln(\\alpha)$ 或 $\\alpha$ 的检验统计量没有给出，因为它们需要特殊的处理，如接下来所讨论的。 在负二项回归模型中，$\\alpha$ 是一个重要的参数，它控制着数据的过度分散程度。然而，由于 $\\alpha$ 必须大于零（因为伽马分布的参数必须为正），所以 nbreg 提供了 $\\ln(\\alpha)$ 的估计值。这个估计值 $\\widehat{\\ln(\\alpha)}$ 是模型输出中的一个组成部分，通常以 /lnalpha 表示。然后，$\\alpha$ 的值（即 $\\exp(\\widehat{\\ln(\\alpha)})$）在下一行给出。 由于 $\\ln(\\alpha)$ 和 $\\alpha$ 本身没有直接的统计检验，因此 nbreg 没有提供这些检验的统计量。这些检验需要通过 $\\ln(\\alpha)$ 和 $\\alpha$ 的估计值来进行特殊处理，这通常涉及到对 $\\alpha$ 的似然比检验或相关统计量。这些特殊处理的方法在统计文献中有详细的讨论，但通常超出了初学者的范围。 总的来说，负二项回归模型的输出提供了 $\\ln(\\alpha)$ 的估计值和 $\\alpha$ 的值，但并没有直接提供这些值的统计检验。这些检验需要通过特殊的方法来进行，通常超出了初学者的理解范围。 ","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:3:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3.3 Testing for overdispersion Because the NBRM reduces to the PRM when $\\alpha = 0$, we can test for overdispersion by testing $H_0: \\alpha = 0$. There are two points to remember: The nbreg command estimates $\\ln(\\alpha)$ rather than $a$. A test of $H_0: \\ln(\\alpha) = 0$ corresponds to testing $H_0: \\alpha = 1$, which is not the hypothesis we want to test. Because $\\alpha$ must be greater than or equal to 0, the sampling distribution of $\\hat{\\alpha}$ when $\\alpha = 0$ is only half of a normal distribution because values less than 0 have a probability of 0. This requires an adjustment to the usual significance level of the test, which is done automatically by Stata. Stata provides a likelihood-ratio (LR) test of $H_0: \\alpha = 0$ that is listed after the estimates of the parameters: Likelihood-ratio test of alpha=0: chibar2(01) = 180.20, Prob\u003e=chibar2 = 0.000 The test statistic chibar2(01) is computed as \\begin{align*} \\begin{array}{rcl}G^{2}\u0026=\u00262\\left(\\operatorname{ln}L_{\\mathrm{NBRM}}-\\operatorname{ln}L_{\\mathrm{PRM}}\\right)\\end{array} \\end{align*} \\begin{align*} \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\quad2\\left(-1560.9583–1651.0563\\right)=180.20 \\end{align*} The log likelihood for the PRM is shown in the iteration log for nbreg under the heading Fitting Poisson model, with the log likelihood for the NBRM shown last in the log. The significance level of the test is adjusted for the truncated sampling distribution for $\\alpha$. For details, you can click on the blue linked chibar2(01) in the Results window. In our example, the test provides strong evidence of overdispersion. You can summarize this by saying the following: Because there is significant evidence of overdispersion $(G^2=180.2,p\u003c0.001)$ , the NBRM is preferred over the PRM. As with other LR tests, this test is not available if robust standard errors are used, including when probability weights, survey estimation, or the cluster() option is used. We talk more about robust standard errors shortly. Given that, in our experience, the test usually indicates overdispersion in cases where the LR test is available, we suggest that investigators using robust standard errors begin by fitting the NBRM - simply assume there is overdispersion. 首先，我们需要理解负二项回归模型（NBRM）和泊松回归模型（PRM）之间的关系。当 $\\alpha = 0$ 时，NBRM 简化为 PRM。这意味着如果数据不是过度分散的，即数据的真实方差等于模型的预测方差，那么 NBRM 就会退化为 PRM。 为了测试数据是否过度分散，我们可以测试 $H_0: \\alpha = 0$。如果数据是过度分散的，那么 $H_0$ 就不会成立，我们需要一个更大的 $\\alpha$ 值来拟合数据的过度分散特性。 估计 $\\ln(\\alpha)$ 而不是 $\\alpha$：在 Stata 中的 nbreg 命令中，我们估计的是 $\\ln(\\alpha)$ 而不是 $\\alpha$。这是因为直接估计 $\\alpha$ 会导致估计值可能为负，而伽马分布的参数必须为正。因此，我们通过估计 $\\ln(\\alpha)$ 来确保估计值是正的。当我们说 “$\\ln(\\alpha) = 0$” 时，我们实际上是在说 “$\\alpha = 1$\"。但这并不是我们想要测试的假设，因为我们真正想要知道的是 $\\alpha = 0$ 是否成立。 调整显著性水平：由于 $\\alpha$ 必须大于或等于 0，当 $\\alpha = 0$ 时，$\\hat{\\alpha}$ 的抽样分布只有正态分布的一半。这意味着在 $\\alpha = 0$ 时，$\\hat{\\alpha}$ 变小的概率只有 50%。因此，我们需要调整显著性水平，以适应这种不对称的分布。Stata 会自动完成这个调整。 在实际应用中，Stata 提供了对 $\\alpha = 0$ 的似然比检验，称为 chibar2(01)。如果这个检验的 p 值小于 0.05，我们就可以说模型存在过度分散，并选择负二项回归模型而不是泊松回归模型。 ","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:3:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3.4 Comparing the PRM and NBRM using estimates table To understand how the PRM and NBRM differ, it is useful to compare the estimates from poisson and nbreg side by side: qui poisson art i.female i.married kid5 phd mentor estimates store PRM qui nbreg art i.female i.married kid5 phd mentor estimates store NBRM estimates table PRM NBRM, b(%9.3f) t p(%9.3f) varlabel /// drop(lnalpha) stats(alpha N) eform vsquish --------------------------------------------------\rVariable | PRM NBRM -------------------------+------------------------\rGender: 1=female 0=male | Female | 0.799 0.805 | -4.11 -2.98 | 0.000 0.003 Married: 1=yes 0=no | Married | 1.168 1.162 | 2.53 1.83 | 0.011 0.067 # of kids \u003c 6 | 0.831 0.838 | -4.61 -3.32 | 0.000 0.001 PhD prestige | 1.013 1.015 | 0.49 0.42 | 0.627 0.672 Mentor's # of articles | 1.026 1.030 | 12.73 8.38 | 0.000 0.000 Constant | 1.356 1.292 | 2.96 1.85 | 0.003 0.065 -------------------------+------------------------\ralpha | 0.442 N | 915 915 --------------------------------------------------\rLegend: b/t/p\rThe estimated parameters from the PRM and the NBRM are close, but the $z$-values for the NBRM are consistently smaller than those for the PRM. This is the expected consequence of overdispersion. If there is overdispersion, estimates from the PRM are inefficient, with standard errors that are biased downward (meaning that the $z$-values are inflated), even if the model includes the correct variables. As a consequence, if the PRM is used when there is overdispersion, the risk is that a variable will mistakenly be considered significant when it is not, as is the case for “married” in our example. Accordingly, it is important to test for overdispersion before using the PRM. ","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:3:4","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3.5 Robust standard errors In the presence of overdispersion, the standard errors in the PRM are downwardly biased. Accordingly, Cameron and Trivedi (2013, 85) recommend that robust standard errors be used—not only with the PRM but also with the NBRM—in case the variance specification of the model is misspecified. To illustrate how robust standard errors can affect the statistical significance of regression coefficients, the following table compares estimates from the PRM and NBRM both with and without using robust standard errors: quietly poisson art i.female i.married kid5 phd mentor estimates store PRM quietly poisson art i.female i.married kid5 phd mentor, vce(robust) estimates store PRMrobust quietly nbreg art i.female i.married kid5 phd mentor estimates store NBRM quietly nbreg art i.female i.married kid5 phd mentor, vce(robust) estimates store NBRMrobust estimates table PRM PRMrobust NBRM NBRMrobust, b(%9.3f) se(%9.4f) p(%9.3f) /// varlabel drop(lnalpha) stats(alpha N) eform vsquish modelwidth(10) ------------------------------------------------------------------------------\rVariable | PRM PRMrobust NBRM NBRMrobust -------------------------+----------------------------------------------------\rGender: 1=female 0=male | Female | 0.799 0.799 0.805 0.805 | 0.0436 0.0573 0.0585 0.0568 | 0.000 0.002 0.003 0.002 Married: 1=yes 0=no | Married | 1.168 1.168 1.162 1.162 | 0.0717 0.0957 0.0954 0.0936 | 0.011 0.058 0.067 0.062 # of kids \u003c 6 | 0.831 0.831 0.838 0.838 | 0.0334 0.0465 0.0445 0.0445 | 0.000 0.001 0.001 0.001 PhD prestige | 1.013 1.013 1.015 1.015 | 0.0267 0.0425 0.0366 0.0381 | 0.627 0.760 0.672 0.684 Mentor's # of articles | 1.026 1.026 1.030 1.030 | 0.0021 0.0039 0.0036 0.0040 | 0.000 0.000 0.000 0.000 Constant | 1.356 1.356 1.292 1.292 | 0.1397 0.1988 0.1790 0.1812 | 0.003 0.038 0.065 0.068 -------------------------+----------------------------------------------------\ralpha | 0.442 0.442 N | 915 915 915 915 ------------------------------------------------------------------------------\rLegend: b/se/p\rThere are several things to note. First, parameter estimates are not affected by using robust standard errors. For example, the coefficient for “female” is 0.799 for both PRM and PRM robust. Second, for the PRM, the robust standard errors are substantially smaller than the nonrobust standard errors. The most noticeable difference is with “married”, where the effect is significant at the 0.01 level when robust standard errors are not used but p = 0.06 with robust standard errors. For the NBRM, the two types of standard errors are similar. Third, the robust standard errors in the PRM are similar to the standard errors in the NBRM, illustrating that robust standard errors for the PRM correct for the downward bias in the nonrobust standard errors. However, even if the coefficients and standard errors for the PRM estimated with robust standard errors and those for the NBRM are similar, predicted probabilities from the two models can be quite different because these probabilities depend on the dispersion parameter for the NBRM. In other words, in the presence of overdispersion, the NBRM is preferred for accurate estimation of predicted probabilities. Cameron and Trivedi (2013, sec. 3.3) show that the NBRM is also robust to distributional misspecification when robust standard errors are used. ","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:3:5","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3.6 Interpretation using E(y | x) Because the mean structure for the NBRM is identical to that for the PRM, the same methods of interpretation with rates can be used. As before, the factor change for an increase of $\\delta$ in $x_k$ equals $$\\frac{E\\left(y\\mid\\mathbf{x},x_k+\\delta\\right)}{E\\left(y\\mid\\mathbf{x},x_k\\right)}=e^{\\beta_k\\delta}$$ and the corresponding percentage change equals $$100\\times\\frac{E\\left(y\\mid\\mathbf{x},x_{k}+\\delta\\right)-E\\left(y\\mid\\mathbf{x},x_{k}\\right)}{E\\left(y\\mid\\mathbf{x},x_{k}\\right)}=100\\times\\left(e^{\\beta_{k}\\delta}-1\\right)$$ 因子变化和百分比变化的概念 在负二项回归模型（NBRM）中，因子变化和百分比变化是用来解释自变量变化对因变量期望值的影响。这些变化可以通过对模型中的参数 $\\beta_k$ 进行计算得出。 因子变化：当自变量 $x_k$ 增加一个单位时，因变量 $y$ 的期望值 $E(y|\\mathbf{x}, x_k)$ 会发生变化。这个变化可以通过因子变化来衡量，即新旧期望值之比。 百分比变化：百分比变化是因子变化的一个百分比表示，它告诉我们自变量变化导致的客流量期望值变化的百分比。 公式分解 现在，让我们分解公式，并用一个例子来解释。 因子变化： $$ \\frac{E\\left(y\\mid\\mathbf{x},x_k+\\delta\\right)}{E\\left(y\\mid\\mathbf{x},x_k\\right)}=e^{\\beta_k\\delta} $$ 这里，$E\\left(y\\mid\\mathbf{x},x_k+\\delta\\right)$ 是新的期望值，$E\\left(y\\mid\\mathbf{x},x_k\\right)$ 是旧的期望值，$\\delta$ 是自变量 $x_k$ 的变化量，$\\beta_k$ 是 $x_k$ 的系数。 例如，假设我们研究的是餐馆的客流量，我们的自变量包括餐馆的位置、菜单价格和促销活动。如果餐馆的位置从市中心移动到郊区，根据我们的模型，客流量会相应地减少。我们可以计算这个变化，即新旧客流量之比，来了解移动对客流量的影响。 百分比变化： $$ 100\\times\\frac{E\\left(y\\mid\\mathbf{x},x_{k}+\\delta\\right)-E\\left(y\\mid\\mathbf{x},x_{k}\\right)}{E\\left(y\\mid\\mathbf{x},x_{k}\\right)}=100\\times\\left(e^{\\beta_{k}\\delta}-1\\right) $$ 这个公式告诉我们，自变量 $x_k$ 的变化导致的客流量期望值变化的百分比。 例如，如果新的客流量是旧的客流量的一半，那么这个百分比变化将是 -50%。 通过这个例子，我们可以看到如何使用因子变化和百分比变化来解释自变量对因变量的影响。这两个概念在实际应用中非常有用，可以帮助我们理解不同因素如何影响我们的结果。希望这个解释能帮助你更好地理解这些概念。 Factor and percentage change coefficients can be obtained using listcoef. For example, the factor change coefficients for female and mentor are nbreg art i.female i.married kid5 phd mentor, nolog listcoef female mentor nbreg (N=915): Factor change in expected count Observed SD: 1.9261\r------------------------------------------------------------------------\r| b z P\u003e|z| e^b e^bStdX SDofX\r-------------+----------------------------------------------------------\rfemale |\rFemale | -0.2164 -2.978 0.003 0.805 0.898 0.499\rmentor | 0.0291 8.381 0.000 1.030 1.318 9.484\r------------------------------------------------------------------------\rand the percentage change coefficients are listcoef female mentor, percent nbreg (N=915): Percentage change in expected count Observed SD: 1.9261\r------------------------------------------------------------------------\r| b z P\u003e|z| % %StdX SDofX\r-------------+----------------------------------------------------------\rfemale |\rFemale | -0.2164 -2.978 0.003 -19.5 -10.2 0.499\rmentor | 0.0291 8.381 0.000 3.0 31.8 9.484\r------------------------------------------------------------------------\rThese coefficients can be interpreted as follows: Being a female scientist decreases the expected number of articles by a factor of 0.805, holding other variables constant. Equivalently, being a female scientist decreases the expected number of articles by 19.5%, holding other variables constant. For every additional article by the mentor, a scientist’s expected rate of productivity increases by 3.0%, holding other variables constant. For a standard deviation increase in the mentor’s productivity, roughly 10 articles, a scientist’s expected productivity increases by 32%, holding other variables constant. As noted earlier, factor and percentage change coefficients for count models are often quite effective for interpretation in contrast to models in previous chapters for which we think factor and percentage changes in odds are not as readily understood. The similarity in magnitude of coefficients in the PRM and the NBRM, as well as the difference in standard errors when there is overdispersion, also affects predictions. To see this, we compute the rate of productivity for women from elite programs who have mentors with different levels of productivity: poisson art i.female i.married kid5 phd mentor, nolog quietly mtable, estname(PRM_mu) ci at(mentor=(0(5)20) female=1 phd=4) /// a","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:3:6","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3.7 Interpretation using predicted probabilities In the presence of overdispersion, predicted probabilities from the NBRM can differ substantially from those for the PRM, even though the predicted rates are similar. The methods used to interpret predicted probabilities, however, are the same with probabilities for the NBRM computed with $$\\Pr\\left(y\\mid\\mathbf{x}\\right)=\\frac{\\Gamma(y+\\alpha^{-1})}{y!\\Gamma(\\alpha^{-1})}\\left(\\frac{\\alpha^{-1}}{\\alpha^{-1}+\\mu}\\right)^{\\alpha^{-1}}\\left(\\frac{\\mu}{\\alpha^{-1}+\\mu}\\right)^{y}$$ where $\\mu=\\exp(\\mathbf{x}\\boldsymbol{\\beta})$ Predicted probabilities for observed values of the independent variables can be computed using predict. Average predicted probabilities or predicted probabilities at specific values can be calculated using mtable, mgen, mchange, or margins. Because there is nothing new in how to use these commands with the NBRM, we provide only a few examples that are designed to illustrate key differences and similarities between the PRM and the NBRM. In this example, we use mtable to compare predicted probabilities for counts 0 to 7, specified with the option pr(0/7), for the two models for a hypothetical case of someone who is average on all characteristics. poisson art i.female i.married kid5 phd mentor, nolog estimates store PRM quietly mtable, rowname(PRM) pr(0/7) atmeans nbreg art i.female i.married kid5 phd mentor, nolog estimates store NBRM mtable, rowname(NBRM) pr(0/7) atmeans below brief decimals(3) width(6) Expression: Pr(art), predict(pr())\r| 0 1 2 3 4 5 6 7\r----------+---------------------------------------------------------------\rPRM | 0.200 0.322 0.259 0.139 0.056 0.018 0.005 0.001\rNBRM | 0.298 0.279 0.189 0.111 0.061 0.031 0.016 0.008\rThe predicted probability of a 0 is 0.20 for the PRM and 0.30 for the NBRM, a substantial difference. This difference is offset by higher probabilities for the PRM for counts 1-3. At 4 and above, the probabilities are larger for the NBRM. The larger probabilities at the higher and lower counts reflect the greater dispersion in the NBRM compared with the PRM. Differences between predictions in the NBRM and the PRM are generally most noticeable for 0s. You can think of it this way. With overdispersion, the variance in the predicted counts increases. Because counts cannot be smaller than 0, the increased variation below the mean leads to predictions “stacking up” at 0. Above the mean, the greater variation affects all values. To highlight the greater probability of a 0 in the NBRM, we plot the probability of 0s as mentor productivity increases from 0 to 50, holding other variables at the mean. Using mgen, we make predictions for the probability of 0 at different levels of the mentor’s productivity for the PRM: poisson art i.female i.married kid5 phd mentor, nolog estimates restore PRM mgen, atmeans at(mentor=(0(5)50)) stub(PRM) pr(0) Predictions from: margins, atmeans at(mentor=(0(5)50)) predict(pr(0))\rVariable Obs Unique Mean Min Max Label\r-------------------------------------------------------------------------------\rPRMpr0 11 11 .110384 .009894 .2760837 pr(y=0) from margins\rPRMll0 11 11 .0954709 .0026402 .2517684 95% lower limit\rPRMul0 11 11 .125297 .0171479 .3003991 95% upper limit\rPRMmentor 11 11 25 0 50 Mentor's # of articles\rPRMCpr0 11 11 .110384 .009894 .2760837 pr(y\u003c=0)\r-------------------------------------------------------------------------------\rSpecified values of covariates\r1. 1. female married kid5 phd ------------------------------------------\r.4601093 .6622951 .495082 3.103109 We make corresponding predictions for the NBRM: nbreg art i.female i.married kid5 phd mentor, nolog estimates restore NBRM mgen, atmeans at(mentor=(0(5)50)) stub(NBRM) pr(0) Predictions from: margins, atmeans at(mentor=(0(5)50)) predict(pr(0))\rVariable Obs Unique Mean Min Max Label\r-----------------------------------------------------------------------------\rNBRMpr0 11 11 .1954706 .0648641 .371642 pr(y=0) from margins\rNBRMll0 11 11 .1629587 ","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:3:7","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4 Models for truncated counts Modeling zeros often poses special problems in count models. One type of problem arises when observations with outcomes equal to 0 are missing from the sample because of the way the data were collected. For example, suppose that we are gathering data to study scientific productivity among chemists but that we do not have a sampling roster of all chemists with PhDs. One solution is to take a sample from those scientists who published at least one article that was listed in Chemical Abstracts, which excludes all chemists who have not published. Other examples of truncation are easy to find. A study of how many times people visit national parks could be based on a survey given to people entering the park. Or, when you fill out a customer satisfaction survey after buying a TV, you might be asked how many TVs are in your household, leading to a dataset in which everyone has at least one TV. Zero-truncated count models are designed for instances like these, where observations with an outcome of 0 exist in the population but have been excluded from the sample. The zero-truncated Poisson model (ZTP) begins with the PRM from section 9.2: $$\\Pr\\left(y_{i}=k\\mid\\mathbf{x}\\right)=\\frac{\\exp\\left(-\\mu_{i}\\right)\\mu_{i}^{y_{i}}}{y_{i}!}$$ where $\\mu_i=\\exp\\left(\\mathbf{x}_i\\boldsymbol{\\beta}\\right)$ For a given the $x_i$,the probability of observing a 0 is $$\\Pr\\left(y_i=0\\mid\\mathbf{x}_i\\right)=\\exp\\left(-\\mu_i\\right)$$ and the probability of a positive (that is, nonzero) count is $$\\Pr\\left(y_{i}\u003e0\\mid\\mathbf{x}{i}\\right)=1-\\exp\\left(-\\mu{i}\\right)$$ Because our counts are truncated at 0, we want to compute the probability for each positive count given that we know the count must be greater than 0 for a case to be observed. By the law of conditional probability, $$\\Pr\\left(A\\mid B\\right)=\\frac{\\Pr\\left(A\\And B\\right)}{\\Pr\\left(B\\right)}$$ Thus the conditional probability of observing a specific value $y = k$ given that we know the count is not 0 is $$\\Pr(y_i=k|y_i\u003e0,\\mathbf{x}_i) = \\frac{\\Pr(y_i=k \\And y_i\u003e0|\\mathbf{x}_i)}{\\Pr(y_i\u003e0|\\mathbf{x}_i)}$$ Given the probability that $y = k$ and $y \u003e 0$ is simply the probability that $y = k$, and substituting $(9.6)$ leads to the conditional probability $$\\Pr\\left(y_{i}=k\\mid y_{i}\u003e0,\\mathbf{x_i}\\right)=\\frac{\\Pr\\left(y_{i}=k\\mid\\mathbf{x_i}\\right)}{1-\\exp\\left(-\\mu_{i}\\right)}$$ This formula increases each unconditional probability by the factor ${1-\\exp\\left(-\\mu\\right)}^{-1}$ forcing the probability mass of the truncated distribution to sum to 1. 好的，我们来一步一步地理解零截断计数模型（Zero-truncated count models），以及它是如何应用于实际情况中的。 首先，我们需要理解什么是截断（truncation）。在统计学中，截断是指数据集中缺失了某些值，通常是因为数据收集的方式导致了某些结果的出现。例如，如果我们在研究化学家的科学生产力，但我们没有所有拥有博士学位的化学家的名单，我们可能会选择从那些在《化学文摘》上至少发表了一篇文章的科学家中抽取样本。这样，那些从未发表过文章的化学家就不会出现在样本中，这就是一个截断的例子。 接下来，我们来看零截断泊松模型（Zero-truncated Poisson model, ZTP）。这个模型是为了处理那些在总体中存在但因为样本收集方式而被排除的零观测值的情况。 我们首先有一个基本的概率公式，它是泊松回归模型（Poisson regression model, PRM）的一部分： $$\\Pr\\left(y_{i}=k\\mid\\mathbf{x}\\right)=\\frac{\\exp\\left(-\\mu_{i}\\right)\\mu_{i}^{y_{i}}}{y_{i}!}$$ 在这个公式中： $\\Pr(y_i = k|\\mathbf{x})$ 表示在给定 $\\mathbf{x}$ 的条件下，观测值 $y_i$ 等于 $k$ 的概率。 $\\mu_i$ 是随机变量 $y_i$ 的期望值，也就是在给定 $\\mathbf{x}$ 的条件下，$y_i$ 的平均值。 $e$ 是自然对数的底数，大约等于 2.71828。 $\\mathbf{x}$ 是解释变量的向量。 $\\boldsymbol{\\beta}$ 是模型参数的向量。 $y_i!$ 是 $y_i$ 的阶乘，即 $y_i$ 乘以所有小于它的正整数的乘积。 接下来，我们来看在给定 $\\mathbf{x}_i$ 的条件下，观测值为 0 的概率： $$\\Pr(y_i=0|\\mathbf{x}_i) = e^{-\\mu_i}$$ 这里的 $e^{-\\mu_i}$ 表示 $\\mu_i$ 取负值后，再求 $e$ 的幂。 然后，我们来看在给定 $\\mathbf{x}_i$ 的条件下，观测值大于 0 的概率： $$\\Pr(y_i\u003e0|\\mathbf{x}_i) = 1 - e^{-\\mu_i}$$ 这个概率是 1 减去观测值为 0 的概率，因为一个计数要么为 0，要么大于 0。 现在，我们想要计算的是，在我们知道观测值已经大于 0 的情况下，观测值为某个正数 $k$ 的条件概率。这就是条件概率的概念： $$\\Pr(A|B) = \\frac{\\Pr(A \\And B)}{\\Pr(B)}$$ 在这里，$A$ 是观测值 $y = k$ 的事件，$B$ 是观测值 $y \u003e 0$ 的事件。根据条件概率的定义，我们可以写出： $$\\Pr(y_i=k|y_i\u003e0,\\mathbf{x}_i) = \\frac{\\Pr(y_i=k \\And y_i\u003e0|\\mathbf{x}_i)}{\\Pr(y_i\u003e0|\\mathbf{x}_i)}$$ 由于 $y = k$ 且 $y \u003e 0$ 的概率就是 $y = k$ 的概率，所以我们可以将 $\\Pr(y_{i}=k|\\mathbf{x}_i)$ 直接代入，得到： $$\\Pr\\left(y_{i}=k\\mid y_{i}\u003e0,\\mathbf{x_i}\\right)=\\frac{\\P","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:4:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4.1 Estimation using tpoisson and tnbreg The ZTP is fit with the command tpoisson, and the ZTNB is fit with the command tnbreg. tpoisson depvar [indepvars] [if] [in] [weight] [, options] tnbreg depvar [indepvars] [if] [in] [weight] [, options] where options are largely the same as those for poisson and nbreg. These commands can also be used when the sample is truncated at a value greater than 0. Imagine a study in which the outcome is number of criminal offenses with data that are collected only on people with multiple offenses, so only people with a count of at least two are included in the dataset. A model for this outcome can be fit by adding the ll(#) option to either tpoisson or tnbreg, where # is the value of the largest nonobserved count. In the example where only individuals with a count of at least two are included, this option should be specified as ll(1). The default for both tpoisson and tnbreg is a zero-truncated model (that is, ll(0)), so you do not have to specify this option for zero-truncated models. Example of zero-truncated model To illustrate zero-truncated count models, we continue our example of scientific productivity with the outcome variable artrunc, which artificially recodes values of 0 in art to missing. We begin by truncating the sample to exclude scientists who have no publications. use couart4, clear quietly nbreg art i.female i.married kid5 phd mentor, nolog vce(robust) estimates store NBRM sum art arttrunc drop if art==0 Variable | Obs Mean Std. dev. Min Max\r-------------+---------------------------------------------------------\rart | 915 1.692896 1.926069 0 19\rarttrunc | 640 2.420313 1.882269 1 19\rAfter dropping those 275 observations, we have a truncated sample of 640 scientists. Next, we fit the ZTNB with the truncated sample, where we used the irr option so that the parameters are shown as factor change coefficients: tnbreg arttrunc i.female i.married kid5 phd mentor, nolog irr estimates store ZTNBRM Truncated negative binomial regression Number of obs = 640\rTruncation point = 0 LR chi2(5) = 44.58\rDispersion: mean Prob \u003e chi2 = 0.0000\rLog likelihood = -1027.3185 Pseudo R2 = 0.0212\r------------------------------------------------------------------------------\rarttrunc | IRR Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rfemale |\rFemale | 0.78 0.08 -2.52 0.012 0.65 0.95\r|\rmarried |\rMarried | 1.11 0.12 0.95 0.345 0.89 1.37\rkid5 | 0.86 0.06 -2.12 0.034 0.74 0.99\rphd | 1.00 0.05 -0.06 0.951 0.91 1.10\rmentor | 1.02 0.00 5.54 0.000 1.02 1.03\r_cons | 1.43 0.28 1.80 0.071 0.97 2.10\r-------------+----------------------------------------------------------------\r/lnalpha | -0.60 0.22 -1.04 -0.16\r-------------+----------------------------------------------------------------\ralpha | 0.55 0.12 0.35 0.85\r------------------------------------------------------------------------------\rNote: Estimates are transformed only in the first equation to incidence-rate ratios.\rNote: _cons estimates baseline incidence rate.\rLR test of alpha=0: chibar2(01) = 105.43 Prob \u003e= chibar2 = 0.000\rThe output looks like that from nbreg except that the title now says “Truncated negative binomial regression” and the truncation point is listed. The LR test provides clear evidence of overdispersion, as was the case with the NBRM. Indeed, the ZTNB is estimating the same parameters as the NBRM but with more limited data. The effects of estimating the parameters with the truncated sample can be seen by comparing the estimates from the two models: estimates table NBRM ZTNBRM, b(%9.3f) stats(N) t eform --------------------------------------\rVariable | NBRM ZTNBRM -------------+------------------------\r| art arttrunc\rfemale |\rFemale | 0.805 0.783 | -3.07 -2.52 |\rmarried |\rMarried | 1.162 1.109 | 1.87 0.95 |\rkid5 | 0.838 0.858 | -3.32 -2.12 phd | 1.015 0.997 | 0.41 -0.06 mentor | 1.030 1.024 | 7.49 5.54 _cons | 1.292 1.426 | 1.83 1.80 -------------+----------------------","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:4:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4.2 Interpretation using E(y | x) If counts of zero are missing from your sample because of the way the data were collected, the parameters can be interpreted in the same way as those for the PRM and the NBRM. Essentially, the model fills in the data that were lost when the data were collected. Accordingly, $exp(\\beta_k)$ is the factor increase in the rate for a unit increase in $x_k$, holding other variables constant. Keep in mind that we are referring to the factor change in the unconditional rate $E(y|x)$ not the conditional rate $E(y|y \u003e 0, x)$. As with the NBRM and the PRM, you can use the irr option (as shown in the output above) or use listcoef to compute factor change coefficients after running tpoisson or tnbreg. listcoef female mentor tnbreg (N=640): Factor change in expected count Observed SD: 1.8823\r------------------------------------------------------------------------\r| b z P\u003e|z| e^b e^bStdX SDofX\r-------------+----------------------------------------------------------\rfemale |\rFemale | -0.2447 -2.517 0.012 0.783 0.886 0.497\rmentor | 0.0237 5.538 0.000 1.024 1.278 10.329\r------------------------------------------------------------------------\rHere are some examples of interpreting these results: Being a female scientist decreases the expected number of papers by a factor of 0.78, holding other variables constant. Each additional publication by the mentor increases the predicted number of publications by 2.4%, holding other variables constant. A standard deviation increase in publication by the mentor increases the predicted number of publications by a factor of 1.28, holding other variables constant. ","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:4:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4.3 Predictions in the estimation sample Running predict after fitting a truncated count model will generate a new variable with predictions based on the values of the independent variable for each observation. The predictions can be conditional or unconditional. Consider our hypothetical example in which scientists are in the sample only if they have published at least once. Unconditional predictions are predictions about the entire population of scientists, regardless of whether they have published. Conditional predictions are predictions conditional on the count being greater than the truncation point. If we want to make predictions about those who have published (that is, the count is greater than 0), we would use conditional predictions. As noted before, both unconditional and conditional predictions are still conditional on the values of the independent variables. By default, predict computes the unconditional rate. We can obtain the unconditional predicted probability of a specific count $Pr(y = k)$ with the option pr(fc) and the predicted probability of a count within a range $Pr(m \u003c y \u003c n)$ with the option pr(m,n). We can also compute conditional predictions that are conditional on the count being greater than the truncation point. We obtain conditional predictions about the rate with the option cm and conditional predicted probabilities with the option cpr() instead of pr(). In our example, we use predict followed by summarize to show the average unconditional and conditional rates and to compare those with the mean number of articles in the sample. predict rate label var rate \"Unconditional rate of productivity\" predict crate, cm label var rate \"Conditional rate of productivity given at least 1 article\" summarize rate crate arttrunc Variable | Obs Mean Std. dev. Min Max\r-------------+---------------------------------------------------------\rrate | 640 1.687704 .641613 .9468752 8.397242\rcrate | 640 2.425643 .5871667 1.774038 8.774148\rarttrunc | 640 2.420313 1.882269 1 19\r","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:4:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4.4 Interpretation using predicted rates and probabilities mtable, mgen, and mchange can compute conditional and unconditional rates and predicted probabilities. Unconditional rates are computed by default. Other types of predictions can be made using the predict() option. For example, to compute the average conditional rate, you could use the command mtable, predict(cm). To compute unconditional probabilities for counts from m to n, we use the option predict(pr(m/n)). To compute conditional probabilities, we use predict(cpr(m/n)). To illustrate this, we compare the conditional and unconditional rates of productivity for married (married=1), female (female=1) scientists without young children (kid5=0), who are average on other characteristics: qui mtable, rowname(Unconditional) at(female=1 married=1 kid5=0) atmeans ci mtable, rowname(Conditional) at(female=1 married=1 kid5=0) atmeans ci /// predict(cm) below brief Expression: Conditional mean of arttrunc \u003e ll(0), predict(cm)\r| mu ll ul\r---------------+-----------------------------\rUnconditional | 1.561 1.243 1.880\rConditional | 2.308 2.062 2.553\rAn otherwise average married, female scientist without young children is predicted to publish 1.56 papers. However, if we know she has published at least one paper, this prediction increases to 2.31 articles. The conditional rate is higher than the unconditional rate, as it must be, because the conditional rate includes only those with at least one paper. To compute unconditional probabilities from 0 to 5 articles for a married female scientist without children, we type mtable, at(female=1 married=1 kid5=0) atmeans pr(0/5) ci brief Expression: Pr(arttrunc), predict(pr())\r| 0 1 2 3 4 5\r----------+-----------------------------------------------------------\rPr(y) | 0.323 0.272 0.177 0.104 0.058 0.031\rll | 0.240 0.253 0.150 0.081 0.042 0.020\rul | 0.407 0.292 0.205 0.127 0.074 0.042\rThe probability of a zero count is 0.32 with a 95% confidence interval from 0.24 to 0.41, indicating that we expect that an otherwise average married female scientist with no children would have a 32% chance of having no publications. Other probabilities can be interpreted similarly. We cannot compute the conditional probability of a zero count because we are conditioning on having at least one publication. Accordingly, we compare unconditional and conditional probabilities of counts greater than 1: qui mtable, rowname(Unconditional) at(female=1 married=1 kid5=0) atmeans /// pr(1/5) mtable, rowname(Conditional) at(female=1 married=1 kid5=0) atmeans /// cpr(1/5) below brief Expression: Pr(arttrunc | arttrunc\u003e0), cpr()\r| 1 2 3 4 5\r---------------+-------------------------------------------------\rUnconditional | 0.272 0.177 0.104 0.058 0.031\rConditional | 0.403 0.262 0.154 0.086 0.046\rAs expected, the conditional probabilities are larger than the unconditional probabilities. This is because the unconditional probability of a 0 is 0.32, so the conditional predictions equal $\\Pr\\left(y=k\\mid\\mathbf{x},y\u003e0\\right)=\\Pr\\left(y=k\\mid\\mathbf{x}\\right)/{1-\\Pr\\left(y=0\\mid\\mathbf{x}\\right)}.$Accordingly, each unconditional probability is 32.3% lower than the corresponding conditional probability. Although we do not illustrate them, other methods of interpretation that were used for the PRM and NBRM can also be used for count models with truncation. ","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:4:4","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"5 (Advanced) The hurdle regression model This section is marked as advanced because it involves fitting a model that is not built-in to Stata. Obtaining estimates is, consequently, more complicated. The section is useful if you have an application of the hurdle model, and it also provides an opportunity to learn about working with matrices and using the suest command to combine results from different models. 这段话是在讨论如何在Stata统计软件中进行一些高级操作，特别是涉及到拟合一个非内置的模型，即障碍模型（hurdle model）。障碍模型是一种用于处理零膨胀数据的模型，通常用于计数数据或存在大量零值的数据集。由于这个模型不是Stata内置的，因此使用它来获取模型估计的过程比使用内置命令更为复杂。 这一部分对于那些需要在实际应用中使用障碍模型的用户来说是有帮助的。同时，它也提供了一个学习如何处理矩阵和如何使用suest命令的机会。suest是Stata中的一个命令，用于对多个模型进行联合估计，特别适用于当需要对不同子样本或不同模型估计的参数进行合并检验时。 总的来说，这段话强调了障碍模型在Stata中的拟合是一个高级操作，需要用户具备一定的统计知识和Stata操作能力，同时指出了这一过程对于学习和理解更复杂的统计技术和Stata命令是有益的。 The hurdle regression model (HRM) combines a binary model to predict Os and a ZTP or ZTNB to predict nonzero counts (Mullahy 1986; Cameron and Trivedi [2013, 136-139]). Let y be a count outcome that is not truncated at 0. Suppose that zero counts are generated by a binary process. Here we use a logit to model the binary outcome $y = 0$ versus $y \u003e 0$, but other binary models could be used: $$\\Pr(y_i=0\\mid\\mathbf{x}_i)=\\frac{\\exp\\left(\\mathbf{x}_i\\boldsymbol{\\gamma}\\right)}{1+\\exp\\left(\\mathbf{x}_i\\boldsymbol{\\gamma}\\right)}=\\pi_i$$ In this two-equation model, 0 is viewed as a hurdle that you have to get past before reaching positive counts. Positive counts are generated either by the ZTP or the ZTNB models from section 9.4. Because there are separate equations to predict zero counts and positive counts, this allows the process that predicts zeros to be different from the process that predicts positive counts. The name “hurdle” might suggest that the process of moving from 0 to 1 with the count outcome is more difficult than subsequent increases. This makes sense in many count processes, including the case of scientific publishing: we might imagine that getting one’s first publication is especially difficult, and publishing is easier after that. In such cases, the number of zero counts would be greater than we would predict using the PRM or NORM. Unlike the zero-inflated models considered in section 9.6, however, the hurdle model can also be applied to cases in which there are fewer Os than expected. In such cases, getting over the hurdle is easier to achieve than subsequent counts. The predicted rates and probabilities for the HRM are computed by mixing the results of the binary model and the zero-truncated model. The probability of a 0 is $$\\Pr(y_i=0\\mid\\mathbf{x}_i)=\\pi_i$$ as estimated by a binary regression model. Because positive counts can occur only if you get past the 0 hurdle, which occurs with probability $1 - \\pi_i$, we rescale the conditional probability from the zero-truncated model to compute the unconditional probability. $$\\Pr\\left(y_i\\mid\\mathbf{x}_i\\right)=\\left(1-\\pi_i\\right)\\Pr\\left(y_i\\mid y_i\u003e0,\\mathbf{x}_i\\right)\\quad\\mathrm{for~}y\u003e0$$ The unconditional rate is computed by combining the mean rate for those with $y = 0$ (which, of course, is 0) and the mean rate for those with positive counts: $$E(y_i|x_i)=(\\pi_i\\times0)+{(1-\\pi_i)}\\times E(y_i|y_i\u003e0,x_i)$$ \\begin{align*} = (1-\\pi_{i})\\times E(y_{i}\\mid y_{i}\u003e0,\\mathbf{x}_{i}) \\end{align*} where $E(y_i\\mid y_i\u003e0,\\mathbf{x}_i)$ is defined by (9.8) for the ZTP and (9.9) for the ZTNB. Although Stata does not include the hurdle model as a command, it can be fit by combining results from logit with those from either tnb reg or tpoisson. Using these estimation commands along with commands for working with predictions, we can compute predictions for the hurdle model that correspond to those for other count models. This process involves a few more steps than the other examples in this book, but these are straightforward and necessary for using this model. The process also provides an example of accomplishing postestimation analyses “by hand”. A bigger problem is that the standard errors f","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:5:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"5.1 Fitting the hurdle model With binary models (see chapter 5), Stata defines the two outcome categories as 0 or any nonmissing value that is not 0. For the hurdle model, this is very convenient. If we use logit or probit for a count outcome, the resulting estimates are for a binary model of any positive count versus 0, which is precisely what we want for the first step in the hurdle model. Here we fit the logit model and store the estimates: use couart4, clear logit art i.female i.married kid5 phd mentor, nolog or predict prlogit est store Hlogit Then, we fit a ZTNB by using if art \u003e 0 to restrict our sample to scientists with at least one article: tnbreg art i.female i.married kid5 phd mentor if art\u003e0, nolog irr est store Hztnb This correctly estimates the coefficients, but the standard errors are incorrect. Accordingly, in the following table, the estimated coefficients are correct but the p-values are not (we only show them for comparison with the correct standard errors, computed with suest below): est table Hlogit Hztnb, b(%9.3f) eform t --------------------------------------\rVariable | Hlogit Hztnb -------------+------------------------\rart |\rfemale |\rFemale | 0.778 0.783 | -1.58 -2.52 |\rmarried |\rMarried | 1.386 1.109 | 1.80 0.95 |\rkid5 | 0.752 0.858 | -2.57 -2.12 phd | 1.022 0.997 | 0.28 -0.06 mentor | 1.083 1.024 | 6.15 5.54 _cons | 1.267 1.426 | 0.80 1.80 -------------+------------------------\r/lnalpha | 0.547 | -2.68 --------------------------------------\rLegend: b/t\rTo obtain the correct standard errors, we use the suest command (see [r] su est), which takes into account that even though the two models were independently estimated, they are dependent on one another. suest Hlogit Hztnb, vce(robust) eform(expB) Simultaneous results for Hlogit, Hztnb Number of obs = 915\r------------------------------------------------------------------------------\r| Robust\r| expB std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rHlogit_art |\rfemale |\rFemale | 0.78 0.12 -1.61 0.108 0.57 1.06\r|\rmarried |\rMarried | 1.39 0.25 1.83 0.068 0.98 1.97\rkid5 | 0.75 0.08 -2.58 0.010 0.61 0.93\rphd | 1.02 0.08 0.28 0.782 0.87 1.20\rmentor | 1.08 0.02 5.61 0.000 1.05 1.11\r_cons | 1.27 0.37 0.81 0.417 0.72 2.24\r-------------+----------------------------------------------------------------\rHztnb_art |\rfemale |\rFemale | 0.78 0.07 -2.64 0.008 0.65 0.94\r|\rmarried |\rMarried | 1.11 0.12 0.98 0.327 0.90 1.36\rkid5 | 0.86 0.06 -2.10 0.036 0.74 0.99\rphd | 1.00 0.05 -0.06 0.954 0.90 1.10\rmentor | 1.02 0.01 4.79 0.000 1.01 1.03\r_cons | 1.43 0.27 1.84 0.065 0.98 2.08\r-------------+----------------------------------------------------------------\r/Hztnb |\rlnalpha | 0.55 0.13 0.34 0.87\r------------------------------------------------------------------------------\rYou can confirm that the parameter estimates are the same as those shown with estimates table earlier but that the $z-values$ differ. With the hurdle model, a variable can be significant in one part of the model but not in the other part. For example, women are not significantly different from men “getting over the hurdle”, but they have a significantly lower rate of publication once over the hurdle. For that matter, coefficients for the same variable can be in opposite directions for the two parts of the model, and there is no need for both parts to include the same independent variables. The logit coefficients can be interpreted as discussed in chapter 6. If the coefficient for $X_k$ is positive, it means that a unit increase in $X_k$ increases the odds of publishing one or more papers by a factor of $exp(\\hat{\\beta}_k)$, holding other variables constant. For example, consider the effect that young children have on publishing: For an additional young child, the odds of publishing at least one article decrease by a factor of 0.75, holding other variables constant. The ZTNB part of the model estimates how independent variables affect the r","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:5:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"5.2 Predictions in the sample Using predict with the estimates from our logit model, we can compute the predicted probability of observing a positive count for each observation: estimates restore Hlogit predict Hprobgt0 label var Hprobgt0 \"Pr(y\u003e0)\" If we subtract this probability from 1, we get the predicted probability of a zero count: gen Hprob0 = 1 - Hprobgt0 label var Hprob0 \"Pr(y=0)\" summarize Hprob0 Hprobgt0 Variable | Obs Mean Std. dev. Min Max\r-------------+---------------------------------------------------------\rHprob0 | 915 .3005464 .1180336 .0015213 .5612055\rHprobgt0 | 915 .6994536 .1180336 .4387945 .9984787\rThe average predicted probability of a 0 is 0.3005, which is exactly the same as the proportion of the 0s in the sample, as shown with tabulate art earlier. When logit is used for the binary portion of the model, the average probability of a 0 will always exactly match the observed proportion of 0s. Accordingly, in the hurdle model, the average predicted probability of a 0 equals the observed proportion of 0s. To compute predictions of positive counts, we restore the results from the ZTNB and use predict with the cm option to generate conditional predictions: estimates restore Hztnb predict Hcrate, cm label var Hcrate \"Conditional rate\" The conditional rate is the expected number of publications for those who have made it over the hurdle of publishing at least one article. To compute the unconditional rate, we multiply the conditional rate by the probability of having published at least one article, which we estimated using the logit portion of the model. gen Hrate = Hcrate * Hprobgt0 label var Hrate \"Unconditional rate\" We use summarize to compare the average conditional and unconditional rates: sum Hcrate Hrate Variable | Obs Mean Std. dev. Min Max\r-------------+---------------------------------------------------------\rHcrate | 915 2.356661 .52643 1.73671 8.774148\rHrate | 915 1.697686 .7016245 .7705519 8.7608\rWe follow the same idea to compute unconditional probabilities for nonzero counts. Because we want to compute predictions for multiple counts, we use a foreach loop. Within the loop, we first compute the conditional predicted probabilities by using predict. Next, we multiply the conditional probabilities by the probability from our logit model of having published at least one article: forvalues icount = 1/8 { predict Hcprob`icount', cpr(`icount') label variable Hcprob`icount' \"Pr(y=`icount'|y\u003e0)\" gen Hprob`icount' = Hcprob`icount' * Hprobgt0 label variable Hprob`icount' \"Pr(y=`icount')\" } The loop executes the code within braces eight times, successively substituting the values 1-8 for the macro icount. The predict command uses the pr() option to compute conditional probabilities. The unconditional probabilities are obtained by multiplying conditional probabilities by the probability of a positive count. We use summarize to obtain the average unconditional probabilities: sum Hprob* Variable | Obs Mean Std. dev. Min Max\r-------------+---------------------------------------------------------\rHprobgt0 | 915 .6994536 .1180336 .4387945 .9984787\rHprob0 | 915 .3005464 .1180336 .0015213 .5612055\rHprob1 | 915 .2772253 .0264024 .0672922 .3398655\rHprob2 | 915 .1783374 .0225025 .07815 .2233984\rHprob3 | 915 .1053164 .0246465 .0489526 .1597355\r-------------+---------------------------------------------------------\rHprob4 | 915 .0599405 .022171 .0196098 .1201168\rHprob5 | 915 .0336258 .0178631 .0075859 .0961457\rHprob6 | 915 .0188354 .0136543 .002865 .0806172\rHprob7 | 915 .0106266 .0102113 .0010633 .0676752\rHprob8 | 915 .0060773 .0075988 .0003894 .0604464\rThe mean predicted probabilities can be compared with the observed predictions, as illustrated for the PRM and NBRM. ","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:5:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"5.3 Predictions at user-specified values We can use mtable or margins to compute predictions at specific values of the independent variables. You should not compute confidence intervals for these predictions: they will be incorrect because they do not use the correct standard errors from suest. For our example, we make predictions for the ideal type of a married, female scientist (female=l married=l) from an elite PhD program (phd=4.5) who studied with a mentor with average productivity. We begin by making predictions from the logit portion of the model: est restore Hlogit mtable, at(female=1 married=1 kid5=0 phd=4.5) atmeans Expression: Pr(art), predict()\rPr(y)\r--------\r0.753\rSpecified values of covariates\r| female married kid5 phd mentor\r----------+-------------------------------------------------\rCurrent | 1 1 0 4.5 8.77\rRecall, that $Pr(y)$ is the probability of a 1 in the logit model, which corresponds to having one or more publications. Accordingly, the predicted probability of publishing one or more papers for a scientist with these characteristics is 0.753. Next, we need the probability of a positive count to compute unconditional predictions with (9.10) and (9.12). Although we could simply type 0.753 in our do-file, a better way is to save the result as a local macro. Not only does this prevent typing errors, but our code will still work correctly if we change the model or sample and the predicted probability is no longer 0.753. To do this, we use information in the matrix r (\\text{table}) that is returned by mtable. If you type matlist r(table), you will see that the prediction we want is in row 1 and column 1 of the matrix. To store this value in the local macro prgtO, we use: local prgt0 = el(r(table),1,1) where the el() function extracts the element in row 1 and column 1 of matrix (r (\\text{table})). After we compute conditional probabilities, we will use the macro when computing unconditional probabilities. Next, we compute conditional probabilities from the truncated portion of the model: est restore Hztnb mtable, at(female=1 married=1 kid5=0 phd=4.5) atmeans noesample cpr(1/8) Expression: Pr(art | art\u003e0), cpr()\r1 2 3 4 5 6 7 8\r------------------------------------------------------------------------------\r0.413 0.264 0.152 0.083 0.044 0.022 0.011 0.006\rSpecified values of covariates\r| female married kid5 phd mentor\r----------+-------------------------------------------------\rCurrent | 1 1 0 4.5 8.77\rThe noesample option is essential; we explain it briefly here, with further discussion below. The predictions for mtable above use estimates from tnbreg, which was fit using only observations where art is nonzero. By default, atmeans would use this sample to compute the mean for mentor. We want to compute predictions with the mean for mentor based on the entire sample not just the estimation sample. The noesample option tells mtable (and other commands based on margins) to use the entire sample. The conditional probabilities computed by mtable were stored in the matrix r(table). To compute unconditional probabilities, we multiply these conditional probabilities by the probability of a positive count, which we saved in the local macro prgtO. This can be done simply with a matrix command: matrix Huncond = `prgt0' * r(table) matlist Huncond, format(%8.3f) title(Unconditional probabilties) names(col) Unconditional probabilties\r1 2 3 4 5 6 7 8 ------------------------------------------------------------------------------\r0.251 0.155 0.086 0.045 0.023 0.011 0.006 0.003 These predictions are for the ideal type of a married, female scientist with no children, who studied with an average mentor at an elite graduate program. The predicted probabilities of publishing one article is 0.31, two articles is 0.20, and so on. We could extend these analyses to compare these predictions with those for scientists with other characteristics. ","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:5:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"5.4 Warning regarding sample specification The two parts of the hurdle model are fit using different samples. The full sample was used to fit the binary model, while the truncated sample was used to fit the zero-truncated model. When computing predictions at specified values of the independent variables—say, the mean—you want the mean for the full sample, not the smaller, truncated sample. Or, if we want average predictions, we want averages for the full sample. By default, margins, mtable, mgen, and mchange use the estimation sample. For example, after fitting the ZTNB, mtable,atmeans computes predictions by using the means for cases with nonzero outcomes. What we want, however, are the means for the sample used for the binary portion of the model. To avoid problems, we suggest the following steps: Before fitting the binary portion of the model, use the drop command to drop cases with missing data or that you would otherwise drop by using an if or in condition. This was not necessary in our example above because we wanted to use all the cases to fit the model. Fit the truncated model with an if condition to drop cases that are 0 on the outcome (for example, tnbreg ... if art \u003e 0). When using margins, mtable, or other margins-based commands, use the option noesample, which specifies that all cases in memory be used to compute averages and other statistics, rather than using only the estimation sample. ","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:5:4","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"6 Zero-inflated count models The NBRM improves upon the underprediction of Os in the PRM by increasing the conditional variance without changing the conditional mean. The hurdle model addresses the underprediction of Os by using two equations: a binary model to predict Os and a zero-truncated model for the remaining counts. We can think of this as allowing the process that generates the first count to be distinct from the process that generates subsequent counts. Zero-inflated count models, introduced by Mullahy (1986) and Lambert (1992), change the mean structure to allow Os to be generated by two distinct processes. Consider our example of scientific productivity. The PRM, NBRM, and HRM assume that every scientist has a nonzero rate of publishing. This implies a chance that any given scientist would have no publications, but it also implies a positive probability for all positive counts. The rates and predicted probabilities differ across individuals according to their characteristics, but all scientists could publish even if that probability is quite small. Substantively, this would be unrealistic if some scientists have no opportunity for publishing. For example, scientists might be employed in an industry where publishing is not allowed, or they might hold jobs that do not involve research. As a result, we would observe more scientists with no publications because zero counts reflect a combination of scientists with no probability of publishing and scientists for whom no publications is the result of a probabilistic process. Zero-inflated models allow for this possibility. A zero-inflated model assumes that there are two latent (that is, unobserved) groups. An individual in the “always 0 group” (group ~A) has outcome 0 with a probability of 1, whereas an individual in the “not always 0” group (group ~A) might have outcome 0, but there is a nonzero probability that the individual has a positive count. For someone with no publications, we cannot determine whether he or she is in group A or group ~A, but we can model the individual’s probability of being in one of the groups based on observed characteristics. This process is developed in three steps. Step 1. Model membership into the latent groups A and ~A. Step 2. Model counts for those in group ~A who can publish Step 3. Compute probabilities for each count as a mixture of the probabilities from the two groups. 抱歉刚才的回复中出现了误解。现在我将按照您的要求，将文中的数学公式用美元符号 $包裹起来，以符合Markdown的公式渲染格式。 零膨胀模型的详细解释 第1步：模型潜在群体的成员资格 在这一步中，我们使用一个二元逻辑回归模型来预测个体是否属于“总是零”的群体（群体A）。逻辑回归模型的形式通常如下： $$ \\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_n X_n $$ 这里： $ p $ 是个体属于群体A的概率。 $ \\log\\left(\\frac{p}{1-p}\\right) $ 是对数几率（logit），它是将概率转换为一个可以被线性模型预测的值。 $ \\beta_0, \\beta_1, \\ldots, \\beta_n $ 是模型参数，它们表示不同特征对个体属于群体A概率的影响。 $ X_1, X_2, \\ldots, X_n $ 是个体的特征，比如年龄、性别、教育水平等。 第2步：模型计数 对于群体~A中的个体，我们使用一个计数模型来预测他们的出版物数量。泊松回归模型是常用的计数模型之一，其形式如下： $$ \\log(\\lambda) = \\alpha_0 + \\alpha_1 Y_1 + \\alpha_2 Y_2 + \\ldots + \\alpha_m Y_m $$ 这里： $ \\lambda $ 是个体预期的出版物数量（即泊松分布的参数）。 $ \\alpha_0, \\alpha_1, \\ldots, \\alpha_m $ 是模型参数。 $ Y_1, Y_2, \\ldots, Y_m $ 是影响个体出版物数量的特征，比如研究资金、工作时间等。 第3步：计算每个计数的概率 在这一步中，我们将前两步的结果结合起来，计算每个可能计数的概率。对于任何给定的计数 $ k $，最终的概率是： $$ P(X = k) = (1 - P(\\text{在群体A})) \\cdot P(X = k | \\text{在群体~A}) + P(\\text{在群体A}) \\cdot P(X = k | \\text{在群体A}) $$ 由于群体A中的个体永远没有出版物，所以 $ P(X = k | \\text{在群体A}) $ 只有在 $ k = 0 $ 时才不为零。因此，上述公式可以简化为： $$ P(X = k) = \\begin{cases} P(\\text{在群体A}) \u0026 \\text{if } k = 0 \\ (1 - P(\\text{在群体A})) \\cdot P(X = k | \\text{在群体~A}) \u0026 \\text{if } k \u003e 0 \\end{cases} $$ 这意味着： 当 $ k = 0 $ 时，概率仅仅是个体属于群体A的概率，因为只有群体A中的个体才总是有零出版物。 当 $ k \u003e 0 $ 时，概率是个体属于群体~A的概率乘以在群体~A中得到 $ k $ 个出版物的概率。 例子 假设我们有两个科学家，Alice和Bob，他们的特征如下： Alice：工业界工作，没有研究资金。 Bob：大学教授，有研究资金。 第1步：预测Alice和Bob属于群体A的概率 假设逻辑回归模型的参数估计结果如下： Alice：$ P(\\text{在群体A}) = 0.9 $（很高的概率属于群体A） Bob：$ P(\\text{在群体A}) = 0.1 $（较低的概率属于群体A） 第2步：预测Bob的出版物数量 假设泊松回归模型预测Bob有1篇、2篇、3篇出版物的概率分别为： $ P(X = 1 | \\text{在群体~A}) = 0.2 $ $ P(X = 2 | \\text{在群体~A}) = 0.3 $ $ P(X","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:6:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"Estimation using zinb and zip The ZIP and ZINB models are fit with the zip and zinb commands, respectively, listed here with their basic options: zip depvar [indepvars] [if] [m] [weight], inflat e(indepvars2) [noconstant probit vce(vcetype) irr vuong] zinb depvar [indepvars] [if] [in] [weight], inflate(indepvars2) [noconstant probit vce(vcetypc) irr vuong] Variable lists depvar is the dependent variable, which must be a count with no negative values or non-integers. indepvars is a list of independent variables that determine counts among those who are not always Os. If indepvars is not included, a model with only an intercept is fit. indepvars2 is a list of inflation variables that determine whether one is in the “always 0” group (group A) or the “not always 0” group (group ~A). indepvars and indepvars2 can be the same variables but do not have to be. Options Here we consider only those options that differ from the options for earlier models in this chapter. probit specifies that the model determining the probability of being in group A versus group -A be a binary probit model. By default, a binary logit model is used. vuong requests a Vuong (1989) test of the ZIP versus the PRM or of the ZINB versus the NBRM. Details are given in section 9.7.2. The vuong option is not available if robust standard errors are used. ","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:6:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"Example of zero-inflated models The output from zip and zinb is similar, so here we show only the output for zinb: zinb art i.female i.married kid5 phd mentor, /// inflate(i.female i.married kid5 phd mentor) nolog vce(robust) estimates store zinb Zero-inflated negative binomial regression Number of obs = 915\rInflation model: logit Nonzero obs = 640\rZero obs = 275\rWald chi2(5) = 52.34\rLog pseudolikelihood = -1549.991 Prob \u003e chi2 = 0.0000\r------------------------------------------------------------------------------\r| Robust\rart | Coefficient std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rart |\rfemale |\rFemale | -0.20 0.08 -2.58 0.010 -0.34 -0.05\r|\rmarried |\rMarried | 0.10 0.09 1.15 0.251 -0.07 0.26\rkid5 | -0.15 0.06 -2.71 0.007 -0.26 -0.04\rphd | -0.00 0.04 -0.02 0.986 -0.08 0.08\rmentor | 0.02 0.00 5.99 0.000 0.02 0.03\r_cons | 0.42 0.15 2.85 0.004 0.13 0.70\r-------------+----------------------------------------------------------------\rinflate |\rfemale |\rFemale | 0.64 1.00 0.63 0.526 -1.33 2.60\r|\rmarried |\rMarried | -1.50 1.13 -1.33 0.184 -3.71 0.71\rkid5 | 0.63 0.45 1.41 0.160 -0.25 1.50\rphd | -0.04 0.31 -0.12 0.904 -0.65 0.58\rmentor | -0.88 0.30 -2.96 0.003 -1.47 -0.30\r_cons | -0.19 1.56 -0.12 0.902 -3.25 2.87\r-------------+----------------------------------------------------------------\r/lnalpha | -0.98 0.15 -6.73 0.000 -1.26 -0.69\r-------------+----------------------------------------------------------------\ralpha | 0.38 0.05 0.28 0.50\r------------------------------------------------------------------------------\rThe top set of coefficients, labeled art in the left margin, corresponds to the NBRM for those in group A. The lower set of coefficients, labeled inflate, corresponds to the binary model predicting group membership. ","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:6:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"6.3 Interpretation of coefficients When interpreting zero-inflated models, it is easy to be confused by the direction of the coefficients. listcoef makes interpretation simpler. For example, consider the results for the ZINB: listcoef, help zinb (N=915): Factor change in expected count Observed SD: 1.9261\rCount equation: Factor change in expected count for those not always 0\r------------------------------------------------------------------------\r| b z P\u003e|z| e^b e^bStdX SDofX\r-------------+----------------------------------------------------------\rfemale |\rFemale | -0.1955 -2.580 0.010 0.822 0.907 0.499\r|\rmarried |\rMarried | 0.0976 1.148 0.251 1.103 1.047 0.473\rkid5 | -0.1517 -2.707 0.007 0.859 0.890 0.765\rphd | -0.0007 -0.018 0.986 0.999 0.999 0.984\rmentor | 0.0248 5.989 0.000 1.025 1.265 9.484\rconstant | 0.4167 2.846 0.004 . . .\r-------------+----------------------------------------------------------\ralpha |\rlnalpha | -0.9764 . . . . .\ralpha | 0.3767 . . . . .\r------------------------------------------------------------------------\rb = raw coefficient\rz = z-score for test of b=0\rP\u003e|z| = p-value for z-test\re^b = exp(b) = factor change in expected count for unit increase in X\re^bStdX = exp(b*SD of X) = change in expected count for SD increase in X\rSDofX = standard deviation of X\rBinary equation: factor change in odds of always 0\r------------------------------------------------------------------------\r| b z P\u003e|z| e^b e^bStdX SDofX\r-------------+----------------------------------------------------------\rfemale |\rFemale | 0.6359 0.634 0.526 1.889 1.373 0.499\r|\rmarried |\rMarried | -1.4995 -1.329 0.184 0.223 0.492 0.473\rkid5 | 0.6284 1.406 0.160 1.875 1.617 0.765\rphd | -0.0377 -0.121 0.904 0.963 0.964 0.984\rmentor | -0.8823 -2.963 0.003 0.414 0.000 9.484\rconstant | -0.1917 -0.123 0.902 . . .\r------------------------------------------------------------------------\rb = raw coefficient\rz = z-score for test of b=0\rP\u003e|z | = p-value for z-test\re^b = exp(b) = factor change in odds for unit increase in X\re^bStdX = exp(b*SD of X) = change in odds for SD increase in X\rSDofX = standard deviation of X\rThe top half of the output, labeled Count equation, contains coefficients for the factor change in the expected count for those who have the opportunity to publish (that is, group ~A). The coefficients can be interpreted in the same way as coefficients from the PRM or the NBRM. For example, Among those who have the opportunity to publish, being a female scientist decreases the expected rate of publication by a factor of 0.82, holding other variables constant. The bottom half of the output, labeled Binary equation, contains coefficients for the factor change in the odds of being in group A compared with being group -A. These can be interpreted just as the coefficients for a binary logit model. For example, Being a female scientist increases the odds of not having the opportunity to publish by a factor of 1.89, holding other variables constant. As found in this example, when the same variables are included in both equations, the signs of the corresponding coefficients from the binary equation are often in the opposite direction of those from the count equation. This makes substantive sense. The count equation predicts the number of publications, so a positive coefficient indicates higher productivity. In contrast, the binary equation is predicting membership in the group that always has zero counts, so a positive coefficient implies lower productivity. This is not, however, required by the model. ","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:6:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"6.4 Interpretation of predicted probabilities For th e ZIP, $$\\widehat{\\Pr}\\left(y=0\\mid\\mathbf{x},\\mathbf{z}\\right)=\\widehat{\\psi}+\\left(1-\\widehat{\\psi}\\right)e^{-\\widehat{\\mu}}$$ where $\\widehat{\\mu}=\\exp\\left(\\mathbf{x}\\widehat{\\boldsymbol{\\beta}}\\right)\\mathrm{~and~}\\widehat{\\psi}=F(\\mathbf{z}\\widehat{\\boldsymbol{\\gamma}}).$ The predicted probability of a positive count applies only to the $1-\\widehat{\\psi}$ observations in group ~A: $$\\widehat{\\Pr}(y\\mid\\mathbf{x})=\\left(1-\\widehat{\\psi}\\right)\\frac{e^{-\\widehat{\\mu}_{t}}\\widehat{\\mu}^{y}}{y!}$$ Similarly, for th e ZINB, $$\\widehat{\\Pr}\\left(y=0\\mid\\mathbf{x},\\mathbf{z}\\right)=\\widehat{\\psi}+\\left(1-\\widehat{\\psi}\\right)\\left(\\frac{\\widehat{\\alpha}^{-1}}{\\widehat{\\alpha}^{-1}+\\widehat{\\mu}_{i}}\\right)^{\\widehat{\\alpha}^{-1}}$$ and the predicted probability for a positive count is $$\\widehat{\\Pr}(y\\mid\\mathbf{x})=\\left(1-\\widehat{\\psi}\\right)\\frac{\\Gamma\\left(y+\\widehat{\\alpha}^{-1}\\right)}{y!\\Gamma(\\widehat{\\alpha}^{-1})}\\left(\\frac{\\widehat{\\alpha}^{-1}}{\\widehat{\\alpha}^{-1}+\\widehat{\\mu}}\\right)^{\\widehat{\\alpha}^{-1}}\\left(\\frac{\\widehat{\\mu}}{\\widehat{\\alpha}^{-1}+\\widehat{\\mu}}\\right)^{y}$$ Predicted probabilities can be computed with margins and with our SPost commands mtable, mgen, and mchange. Predicted probabilities with mtable Suppose that we want to compare the predicted probabilities for a married female scientist with young children who came from a weak graduate program with those for a married male from a strong department who had a productive mentor. We can use the mtable command to display these two predictions in one table. First, we use a vars option to include in the table the variables on which we are focusing. By default, these would not be included because they do not vary within either of the mtable commands. Second, because the at() variables in the table make it clear what each row contains, we use norownumbers so that only column labels are shown. The width(7) option makes the results fit without wrapping. quietly mtable, at(female=0 married=1 kid5=3 phd=3 mentor=10) /// atvars(female phd mentor) pr(0/5) mtable, at(female=1 married=1 kid5=3 phd=1 mentor=0) /// atvars(female phd mentor) pr(0/5) below width(7) norownumbers Expression: Pr(art), predict(pr())\r1. female phd mentor 0 1 2 3 4 5\r--------------------------------------------------------------------------------\r0 3 10 0.334 0.300 0.185 0.097 0.047 0.021\r1 1 0 0.835 0.096 0.043 0.017 0.006 0.002\rSpecified values of covariates\r| female married kid5 phd mentor\r----------+--------------------------------------------\rSet 1 | 0 1 3 3 10\rCurrent | 1 1 3 1 0\rThe predicted probabilities of a 0 include both scientists from group A and scientists from group ~A who by chance did not publish. To compute the probability of being in group A , we use the predict(pr) option:9 quietly mtable, at(female=0 married=1 kid5=3 phd=3 mentor=10) /// atvars(female phd mentor) predict(pr) mtable, at(female=1 married=1 kid5=3 phd=1 mentor=0) /// atvars(female phd mentor) predict(pr) below norownumbers decimals(4) Expression: Pr(art = always 0), predict(pr)\r1. female phd mentor PrAll0\r--------------------------------------\r0 3 10 0.0002\r1 1 0 0.6883\rSpecified values of covariates\r| female married kid5 phd mentor\r----------+-------------------------------------------------\rSet 1 | 0 1 3 3 10\rCurrent | 1 1 3 1 0\rWe used the option decimals(4) to show that the probability of being in group A for the men is small but is not 0. Plotting predicted probabilities with mgen In this section, we explore the two sources of 0s and how they each contribute to the predicted proportion of 0s as the number of publications by a scientist’s mentor changes. First, we use mgen to compute the predicted probability of a 0 of either type as mentor’s publications range from 0 to 6, holding other variables to their means: mgen, at(mentor=(0/6)) atmeans pr(0) stub(ZINB) replace Next, we compute the probability of being in group A by specifying ","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:6:4","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"7 Comparisons among count models There are two approaches that can be used to compare count models. First, we can compare the mean predicted probabilities and the observed proportions for each count. This was done when we compared predictions from the PRM and NBRM earlier. Second, we can use various tests and measures of fit to compare models, such as the LR test of overdispersion or the BIC statistic. We begin by showing you how to make these computations using official Stata commands. Then, we demonstrate the SPost command countfit, which automates this process. Although countfit is the simplest way to compare models, it is useful to understand how these computations are made to more fully understand the output of countfit. ","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:7:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"7.1 Comparing mean probabilities One way to compare count models is to compute average predicted probabilities and compare their fit to the observed data across models. First, we compute the mean predicted probability. For example, in the PRM, This is simply the average across all observations of the probability of each count. $\\widehat{\\mathrm{Pr}}_{\\mathrm{Observed}}(y=k)$ is the observed probability or proportion of observations with the count equal to $ k $. The difference between the observed probability and the mean probability is To compute these measures, we first fit each of the four models and then use mgen, meanpred to create variables containing average predictions: poisson art i.female i.married kid5 phd mentor, nolog mgen, stub(PRM) pr(0/9) meanpred Poisson regression Number of obs = 915\rLR chi2(5) = 183.03\rProb \u003e chi2 = 0.0000\rLog likelihood = -1651.0563 Pseudo R2 = 0.0525\r------------------------------------------------------------------------------\rart | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rfemale |\rFemale | -0.22 0.05 -4.11 0.000 -0.33 -0.12\r|\rmarried |\rMarried | 0.16 0.06 2.53 0.011 0.03 0.28\rkid5 | -0.18 0.04 -4.61 0.000 -0.26 -0.11\rphd | 0.01 0.03 0.49 0.627 -0.04 0.06\rmentor | 0.03 0.00 12.73 0.000 0.02 0.03\r_cons | 0.30 0.10 2.96 0.003 0.10 0.51\r------------------------------------------------------------------------------\r. mgen, stub(PRM) pr(0/9) meanpred\rPredictions from: Variable Obs Unique Mean Min Max Label\r---------------------------------------------------------------------------------------\rPRMval 10 10 4.5 0 9 Articles in last 3 yrs of PhD\rPRMobeq 10 10 .0993443 .0010929 .3005464 Observed proportion\rPRMoble 10 10 .8328962 .3005464 .9934426 Observed cum. proportion\rPRMpreq 10 10 .0998819 .0009304 .3098447 Avg predicted Pr(y=#)\rPRMprle 10 10 .8308733 .2092071 .9988187 Avg predicted cum. Pr(y=#)\rPRMob_pr 10 10 -.0005376 -.0475604 .0913393 Observed - Avg Pr(y=#)\r---------------------------------------------------------------------------------------\rnbreg art i.female i.married kid5 phd mentor, nolog mgen, stub(NBRM) pr(0/9) meanpred Negative binomial regression Number of obs = 915\rLR chi2(5) = 97.96\rDispersion: mean Prob \u003e chi2 = 0.0000\rLog likelihood = -1560.9583 Pseudo R2 = 0.0304\r------------------------------------------------------------------------------\rart | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rfemale |\rFemale | -0.22 0.07 -2.98 0.003 -0.36 -0.07\r|\rmarried |\rMarried | 0.15 0.08 1.83 0.067 -0.01 0.31\rkid5 | -0.18 0.05 -3.32 0.001 -0.28 -0.07\rphd | 0.02 0.04 0.42 0.672 -0.06 0.09\rmentor | 0.03 0.00 8.38 0.000 0.02 0.04\r_cons | 0.26 0.14 1.85 0.065 -0.02 0.53\r-------------+----------------------------------------------------------------\r/lnalpha | -0.82 0.12 -1.05 -0.58\r-------------+----------------------------------------------------------------\ralpha | 0.44 0.05 0.35 0.56\r------------------------------------------------------------------------------\rLR test of alpha=0: chibar2(01) = 180.20 Prob \u003e= chibar2 = 0.000\r. mgen, stub(NBRM) pr(0/9) meanpred\rPredictions from: Variable Obs Unique Mean Min Max Label\r-------------------------------------------------------------------------------------------\rNBRMval 10 10 4.5 0 9 Articles in last 3 yrs of PhD\rNBRMobeq 10 10 .0993443 .0010929 .3005464 Observed proportion\rNBRMoble 10 10 .8328962 .3005464 .9934426 Observed cum. proportion\rNBRMpreq 10 10 .0993102 .0035078 .3035957 Avg predicted Pr(y=#)\rNBRMprle 10 10 .8314587 .3035957 .9931023 Avg predicted cum. Pr(y=#)\rNBRMob_pr 10 10 .000034 -.0145355 .0144863 Observed - Avg Pr(y=#)\r-------------------------------------------------------------------------------------------\rzip art i.female i.married kid5 phd mentor, /// inflate(i.female i.married kid5 phd mentor) nolog mgen, stub(ZIP) pr(0/9) meanpred Zero-inflated Poisson ","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:7:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"7.2 Tests to compare count models Plotting predictions is only an informal method of assessing the fit of a count model. More formal testing can be done with an LR test of overdispersion and a Vuong test to compare two models. LR tests of α Because the NBRM reduces to the PRM when α = 0, the PRM and NBRM can be compared by testing $H_{0}:\\alpha=0$ As shown in section 9.3.3, we find that Likelihood-ratio test of alpha=0: chibar2(01) = 180.20 prob\u003e=chibar2 = 0.000\rquietly zip art i.female i.married kid5 phd mentor, /// inflate(i.female i.married kid5 phd mentor) estimates store zip quietly zinb art i.female i.married kid5 phd mentor, /// inflate(i.female i.married kid5 phd mentor) estimates store zinb lrtest zip zinb, force Likelihood-ratio test\rAssumption: zip nested within zinb\rLR chi2(1) = 109.56\rProb \u003e chi2 = 0.0000\rGiven the significant LR test statistics, we conclude that the ZINB significantly improves the fit over the ZIP. Vuong test of nonnested models Greene (1994) points out that the PRM and the ZIP are not nested.11 For the ZIP model to reduce to the PRM, $\\psi $ must equal 0, but this does not occur when $\\gamma=0$ because $\\psi=F\\left(\\mathbf{z0}\\right)=0.5$ Similarly, the NBRM and the ZINB are not nested. Greene proposes using a test by Vuong (1989, 319) for nonnested models. This test considers two models, where $\\widehat{Pr_1}\\left(y_{i}\\mid\\mathbf{x}_{i}\\right)$ is the predicted probability of observing $y_i$ in the first model and $\\widehat{\\mathrm{Pr}}_2\\left(y_i\\mid\\mathbf{x}_i\\right)$ is the predicted probability for the second model. If there are inflation variables, we are assuming they are part of x. Define and let $\\overline{m}$ be the mean and $s_m$ be the standard deviation of $m_i$.The Vuong statistic to test the hypothesis that $E\\left(m\\right)=0$ is $$V=\\frac{\\sqrt{N}\\overline{m}}{s_m}$$ $V$ has an asymptotic normal distribution. If $V \u003e 1.96$, the first model is favored; if $V \u003c -1.96$, the second model is favored. For zip. the vuong option computes the Vuong statistic comparing the ZIP with the PRM; for zinb , the vuong option compares the ZINB with the NBRM . For example, 基本概念 PRM (Poisson Regression Model) 和 ZIP (Zero-Inflated Poisson) 是两种统计模型，用于处理计数数据，特别是当数据中存在大量零值时。 NBRM (Negative Binomial Regression Model) 和 ZINB (Zero-Inflated Negative Binomial) 也是用于计数数据的模型，特别是当数据的方差大于均值时。 Nested Models 指的是一种模型可以简化为另一种模型的特殊情况。如果模型A在某些参数条件下可以变成模型B，那么我们说模型A嵌套于模型B。 非嵌套模型 (Non-nested models) 指的是两种模型之间不存在简化关系。 符号解释 $ \\psi $：某个参数，当它等于0时，ZIP模型可以简化为PRM模型。 $ \\gamma $：另一个参数，当它等于0时，$ \\psi $ 不等于0。 $ F(\\mathbf{z0}) $：一个函数，这里用来说明 $ \\psi $ 的值。 $ \\widehat{Pr_1} $ 和 $ \\widehat{\\mathrm{Pr}}_2 $：分别是两个模型预测的观察到 $ y_i $ 的概率。 Vuong检验 Vuong检验是一种用于比较两个非嵌套模型拟合优度的统计方法。它通过计算Vuong统计量 $ V $ 来决定哪个模型更优。 步骤1：定义 $ m_i $ 和它的统计量 $ m_i $ 是对于每个观测值 $ i $，第一个模型预测概率与第二个模型预测概率的比值的自然对数。 $ \\overline{m} $ 是 $ m_i $ 的均值。 $ s_m $ 是 $ m_i $ 的标准差。 2. 步骤2：计算Vuong统计量 $ V $ $ V $ 的计算公式是 $ \\frac{\\sqrt{N}\\overline{m}}{s_m} $，其中 $ N $ 是样本大小。 3. 步骤3：判断模型 $ V $ 服从渐近正态分布。 如果 $ V \u003e 1.96 $，则偏好第一个模型。 如果 $ V \u003c -1.96 $，则偏好第二个模型。 4. 示例 在统计软件中，对于ZIP模型和PRM模型，或者ZINB模型和NBRM模型，可以通过设置 vuong 选项来计算Vuong统计量，以比较这两个模型。 5. 总结 Vuong检验为我们提供了一种在两个非嵌套模型之间做出选择的方法。通过计算每个模型的预测概率，然后使用这些预测概率来计算Vuong统计量，我们可以判断哪个模型更适合我们的数据。这种方法特别适用于存在大量零值或者方差大于均值的计数数据。 * Vuong test of non-nested models zip art i.female i.married kid5 phd mentor, /// inflate(i.female i.married kid5 phd mentor) forcevuong nolog listcoef, help zip (N=915): Factor change in expected count Observed SD: 1.9261\rCount equation: Factor change in expected count for those not always 0\r------------------------------------------------------------------------\r| b z P\u003e|z| e^b e^bStdX SDofX\r-------------+----------------------------------------------------------\rfemale |\rFemale | -0.2091 -3.299 0.001 0.811 0.901 0.499\r|\rmarried |\rMarried | 0.1038 1.459 0.145 1.109 1.050 0.473\rkid5 | -0.1433 -3.022 0.003 0.866 0.896 0.765\rphd | -0.0062 -0.199 0.842 0.994 0.994 0.984\rmentor | 0.0181 7.886 0.000 1.018 1.187 9.484\rconstant | 0.6408 5.283 0.000 .","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:7:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"7.3 Using countfit to compare count models The countfit command automates the analyses described in the last two sections for the PRM, NBRM, ZIP, and ZINB. The command can provide a table of estimates, a table of differences between observed and average estimated probabilities, a graph of these differences, and various tests and measures of fit used to compare count models. The syntax is countfit varlist [if] [in] [, inflate(varlist2) noconst prm nbreg zip zinb stub(prefix) replace note(string) nograph nodifferences noptable noestimates nofit nodash maxcount(#) noisily] Options for specifying the model varlist is the variable list for the model, beginning with the count outcome variable if and in specify the sample used for fitting the models. inflate(varlist2) specifies the inflation variables for ZIP and ZINB. noconstant specifies that no constant be included in the model. Options to select the models to fit By default, PRM, NBRM, ZIP, and ZINB are all fit. If you want only some of these models, specify the models you want. prm fits the PRM. nbreg fits the NBRM. zip fits the ZIP. zinb fits the ZINB. Options to label and save results stub(prefix) can be up to five letters to prefix the variables that are created and to label the models in the output. This name is placed in front of the type of model (for example, namePRM). These labels help keep track of results from multiple specifications of models. replace replaces variables created by stub() if they already exist. note(string) adds a label to the graph. Options to control what is printed nograph suppresses the graph of differences from observed counts. nodifferences suppresses the table of differences from observed counts. noptable suppresses the table of predictions for each model. noestimates suppresses the table of estimated coefficients. nofit suppresses the table of fit statistics and tests of fit. nodash suppresses dashed lines between measures of fit. maxcount(#) specifies the number of counts to evaluate. noisily includes output from Stata estimation commands; without this option, the results are shown only in the estimates table output. To illustrate what countfit does, we use countfit with our publication example and discuss the output that is generated. countfit estimates each of the models, so our command includes the specification of the outcome, the $x$ variables, and the $z$ variables for zero-inflated models. We do not use any of the options that limit the output that is generated. use couart4, clear countfit art i.female i.married kid5 phd mentor, inflate(i.mentor i.female) --------------------------------------------------------------------------------\rVariable | PRM NBRM ZIP ZINB -------------------------------+------------------------------------------------\rart |\rGender: 1=female 0=male | Female | 0.799 0.805 0.802 0.814 | -4.11 -2.98 -3.48 -2.59 | Married: 1=yes 0=no | Married | 1.168 1.162 1.131 1.146 | 2.53 1.83 1.85 1.68 # of kids \u003c 6 | 0.831 0.838 0.849 0.843 | -4.61 -3.32 -3.75 -3.27 PhD prestige | 1.013 1.015 0.994 1.002 | 0.49 0.42 -0.22 0.05 Mentor's # of articles | 1.026 1.030 1.020 1.025 | 12.73 8.38 8.78 7.04 Constant | 1.356 1.292 1.891 1.557 | 2.96 1.85 5.62 3.10 -------------------------------+------------------------------------------------\rlnalpha |\rConstant | 0.442 0.309 | -6.81 -6.71 -------------------------------+------------------------------------------------\rinflate |\r| Mentor's # of articles | 1 | 1.121 1.153 | 0.26 0.24 2 | 0.678 0.565 | -0.93 -0.89 3 | 0.351 0.129 | -2.00 -1.17 4 | 0.139 0.000 | -2.40 -0.02 5 | 0.341 0.119 | -2.13 -1.13 6 | 0.343 0.159 | -1.88 -1.21 7 | 0.369 0.203 | -1.68 -1.19 8 | 0.380 0.223 | -1.75 -1.21 9 | 0.276 0.089 | -1.68 -0.81 10 | 0.066 0.000 | -1.46 -0.00 11 | 0.097 0.000 | -1.57 -0.00 12 | 0.096 0.000 | -1.49 -0.00 13 | 0.647 0.605 | -0.68 -0.54 14 | 0.172 0.006 | -1.59 -0.11 15 | 0.146 0.000 | -1.38 -0.02 16 | 0.033 0.000 | -0.76 -0.00 17 | 0.000 0.000 | -0.00 -0.00 18 | 0.075 0.000 | -1.12 -0","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:7:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"8 Conclusion Count outcomes are not categorical variables in the sense that binary, ordinal, and nominal variables are. Although they are discrete, there is no sense in which the values assigned to a count variable are arbitrary. Indeed, count outcomes support both additive and multiplicative operations, and the number of potential outcome values is not limited. Even though count variables are thus not categorical, we hope that it is clear how the study of count outcomes can benefit from the same strategies of modeling and interpretation that we introduced in earlier chapters. Instead of only offering interpretations in terms of expected counts, we offered interpretations based on the probabilities of observing a specific count or range of counts. Also, as we showed, the outcomes of 0 can be conceptualized and even modeled as though they are categorically different from the process by which positive counts accumulate. Consequently, as with other types of outcomes in this book, we can learn much by thinking about and testing our ideas about how outcome values are generated. The combination of Stata with the extra tools we provide in this book makes it easy to interpret the relationship between independent variables and count outcomes in ways that are much more effective than vast tables of untransformed coefficients. ","date":"2024-04-01","objectID":"/chapter_9-models-for-count-outcomes/:8:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_9 ：Models for count outcomes","uri":"/chapter_9-models-for-count-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"In this chapter, we focus on the multinomial logit model (MNL), which is the most frequently used nominal regression model.","date":"2024-03-16","objectID":"/chapter_8-models-for-nominal-outcomes/","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_8 ：Models for nominal outcomes","uri":"/chapter_8-models-for-nominal-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"An outcome is nominal when the categories are assumed to be unordered. For example, marital status can be grouped nominally into the categories of divorced, never married, married, or widowed. Occupations might be organized as professional, white collar, blue collar, craft, and menial. Other examples include reasons for leaving the parents’ home, the organizational context of scientific work such as industry, government, and academia, and the choice of language in a multilingual society. Further, in some cases, you might prefer to treat an outcome as nominal even though it is ordered, ordered on multiple dimensions, or partially ordered. For example, if the response categories are strongly agree, agree, disagree, strongly disagree, and don’t know, the category “don’t know” invalidates models for ordinal outcomes. Or, you might decide to use a nominal regression model when the assumption of parallel regressions is rejected. In general, if you have concerns about the ordinality of the dependent variable, the potential loss of efficiency in using models for nominal outcomes is outweighed by avoiding potential bias. In this chapter, we focus on the multinomial logit model (MNL), which is the most frequently used nominal regression model. This model essentially fits separate binary logits for each pair of outcome categories. Next, we consider the stereotype logistic regression model. Although this model is often used for ordinal outcomes, it is closely related to the MNL. These models assume that the data are case-specific, meaning that each independent variable has one value for each individual. Examples of such variables are an individual’s race or education. After that, we consider several models that include alternative-specific data. Alternative-specific variables vary not only by individual but also by the alternative. For example, if a commuter is selecting one of three modes of travel, an alternative-specific predictor might be her travel time using each alternative. We use “alternative” to refer to a possible outcome. Sometimes we refer to an alternative as an outcome, a category, or a comparison group to be consistent with the usual terminology for a model or the output generated by Stata. The term “choice” refers to the alternative that is actually observed, which can be thought of as the “most preferred” alternative. For example, if the dependent variable is the party voted for in the last presidential election, the alternatives might be Republican, Democrat, and Independent. If a person voted for the alternative of Democrat, we would say that the choice for this case is Democrat. But you should not infer from the term “choice” that the models we describe can be used only for data where the outcome occurs through a process of choice. For example, if we were modeling the type of injuries that people entering the emergency room of a hospital have, we would use the term “choice” even though the injury sustained is unlikely to be a choice. We begin by discussing the MNL, where the biggest challenge is that the model includes many parameters and so it is easy to be overwhelmed by the complexity of the results. This complexity is compounded by the nonlinearity of the model, which leads to the same difficulties of interpretation found for models in prior chapters. Although fitting the model is straightforward, interpretation involves challenges that are the focus of this chapter. We begin by reviewing the statistical model, followed by a discussion of testing, fit, and finally, methods of interpretation. For a more technical discussion of the model, see Long (1997). As discussed in chapter 1, you can obtain sample do-files and data files by downloading the spostl3_do package. The outcome for the primary example we have chosen for this chapter is political party affiliation, collected from a survey that used the categories strong Democrat, Democrat, Independent, Republican, and strong Republican. Although this variable may init","date":"2024-03-16","objectID":"/chapter_8-models-for-nominal-outcomes/:0:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_8 ：Models for nominal outcomes","uri":"/chapter_8-models-for-nominal-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1 The multinomial logit model The MNLM can be thought of as simultaneously fitting binary logits for all comparisons among the alternatives. For example, let party3 be a categorical variable with the outcomes D for Democrat, I for Independent, and R for Republican. Assume that there is one independent variable measuring income in $1,000$s. We can examine the effect of income on party3 by fitting three binary logits: $$\\ln\\frac{\\Pr\\left(D\\mid\\mathbf{x}\\right)}{\\Pr\\left(I\\mid\\mathbf{x}\\right)}=\\beta_{0,\\mathrm{D}\\mid I}+\\beta_{1,\\mathrm{D}\\mid I}\\text{income}$$ $$\\ln\\frac{\\Pr\\left(R\\mid\\mathbf{x}\\right)}{\\Pr\\left(I\\mid\\mathbf{x}\\right)}=\\beta_{0,\\mathrm{R}\\mid I}+\\beta_{1,\\mathrm{R}\\mid I}\\text{income}$$ $$\\ln\\frac{\\Pr(D\\mid\\mathbf{x})}{\\Pr(R\\mid\\mathbf{x})}=\\beta_{0,\\mathrm{D}\\mid\\mathrm{R}}+\\beta_{1,\\mathrm{D}\\mid\\mathrm{R}}\\text{income}$$ Where the subscripts to the $\\beta ’ s$ indicate which comparison is being made. For example, $\\beta_{\\mathrm{1,D|I}}$ is the coefficient for the first independent variable for the comparison of Democrat and Independent. The three binary logits include redundant information. Because $\\ln a/b=\\ln a-\\ln b$, the following equality must hold: $$\\ln\\frac{\\Pr\\left(D\\mid\\mathbf{x}\\right)}{\\Pr\\left(I\\mid\\mathbf{x}\\right)}-\\ln\\frac{\\Pr\\left(R\\mid\\mathbf{x}\\right)}{\\Pr\\left(I\\mid\\mathbf{x}\\right)}=\\ln\\frac{\\Pr\\left(D\\mid\\mathbf{x}\\right)}{\\Pr\\left(R\\mid\\mathbf{x}\\right)}$$ This implies that $$\\beta_{0,\\mathrm{D}|1}-\\beta_{0,\\mathrm{R}|1}=\\beta_{0,\\mathrm{D}|\\mathrm{R}}$$ $$\\beta_{\\mathrm{1,D|I}}-\\beta_{\\mathrm{1,R|I}}=\\beta_{\\mathrm{1,D|R}}$$ In general, with $J$ alternatives, only $J - 1$ binary logits need to be fit. These $J - 1$ logits are referred to as a minimal set. Estimates for the remaining coefficients can be computed using equalities of the sort shown in (8.1). 当你在学习这些概念时，确保理解是至关重要的，我会尽力详细解释。首先，我们来看 MNLM（多项式 Logit 模型）。这个模型的核心思想是同时为所有可比较的选择（例如，民主党、独立派、共和党）拟合二元逻辑回归模型。这意味着我们想要了解一些因素（例如收入）对于一个人选择哪个政党的影响。 首先，模型中使用了一些数学符号和公式，我们会一步步解释。首先，公式中的 $\\ln$ 表示自然对数，它可以将一个概率转换为一个连续的值，这样我们可以更容易地进行建模和分析。$\\Pr$ 表示概率，它是一个事件发生的可能性。$\\beta$ 是我们要估计的参数，它们告诉我们不同因素对于选择的影响程度。 让我们以一个具体的例子来解释。假设我们有一个名为 party3 的变量，它有三种取值：D（民主党）、I（独立派）和 R（共和党）。我们想知道收入对于一个人选择这三个政党的影响。 我们将收入用 $\\text{income}$ 表示，单位是千美元。然后，我们可以分别观察收入对于三种政党的影响： 民主党与独立派之间的比较： $$\\ln\\frac{\\Pr(D|\\mathbf{x})}{\\Pr(I|\\mathbf{x})}=\\beta_{0,\\mathrm{D}|I}+\\beta_{1,\\mathrm{D}|I}\\text{income}$$ 共和党与独立派之间的比较： $$\\ln\\frac{\\Pr(R|\\mathbf{x})}{\\Pr(I|\\mathbf{x})}=\\beta_{0,\\mathrm{R}|I}+\\beta_{1,\\mathrm{R}|I}\\text{income}$$ 民主党与共和党之间的比较： $$\\ln\\frac{\\Pr(D|\\mathbf{x})}{\\Pr(R|\\mathbf{x})}=\\beta_{0,\\mathrm{D}|R}+\\beta_{1,\\mathrm{D}|R}\\text{income}$$ 这里的 $\\mathbf{x}$ 表示其他可能影响选择的变量，例如教育水平、地理位置等等。 让我们详细解释每个公式： 民主党与独立派之间的比较： 这个公式告诉我们，我们想知道一个人更倾向于选择民主党还是独立派的可能性。我们用 $\\ln\\frac{\\Pr(D|\\mathbf{x})}{\\Pr(I|\\mathbf{x})}$ 表示这个可能性的对数比率。这里 $\\Pr(D|\\mathbf{x})$ 是在给定其他因素 $\\mathbf{x}$ 的情况下选择民主党的概率，$\\Pr(I|\\mathbf{x})$ 是选择独立派的概率。$\\beta_{0,\\mathrm{D}|I}$ 是一个常数项，$\\beta_{1,\\mathrm{D}|I}$ 是收入对于选择民主党的影响系数。 共和党与独立派之间的比较： 这个公式告诉我们，我们想知道一个人更倾向于选择共和党还是独立派的可能性。同样地，我们用 $\\ln\\frac{\\Pr(R|\\mathbf{x})}{\\Pr(I|\\mathbf{x})}$ 表示这个可能性的对数比率。其中 $\\Pr(R|\\mathbf{x})$ 是在给定其他因素 $\\mathbf{x}$ 的情况下选择共和党的概率，$\\Pr(I|\\mathbf{x})$ 是选择独立派的概率。$\\beta_{0,\\mathrm{R}|I}$ 是一个常数项，$\\beta_{1,\\mathrm{R}|I}$ 是收入对于选择共和党的影响系数。 民主党与共和党之间的比较： 这个公式告诉我们，我们想知道一个人更倾向于选择民主党还是共和党的可能性。同样地，我们用 $\\ln\\frac{\\Pr(D|\\mathbf{x})}{\\Pr(R|\\mathbf{x})}$ 表示这个可能性的对数比率。其中 $\\Pr(D|\\mathbf{x})$ 是在给定其他因素 $\\mathbf{x}$ 的情况下选择民主党的概率，$\\Pr(R|\\mathbf{x})$ 是选择共和党的概率。$\\beta_{0,\\mathrm{D}|R}$ 是一个常数项，$\\beta_{1,\\mathrm{D}|R}$ 是收入对于选择民主党的影响系数。 现在，让我们结合一个具体的例子来说明这些公式。 假设我们有两个人，他们的收入分别为 50,000 美元和 100,000 美元。我们想知道他们更倾向于选择哪个政党。我们还知道他们的教育水平、居住地等其他因素。 根据我们的模型，我们可以计算出两个人选择民主党、独立派和共和党的可能性。然后，我们可以使用模型中的系数来计算收入对于他们的影响。 例如，对于第一个人： 民主党与独立派之间的可能性对数比率是 0.5 共和党与独立派之间的可能性对数比率是 -0.3 民主党与共和党之间的可能性对数比率是 0.8 现在，我们可以使用模型中的系数来计算收入对于这些可能性的影响。这样，我们就可以了解收入对于不同政党偏好的影响程度。 首先，我们注意到这些公式之间存在一些冗余，因为对数的性质。作者提到了一个重要的等式，即 $\\ln a/b = \\ln a - \\ln b$。这意味着我们可以用一个公式推导出其他的。在这里，作者使用这个等式来说明我们不需要拟合所有可能的组合，而只需拟合其中一部分，然后通过这些等式来推断其他部分。这样，我们","date":"2024-03-16","objectID":"/chapter_8-models-for-nominal-outcomes/:1:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_8 ：Models for nominal outcomes","uri":"/chapter_8-models-for-nominal-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.1 Formal statement of the model The MNLM can be written as $$\\ln\\Omega_{m|b}\\left(\\mathbf{x}\\right)=\\ln\\frac{\\Pr\\left(y=m\\mid\\mathbf{x}\\right)}{\\Pr\\left(y=b\\mid\\mathbf{x}\\right)}=\\text{x}\\beta_{m|b}\\quad\\mathrm{for~}m=1\\mathrm{~to~}J$$ Where b is the base outcome, sometimes called the reference category. As $\\ln\\Omega_{b|b}(x) = \\ln 1 = 0$, it follows that $\\beta_{b|b}= 0$. That is, the log odds of an outcome compared with itself is always 0, and thus the effects of any independent variables must also be 0. These J equations can be solved to compute the probabilities for each outcome: $$\\Pr\\left(y=m\\mid\\mathbf{x}\\right)=\\frac{\\exp\\left(\\mathbf{x}\\beta_{m\\mid b}\\right)}{\\sum_{j=1}^{j}\\exp\\left(\\mathbf{x}\\beta_{j\\mid b}\\right)}$$ The probabilities will be the same regardless of the base outcome b that is used. For example, suppose that you have three outcomes and fit the model with alternative 1 as the base, where you would obtain estimates $\\hat\\beta_{2|1}$ and $\\hat\\beta_{3|1}$, with $\\beta_{1|1} = 0$. The probability equation is $$\\Pr\\left(y=m\\mid\\mathbf{x}\\right)=\\frac{\\exp\\left(\\mathbf{x}\\beta_{m\\mid1}\\right)}{\\sum_{j=1}^{J}\\exp\\left(\\mathbf{x}\\beta_{j\\mid1}\\right)}$$ If someone else set up the model with base outcome 2, they would obtain estimates $\\hat\\beta_{3|2}$ and $\\beta_{1|2} = 0$. Their probability equation would be $$\\Pr\\left(y=m\\mid\\mathbf{x}\\right)=\\frac{\\exp\\left(\\mathbf{x}\\beta_{m\\mid2}\\right)}{\\sum_{j=1}^{J}\\exp\\left(\\mathbf{x}\\beta_{j\\mid2}\\right)}$$ The estimated parameters are different because they are estimating different things, but they produce exactly the same predictions. Confusion arises only if you are not clear about which parameterization you are using. We return to this issue when we discuss how Stata’s mlogit parameterizes the model in the next section. 在这段文字中，我们深入了解了多项 Logit 模型（MNLM）的关键概念和公式。 首先，MNLM 可以写成如下形式： $$\\ln\\Omega_{m|b}\\left(\\mathbf{x}\\right)=\\ln\\frac{\\Pr\\left(y=m\\mid\\mathbf{x}\\right)}{\\Pr\\left(y=b\\mid\\mathbf{x}\\right)}=\\text{x}\\beta_{m|b}\\quad\\mathrm{for~}m=1\\mathrm{~to~}J$$ 这个方程告诉我们，选择结果 $m$ 相对于基准结果 $b$ 的对数比率 $\\ln\\Omega_{m|b}$ 可以通过自变量 $\\mathbf{x}$ 的线性函数来表示，系数为 $\\beta_{m|b}$。作者指出了一个重要的性质，即基准结果相对于自身的对数比率始终为 0，因此 $\\beta_{b|b}=0$。这意味着相对于自身的对数几率永远为 0，因此任何自变量的影响也必须为 0。 在这个公式中，$\\Omega_{m|b}$ 表示选择结果 $m$ 相对于基准结果 $b$ 的对数比率。$\\mathbf{x}$ 是自变量向量，$\\beta_{m|b}$ 是相应的系数。选择不同的基准结果 $b$ 会导致不同的参数估计值。 举个例子，假设我们正在研究一个人选择早餐的情况，可能选择吃麦片、吃鸡蛋或吃面包。我们使用年龄和性别作为自变量来预测选择结果。如果我们选择吃麦片作为基准结果，我们将得到一组系数估计值，如 $\\beta_{2|1}$ 和 $\\beta_{3|1}$，分别对应吃鸡蛋和吃面包相对于吃麦片的对数比率。然而，如果我们选择吃鸡蛋作为基准结果，我们将得到另一组系数估计值，如 $\\beta_{1|2}$ 和 $\\beta_{3|2}$，分别对应吃麦片和吃面包相对于吃鸡蛋的对数比率。 总之，不同基准结果会导致不同的系数估计值，但模型的预测能力并不会受到影响。换句话说，无论我们选择哪个结果作为基准，模型都会产生相同的预测结果。因此，在实际应用中，我们通常不需要过多地关注选择基准结果的影响，只需专注于模型的预测和解释能力即可。 接着，我们了解了如何使用这些方程来计算每个结果的概率。概率公式为： $$\\Pr\\left(y=m\\mid\\mathbf{x}\\right)=\\frac{\\exp\\left(\\mathbf{x}\\beta_{m\\mid b}\\right)}{\\sum_{j=1}^{J}\\exp\\left(\\mathbf{x}\\beta_{j\\mid b}\\right)}$$ 这个公式告诉我们，在给定自变量 $\\mathbf{x}$ 的情况下，选择结果 $m$ 的概率是多少。分子是选择结果 $m$ 的对数几率的指数形式，分母是所有可能结果的对数几率的指数形式之和。这种形式确保了所有概率之和为 1。 举个例子，我们考虑了三个可能的选择，并且将第一个选择作为基准结果。我们得到了估计值 $\\hat\\beta_{2|1}$ 和 $\\hat\\beta_{3|1}$，其中 $\\beta_{1|1}=0$。概率方程为： $$\\Pr\\left(y=m\\mid\\mathbf{x}\\right)=\\frac{\\exp\\left(\\mathbf{x}\\beta_{m\\mid1}\\right)}{\\sum_{j=1}^{J}\\exp\\left(\\mathbf{x}\\beta_{j\\mid1}\\right)}$$ 如果另外有人将第二个选择作为基准结果建立模型，他们会得到估计值 $\\hat\\beta_{3|2}$ 和 $\\beta_{1|2}=0$。他们的概率方程与之前的形式相同，但系数向量会有所不同。 这种灵活性使得在实际应用中，我们可以自由选择基准结果，而不会影响到模型的预测能力。这也说明了参数估计值的变化不会影响到模型的预测结果，只有在不清楚使用的参数化方式时才会产生混淆。 接下来，我们可以进一步探讨不同基准结果之间的参数估计值的变化，以及这种变化对模型的解释和应用的影响。 在实际应用中，选择不同的基准结果可能会导致不同的参数估计值。例如，在我们的例子中，选择第一个选择作为基准结果会得到一组估计值，而选择第二个选择作为基准结果会得到另一组估计值。这是因为不同的基准结果会导致对比较的选择组合进行重新参数化。 然而，重要的是要理解，虽然参数估计值可能会因基准结果的选择而有所不同，但模型的预测结果是一样的。这是因为参数估计值的变化只会影响到模型的参数化方式，而不会影响到模型的预测能力。因此，在实际应用中，我们通常不需要过于关注基准结果的选择，只需专注于模型的预测能力和解释能力即可。 举个例子，假设我们想预测一个人的早餐选择：吃麦片、吃鸡蛋还是吃面包。我们用年龄和性别作为特征来预测。假设我们选择吃麦片作为基准结果。然后我们会计算出吃鸡蛋和吃面包相对于吃麦片的对数比率，以及吃鸡蛋和吃面包的概率。即使我们选择了不同的基准结果，我们仍然可以得到相同的预测结果。 假设我们继续使用早餐选择的例子，但这次我们选择吃鸡蛋作为基准结果。那么，我们将重新计算吃麦片和吃面包相对于吃鸡蛋的对数比率，并且重新计算吃麦片和吃面包的概率。尽管参数估计值可能会不同，但我们得到的预测结果仍","date":"2024-03-16","objectID":"/chapter_8-models-for-nominal-outcomes/:1:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_8 ：Models for nominal outcomes","uri":"/chapter_8-models-for-nominal-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2 Estimation using the mlogit command The MNLM is fit with the following command and its basic options: mlogit depvar [indepvars] [if] [in] [weight], baseoutcome(#) vce(vcetype) For other options, run help mlogit. In our experience, the model converges quickly, even when there are many outcome categories and independent variables. Variable lists depvar is the dependent variable. The specific values taken on by the dependent variable are irrelevant as long as they are integers. For example, if you had three outcomes, you could use the values 1, 2, and 3 or -1, 0, and 999. Nevertheless, to avoid confusion, we strongly recommend coding your outcome as consecutive integers beginning with 1. indepvars is a list of independent variables. If indepvars is not included, Stata fits a model with only constants. Specifying the estimation sample The if and in qualifiers can be used to restrict the estimation sample. For example, if you want to fit the model with only white respondents, use the command mlogit party i.edu income10 if black==0. Listwise deletion. Stata excludes cases in which there are missing values for any of the variables. Accordingly, if two models are fit using the same dataset but have different sets of independent variables, it is possible to have different samples. We recommend that you use mark and markout (discussed in chapter 3) to explicitly remove cases with missing data. Weights and complex samples mlogit can be used with fweights, pweights, and iweights. Survey estimation is supported. See chapter 3 for details. Options noconstant excludes the constant terms from the model. baseoutcome(#) specifies the value of depvar that is the base outcome (that is, reference group) for the coefficients that are listed. This determines how the model is parameterized. If baseoutcome() is not specified, the most frequent outcome in the estimation sample is used as the base. The base is reported as (base outcome) in the table of estimates. vce(vcetype) specifies the type of standard errors to be computed. See section 3.9.1 for details. rrr reports the estimated coefficients transformed to relative-risk ratios, defined as exp(b) rather than b, along with standard errors and confidence intervals for these ratios. Relative risk ratios are also referred to as odds ratios. ","date":"2024-03-16","objectID":"/chapter_8-models-for-nominal-outcomes/:2:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_8 ：Models for nominal outcomes","uri":"/chapter_8-models-for-nominal-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.1 Example of MNLM The 1992 American National Election Study asked respondents to indicate their political party using one of eight categories. We used these to create party with five categories: strong Democrat (1 = StrDem), Democrat (2 = Dem), independent (3 = Indep), Republican (4 = Rep), and strong Republican (5 = StrRep): use partyid4, clear tabulate party, miss Party ID | Freq. Percent Cum.\r------------+-----------------------------------\rStrDem | 266 19.25 19.25\rDem | 427 30.90 50.14\rIndep | 151 10.93 61.07\rRep | 369 26.70 87.77\rStrRep | 169 12.23 100.00\r------------+-----------------------------------\rTotal | 1,382 100.00\rTo simplify our notation, at times we abbreviate StrDem as SD, Dem as D, Indep as I, Rep as R, and StrRep as SR. Five regressors are included in the model: age, income, race (indicated as black or not), gender, and education (measured as not completing high school, completing high school but not college, and completing college). Descriptive statistics for the continuous and binary variables are: sum age income black female Variable | Obs Mean Std. dev. Min Max\r-------------+---------------------------------------------------------\rage | 1,382 45.94645 16.78311 18 91\rincome | 1,382 37.45767 27.78148 1.5 131.25\rblack | 1,382 .1374819 .34448 0 1\rfemale | 1,382 .4934877 .5001386 0 1\rThe distribution of educational attainment is tabulate educ, miss Level of |\reducation | Freq. Percent Cum.\r------------+-----------------------------------\rnot hs grad | 222 16.06 16.06\rhs only | 802 58.03 74.10\rcollege | 358 25.90 100.00\r------------+-----------------------------------\rTotal | 1,382 100.00\rUsing these variables, we fit the model mlogit party age income i.black i.female i.educ Because educ has three categories, i.educ is expanded to 2.educ and 3.educ, leading to the following minimal set of equations: \\begin{align*} \\ln\\Omega_{\\mathrm{SD|SR}}\\left(\\mathbf{x_i}\\right)=\\beta_{0,\\mathrm{SD|SR}}+\\beta_{1,\\mathrm{SD|SR}}\\mathbf{age}+\\beta_{2,\\mathrm{SD|SR}}\\mathbf{income}+\\beta_{3,\\mathrm{SD|SR}}\\mathbf{black} \\ \\end{align*} \\begin{align*} \\qquad +\\beta_{4,\\mathrm{SD|SR}}\\mathbf{female}+\\beta_\\text{5,SD|SR}2.\\mathbf{educ}+\\beta_\\text{6,SD|SR}3.\\mathbf{educ} \\end{align*} \\begin{align*} \\ln\\Omega_{\\mathrm{D|SR}}\\left(\\mathbf{x_i}\\right)=\\beta_{0,\\mathrm{D|SR}}+\\beta_{1,\\mathrm{D|SR}}\\mathbf{age}+\\beta_{2,\\mathrm{D|SR}}\\mathbf{incone}+\\beta_{3,\\mathrm{D|SR}}\\mathbf{black}\\ \\end{align*} \\begin{align*} \\qquad +\\beta_{4,\\mathrm{D}|\\mathrm{SR}}\\mathbf{female}+\\beta_{5,\\mathrm{D}|\\mathrm{SR}}2.\\textbf{educ}+\\beta_{6,\\mathrm{D}|\\mathrm{SR}}3.\\textbf{educ} \\end{align*} \\begin{align*} \\ln\\Omega_{\\mathrm{I|SR}}\\left(\\mathbf{x_i}\\right)=\\beta_{0,\\mathrm{I|SR}}+\\beta_{1,\\mathrm{I|SR}}\\mathbf{age}+\\beta_{2,\\mathrm{I|SR}}\\mathbf{income}+\\beta_{3,\\mathrm{I|SR}}\\mathbf{b}\\text{lack} \\ \\end{align*} \\begin{align*} \\qquad +\\beta_{4,1|\\text{SR}}\\mathbf{female}+\\beta_\\text{5,1|SR}2.\\mathbf{educ}+\\beta_\\text{6,1[SR}3.\\mathbf{educ} \\end{align*} \\begin{align*} \\ln\\Omega_{\\mathrm{R|SR}}\\left(\\mathbf{x_i}\\right)=\\beta_{0,R|SR}+\\beta_{1,R|\\mathrm{SR}}\\mathbf{age}+\\beta_{2,R|\\mathrm{SR}}\\mathbf{income}+\\beta_{3,\\mathrm{R|SR}}\\mathbf{black} \\ \\end{align*} \\begin{align*} \\qquad +\\beta_{4,R|\\text{SR}}\\mathbf{female}+\\beta_\\text{5,R|SR}2.\\mathbf{educ}+\\beta_\\text{6,R|SR}3.\\mathbf{educ} \\end{align*} where the fifth outcome, StrRep, is the base. The (lengthy) results are mlogit party age income i.black i.female i.educ, base(5) vsquish Iteration 0: Log likelihood = -2116.5357 Iteration 1: Log likelihood = -1973.8136 Iteration 2: Log likelihood = -1961.2327 Iteration 3: Log likelihood = -1960.9125 Iteration 4: Log likelihood = -1960.9107 Iteration 5: Log likelihood = -1960.9107 Multinomial logistic regression Number of obs = 1,382\rLR chi2(24) = 311.25\rProb \u003e chi2 = 0.0000\rLog likelihood = -1960.9107 Pseudo R2 = 0.0735\r------------------------------------------------------------------------------\rparty | Coefficient Std. err. z P\u003e|z| [95% conf. interv","date":"2024-03-16","objectID":"/chapter_8-models-for-nominal-outcomes/:2:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_8 ：Models for nominal outcomes","uri":"/chapter_8-models-for-nominal-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.2 Selecting different base outcomes By default, mlogit sets the base outcome to the alternative with the most observations in the estimation sample. Or, as illustrated in the last example, you can select the base with the option baseoutcome(), which can be abbreviated simply as b(). mlogit reports the coefficients for each independent variable on each outcome relative to the base outcome. Although mlogit only shows coefficients comparing outcomes with the base outcome, it is important to examine the coefficients for other pairs of outcomes. For example, you might be interested in the effect of being female on being Dem compared with Indep (that is, $\\beta_{female,D|I}$), which was not estimated in the output above. Although this coefficient can be estimated by running mlogit with a different base outcome (for example, mlogit party ..., base(3)), it is easier to use listcoef, which presents estimates for all pairs of outcome categories. Because listcoef can generate lengthy output, we illustrate several options that limit which coefficients are listed. First, if you specify a list of variables, only coefficients for those variables are shown. For example, mlogit party age income i.black i.female i.educ, base(5) vsquish listcoef female, help mlogit (N=1382): Factor change in the odds of party Variable: 1.female (sd=0.500)\r------------------------------------------------------------------------------\r| b z P\u003e|z| e^b e^bStdX\r-----------------------------+------------------------------------------------\rStrDem vs Dem | -0.2408 -1.443 0.149 0.786 0.887\rStrDem vs Indep | 0.1889 0.889 0.374 1.208 1.099\rStrDem vs Rep | -0.0079 -0.044 0.965 0.992 0.996\rStrDem vs StrRep | 0.2368 1.101 0.271 1.267 1.126\rDem vs StrDem | 0.2408 1.443 0.149 1.272 1.128\rDem vs Indep | 0.4298 2.217 0.027 1.537 1.240\rDem vs Rep | 0.2330 1.587 0.112 1.262 1.124\rDem vs StrRep | 0.4777 2.493 0.013 1.612 1.270\rIndep vs StrDem | -0.1889 -0.889 0.374 0.828 0.910\rIndep vs Dem | -0.4298 -2.217 0.027 0.651 0.807\rIndep vs Rep | -0.1968 -0.983 0.326 0.821 0.906\rIndep vs StrRep | 0.0479 0.203 0.839 1.049 1.024\rRep vs StrDem | 0.0079 0.044 0.965 1.008 1.004\rRep vs Dem | -0.2330 -1.587 0.112 0.792 0.890\rRep vs Indep | 0.1968 0.983 0.326 1.217 1.103\rRep vs StrRep | 0.2447 1.268 0.205 1.277 1.130\rStrRep vs StrDem | -0.2368 -1.101 0.271 0.789 0.888\rStrRep vs Dem | -0.4777 -2.493 0.013 0.620 0.787\rStrRep vs Indep | -0.0479 -0.203 0.839 0.953 0.976\rStrRep vs Rep | -0.2447 -1.268 0.205 0.783 0.885\r------------------------------------------------------------------------------\rb = raw coefficient\rz = z-score for test of b=0\rP\u003e|z| = p-value for z-test\re^b = exp(b) = factor change in odds for unit increase in X\re^bStdX = exp(b*SD of X) = change in odds for SD increase in X\rNotice that none of the coefficients with the base outcome StrDem are statistically significant, even at the 0.10 level. Although these are the only coefficients that are shown in the output from mlogit, two of the coefficients relative to outcome Dem are significant at the 0.05 level and two more are almost significant at the 0.10 level. Before concluding that a variable has no effect, you should examine all the contrasts and compute an omnibus test for the effect of a variable, as discussed in section 8.3.2. By default, listcoef shows coefficients for all contrasts. For example, it even shows you the coefficient comparing StrRep with Rep, which equals -0.2447, and the coefficient comparing Rep with StrRep, which equals 0.2447. You can limit which contrasts are shown with the gt, lt, or adjacent options. With the gt option, only coefficients in which the category number of the first alternative is greater than that of the second are shown; lt shows comparisons when the first alternative is less than the second; and adjacent limits coefficients to those from adjacent outcomes. For example, listcoef income age, lt adjacent mlogit (N=1382): Factor change in the odds of party Variable: age (sd=16.783)\r---------------","date":"2024-03-16","objectID":"/chapter_8-models-for-nominal-outcomes/:2:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_8 ：Models for nominal outcomes","uri":"/chapter_8-models-for-nominal-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.3 Predicting perfectly The mlogit command handles perfect prediction in the same way as the ologit and oprobit commands, but somewhat differently than estimation commands for binary models, logit and probit. logit and probit automatically remove the observations that imply perfect prediction and compute the estimates accordingly. mlogit and oprobit keep these observations in the model, set the χ² statistics for the problem variables to 0, warn that standard errors are questionable, and indicate that a given number of observations are completely determined. You should refit the model after excluding the problem variable and deleting the observations that imply the perfect predictions. Using tabulate to cross-tabulate the problem variable and the dependent variable should reveal the combination of values that results in perfect prediction. **mlogit命令处理完美预测的方式与ologit和oprobit命令相同，但与二元模型的估计命令logit和probit有些不同。logit和probit会自动删除导致完美预测的观测，并相应地计算估计值。mlogit和oprobit会保留这些观测值，并将问题变量的χ²统计量设为0，警告标准误差可能存在问题，并指出一定数量的观测完全确定。在排除问题变量并删除导致完美预测的观测后，应重新拟合模型。使用tabulate**对问题变量和因变量进行交叉表分析应该可以显示出导致完美预测的值组合。 ","date":"2024-03-16","objectID":"/chapter_8-models-for-nominal-outcomes/:2:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_8 ：Models for nominal outcomes","uri":"/chapter_8-models-for-nominal-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3 Hypothesis testing In the MNLM, you can test individual coefficients with the reported z statistics, with a Wald test by using test, or with an LR test by using lrtest. As the methods of testing a single coefficient that were discussed in chapters 3, 5, and 7 apply fully, they are not considered further here. However, in the MNLM, there are new reasons for testing sets of coefficients. First, testing that a variable has no effect requires a test that J - 1 coefficients in a minimal set are simultaneously equal to 0. Second, testing whether the independent variables as a group differentiate between two alternatives requires a test of K coefficients, where K is the number of independent variables, including those created by expanding factor-variable notation. In this section, we focus on these two kinds of tests. Caution regarding specification searches: Given the difficulties of interpretation that are associated with the MNLM, it is tempting to search for a more parsimonious model by excluding variables or combining outcome categories based on a sequence of tests. Such a search requires great care. First, these tests involve multiple coefficients. Although the overall test might indicate that as a group the coefficients are not significantly different from 0, an individual coefficient could still be substantively and statistically significant. Accordingly, you should examine the individual coefficients involved in each test before deciding to revise your model. Second, as with all searches that use repeated, sequential tests, there is a danger of overfitting the model to the data. Whenever model specifications are determined based on prior testing using the same data, significance levels should be used only as rough guidelines. ","date":"2024-03-16","objectID":"/chapter_8-models-for-nominal-outcomes/:3:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_8 ：Models for nominal outcomes","uri":"/chapter_8-models-for-nominal-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3.1 mlogtest for tests of the MNLM Although the tests in this section can be computed using test or lrtest, in practice, this is tedious. The mlogtest command by Freese and Long (2000) makes the computation of these tests easy. The syntax is mlogtest [varlist] [, lr wald set([setname=] varlist [\\ [setname=] varlist [\\...]) combine lrcombine iia hausman smhsiao detail base all] varlist indicates the variables for which tests of significance should be computed. If no varlist is given, tests are run for all independent variables. Options lr requests a likelihood-ratio (LR) test for each variable in varlist. If varlist is not specified, tests for all variables are computed. wald requests a Wald test for each variable in varlist. If varlist is not specified, tests for all variables are computed. set([setname1=] varlist1 [\\ [setname2=] varlist2 ] [\\ ...])** specifies that a set of variables be considered together for the LR test or Wald test. \\ is used to indicate that a new set of variables is being specified. For example, **mlogtest, lr set(age income \\ 2.educ 3.educ)** computes one LR test for the hypothesis that the effects of age and income are jointly 0 and a second LR test that the effects of 2.educ and 3.educ are jointly 0. The option **set()`` is used to label the output. combine requests Wald tests of whether dependent categories can be combined. lrcombine requests LR tests of whether dependent categories can be combined. These tests use constrained estimation and overwrite constraint 999 if it is already defined. For other options, type help mlogtest. ","date":"2024-03-16","objectID":"/chapter_8-models-for-nominal-outcomes/:3:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_8 ：Models for nominal outcomes","uri":"/chapter_8-models-for-nominal-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3.2 Testing the effects of the independent variables With $J$ dependent categories, there are $J-1$ nonredundant coefficients associated with each independent variable $x_k$. For example, in our model of party affiliation, there are four coefficients associated with female: $\\beta_{female,SD|SR}$, $\\beta_{female,D|SR}$, $\\beta_{female,I|SR}$, and $\\beta_{female,R|SR}$. The hypothesis that $x_k$ does not affect the dependent variable can be written as: where $b$ is the base outcome. Because $\\beta_{k,b|b}$ is necessarily 0, the hypothesis imposes constraints on $J - 1$ parameters. This hypothesis can be tested with either a Wald or an LR test. Likelihood-ratio test The LR test involves fitting the full model that includes all the variables, resulting in the LR statistic $LR \\chi_{F}^{2};$ . fitting the restricted model th at excludes variable $x_k$,resulting in $\\mathrm{LR}\\chi_{R}^{2}$; computing the difference $\\mathrm{LR}\\chi_{R\\mathrm{vs}F}^{2}=\\mathrm{LR}\\chi_{F}^{2}-\\mathrm{LR}\\chi_{R}^{2}$, which is distributed as chi-squared with $J-1$ degrees of freedom if the null hypothesis is true. This can be done using lrtest by first fitting the full model and storing the estimates: 当有 $J$ 个因变量类别时，每个自变量 $x_k$ 都会有 $J-1$ 个与之相关的系数。比如，在我们对党派隶属的模型中，针对女性就有四个系数：$\\beta_{female,SD|SR}$、$\\beta_{female,D|SR}$、$\\beta_{female,I|SR}$ 和 $\\beta_{female,R|SR}$。 现在假设 $x_k$ 不影响因变量，这就意味着我们要做一个假设检验，检验 $x_k$ 的系数是否为零。这个假设可以写成： $$ H_0: \\beta_{k1|1} = \\beta_{k2|2} = \\ldots = \\beta_{kJ|J} = 0 $$ 其中 $b$ 是基准结果，例如在党派隶属模型中可能是某个特定的党派。 因为基准结果下的系数 $\\beta_{k,b|b}$ 必然为0，所以这个假设实际上对 $J - 1$ 个系数进行了约束。我们可以通过似然比检验来验证这个假设。 似然比检验包括以下步骤： 拟合一个包含所有变量的全模型，得到似然比统计量 $LR \\chi_{F}^{2}$。 拟合一个不包含变量 $x_k$ 的受限模型，得到 $\\mathrm{LR}\\chi_{R}^{2}$。 计算两者之间的差异 $\\mathrm{LR}\\chi_{R\\mathrm{vs}F}^{2}=\\mathrm{LR}\\chi_{F}^{2}-\\mathrm{LR}\\chi_{R}^{2}$，如果零假设成立，则这个差异服从自由度为 $J-1$ 的卡方分布。 你也可以使用 lrtest 来完成这个检验，首先拟合全模型并存储参数估计值。 use partyid4, clear mlogit party age income i.black i.female i.educ, base(5) nolog estimates store full_model Next, we fit a model that drops the variable age, again storing the estimates: mlogit party income i.black i.female i.educ, base(5) nolog estimates store drop_age Finally, we compute the LR test. lrtest full_model drop_age Likelihood-ratio test\rAssumption: drop_age nested within full_model\rLR chi2(4) = 45.16\rProb \u003e chi2 = 0.0000\rAlthough using lrtest is straightforward, the command mlogtest, lr is even simpler because **it automatically fits the needed models and computes the tests for all variables by making repeated calls to **lrtest****: mlogit party age income i.black i.female i.educ, base(5) nolog mlogtest, lr LR tests for independent variables (N=1382)\rHo: All coefficients associated with given variable(s) are 0\r| chi2 df P\u003echi2\r-----------------+-------------------------\rage | 45.165 4 0.000\rincome | 24.361 4 0.000\r1.black | 126.467 4 0.000\r1.female | 9.143 4 0.058\r2.educ | 5.567 4 0.234\r3.educ | 21.582 4 0.000\rThe results of the LR test, regardless of how they are computed, can be interpreted as follows: The effect of age on party affiliation is significant at the 0.01 level (LR $\\chi^2 = 45.17$, df = 4, $p \u003c 0.01$). The effect of being female is significant at the 0.10 level but not at the 0.05 level (LR $\\chi^2 = 9.14$, df = 4, $p = 0.06$). This can also be stated more formally: The hypothesis that all the coefficients associated with income are simultaneously equal to 0 can be rejected at the 0.01 level (LR $\\chi^2 = 24.36$, df = 4, $p \u003c 0.01$). Wald test Although we consider the LR test superior, its computational costs can be prohibitive if the model is complex or the sample is very large. Also, LR tests cannot be used if robust standard errors or survey estimation is used. Wald tests are computed using test and can be used with robust standard errors and survey estimation. As an example, to compute a Wald test of the null hypothesis that the effect of being female is 0, type: mlogit party age income i.black i.female i.educ, base(5) nolog test 1.female ( 1) [S","date":"2024-03-16","objectID":"/chapter_8-models-for-nominal-outcomes/:3:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_8 ：Models for nominal outcomes","uri":"/chapter_8-models-for-nominal-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3.3 Tests for combining alternatives If none of the independent variables significantly affects the odds of alternative m versus alternative n, we may say that m and n are indistinguishable with respect to the variables in the model (Anderson 1984). To say that alternatives m and n are indistinguishable corresponds to the hypothesis that $$H_{0}\\colon\\beta_{1,m|n}=\\cdots\\beta_{K,m|n}=0$$ which can be tested with either a Wald or an LR test. If alternatives are indistinguishable with respect to the variables in the model, then you can obtain more efficient estimates by combining them. Note, however, that while the mlogtest command makes it easier to test the hypotheses that each pair of outcomes can be combined, we do not recommend combining categories simply because the null hypothesis is not rejected. This is likely to lead to over-fitting your data and creating outcome variables that do not make substantive sense. Instead, these tests should be used to test a substantively motivated hypothesis that two categories are indistinguishable. 这段话讲的是在统计建模中，当独立变量对于比较两个不同选择（称为m和n）的概率没有显著影响时，可以认为这两个选择在模型中是无法区分的。这意味着对于这些变量，无法区分m和n。在这种情况下，我们可以用统计方法来检验假设，即$$H_{0}\\colon\\beta_{1,m|n}=\\cdots\\beta_{K,m|n}=0$$，其中β表示模型中的系数，K表示独立变量的数量。这个假设可以用瓦尔德检验或LR检验来检验。如果根据模型中的变量来说，两个选择是无法区分的，那么可以通过将它们合并来获得更有效的估计值。但是需要注意的是，虽然**mlogtest**命令可以更容易地测试每一对结果是否可以合并，但我们不建议仅仅因为零假设没有被拒绝就合并类别。这样做可能会导致过度拟合数据，并创建出不具有实质意义的结果变量。相反，这些测试应该用于测试一个有实质意义的假设，即两个类别是无法区分的。 举例：让我们考虑一个市场调查的情景。假设我们想要了解两种不同的产品（产品m和产品n）在不同年龄段和收入水平下的受欢迎程度。我们使用年龄和收入作为独立变量，并观察它们对于选择产品m或产品n的影响。 如果统计分析显示，无论是年龄还是收入，都对产品m和产品n的选择没有显著影响，那么我们就无法通过这些因素来区分这两种产品。在这种情况下，我们可以考虑将这两种产品合并，以获得更可靠的市场反应估计。然而，需要注意的是，我们不应该仅仅因为统计检验中的零假设未被拒绝而合并这两种产品。相反，我们应该有充分的理由相信这两种产品在这些变量下是无法区分的，比如市场调查的其他结果或者消费者行为理论的支持。 Wald test for combining alternatives The command mlogtest, combine computes Wald tests of the null hypothesis that two alternatives can be combined for all pairs of alternatives. For example, mlogit party age income i.black i.female i.educ, base(5) nolog mlogtest, combine Wald tests for combining alternatives (N=1382)\rHo: All coefficients except intercepts associated with a given pair\rof alternatives are 0 (i.e., alternatives can be combined)\r| chi2 df P\u003echi2\r-----------------+-------------------------\rStrDem \u0026 Dem | 72.854 6 0.000\rStrDem \u0026 Indep | 40.334 6 0.000\rStrDem \u0026 Rep | 126.561 6 0.000\rStrDem \u0026 StrRep | 83.272 6 0.000\rDem \u0026 Indep | 15.141 6 0.019\rDem \u0026 Rep | 44.862 6 0.000\rDem \u0026 StrRep | 56.580 6 0.000\rIndep \u0026 Rep | 49.879 6 0.000\rIndep \u0026 StrRep | 60.203 6 0.000\rRep \u0026 StrRep | 22.286 6 0.001\rFrom these results, we can reject the hypothesis that categories StrDem and Dem are indistinguishable. Indeed, our results indicate that all the categories are distinguishable. (Advanced) Using test [outcome] The mlogtest, combine command makes its computations by using the test command for multiple-equation models. Although most researchers will find that mlogtest is sufficient for their needs, there might be situations in which you want to conduct tests that are unique to your application. If so, this section provides some insights into how to use the test command to test hypotheses involving combining outcomes. To test that StrDem is indistinguishable from the base outcome StrRep, type: test [StrDem] ( 1) [StrDem]age = 0\r( 2) [StrDem]income = 0\r( 3) [StrDem]0b.black = 0\r( 4) [StrDem]1.black = 0\r( 5) [StrDem]0b.female = 0\r( 6) [StrDem]1.female = 0\r( 7) [StrDem]1b.educ = 0\r( 8) [StrDem]2.educ = 0\r( 9) [StrDem]3.educ = 0\rConstraint 3 dropped\rConstraint 5 dropped\rConstraint 7 dropped\rchi2( 6) = 83.27\rProb \u003e chi2 = 0.0000\rThe result matches the results from mlogtest in row StrDem \u0026 StrRep. The command test [outcome] indicates which equation is being referenced in multiple-equation commands. mlogit is a multiple-equation command with .7—1 equations that are named by the value label for the outcome categories. In the output above, constraints 3, 5, and 7 were dropped. These constraints correspond to the base categories for factor varia","date":"2024-03-16","objectID":"/chapter_8-models-for-nominal-outcomes/:3:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_8 ：Models for nominal outcomes","uri":"/chapter_8-models-for-nominal-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4 Independence of irrelevant alternatives The MNLM, as well as the conditional logit and rank-ordered logit models discussed below, make the assumption known as the independence of irrelevant alternatives (IIA). Here we describe the assumption in terms of the MNLM. In this model, where the odds do not depend on other alternatives that are available. In this sense, those alternatives are “irrelevant”. What this means is that adding or deleting alternatives does not affect the odds among the remaining alternatives. This point is often made with the red bus blue bus example. Suppose people in a city have three ways of getting to work: by car, by taking a bus operated by a company that uses red buses, or by taking a bus operated by an identical company that uses blue buses. We might expect that many people have a clear preference between taking the car versus taking a bus but are indifferent about whether they take a red bus or a blue bus. Suppose the odds of a person taking a red bus compared with those of taking a car are 1:1. IIA implies the odds will remain 1:1 between these two alternatives even if the blue bus company were to go out of business. The assumption is dubious because we would expect the vast majority of those who take the blue bus to have the red bus as their next preference. Consequently, eliminating the blue bus will increase the probability of traveling by red bus much more than it will increase the probability of someone traveling by car, yielding odds more like 2:1 than 1:1. In other words, because the blue bus and red bus are close substitutes, having the blue bus as an available alternative leads the MNLM to underestimate the preference for red bus versus car. Tests of IIA involve comparing the estimated coefficients from the full model to those from a restricted model that excludes at least one of the alternatives. If the test statistic is significant, the assumption of IIA is rejected, indicating that the MNLM is inappropriate. In this section, we consider the two most common tests of IIA: the Hausman-McFadden (HM) test (Hausman and McFadden 1984) and the Small-Hsiao (SH) test (Small and Hsiao 1985). For details on other tests, see Fry and Harris (1996, 1998). For a model with .7 alternatives, we consider J ways of computing each test. If you remove the first alternative and refit the model, you get the first restricted model leading to the first variation of the test. If you remove the second alternative, you get the second variation, and so on. Each restricted model will lead to a different test statistic, as we demonstrate below. Both the HM test and the SH test are computed by mlogtest, and for both tests, we compute J variations. As many users of mlogtest have told us, the HM and SH tests often provide conflicting information on whether IIA has been violated, with some of the tests rejecting the null hypothesis, while others do not. To explore this further, Cheng and Long (2007) ran Monte Carlo experiments to examine the properties of these tests. Their results show that the HM test has poor size properties even with sample sizes of more than 1,000. For some data structures, the SH test has reasonable size properties for samples of 500 or more. But with other data structures, the size properties are extremely poor and do not get better as the sample size increases. Overall, they conclude that these tests are not useful for assessing violations of the IIA property. It appears that the best advice regarding IIA goes back to an early statement by McFadden (1974), who wrote that the multinomial and conditional logit models should be used only in cases where the alternatives “can plausibly be assumed to be distinct and weighted independently in the eyes of each decision maker”. Similarly, Amemiya (1981) suggests that the MNLM works well when the alternatives are dissimilar. Care in specifying the model to involve distinct alternatives that are not substitutes for one another seems to be reasonable albeit","date":"2024-03-16","objectID":"/chapter_8-models-for-nominal-outcomes/:4:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_8 ：Models for nominal outcomes","uri":"/chapter_8-models-for-nominal-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4.1 Hausman-McFadden test of IIA The HM test of IIA involves the following steps: Fit the full model with all $J$ alternatives included, with estimates in $\\widehat{\\beta}_{F}$. Fit a restricted model by eliminating one or more alternatives, with estimates in $\\widehat{\\beta}_{R}$. Let $\\hat{\\beta_F}^{*}$ be a subset of $\\widehat{\\beta}_{F}$ after eliminating coefficients not fit in the restricted model. The test statistic is where HM is asymptotically distributed as chi-squared with degrees of freedom equal to the rows in $\\widehat{\\beta}_{R}$ if IIA is true. Significant values of HM indicate that the IIA assumption has been violated. The HM test of IIA can be computed with mlogtest: mlogit party age income i.black i.female i.educ, base(5) mlogtest, hausman Hausman tests of IIA assumption (N=1382)\rHo: Odds(Outcome-J vs Outcome-K) are independent of other alternatives\r| chi2 df P\u003echi2\r-----------------+-------------------------\rStrDem | 4.622 20 1.000\rDem | 0.919 21 1.000\rIndep | -2.244 19 .\rRep | 3.030 21 1.000\rStrRep | -0.580 21 .\rNote: A significant test is evidence against Ho.\rNote: If chi2\u003c0, the estimated model does not meet asymptotic assumptions.\rFive tests of IIA are reported. The first four correspond to excluding one of the four nonbase categories. The fifth test, in row StrRep, is computed by refitting the model with the largest remaining outcome as the base category. Three of the tests produce negative chi-squareds, something that is common with this test. Hausman and McFadden (1984, 1226) note this possibility and conclude that a negative result is evidence that IIA has not been violated. Our simulations suggest that negative chi-squareds indicate problems with the test, consistent with the warning that the mlogtest output provides. ","date":"2024-03-16","objectID":"/chapter_8-models-for-nominal-outcomes/:4:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_8 ：Models for nominal outcomes","uri":"/chapter_8-models-for-nominal-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4.2 Small-Hsiao test of IIA To compute an SH test, the sample is divided randomly into two subsamples of about equal size. The unrestricted MNLM is fit on both subsamples, where $\\widehat{\\beta_n}^{S_{1}}$ contains estim ates from th e unrestricted model on the first subsample and $\\widehat{\\beta_n}^{S_{2}}$ is its counterpart for the second subsample. A weighted average of the coefficients is computed as Next, a restricted sample is created from the second subsample by eliminating all cases with a chosen value of the dependent variable. The MNLM is fit using the restricted sample, yielding the estimates $\\widehat{\\beta_r}^{S_{2}}$ and the likelihood $L(\\widehat{\\beta_r}^{S_{2}})$ The SH statistic is which is asymptotically distributed as chi-squared with degrees of freedom equal to the number of coefficients that are fit in both the full model and the restricted model. To compute the SH test, use the command mlogtest, smhsiao (our program uses code from smhsiao by Nick Winter [2000] available at the Statistical Software Components archive). Because the SH test requires randomly dividing the data into subsamples, the results will differ with successive calls of the command, because the sample will be randomly divided differently. To obtain test results that can be replicated, you must explicitly set the seed used by the random-number generator. For example, set seed 124386 mlogtest, smhsiao Small-Hsiao tests of IIA assumption (N=1382)\rHo: Odds(Outcome-J vs Outcome-K) are independent of other alternatives\r| df -----------------+-----------------------------------------------\rStrDem | -703.527 -693.584 19.886 21 0.528\rDem | -554.226 -547.159 14.134 21 0.864\rIndep | -759.018 -746.810 24.416 21 0.273\rRep | -594.421 -584.722 19.397 21 0.560\rStrRep | -747.457 -734.967 24.980 21 0.248\rNote: A significant test is evidence against Ho.\rThese results are consistent with those from the HM test, with none of the tests being significant. Before taking these results seriously, we tried three other seeds to produce a different random division of the sample. The results varied widely. For example, set seed 254331 mlogtest, smhsiao Small-Hsiao tests of IIA assumption (N=1382)\rHo: Odds(Outcome-J vs Outcome-K) are independent of other alternatives\r| df -----------------+-----------------------------------------------\rStrDem | -685.597 -674.385 22.425 21 0.375\rDem | -569.113 -560.231 17.764 21 0.664\rIndep | -750.556 -740.408 20.296 21 0.503\rRep | -613.921 -603.605 20.631 21 0.482\rStrRep | -744.059 -732.629 22.860 21 0.352\rNote: A significant test is evidence against Ho.\rUsing the new seed, we reject the null at the 0.001 level in four of the five tests, illustrating a common problem when using the SH test you often get very different results depending on how the sample is randomly divided. Tip: Setting the random seed. The random numbers that divide the sample for the SH test are based on the runiform() function, which uses a pseudorandom-number generator to create a sequence of numbers based on a seed number. Although these numbers appear to be random, the same sequence will be generated each time you start with the same seed. In this sense (and some others), these numbers are pseudorandom rather than random. If you specify the seed with set seed #, you ensure that you can replicate your results later. See [r] set seed for more details. ","date":"2024-03-16","objectID":"/chapter_8-models-for-nominal-outcomes/:4:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_8 ：Models for nominal outcomes","uri":"/chapter_8-models-for-nominal-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"5 Measures of fit As with models for binary and ordinal outcomes, many scalar measures of fit for the MNLM model can be computed with the SPost command fitstat, and information criteria can be computed with estat ic. The same caveats against overstating the importance of these scalar measures apply here as to the other models we have considered (see also chapter 3). To examine the fit of individual observations, you can fit the series of binary logits implied by the MNLM and use the established methods of examining the fit of observations to binary logit estimates. ","date":"2024-03-16","objectID":"/chapter_8-models-for-nominal-outcomes/:5:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_8 ：Models for nominal outcomes","uri":"/chapter_8-models-for-nominal-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"6 Overview of interpretation Although the MNLM is a mathematically simple extension of the binary model, interpretation is difficult because of the many possible comparisons. Even in our simple example with five outcomes, we have 10 comparisons: StrDem versus StrRep, Dem versus StrRep, Indep versus StrRep, Rep versus StrRep, StrDem versus Rep, Dem versus Rep, Indep versus Rep, StrDem versus Indep, Dem versus Indep, and StrDem versus Dem. It is tedious to write all of them, let alone to interpret all of them for each independent variable. The key to effective interpretation is to avoid overwhelming yourself or your audience with the many comparisons. As with models for binary and ordinal outcomes, we prefer methods of interpretation that are based on predicted probabilities. Fortunately, these methods are essentially unchanged from those used for ordinal models in the last chapter, where the predicted probability is now computed with the formula Here w’e assume that the base outcome is J. but any base could be used. We follow a similar order of presentation to that used in the last chapter,providing new variations in some cases and excluding some topics. In all cases,however,methods from chapter 7 could be used for nominal models, and new ideas shown in this chaptercould be applied to ordinal models. For example, while we do not consider ideal typesin this chapter, they are just as useful for nominal outcomes as they were for the ordinalregression model (ORM ). We begin by examining the distribution of predictions for each observation in theestimation sample. Then, we consider how marginal effects can be used as an overall assessment of the im pact of each variable, but we also show what can be learned by lookingat the distribution of effects for each observation in the estimation sample. We extendearlier methods for examining tables of predictions and show how to test a differenceof differences. Next, we plot predictions as a continuous independent variable changes,which we use to highlight how results from an ordinal model can be misleading whenan outcome does not behave as if it were ordinal. Finally, we consider interpretationusing odds ratios. Although odds ratios in the MNLM have all the limitations discussedin chapter 6, they are im portant for understanding how independent variables affect thedistribution of observations between pairs of outcomes, something th at cannot be doneusing predicted probabilities alone. Before beginning, we must also emphasize once again th at, as with other modelsconsidered in this book, the MNLM is nonlinear in the outcome probabilities, and noapproach can fully describe the relationship between an independent variable and theoutcome probabilities. You should experiment with each of these methods before deciding which approach is most effective in your application. ","date":"2024-03-16","objectID":"/chapter_8-models-for-nominal-outcomes/:6:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_8 ：Models for nominal outcomes","uri":"/chapter_8-models-for-nominal-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"7 Predicted probabilities with predict The most basic command for computing probabilities is predict. After fitting the model with mlogit, predicted probabilities for all outcomes within the sample can be calculated with predict newvarlist, [if] [in] where you must provide one new variable name for each of the J categories of the dependent variable, ordered from the lowest to the highest numerical values. For example, qui mlogit party age income i.black i.female i.educ, base(5) predict mnlmSD mnlmD mnlmI mnlmR mnlmSR The variables created by predict are codebook mnlmSD mnlmD mnlmI mnlmR mnlmSR, compact Variable Obs Unique Mean Min Max Label\r---------------------------------------------------------------------------------\rmnlmSD 1382 1193 .1924747 .0212015 .7654323 Pr(party==StrDem)\rmnlmD 1382 1193 .3089725 .130991 .5125323 Pr(party==Dem)\rmnlmI 1382 1193 .1092619 .0266534 .2838254 Pr(party==Indep)\rmnlmR 1382 1193 .2670043 .0141873 .5036324 Pr(party==Rep)\rmnlmSR 1382 1193 .1222865 .0047189 .4662779 Pr(party==StrRep)\r---------------------------------------------------------------------------------\ras with the ordinal model, if you specify a single variable name after predict, you will obtain predicted probabilities for one outcome category, which you can specify using the outcome() option. As discussed in section 7.10, examining the distribution of the in-sample predictions can be used to get a general sense of what is going on in your model and can sometimes uncover problems in your data. The distribution of predictions can also be used to informally compare competing models, which we illustrate next. We could reasonably argue that the five categories of our dependent variable party are an ordinal scale of party affiliation. Accordingly, it seems reasonable to model these data with an ordinal logit model. First, we fit the model and compute predictions: qui ologit party age income i.black i.female i.educ predict olmSD olmD olmI olmR olmSR codebook olm*, compact Variable Obs Unique Mean Min Max Label\r--------------------------------------------------------------------------\rolmSD 1382 1193 .1934016 .042849 .6546781 Pr(party==1)\rolmD 1382 1193 .30611 .1393038 .3808963 Pr(party==2)\rolmI 1382 1193 .1091385 .0349378 .1221904 Pr(party==3)\rolmR 1382 1193 .269342 .0484485 .3877065 Pr(party==4)\rolmSR 1382 1193 .1220079 .0124719 .3484676 Pr(party==5)\r--------------------------------------------------------------------------\rNext, we plot the predicted probability of being a strong Democrat (outcome 1) with the dotplot command: label var olmSD \"ologit\" label var mnlmSD \"mlogit\" dotplot olmSD mnlmSD, ylabel(0(.2).8, grid) ytitle(Pr(Strong Democrat)) pwcorr olmSD mnlmSD | olmSD mnlmSD\r-------------+------------------\rolmSD | 1.0000 mnlmSD | 0.9374 1.0000 The histograms are similar, and the correlation between the predictions for the ordered logit model (olm) and the MNLM is 0.94. This suggests that the conclusions from the two models might be similar. If we look at the middle category of Independent, however, things look quite different, reflecting the abrupt truncation of the distribution of predictions for middle categories that is often found with the OLM: dotplot olmI mnlmI, ylabel(0(.1).3, grid) ytitle(Pr(Independent)) pwcorr olmI mnlmI | olmI mnlmI\r-------------+------------------\rolmI | 1.0000 mnlmI | -0.1924 1.0000 Not only are the distributions quite different in shape, but the correlation between the predicted probabilities is negative: -0.19! A scatterplot of the predictions shows striking differences between the two models: way scatter olmI mnlmI, /// xtitle(\"mlogit: Pr(Independent)\") ytitle(\"ologit: Pr(Independent)\") /// xlabel(0(.1).3, grid gmin gmax) ylabel(0(.1).3, grid gmin gmax) /// sort aspect(1) A low correlation between the predictions from the MNLM and the ORM could reflect a lack of ordinality, but this is not necessarily so. For example, in simulations where data were generated to meet the assumptions of the ORM, we f","date":"2024-03-16","objectID":"/chapter_8-models-for-nominal-outcomes/:7:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_8 ：Models for nominal outcomes","uri":"/chapter_8-models-for-nominal-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"8 Marginal effects Average marginal effects are a quick and valuable way to assess the effects of all the independent variables in your model. Because methods for using marginal effects to interpret the MNLM are identical to those used for the OLM in section 7.11, we only review key points here. We then extend materials from chapter 7 by examining the distribution of effects within the estimation sample. Marginal effects are also used in section 8.11-2 when we plot odds ratios. The marginal change is the slope of the curve relating $ x_k $ to $ Pr(y = m | x) $, holding all other variables constant, where $ Pr(y = m | x) $ is defined by (8.2). For the MNLM, the marginal change is: Because this equation combines all the $\\beta_{k,j|J^{\\prime}S}$, the value of the marginal change depends on the levels of all variables in the model and can switch sign at different values of these variables. Also, because the marginal change is the instantaneous rate of change, it can be misleading when the probability curve is changing rapidly. For this reason, we generally prefer using a discrete change and do not discuss the marginal change function in this chapter. 假设我们正在研究一个医疗调查，想要了解一个人是否患有某种疾病（$y$）与他们的年龄（$x_1$）和体重指数（$x_2$）之间的关系。我们使用多项逻辑回归模型来建模这个问题。 平均边际效应公式： $$ \\frac{\\partial Pr(y = m | x)}{\\partial x_k} = \\beta_{k,0} + \\sum_{j=1}^{J} \\beta_{k,j|J^{\\prime}S} x_{j} $$ 让我们假设我们的模型只有两个自变量 $x_1$（年龄）和 $x_2$（体重指数），我们希望了解 $x_1$ 和 $x_2$ 对患病率的影响。这时，$k$ 可以取 $1$ 或 $2$，分别代表年龄和体重指数。 假设我们想了解年龄对患病率的影响，即 $k=1$。在这种情况下，公式可以简化为： $$ \\frac{\\partial Pr(y = m | x)}{\\partial x_1} = \\beta_{1,0} + \\beta_{1,1} x_1 + \\beta_{1,2} x_2 $$ 其中，$\\beta_{1,0}$、$\\beta_{1,1}$ 和 $\\beta_{1,2}$ 是模型的参数。 “The discrete change is the change in the probability of $m$ for a change in $x_k$ from the start value $x_k^\\text{start}$ to the end value $x_k^\\text{end}$ (for example, a change from $x_k^\\text{start} = 0$ to $x_k^\\text{end} = 1$), holding other $x$’s constant. Formally, where $Pr(y = m | x, x_k)$ is the probability that $y = m$ given $x$, noting a specific value for $X_k$. The change indicates that when $x_k$ changes from $x_k^\\text{start}$ to $x_k^\\text{end}$, the probability of outcome $m$ changes by $\\Delta Pr(y = m | x) / \\Delta x_k$, holding all other variables at $x$. The magnitude of the change depends on the levels of all variables, including $X_k$, and the size of the change in $X_k$ that is being evaluated. 边际变化公式： $$ \\Delta Pr(y = m | x) = Pr(y = m | x, x_k^\\text{end}) - Pr(y = m | x, x_k^\\text{start}) $$ 这个公式表示当一个自变量 $x_k$ 从一个值变化到另一个值时，目标变量的概率会发生多大的变化。我们可以用同样的例子来说明，比如假设我们想了解体重指数对患病率的影响，即 $k=2$。那么公式可以简化为： $$ \\Delta Pr(y = m | x) = Pr(y = m | x, x_2^\\text{end}) - Pr(y = m | x, x_2^\\text{start}) $$ 这里 $x_2^\\text{end}$ 是体重指数的一个结束值，$x_2^\\text{start}$ 是体重指数的一个起始值。 在实际应用中，我们将模型参数代入这些公式，并对给定的起始值和结束值进行计算，以获得具体的边际效应和边际变化。 The use of marginal change can be misleading when the probability curve is changing rapidly. For this reason, we generally prefer using a discrete change and do not discuss the marginal change further in this chapter. The discrete change is the change in the probability of $m$ for a change in $X_k$ from the start value $x_k^\\text{start}$ to the end value $x_k^\\text{end}$ (for example, a change from $x_k^\\text{start} = 0$ to $x_k^\\text{end} = 1$), holding other $x$’s constant. Formally, $$\\frac{\\Delta\\Pr\\left(y=m\\mid\\mathbf{x}\\right)}{\\Delta x_k\\left(x_k^{\\mathrm{start}}\\rightarrow x_k^{\\mathrm{end}}\\right)}=\\Pr\\left(y=m\\mid\\mathbf{x},x_k=x_k^{\\mathrm{end}}\\right)-\\Pr\\left(y=m\\mid\\mathbf{x},x_k=x_k^{\\mathrm{start}}\\right)$$ where $\\Pr\\left(y=m\\mid\\mathbf{x},x_k\\right)$ is the probability that $y = m$ given $x$,noting a specific value for $x_k$. The change indicates that when $x_k$ changes from $x_k^\\text{start}$ to $x_k^\\text{end}$, the probability of outcome m changes by $\\Delta\\Pr\\left(y=m\\mid\\mathbf{x}\\right)/\\Delta x_k$, holding all other variables at $x$.The magnitude of th e change depends on the levels of all variables, including $x_k$,and the size of the c","date":"2024-03-16","objectID":"/chapter_8-models-for-nominal-outcomes/:8:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_8 ：Models for nominal outcomes","uri":"/chapter_8-models-for-nominal-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"8.1 (Advanced) The distribution of marginal effects Because the average marginal effect (AME) is an average, it does not indicate variation in the sample. We find examining the distribution of marginal effects is often very useful, but it requires using loops, macros, and returns and is computationally intensive. After you are familiar with marginal effects and Stata tools for automation, we encourage you to study this section carefully. Initially, however, we hope you will at least skim it. The value of a marginal effect depends on the level of all variables in the model (see section 6.2.5). Neither the AME nor the marginal effect at the mean provide information about how much variation there is within the sample in the size of the effects. In this section, we extend the methods from chapter 6 to the MNLM. The same techniques can also be used for the OLM. The following commands generate the variable incoraedc containing the discrete change for a standard deviation increase in income, where we assume that the estimation results from mlogit are in memory: mlogit party age income i.black i.female i.educ, base(1) estimates store mymodel gen incomedc = . label var incomedc /// \"Effect of a one standard deviation change in income on Pr(Dem)\" sum income local sd = r(sd) local nobs = _N forvalues i = 1/`nobs' { qui { margins in `i', nose predict(outcome(2)) /// Dem at(income=gen(income)) at(income=gen(income+`sd')) local prstart = el(r(b),1,1) local prend = el(r(b),1,2) local dc = `prend' - `prstart' replace incomedc = `dc' in `i' } } Lines 1 and 2 create and label the variable that will hold the discrete change for each observation. Because we want to compute the effect of a standard deviation change in income, lines 3 and 4 compute the standard deviation and create the local macro sd containing the standard deviation. This is used in the margins command in line 8. Line 5 creates the macro nobs with the number of observations, which we use in line 6 to begin a forvalues loop through the 1,382 observations in the estimation sample. The loop over observations is defined in lines 6 through 14. Because we do not want to see the output from margins, line 7 uses quietly to suppress the output. Line 8 uses margins to compute predictions for observation ‘i’ for the second outcome category, where noSE suppresses the computation of the standard error (this speeds up the computations). The first at() statement specifies the observed value of income, with all other variables held at their observed values; the second at() specifies the prediction at one standard deviation more than the observed value. margins returns the predictions to the matrix r(b), and lines 9 and 10 retrieve the starting and ending probabilities. Line 11 computes the discrete change. Line 12 saves the effect for observation ‘i’ of variable income_dc. The last two lines terminate the quietly command and the forvalues loop. Computing the distribution of effects is slow because computing effects for individual observations is slow. Every time margins is run, it computes predictions for all observations in the sample before it computes predictions at values specified by at(). In line 8, we need the predictions for a single observation, but margins computes predictions for all observations. You cannot turn off this behavior. Unfortunately, margins does not save the predictions it computes for each observation. If it did, we would not need the loop! Accordingly, for each observation in our loop, margins is computing $2N - 1 - 2$ predictions, for a total of $2(N^2 + N)$ predictions — nearly 4 million in our example! Using Stata/MP for eight cores, our loop took 60 seconds to complete. But we believe it is worth the time. Although these commands might seem complex at first, the good news is that you can easily modify our code to work for other variables (for example, change income to age), for different outcomes (for example, change outcome(2) to outcome(5)), or for different amoun","date":"2024-03-16","objectID":"/chapter_8-models-for-nominal-outcomes/:8:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_8 ：Models for nominal outcomes","uri":"/chapter_8-models-for-nominal-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"9 Tables of predicted probabilities As with models for binary and ordinal outcomes, tables of predictions can provide useful insights into models when there are substantively important, categorical independent variables. Because exactly the same commands can be used with nominal models as with ordinal models, we do not repeat the types of examples shown before (see section 7.13 for details). Instead, we focus on using tables of probabilities to compare and elaborate discrete changes across groups. To show the effects of race and gender on party affiliation, we use mtable to compute probabilities for each combination of race and gender, holding other variables at their means. Although we could use the specification at(black = (0 1) female=(0 1)), we instead use multiple at() options to arrange the output in the order we prefer: mlogit party age income i.black i.female i.educ, base(5) mtable, atmeans noci norownumbers /// at(black=0 female=1) at(black=1 female=1) /// white women, black women at(black=0 female=0) at(black=1 female=0) /// white men, black men Expression: Pr(party), predict(outcome())\rblack female StrDem Dem Indep Rep StrRep\r--------------------------------------------------------------------\r0 0 0.142 0.287 0.115 0.311 0.145\r1 0 0.440 0.328 0.162 0.049 0.021\r0 1 0.138 0.354 0.092 0.304 0.111\r1 1 0.417 0.394 0.127 0.047 0.015\rSpecified values of covariates\r| 2. 3.\r| age income educ educ\r----------+---------------------------------------\rCurrent | 45.9 37.5 .58 .259\rWe can interpret this as follows: For those who are average on all other characteristics, blacks are far more likely than whites to be strong Democrats and far less likely to be Republicans or strong Republicans. Much smaller differences are found between men and women in party affiliation. Supposing our substantive interest focuses on race and gender differences in party affiliation, we would want to test the differences in predictions between groups. The easiest way to do this is with mchange, using the option statistics(start end change pvalue) to list the predicted probabilities for each group as well as the discrete changes. The effects of race for women (at female = 1) are mchange black, at(female=1) atmeans brief /// statistics(start end change pvalue) title(Effect of race for women) Effect of race for women | Number of obs = 1382\rExpression: Pr(party), predict(outcome())\r| StrDem Dem Indep Rep StrRep -------------+------------------------------------------------------\rblack | From | 0.138 0.354 0.092 0.304 0.111 To | 0.417 0.394 0.127 0.047 0.015 yes vs no | 0.278 0.040 0.035 -0.257 -0.096 p-value | 0.000 0.342 0.169 0.000 0.000 We interpret this as follows: Compared with a white woman who is average on all characteristics, an otherwise similar black woman has a 0.28 higher probability of being a strong Democrat, a 0.26 lower probability of being a Republican, and a 0.10 lower probability of being a strong Republican. All differences are significant at the 0.001 level. Similarly, we compute the effects for men: mchange black, at(female=1) atmeans brief /// statistics(start end change pvalue) title(Effect of race for women) Effect of race for women | Number of obs = 1382\rExpression: Pr(party), predict(outcome())\r| StrDem Dem Indep Rep StrRep -------------+------------------------------------------------------\rblack | From | 0.138 0.354 0.092 0.304 0.111 To | 0.417 0.394 0.127 0.047 0.015 yes vs no | 0.278 0.040 0.035 -0.257 -0.096 p-value | 0.000 0.342 0.169 0.000 0.000 Although the effects for race are of roughly the same size for men and women, we would like to test whether they are equal. For example, the discrete change for women is 0.278 and for men is 0.298. Can we say these differences are significantly different from one another? We consider this question in the next section. ","date":"2024-03-16","objectID":"/chapter_8-models-for-nominal-outcomes/:9:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_8 ：Models for nominal outcomes","uri":"/chapter_8-models-for-nominal-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"9.1 (Advanced) Testing second differences Computing and testing second differences is extremely useful, especially in group comparisons. To make these computations requires more advanced programming and a deeper understanding of how margins and lincom work. On first reading, you might want to only skim this section. However, we hope you return to it later. Earlier, we used mchange to compute the first difference for race holding female at either 0 or 1 with other variables held at their means. Now, we want to test the null hypothesis that the discrete change for men is equal to the discrete change for women: $$H_0{:\\frac{\\Delta\\Pr\\left(y=j\\mid\\overline{\\mathrm{x}},\\textbf{female}=0\\right)}{\\Delta\\text{b}1\\text{ack}\\left(0\\to1\\right)}=\\frac{\\Delta\\Pr\\left(y=j\\mid\\overline{\\mathrm{x}},\\textbf{female}=1\\right)}{\\Delta\\text{b}1\\text{ack}\\left(0\\to1\\right)}}$$ We begin by fitting the model and storing the estimates so that we can restore them after posting estimates from margins: mlogit party age income i.black i.female i.educ, base(5) estimates store mymodel Now, we use margins to compute predictions for all combinations of gender and race for outcome 1. Later, we will use a loop to make computations for all outcomes. Because the atlegend produced by margins is in this case quite long, we suppress it with the noatlegend option and use m listat to list the values at which the independent variables are held: margins, predict(outcome(1)) post atmeans noatlegend /// at(black=0 female=0) at(black=1 female=0) /// white men, black men at(black=0 female=1) at(black=1 female=1) // white women, black women Adjusted predictions Number of obs = 1,382\rModel VCE: OIM\rExpression: Pr(party==StrDem), predict(outcome(1))\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_at |\r1 | 0.14 0.01 10.21 0.000 0.12 0.17\r2 | 0.44 0.04 10.09 0.000 0.35 0.53\r3 | 0.14 0.01 9.79 0.000 0.11 0.17\r4 | 0.42 0.04 9.71 0.000 0.33 0.50\r------------------------------------------------------------------------------\rmlistat 1. 3.\rage income educ educ --------------------------------------\r45.9 37.5 .58 .259 at() values vary\r_at | black female -------+-------------------\r1 | 0 0 2 | 1 0 3 | 0 1 4 | 1 1 The post option saves the predictions so they can be used with lincom or mlincom to compute second differences. Why are we using margins, which computes predictions for only one outcome, instead of m table, which computes predictions for all outcomes? To test predictions, those predictions must be saved to the e(b) and e(V) matrices. Because margins computes predictions for only one outcome at a time, we can only post predictions for one outcome. Although m table can collect predictions for all the outcomes, it can only post predictions for a single outcome, just like margins. Accordingly, there is no advantage to using m table. The second difference is computed by taking the difference between two differences: the difference between the probability for black men contained in _b[2._at] and for white men in _b[1._at]; and the difference between the probability for black women in _b[4._at] and for white women in _b[3._at]: lincom _b[2._at] - _b[1._at] // 1st difference if female ( 1) - 1bn._at + 2._at = 0\r------------------------------------------------------------------------------\r| Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r(1) | 0.30 0.04 7.16 0.000 0.22 0.38\r------------------------------------------------------------------------------\rlincom _b[4._at] - _b[3._at] // 1st difference if male ( 1) - 3._at + 4._at = 0\r------------------------------------------------------------------------------\r| Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+--------------------------------------------------------------","date":"2024-03-16","objectID":"/chapter_8-models-for-nominal-outcomes/:9:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_8 ：Models for nominal outcomes","uri":"/chapter_8-models-for-nominal-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"9.2 (Advanced) Predictions using local means and subsamples Comparing groups by making predictions with local means or by average predictions within subsamples is important for nuanced interpretations of group differences. To do this requires Stata programming and using the over() option with margins. Although you might want to skip this section on first reading, we encourage you to return to this section when you are comfortable with the commands used in other sections on interpretation with predictions. To compute the predicted probabilities of party affiliation by race and gender, we used the atmeans option to hold all other variables at the means for the estimation sample. Accordingly, the predictions are comparing white men, black men, white women, and black women who have the same values for age, income, and education. Because the four race-gender groups are likely to differ on these variables, the predictions must be viewed as a “what if” experiment: what would happen if these groups had the same distributions of other characteristics? We could compute predictions by using within-group means, which we refer to as local means. Using methods discussed on page 273, we use if conditions to select the sample for each mtable command: mlogit party age income i.black i.female i.educ, base(5) estimates store mymodel qui mtable if black==0 \u0026 female==0, atmeans noci rowname(White Men) clear qui mtable if black==1 \u0026 female==0, atmeans noci rowname(Black Men) below qui mtable if black==0 \u0026 female==1, atmeans noci rowname(White Women) below mtable if black==1 \u0026 female==1, atmeans noci rowname(Black Women) below Expression: Pr(party), predict(outcome())\r| StrDem Dem Indep Rep StrRep\r-------------+-------------------------------------------------\rWhite Men | 0.128 0.283 0.109 0.325 0.156\rBlack Men | 0.461 0.310 0.168 0.044 0.017\rWhite Women | 0.145 0.354 0.093 0.298 0.109\rBlack Women | 0.460 0.364 0.129 0.037 0.011\rSpecified values of covariates\r| 2. 3.\r| age income black female educ educ\r----------+-----------------------------------------------------------\rSet 1 | 45.1 43.5 0 0 .546 .324\rSet 2 | 45 29.8 1 0 .518 .188\rSet 3 | 47.1 35.2 0 1 .624 .222\rCurrent | 45.4 20.4 1 1 .59 .143\rThe values of the covariates show substantial differences among the groups, especially with respect to income, where white men have more than twice the average income of black women. Consequently, the predicted probabilities with local means differ from those computed with global means. For example, for black women, the probability of being a strong Democrat is 0.417 when global means are used compared with 0.460 when local means are used. Although we can create the predictions we want by using mtable with if conditions, this approach does not allow us to compute first and second differences across the groups. Technically, the problem is that at the end of each mtable command (or margins, if we had used that command instead), only predictions for the current group can be posted to e(b) and e(V) for use by lincom or mlincom. A relatively efficient way to deal with this limitation is to use the over (over-variables) option. With the over() option, margins computes predictions based on the subsample of cases defined by the over-variables. The over-variables can be any categorical variables in the dataset, even if they are not used in the regression model. For our purposes, we want to use over(female black) to compute predictions based on subsamples defined by race and gender: margins, over(female black) atmeans post estimates restore mymodel Adjusted predictions Number of obs = 1,382\rModel VCE: OIM\rOver: female black\r1._predict: Pr(party==StrDem), predict(pr outcome(1))\r2._predict: Pr(party==Dem), predict(pr outcome(2))\r3._predict: Pr(party==Indep), predict(pr outcome(3))\r4._predict: Pr(party==Rep), predict(pr outcome(4))\r5._predict: Pr(party==StrRep), predict(pr outcome(5))\rAt: 0.female#0.black\rage = 45.13171 (mean)\rincome = 43.54512 (mean)\rblack = 0\rfemale =","date":"2024-03-16","objectID":"/chapter_8-models-for-nominal-outcomes/:9:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_8 ：Models for nominal outcomes","uri":"/chapter_8-models-for-nominal-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"10 Graphing predicted probabilities Graphing predicted probabilities for each outcome can also be useful for the MNLM and is done exactly as it was for the ORM. To illustrate this, we create plots to show the effects of age and income on party affiliation. After fitting the model, we compute predictions as income increases from $0$ to $100,000$, holding other variables at their means. qui mlogit party age income i.black i.female i.educ, base(5) vsquish mgen, atmeans at(income=(0(10)100)) stub(mnlmI) replace Predictions from: margins, atmeans at(income=(0(10)100)) predict(outcome())\rVariable Obs Unique Mean Min Max Label\r--------------------------------------------------------------------------------\rmnlmIpr1 11 11 .1604032 .090648 .2445128 pr(y=StrDem) from margins\rmnlmIll1 11 11 .1253144 .0479288 .1908274 95% lower limit\rmnlmIul1 11 11 .1954919 .1333672 .2981981 95% upper limit\rmnlmIincome 11 11 50 0 100 Income in $1,000s\rmnlmICpr1 11 11 .1604032 .090648 .2445128 pr(y\u003c=StrDem)\rmnlmIpr2 11 11 .3325221 .2827661 .3683488 pr(y=Dem) from margins\rmnlmIll2 11 11 .2882002 .2133308 .3252223 95% lower limit\rmnlmIul2 11 11 .3768441 .3522013 .420665 95% upper limit\rmnlmICpr2 11 11 .4929253 .3734141 .6128616 pr(y\u003c=Dem)\rmnlmIpr3 11 11 .1120243 .1014983 .117243 pr(y=Indep) from margins\rmnlmIll3 11 11 .0816545 .0528909 .0967365 95% lower limit\rmnlmIul3 11 11 .142394 .1344071 .152314 95% upper limit\rmnlmICpr3 11 11 .6049496 .4749124 .729903 pr(y\u003c=Indep)\rmnlmIpr4 11 11 .2785857 .1944607 .3642161 pr(y=Rep) from margins\rmnlmIll4 11 11 .2354603 .1558902 .2878424 95% lower limit\rmnlmIul4 11 11 .3217112 .2330312 .4405898 95% upper limit\rmnlmICpr4 11 11 .8835353 .8391285 .9243637 pr(y\u003c=Rep)\rmnlmIpr5 11 11 .1164647 .0756363 .1608715 pr(y=StrRep) from margins\rmnlmIll5 11 11 .0857181 .0519797 .1041749 95% lower limit\rmnlmIul5 11 11 .1472114 .0992929 .2175682 95% upper limit\rmnlmICpr5 11 4 1 1 1 pr(y\u003c=StrRep)\r--------------------------------------------------------------------------------\rSpecified values of covariates\r1. 1. 2. 3.\rage black female educ educ -----------------------------------------------------\r45.94645 .1374819 .4934877 .5803184 .2590449 Using variable labels to assign labels to the lines, we can plot the predictions with these commands: label var mnlmIpr1 \"Strong Dem\" label var mnlmIpr2 \"Democrat\" label var mnlmIpr3 \"Independent\" label var mnlmIpr4 \"Republican\" label var mnlmIpr5 \"Strong Rep\" graph twoway connected /// mnlmIpr1 mnlmIpr2 mnlmIpr3 mnlmIpr4 mnlmIpr5 mnlmIincome, /// title(\"Multinomial logit model: other variables held at their means\", /// pos(11) size(medium)) /// ytitle(Probability of party affiliation) ylab(0(.1).4, grid gmax gmin) /// msym(O Oh dh sh s) mcol(gs1 gs5 gs8 gs5 gs1) /// lpat(solid dash shortdash dash solid) lcol(gs1 gs5 gs8 gs5 gs1) /// legend(rows(2)) The probabilities of being a strong Democrat (•) or a Democrat (O) decrease with income, while the probabilities of being a Republican (□) or a strong Republican (■) increase, with little change in the probability of being Independent (O). Using ologit to fit the ordinal model, we obtain a nearly identical graph: qui ologit party age income i.black i.female i.educ, vsquish mgen, atmeans at(income=(0(10)100)) stub(olmI) replace label var olmIpr1 \"Strong Dem\" label var olmIpr2 \"Democrat\" label var olmIpr3 \"Independent\" label var olmIpr4 \"Republican\" label var olmIpr5 \"Strong Rep\" graph twoway connected /// olmIpr1 olmIpr2 olmIpr3 olmIpr4 olmIpr5 olmIincome, /// title(\"Ordered logit model\", pos(11) size(medium)) /// ytitle(Probability of party affiliation) ylab(0(.1).4, grid gmax gmin) /// msym(O Oh dh sh s) msiz(*1.5 *1.5 *1.7 *1.8 *1.8) mcol(gs1 gs5 gs8 gs5 gs1) /// lpat(solid dash shortdash dash solid) lcol(gs1 gs5 gs8 gs5 gs1) /// legend(rows(2)) The results are quite different when we examine the effect of age on party affiliation. For the MNLM, we plot predictions as age increases from 20 to 85, holding other variables at their means. qui mlogit party age","date":"2024-03-16","objectID":"/chapter_8-models-for-nominal-outcomes/:10:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_8 ：Models for nominal outcomes","uri":"/chapter_8-models-for-nominal-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"11 Odds ratios Discrete change coefficients do not show the dynamics among the outcomes. For example, being black increases the probability of being a Democrat or an Independent, but how does it affect the probability of being Democrat relative to being Independent? To deal with these questions, odds ratios, also referred to as relative-risk ratios and factor change coefficients, can be used to explore how variables affect the choice of one outcome compared with another outcome. Odds ratios do not provide a complete picture of the effects of variables on the outcomes and have the same limitations discussed for the binary logit model in chapter 6; however, in the MNLM, odds ratios complement the information provided by marginal effects and other types of predictions. The factor change in the odds of outcome $m$ versus outcome $n$ as $X_k$ increases by $\\delta$, holding other variables constant, equals: $$\\frac{\\Omega_{m|n}\\left(\\mathbf{x},x_k+\\delta\\right)}{\\Omega_{m|n}\\left(\\mathbf{x},x_k\\right)}=e^{\\beta_{k,m|n}\\delta}$$ If the amount of change is $\\delta=1$,the odds ratio can be interpreted as follows: For a unit increase in $x_k$,the odds of $m$ versus $n$ are expected to change by a factor of $\\exp(\\beta_{k,m|n})$, holding all other variables constant. If the amount of change is $$\\delta=s_{x_k}$$,then the odds ratio can be interpreted as follows: For a standard deviation increase in $X_k$, the odds of $m$ versus $n$ are expected to change by a factor of $\\exp(\\beta_{\\text{fc}}\u003em|n \\times s^*)$, holding all other variables constant. Other values of $\\delta$ can also be used, such as $\\delta = 4$ for four years of education or $\\delta = 10$ for $10,000$ in income. 当我们研究某个变量对选择一种结果而不是另一种结果的影响时，赔率比是一个有用的概念。赔率比告诉我们，当一个变量增加一定量时，选择一个结果相对于选择另一个结果的赔率会如何改变。让我们用一个例子来解释这个概念。 假设我们对选民进行调查，想知道他们的年龄对于投票给民主党或独立党的影响。我们使用了一个公式来表示年龄变化如何影响投票选择的赔率： $$ \\frac{\\Omega_{\\text{民主党|独立党}}(\\textbf{x},\\text{年龄}+\\delta)}{\\Omega_{\\text{民主党|独立党}}(\\textbf{x},\\text{年龄})} = e^{\\beta_{\\text{年龄,民主党|独立党}}\\delta} $$ 现在让我们解释这个公式。假设我们发现，当年龄增加1岁时，投票给民主党相对于独立党的赔率增加了3。这意味着，当年龄增加1岁时，投票给民主党相对于独立党的赔率会乘以$ e^3 $。 换句话说，随着年龄增长，一个人更有可能投票给民主党而不是独立党，而且这种可能性会以指数级别增加。 所以，赔率比告诉我们了解不同变量如何影响结果选择的关键信息。它帮助我们理解特定变量的变化对于结果选择的影响，而且通常可以用具体的倍数来量化这种影响，使我们更好地理解数据中的模式和趋势。 另一个例子是关于肤色对选民投票偏好的影响。假设我们有两个群体，黑人和白人，我们想知道他们投票给民主党相对于独立党的赔率有何不同。 我们可以使用相同的公式来分析这个问题： $$ \\frac{\\Omega_{\\text{民主党|独立党}}(\\textbf{x},\\text{肤色}+\\delta)}{\\Omega_{\\text{民主党|独立党}}(\\textbf{x},\\text{肤色})} = e^{\\beta_{\\text{肤色,民主党|独立党}}\\delta} $$ 假设我们发现，当肤色从白变为黑时，投票给民主党相对于独立党的赔率增加了2。这意味着，黑人相对于白人，投票给民主党相对于独立党的赔率是白人的两倍。 通过这个例子，我们可以看到赔率比如何帮助我们理解不同变量对于结果选择的影响，并且通过具体的倍数来量化这种影响，使我们更好地理解数据中的模式和趋势。 ","date":"2024-03-16","objectID":"/chapter_8-models-for-nominal-outcomes/:11:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_8 ：Models for nominal outcomes","uri":"/chapter_8-models-for-nominal-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"11.1 Listing odds ratios with listcoef The difficulty in interpreting odds ratios for the MNLM is that to understand the effect of a variable, you need to examine the coefficients for comparisons among all pairs of outcomes. The standard output from mlogit includes only a minimal set of $J - 1$ comparisons with the base outcome. Although you could estimate coefficients for all possible comparisons by rerunning mlogit with different bases (for example, mlogit party female black ..., base(1); mlogit party female black ..., base(2); etc.), using listcoef is simpler. For example, to examine the odds ratios for variable black, type: use partyid4.dta, clear mlogit party age income i.black i.female i.educ, base(5) listcoef black, help mlogit (N=1382): Factor change in the odds of party Variable: 1.black (sd=0.344)\r------------------------------------------------------------------------------\r| b z P\u003e|z| e^b e^bStdX\r-----------------------------+------------------------------------------------\rStrDem vs Dem | 0.9963 5.022 0.000 2.708 1.409\rStrDem vs Indep | 0.7845 3.052 0.002 2.191 1.310\rStrDem vs Rep | 2.9692 7.666 0.000 19.475 2.781\rStrDem vs StrRep | 3.0754 5.091 0.000 21.659 2.885\rDem vs StrDem | -0.9963 -5.022 0.000 0.369 0.709\rDem vs Indep | -0.2118 -0.830 0.407 0.809 0.930\rDem vs Rep | 1.9728 5.132 0.000 7.191 1.973\rDem vs StrRep | 2.0791 3.448 0.001 7.997 2.047\rIndep vs StrDem | -0.7845 -3.052 0.002 0.456 0.763\rIndep vs Dem | 0.2118 0.830 0.407 1.236 1.076\rIndep vs Rep | 2.1846 5.220 0.000 8.887 2.122\rIndep vs StrRep | 2.2909 3.658 0.000 9.884 2.202\rRep vs StrDem | -2.9692 -7.666 0.000 0.051 0.360\rRep vs Dem | -1.9728 -5.132 0.000 0.139 0.507\rRep vs Indep | -2.1846 -5.220 0.000 0.113 0.471\rRep vs StrRep | 0.1063 0.155 0.877 1.112 1.037\rStrRep vs StrDem | -3.0754 -5.091 0.000 0.046 0.347\rStrRep vs Dem | -2.0791 -3.448 0.001 0.125 0.489\rStrRep vs Indep | -2.2909 -3.658 0.000 0.101 0.454\rStrRep vs Rep | -0.1063 -0.155 0.877 0.899 0.964\r------------------------------------------------------------------------------\rb = raw coefficient\rz = z-score for test of b=0\rP\u003e|z| = p-value for z-test\re^b = exp(b) = factor change in odds for unit increase in X\re^bStdX = exp(b*SD of X) = change in odds for SD increase in X\rThe odds ratios of interest are in the column labeled e(b). For example, the odds ratio for black for the outcomes StrDera vs Dera is 2.708, which is significant at the 0.001 level. It can be interpreted as follows: Being black increases the odds of having a strong Democratic affiliation compared with a Democratic affiliation by a factor of 2.7, holding other variables constant. Even for a single variable, there are a lot of coefficients; we often hear people lament that there are “too many” coefficients to interpret. Fortunately, a simple graph makes this task manageable, even for complex models. ","date":"2024-03-16","objectID":"/chapter_8-models-for-nominal-outcomes/:11:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_8 ：Models for nominal outcomes","uri":"/chapter_8-models-for-nominal-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"11.2 Plotting odds ratios An odds-ratio plot lets you quickly see patterns in coefficients, even for complex models with many outcomes. Methods for plotting odds ratios were developed by Long (1987) while using the MNLM to examine factors that determine the organizational contexts in which scientists work (Long and McGinnis 1981). Although an odds-ratio plot was included in that paper, this is generally not the most effective way to use these graphs. Rather, the graphs provide a quick way to assess all the parameters in the model and to get a general sense of what is going on to help plan further analyses. Experience in teaching suggests that within an hour, students gain a “feel” for these graphs that allows them to evaluate the results of an MNLM in only a few minutes. Building on these insights, more detailed analyses can be planned using other methods of interpretation. To explain how to interpret an odds-ratio plot, we begin with hypothetical results from an MNLM with three outcomes and three independent variables: These coefficients were constructed to have specific types of relationships among outcomes and variables: The $\\beta$ coefficients for xi and on $B | A$ (which you can read as $B$ versus $A$) are equal but of opposite sign. The coefficient for $x3$ is half as large. The $\\beta$ coefficients for $x1$ and $x2$ on $C | A$ are half as large and in opposite directions as the coefficients on $B | A$ , whereas the coefficient for $x3$ is in the same direction but is twice as large. In an odds-ratio plot, each independent variable is presented on a separate row, with the horizontal axis indicating the magnitude of the $\\beta$ coefficients associated with each contrast of outcomes. Here is the plot, where the letters correspond to the outcome categories: We now explain how the graph conveys the information from the table of coefficients. Sign of coefficients. If a letter is to the right of another letter, increases in the independent variable make the outcome to the right more likely relative to outcomes located to the left, holding all other variables constant. Thus, relative to outcome A, an increase in $X$ increases the odds of C and decreases the odds of D. This corresponds to the positive sign of $\\beta1_{\\text{C|A}}$ and the negative sign of $\\beta_{\\text{B|A}}$. The signs of these coefficients are reversed for $X_2$, and accordingly, the plot for $X_2$ is a mirror image of that for $X_1$. Magnitude of coefficients: The distance between a pair of letters indicates the magnitude of the coefficient. The additive scale on the bottom axis measures the value of the $\\beta_{k,m|n}’s$ . The multiplicative scale on the top axis measures the odds ratios $\\exp(\\beta_{k,m|n})$. For both $X_1$ and $X_2$, the distance between A and B is twice the distance between A and C, which reflects that $\\beta1_{\\text{B|A}}$ is twice as large as $\\beta1_{\\text{C|A}}$, and $\\beta2_{\\text{B|A}}$ is twice as large as $\\beta2_{\\text{C|A}}$. For $X_3$, the distance between A and B is half the distance between A and C, reflecting that $\\beta3_{\\text{B|A}}$ is twice as large as $\\beta3_{\\text{C|A}}$. The additive relationship: The additive relationships among coefficients in (8.1) are shown in the graph. For all the independent variables,$\\beta_{k,C|A}=\\beta_{k,B|A}+\\beta_{k,C|B}.$Accordingly, the distance from letters $A$ to $C$ in the graph is the sum of the distances from $A$ to $B$ and $B$ to $C$.This is easiest to see in the row for variable $X_3$, where all the coefficients are positive. By plotting the $J - 1$ coefficients from a minimal set, it is possible to visualize the relationships among all pairs of outcomes. The base outcome: In the graph above, the $As$ are aligned vertically because the plot uses $A$ as the base outcome when graphing the coefficients. The choice of the base is arbitrary. We could have used alternative $B$ as the base instead, which would shift the rows of the graph to the left or right so that the $Bs$ lined up.","date":"2024-03-16","objectID":"/chapter_8-models-for-nominal-outcomes/:11:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_8 ：Models for nominal outcomes","uri":"/chapter_8-models-for-nominal-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.12 (Advanced) Additional models for nominal outcomes ","date":"2024-03-16","objectID":"/chapter_8-models-for-nominal-outcomes/:12:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_8 ：Models for nominal outcomes","uri":"/chapter_8-models-for-nominal-outcomes/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"This chapter we focus on the logit and probit versions of the ordinal regression model (ORM).","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"Ordinal variables are often coded as consecutive integers from 1 to the number of categories. Perhaps because of this coding, it is tempting to analyze ordinal outcomes with the linear regression model (LRM). However, an ordinal dependent variable violates the assumptions of the LRM, which can lead to incorrect conclusions, as demonstrated strikingly by McKelvey and Zavoina (1975, 117) and Winship and Mare (1984, 521-523). With ordinal outcomes, it is much better to use models that avoid the assumption that the distances between categories are equal. Although many models have been designed for ordinal outcomes, in this chapter we focus on the logit and probit versions of the ordinal regression model (ORM). The model was introduced by McKelvey and Zavoina (1975) in terms of an underlying latent variable, and in biostatistics by McCullagh (1980), who referred to the logit version as the proportional-odds model. In section 7.16, we review several less commonly used models for ordinal outcomes. As with the binary regression model (BRM), the ORM is nonlinear, and the magnitude of the change in the outcome probability for a given change in one of the independent variables depends on the levels of all the independent variables. As with the BRM, the challenge is to summarize the effects of the independent variables in a way that fully reflects key substantive processes without overwhelming and distracting detail. For ordinal outcomes, as well as for the models for nominal outcomes in chapter 8, the difficulty of this task is increased by having more than two outcomes to explain. Before proceeding, we caution that researchers should think carefully before concluding that their outcome is indeed ordinal. Do not assume that a variable should be analyzed as ordinal simply because the values of the variable can be ordered. A variable that can be ordered when considered for one purpose could be unordered or ordered differently when used for another purpose. Miller and Volker (1985) show how different assumptions about the ordering of occupations result in different conclusions. A variable might also reflect ordering on more than one dimension, such as attitude scales that reflect both the intensity and the direction of opinion. Moreover, surveys commonly include the category “don’t know”, which probably does not correspond to the middle category in a scale, even though analysts might be tempted to treat it this way. In general, ORMs restrict the nature of the relationship between the independent variables and the probabilities of outcome categories, as discussed in section 7.15. Even when an outcome seems clearly to be ordinal, such restrictions can be unrealistic, as illustrated in chapter 8. Indeed, we suggest that you always compare the results from ordinal models with those from a model that does not assume ordinality. We begin by reviewing the statistical model, followed by an examination of testing, fit, and methods of interpretation. These discussions are intended as a review for those who are familiar with the models. For a complete discussion, see Agresti (2010), Long (1997), or Hosmer, Lemeshow, and Sturdivant (2013). As explained in chapter 1, you can obtain sample do-files and data files by installing the spostl3_do package. ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:0:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.1 The statistical model The ORM can be developed in different ways, each of which leads to the same form of the model. These approaches to the model parallel those for the BRM. Indeed, the BRM can be viewed as a special case of the ordinal model in which the ordinal outcome has only two categories. ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:1:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.1.1 A latent-variable model The ORM is commonly presented as a latent-variable model. Defining y* as a latent variable ranging from −∞ to ∞, the structural model is $$y_{i}^{*}=\\mathbf{x_i}\\beta+\\varepsilon_{i}$$ where $i$ is the observation and $\\varepsilon$ is a random error, as discussed further below. For the case of one independent variable, The measurement model for binary outcomes from chapter 5 is expanded to divide $y^*$ into $J$ ordinal categories, $$y_i=m\\quad\\text{if }\\tau_{m-1}\\leq y_i^*\u003c\\tau_m\\quad\\text{for }m=1\\text{to }J$$ where the cutpoints $\\tau_1$ through $\\tau_j-1$ are estimated.(Some authors refer to these as thresholds.) We assume $t_0 = -\\infty$ and $t_j = \\infty$ for reasons that will be clear shortly. To illustrate the measurement model, consider the example used in this chapter. People are asked to respond to the following statement: If you were asked to use one of four names for your social class, which would you say you belong in: the lower class, the working class, the middle class, or the upper class? The underlying, continuous latent variable can be thought of as the propensity to identify oneself as having higher socioeconomic standing. The observed response categories are tied to the latent variable by the measurement model. Thus when the latent $y^*$ crosses a cutpoint, the observed category changes. Anderson (1984) referred to ordinal variables created in this fashion as grouped continuous variables and referred to the ORM as the grouped continuous model. 7.1 Relationship between observed y and latent y* in ORM with one independent variable\rFor a single independent variable, the structural model is $y^* = \\alpha + \\beta x + \\epsilon$, which is plotted in figure 7.1 along with the cutpoints for the measurement model. This figure is similar to that for the BRM, except that there are now three horizontal lines representing the cutpoints $\\tau_1$, $\\tau_2$, and $\\tau_3$. The three cutpoints lead to four levels of $y$ that are labeled on the right-hand side of the graph. The probability of an observed outcome $y$ for a given value of $x$, represented by the three vertical lines in the figure, is the area under the curve between a pair of cutpoints. For example, the probability of observing $y = m$ for given values of the $x$’s corresponds to the region of the distribution where $y^*$ falls between $\\tau_{m-1}$ and $\\tau_m$: $$\\Pr(y=m\\mid\\mathbf{x})=\\Pr\\left(\\tau_{m-1}\\leq y^*\u003c\\tau_m\\mid\\mathbf{x}\\right)$$ Substituting $x\\beta + \\epsilon$ for $y^*$ and using some algebra leads to the standard formula for the predicted probability in the ORM. $$\\mathrm{Pr}\\left(y=m\\mid\\mathbf{x}\\right)=F\\left(\\tau_{m}-\\mathbf{x}\\beta\\right)-F\\left(\\tau_{m-1}-\\mathbf{x}\\beta\\right)$$ Where $F$ is the cumulative distribution function for $\\epsilon$. In the ordinal probit model, $F$ is normal with $Var(e) = 1$; in the ordinal logit model, $F$ is logistic with $Var(e) = \\frac{\\pi^2}{3}$. For $y = 1$, the second term on the right drops out because $F(-\\infty - x\\beta) = 0$, and for $y = J$, the first term equals $F(\\infty - x\\beta) = 1$. 假设我们想了解学生的考试成绩 $ y $ 如何受到学习时间 $ x $ 的影响。我们可以使用一个称为“有序回归模型（ORM）”的统计工具来分析这个关系。在这个模型中，我们假设学习时间 $ x $ 对考试成绩 $ y $ 有影响，但不是直接的影响，而是通过一个隐变量 $ y^* $ 来间接影响。这个隐变量 $ y^* $ 可以被理解为一个学生的潜在考试能力，即在没有任何其他影响因素的情况下，学生能够取得的最佳成绩。 结构模型：结构模型描述了潜在变量 $y^※$ 和自变量 $x$ 之间的关系。在这个例子中，我们假设学生的考试成绩 $y^※$ 受到学习时间 $x$ 的影响。这种影响被表示为一个线性关系：$y^※= \\alpha + \\beta x + \\epsilon$。其中，$\\alpha$和 $\\beta$ 是模型的参数，表示了$x$对$y$的影响程度，$\\epsilon$ 是一个随机误差项，代表了其他未考虑到的因素对 $y^*$ 的影响。让我们通过一个例子来解释这个结构模型： 假设我们有一组学生，他们的学习时间 $x$ 分别为10小时、20小时和30小时。我们想知道他们的潜在考试能力 $y^*$ 是多少。根据结构模型，我们可以用下面的公式来计算： $$ \\begin{align*} \\text{学生1的 } y^* \u0026= \\alpha + \\beta \\times 10 + \\epsilon_1 \\end{align*} $$ $$ \\begin{align*} \\text{学生2的 } y^* \u0026= \\alpha + \\beta \\times 20 + \\epsilon_2 \\end{align*} $$ $$ \\begin{align*} \\text{学生3的 } y^* \u0026= \\alpha + \\beta \\times 30 + \\epsilon_3 \\end{align*} $$ 其中，$\\epsilon_1, \\epsilon_2, \\epsilon_3$ 分别代表了每个学生的随机误差。 测量模型：测量模型描述了潜在变量 $y^※$ 和观测到的变量 $y","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:1:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.1.2 A nonlinear probability model The ORM can also be developed as a nonlinear probability model without appealing to an underlying latent continuous variable. Here we show how this is done for the ordinal logit model. First, we define the odds that an outcome is less than or equal to $m$ versus greater than $m$ given $x$: $$\\Omega_{\\leq m|\u003em}\\left(\\mathbf{x}\\right)\\equiv\\frac{\\mathrm{Pr}\\left(y\\leq m\\mid\\mathbf{x}\\right)}{\\mathrm{Pr}\\left(y\u003em\\mid\\mathbf{x}\\right)}\\quad\\mathrm{for}m=1,J-1$$ For example, we could compute the odds of lower- or working-class identification (that is, $rn \u003c 2$) versus middle- or upper-class identification ($rn \u003e 2$). The log of the odds is assumed to equal $$\\ln\\Omega_{\\leq m|\u003em}\\left(\\mathbf{x}\\right)=\\tau_m-\\mathbf{x}\\boldsymbol{\\beta}$$ Critically, the $\\beta$’s are the same for all values of $m$. For a single independent variable and three categories, where we are fixing the intercept to equal 0 and estimating the $r$’s, the model is: $$\\ln\\frac{\\Pr\\left(y\\leq1\\mid\\mathbf{x}\\right)}{\\Pr\\left(y\u003e1\\mid\\mathbf{x}\\right)}=\\tau_{1}-\\beta x$$ $$\\ln\\frac{\\operatorname*{Pr}\\left(y\\leq2\\mid\\mathbf{x}\\right)}{\\operatorname*{Pr}\\left(y\u003e2\\mid\\mathbf{x}\\right)}=\\tau_{2}-\\beta x$$ Although it may seem confusing that we subtract $r \\cdot x$ rather than adding it, this is a consequence of computing the logit of $y \u003c r_m$ versus $y \u003e r_m$. We agree that it would be simpler to stick with $r_m + fix$, but this is not the way the model is normally presented. 在有序Logit模型中，我们想要研究一个有序分类变量 $y$，比如社会阶层。这个模型是如何描述和预测这个变量的呢？让我们一步步来看。 首先，我们有一个称为“几率比”的概念，它描述了一个结果小于或等于某个值 $m$ 的概率与大于 $m$ 的概率之间的关系。这个比率表示为 $\\Omega_{\\leq m|\u003em}(\\mathbf{x})$，其中 $\\mathbf{x}$ 是自变量的值。这个比率可以用以下公式表示： $$ \\Omega_{\\leq m|\u003em}(\\mathbf{x}) = \\frac{\\mathrm{Pr}(y \\leq m \\mid \\mathbf{x})}{\\mathrm{Pr}(y \u003e m \\mid \\mathbf{x})} \\quad \\text{for } m=1,J-1 $$ 这个比率告诉我们在给定自变量 $\\mathbf{x}$ 的情况下，结果 $y$ 小于或等于 $m$ 的概率与大于 $m$ 的概率之间的关系。这对于理解结果的相对概率非常有帮助。 接下来，我们使用对数几率来描述这个比率。对数几率是对上面比率的对数，可以用以下公式表示： $$ \\ln\\Omega_{\\leq m|\u003em}(\\mathbf{x}) = \\tau_m - \\mathbf{x}\\boldsymbol{\\beta} $$ 在这里，$\\tau_m$ 是与结果 $m$ 相关的常数参数，$\\boldsymbol{\\beta}$ 是自变量 $\\mathbf{x}$ 的参数。这个公式告诉我们，对数几率由两部分组成：一个与结果 $m$ 相关的常数部分 $\\tau_m$，以及一个与自变量 $\\mathbf{x}$ 相关的部分，通过参数 $\\boldsymbol{\\beta}$ 来表示。 最后，当我们只有一个自变量和三个结果时，模型可以简化为两个方程，它们描述了不同结果之间的对数几率。这些方程是： $$ \\ln\\frac{\\Pr(y \\leq 1 \\mid \\mathbf{x})}{\\Pr(y \u003e 1 \\mid \\mathbf{x})} = \\tau_{1} - \\beta x $$ $$ \\ln\\frac{\\Pr(y \\leq 2 \\mid \\mathbf{x})}{\\Pr(y \u003e 2 \\mid \\mathbf{x})} = \\tau_{2} - \\beta x $$ 在这里，参数 $r$ 是一个特殊的参数，表示不同结果之间的分界点。 困惑的一点是为什么在计算对数几率时要减去 $\\beta x$ 而不是加上它。这其实是因为我们在计算 $y \u003c r_m$ 与 $y \u003e r_m$ 的Logit几率时所采取的方法不同。在Logit模型中，我们通常对一个事件发生的概率取对数几率，以便于建模和分析。在这种情况下，对于有序Logit模型，我们计算的是结果小于某个分界点 $r_m$ 的对数几率和结果大于 $r_m$ 的对数几率。因此，为了计算这两个对数几率，我们需要考虑 $\\beta x$ 的影响。 作者提到了一种更简单的处理方法，即使用 $r_m$ + $\\betax$，但这不是该模型通常被呈现的方式。这可能是因为 $\\beta x$ 的形式更加直观，能够更清晰地表达出结果小于或大于分界点 $r_m$ 的对数几率之间的关系。尽管这种处理方法可能会导致一些困惑，但它在实践中是有效的，并且能够更好地解释模型的原理。 ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:1:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.2 Estimation using ologit and oprobit The ordered logit and probit models can be fit with the following commands and their basic options: ologit depvar [indepvars] [if] [m] [weight] [, vce(vcetype) or] oprobit depvar [indepvars] [if] [in] [weight] [, vce(vcetype)] In our experience, these models take more steps to converge than either models for binary outcomes fit using logit or probit or models for nominal outcomes fit using mlogit. Variable lists depvar is the dependent variable. The values assigned to the outcome categories are irrelevant, except that larger values are assumed to correspond to “higher” outcomes. For example, if you had three outcomes, you could use the values 1, 2, and 3, or —1.23, 2.3, and 999. To avoid confusion, however, we recommend coding your dependent variable as consecutive integers beginning with 1. indepvars is a list of independent variables. If indepvars is not included, Stata fits a model with only cutpoints. Specifying the estimation sample if and in qualifiers. These can be used to restrict the estimation sample. For example, if you want to fit an ordered logit model for only those surveyed in 1980 (year = 1), you could specify ologit class i.female i.white i.educ age inc if year == 1. Listwise deletion. Stata excludes cases in which there are missing values for any of the variables in the model. Accordingly, if two models are fit using the same dataset but have different sets of independent variables, it is possible to have different samples. We recommend that you use mark and markout (discussed in section 3.1.G) to explicitly remove cases with missing data. Weights and complex samples Both ologit and oprobit can be used with fweights, pweights, and iweights. Survey estimation can be done using the svy prefix. See section 3.1.7 for details. Options vce(vcetype) specifies the type of standard errors to be computed. See section 3.1.9 for details. or reports odds ratios for the ordered logit model. Additional options and information can be found in the Stata manual entries [R] ologit and [r] oprobit. ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:2:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.2.1 Example of ordinal logit model Our example is based on a question asked in the 1980, 1996, and 2012 General Social Surveys. These are repeated cross-sectional data, not panel data. That is, in each wave the survey was administered to new respondents from a new nationally representative sample. The following variables are in the model: use gssclass4, clear codebook class female white year ed age income, compact Variable Obs Unique Mean Min Max Label\r------------------------------------------------------------------------------\rclass 5620 4 2.437544 1 4 subjective class id\rfemale 5620 2 .5491103 0 1 respondent is female\rwhite 5620 2 .8140569 0 1 resondent is whte\ryear 5620 3 2.070996 1 3 year of GSS survey\reduc 5620 3 2.064769 1 3 educational attainment\rage 5620 72 45.15712 18 89 age of respondent\rincome 5620 62 68.07737 .51205 324.2425 household income\r------------------------------------------------------------------------------\rRespondents were asked to indicate the social class to which they think they most belong, using categories coded 1 = lower, 2 = working, 3 = middle, and 4 = upper. The resulting variable class has the distribution: tab class subjective |\rclass id | Freq. Percent Cum.\r------------+-----------------------------------\rlower | 394 7.01 7.01\rworking | 2,567 45.68 52.69\rmiddle | 2,465 43.86 96.55\rupper | 194 3.45 100.00\r------------+-----------------------------------\rTotal | 5,620 100.00\rThe variable educ is a categorical variable in which the categories are less than a high school diploma, high school diploma, and college diploma. The variable income is measured in 2012 dollars for all years of the survey. Using these data, we use ologit to fit the model $$\\mathrm{Pr}(\\mathbf{c}1\\mathbf{ass}=m\\mid\\mathbf{x_i})=F(\\tau_{m}-\\mathbf{x}\\beta)-F(\\tau_{m-1}-\\mathbf{x}\\beta)$$ where $$\\mathbf{x\\beta}=\\beta_\\text{female}{\\text{female}+\\beta_\\text{white}{ \\mathrm{white}}}$$ $$\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad+\\beta_{\\mathrm{year}\\left[1996\\right]}\\text{(year==1996)}+\\beta_{\\mathrm{year}\\left[2012\\right]}\\text{(year==2012)}$$ $$\\qquad \\qquad \\qquad \\qquad \\qquad \\quad +\\beta_{\\text{educ [ha only]}}\\text{(educ==2)}+\\beta_{\\text{educ}[\\text{college}]}\\text{(educ==3)}$$ + β c . a g e c.age + β c . a g e # c . a g e c.age#c.age + β income i n c o m e To specify the model, we use the factor-variable notation i.year to create indicators for the year of the survey, i.educ for education, and c.age##c.age to include age and age-squared, estimates store is used so that we can later make a table containing these results. ologit class i.female i.white i.year i.educ c.age##c.age income, nolog estimates store ologit Ordered logistic regression Number of obs = 5,620\rLR chi2(9) = 1453.95\rProb \u003e chi2 = 0.0000\rLog likelihood = -5016.2107 Pseudo R2 = 0.1266\r------------------------------------------------------------------------------\rclass | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rfemale |\rfemale | 0.02 0.05 0.30 0.765 -0.09 0.12\r|\rwhite |\rwhite | 0.24 0.07 3.28 0.001 0.09 0.38\r|\ryear |\r1996 | -0.08 0.07 -1.16 0.247 -0.22 0.06\r2012 | -0.50 0.08 -6.59 0.000 -0.65 -0.35\r|\reduc |\rhs only | 0.37 0.08 4.73 0.000 0.22 0.52\rcollege | 1.57 0.10 15.99 0.000 1.37 1.76\r|\rage | -0.05 0.01 -5.31 0.000 -0.07 -0.03\r|\rc.age#c.age | 0.00 0.00 7.65 0.000 0.00 0.00\r|\rincome | 0.01 0.00 22.20 0.000 0.01 0.01\r-------------+----------------------------------------------------------------\r/cut1 | -2.14 0.23 -2.59 -1.69\r/cut2 | 0.92 0.23 0.47 1.36\r/cut3 | 4.93 0.24 4.46 5.41\r------------------------------------------------------------------------------\rThe information in the header and the table of coefficients is in the same form as discussed in chapters 3 and 5, with the addition of estimates for the cutpoints at the end. Next, we fit the ordered probit model: oprobit class i.female i.white i.year i.educ c.age##c.age income, nolog estimates store oprobit","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:2:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.2.2 Predicting perfectly If either the highest or the lowest category of the dependent variable does not vary within one of the categories of an independent variable, there will be a problem with estimation. To see what happens, we created an artificial example with a dummy variable for whether respondents have a college degree. Tabulating college against class shows that in all cases where college is 1, respondents have values of class equal to 4, indicating upper-class identification. capture drop college gen college = (educ == 3 \u0026 class == 4) label var college \"Has college degree?\" label def college 0 no 1 yes label val college college tab class college subjective | Has college degree?\rclass id | no yes | Total\r-----------+----------------------+----------\rlower | 394 0 | 394 working | 2,567 0 | 2,567 middle | 2,465 0 | 2,465 upper | 81 113 | 194 -----------+----------------------+----------\rTotal | 5,507 113 | 5,620 Accordingly, if you know college is 1, you can predict perfectly that class is 4. Although we purposely constructed college so this would happen, perfect prediction occurs in real data, especially when samples are small or one of the outcome values is infrequent. When we fit the ordered logit model with college as a regressor, the perfectly predicted observations are retained in the estimation sample with a warning message appearing below the table of estimates. ologit class i.female i.white i.year i.college c.age##c.age income, nolog The note reflects that the standard error for college is enormous, indicating the problem that occurs when trying to estimate a coefficient that is effectively infinite. Another way of thinking about the large standard error is that the lack of variation in the outcome when college equals 1 means we do not have any information that would permit us to estimate the coefficient with precision. When this happens, our next step is to drop the 113 cases for which college equals 1 (you could use the command drop if college==1 to do this) and refit the model without college. This is done automatically for binary models fit by logit and probit (see section 5.2.3). There is no problem if an independent variable perfectly predicts one of the middle categories. For example, if all observations for which college is 1 reported being middle class, this would not cause problems for estimation. ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:2:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.3 Hypothesis testing Hypothesis tests of regression coefficients can be evaluated with the $z$ statistics in the estimation output, with test and testparm for Wald tests of simple and complex hypotheses, and with lrttest for likelihood-ratio tests. We briefly review each. See section 3.2 for additional information on these commands. ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:3:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.3.1 Testing individual coefficients If the assumptions of the model hold, the maximum likelihood estimators from ologit and oprobit are distributed asymptotically normally. The hypothesis $H_{0}:\\beta_{k}=\\beta^{*}$ can be tested with z = ( β ^ k − β ∗ ) / σ ^ β ^ k Under the assumptions justifying maximum likelihood, if $H_0$ is true, then $z$ is distributed approximately normally with a mean of 0 and a variance of 1 for large samples. For example, consider the results for the variable white from the ologit output above. We are using the sformat option to show more decimal places for the $z$ statistic 2: 当我们使用**ologit和oprobit**模型时，我们试图找到一组参数，这些参数可以最好地描述我们观察到的数据。其中一些参数代表我们感兴趣的变量对结果的影响。 假设我们对其中一个参数感兴趣，我们想知道这个参数的值是否等于某个特定值，比如说 $\\beta^*$。我们可以进行一种假设检验来验证这个问题。这种检验告诉我们，如果我们的假设是正确的，那么我们从数据中计算的统计量（称为 $z$ 统计量）应该会遵循一个特定的分布，即正态分布。 这个 $z$ 统计量实际上是一个数字，它告诉我们观察到的参数值与我们假设的值之间有多大的差异，以及这个差异有多大可能是由随机因素引起的。如果这个差异很大，而且很少可能是由随机因素引起的，那么我们就会得到一个很大的 $z$ 值。 当我们对很多样本进行分析时，我们可以看到这个 $z$ 值的分布。如果它们聚集在某个特定的范围内，而不是分散在整个范围内，那么我们就可以相信我们的假设是正确的。 在这种情况下，如果我们的假设是正确的，那么这个 $z$ 统计量的平均值应该接近于零，而且不同样本的 $z$ 值的变化应该是一样的。这就是所谓的“均值为0，方差为1”的正态分布。 We are displaying more decimal places to later demonstrate the equivalence of the z test and the corresponding chi-squared test. With any estimation command, the option format(fmt)(fmt) can be used to format the display of coefficients and standard errors. Likewise, option pformat formats the display of p-values and option sformat(fmt) formats the display of the test. Alternatively, the set command can also be used to change these formats either permanently for the rest of the current Stata session. See [R] set format in the Stata manual for details ologit class i.female i.white i.year i.educ c.age##c.age income, /// nolog sformat(%8.3f) Ordered logistic regression Number of obs = 5,620\rLR chi2(9) = 1453.95\rProb \u003e chi2 = 0.0000\rLog likelihood = -5016.2107 Pseudo R2 = 0.1266\r------------------------------------------------------------------------------\rclass | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rfemale |\rfemale | 0.02 0.05 0.298 0.765 -0.09 0.12\r|\rwhite |\rwhite | 0.24 0.07 3.277 0.001 0.09 0.38\r|\ryear |\r1996 | -0.08 0.07 -1.158 0.247 -0.22 0.06\r2012 | -0.50 0.08 -6.594 0.000 -0.65 -0.35\r|\reduc |\rhs only | 0.37 0.08 4.730 0.000 0.22 0.52\rcollege | 1.57 0.10 15.994 0.000 1.37 1.76\r|\rage | -0.05 0.01 -5.308 0.000 -0.07 -0.03\r|\rc.age#c.age | 0.00 0.00 7.646 0.000 0.00 0.00\r|\rincome | 0.01 0.00 22.203 0.000 0.01 0.01\r-------------+----------------------------------------------------------------\r/cut1 | -2.14 0.23 -2.59 -1.69\r/cut2 | 0.92 0.23 0.47 1.36\r/cut3 | 4.93 0.24 4.46 5.41\r------------------------------------------------------------------------------\rWe conclude the following: Whites and nonwhites significantly differ in their subjective social class identification (z = 3.28, p \u003c 0.01, two-tailed) Either a one-tailed or a two-tailed test can be used, as discussed in chapter 5 The z test in the output of estimation commands is a Wald test, which can also be computed using test. For example, to test $H_0{:}\\beta_{\\mathrm{white}}=0$,type test 1.white ( 1) [class]1.white = 0\rchi2( 1) = 10.74\rProb \u003e chi2 = 0.0011\rWe conclude the following: Whites and nonwhites significantly differ in their class identification $(x^{2}=10.74,df=1,p\u003c0.01).$ The value of a chi-squared test with 1 degree of freedom is identical to the square of the corresponding z test, which can be demonstrated with the display command: display \"z*z=\" 3.277*3.277 z*z=10.738729 A likelihood-ratio LR test is computed by comparing the log likelihood from a full model with that from a restricted model. To test a single coefficient, we begin by fitting the full model and storing the estimates: ologit class i.female i.white i.year i.educ c.age##c.age income, nolog estimates store fullmodel Then, we fit a model that excludes the variable white th at we want to test ologit class i.female","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:3:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.3.2 Testing multiple coefficients We can also test complex hypotheses that involve more than one coefficient. For example, our model has the demographic variables white, female, and age. To test that the effects of these variables are simultaneously equal to 0—that is, $H_0{:}\\beta_{\\mathrm{white}}=\\beta_{\\mathrm{female}}=\\beta_{\\mathrm{age}}=\\beta_{\\mathrm{age*age}}=0$ We can use either a Wald or an LR test. For the Wald test, we fit the full model and then use the test command: ologit class i.female i.white i.year i.educ c.age##c.age income, nolog test 1.white 1.female age age#age ( 1) [class]1.white = 0\r( 2) [class]1.female = 0\r( 3) [class]age = 0\r( 4) [class]c.age#c.age = 0\rchi2( 4) = 226.75\rProb \u003e chi2 = 0.0000\rBefore we interpret the results of the test, we want to clarify how coefficients are specified in the test command when factor-variable notation is used. The specification i.white added the variable 1.white to the model as shown in the output to ologit above. Accordingly, we are testing the coefficient associated with the variable 1.white, not i.white or white. The same rule applies for female. Age was entered into the model as c.age##c.age, which was expanded to estimate coefficients for c.age and c.age#c.age. When entering these coefficients into test, we do not need to include the c. prefix (although we could do so). Regardless of how we specify the test command, we conclude the following: The hypothesis that the demographic effects of age, race, and sex are simultaneously equal to 0 can be rejected at the 0.01 level (χ² = 226.8, df = 4, p \u003c 0.01). To compute an LR test of multiple coefficients, we first fit the full model and store the results with estimates store. Suppose we are interested in whether demographic characteristics matter at all for subjective class identification or whether identification is only a function of socioeconomic status and changes over time. To test $H_0{:}\\beta_{\\mathrm{white}}=\\beta_{\\mathrm{female}}=\\beta_{\\mathrm{age}}=\\beta_{\\mathrm{age*age}}=0$, we fit the model that excludes these four coefficients and run lrtest: ologit class i.female i.white i.year i.educ c.age##c.age income, nolog estimates store fullmodel ologit class i.year i.educ income, nolog estimates store dropdemog lrtest fullmodel dropdemog Likelihood-ratio test\rAssumption: dropdemog nested within fullmodel\rLR chi2(4) = 236.53\rProb \u003e chi2 = 0.0000\rWe conclude the following: The hypothesis that the demographic effects of age, race, and sex are simultaneously equal to 0 can be rejected at the 0.01 level (LR χ² = 236.5, df = 4, p \u003c 0.01). We find that the Wald and LR tests usually lead to the same decisions, and there is no reason why you would typically want to compute both tests. When there are differences, they generally occur when the tests are near the cutoff for statistical significance. Because the LR test is invariant to reparameterization, we prefer the LR test when both are available. However, only the Wald test can be used if robust standard errors, probability weights, or survey estimation are used. When a factor variable has more than two categories, such as year and educ in our model, you can specify each of the coefficients with test (for example, test 2.educ 3.educ) or you can use testparm: ologit class i.female i.white i.year i.educ c.age##c.age income, nolog testparm i.educ ( 1) [class]2.educ = 0\r( 2) [class]3.educ = 0\rchi2( 2) = 329.48\rProb \u003e chi2 = 0.0000\rtest can also be used to test the equality of coefficients, as shown in section 5.3.2. ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:3:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.4 Measures of fit using fitstat As we discussed in greater detail in chapter 3, scalar measures of fit can be used when comparing competing models (also see Long [1997, 85-113]). Several measures can be computed after either ologit or oprobit by using the SPost command fits. In this example, we compare a model for class identification that includes age but not age-squared with the model we have been using that includes age-squared: ologit class i.female i.white i.year i.educ age income, nolog quietly fitstat, save ologit class i.female i.white i.year i.educ c.age##c.age income, nolog fitstat, diff | Current Saved Difference -------------------------+--------------------------------------\rLog-likelihood | Model | -5016.211 -5045.903 29.692 Intercept-only | -5743.186 -5743.186 0.000 -------------------------+--------------------------------------\rChi-square | D(df=5608/5609/-1) | 10032.421 10091.806 -59.385 LR(df=9/8/1) | 1453.951 1394.566 59.385 p-value | 0.000 0.000 0.000 -------------------------+--------------------------------------\rR2 | McFadden | 0.127 0.121 0.005 McFadden(adjusted) | 0.124 0.119 0.005 McKelvey \u0026 Zavoina | 0.284 0.274 0.011 Cox-Snell/ML | 0.228 0.220 0.008 Cragg-Uhler/Nagelkerke | 0.262 0.252 0.009 Count | 0.605 0.600 0.005 Count(adjusted) | 0.273 0.264 0.009 -------------------------+--------------------------------------\rIC | AIC | 10056.421 10113.806 -57.385 AIC divided by N | 1.789 1.800 -0.010 BIC(df=12/11/1) | 10136.030 10186.781 -50.751 -------------------------+--------------------------------------\rVariance of | e | 3.290 3.290 0.000 y-star | 4.596 4.528 0.067 Note: Likelihood-ratio test assumes saved model nested in current model.\rDifference of 50.751 in BIC provides very strong support for current model.\rThe Bayesian information criterion (BIC), Akaike’s information criterion (AIC), and the LR test each provide evidence supporting the inclusion of age-squared in the model. Using simulations, Hagle and Mitchell (1992) and Windmeijer (1995) found that with ordinal outcomes, McKelvey and Zavoina’s pseudo-R² most closely approximates the R² obtained by fitting the LRM on the underlying latent variable. If you are using y*-standardized coefficients to interpret the ORM (see section 7.8.1), McKelvey and Zavoina’s R² could be used as a counterpart to the R² from the LRM. ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:4:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.5 (Advanced) Converting to a different parameterization We mark this section as advanced because the conversion we show is likely only pertinent to readers who also work with other statistics packages that fit the model by using the alternative parameterization. The section may still be useful to strengthen your understanding of how the intercept and thresholds of these models are related, as well as how the lincom command works. Earlier, we noted that the model can be identified by fixing either the intercept or one of the thresholds to equal 0. Stata sets $\\beta_{0}=0$ and estimates $\\tau_{1}$,whereas some programs fix $\\tau_{1}=0$ and estimate $\\beta_{0}$ Although all quantities of interest for interpretation (for example, predicted probabilities) are the same under both parameterizations, it is useful to see how Stata can fit the model with either parameterization. We can understand how this is done with the following equation, where we are simply adding $0=\\delta-\\delta $ and rearranging terms: Without further constraints, it is possible to estimate the differences $\\tau_{m}-\\delta $ and $\\beta_{0}-\\delta $, but not the parameters $\\tau_m$ and $\\beta_{0}$. To identify the model, Stata assumes $\\delta=\\beta_{0}$ which forces the estimate of $\\beta_{0}$ to be 0, Some programs assume $\\delta=\\tau_{1}$, which forces the estimate of $\\tau_{1}$ to be 0. The following table shows the differences in the parameterizations: Although you would only need to estimate the alternative parameterization if you wanted to compare your results with those produced by another statistics package,Seeing how this is done illustrates why the intercept and thresholds are arbitrary. To estimate the alternative parameterization, we use lincom to compute the difference between Stata’s estimates and the estimated value of the first cutpoint. We begin with the alternative parameterization of the intercept: ologit class i.female i.white i.year i.educ c.age##c.age income, nolog lincom 0 - _b[/cut1] // intercept ( 1) - [/]cut1 = 0\r------------------------------------------------------------------------------\rclass | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r(1) | 2.14 0.23 9.39 0.000 1.69 2.59\r------------------------------------------------------------------------------\rTo understand the lincom command, you need to know that _b[/cut1] is how Stata refers to the estimate of the first cutpoint. Accordingly, 0 - _b[/cut1] is the difference between 0 and the estimate of the first cutpoint, which simply reverses the sign of the first estimated cutpoint. For the other cutpoints, we are estimating $\\tau_2-\\tau_1$ and $\\tau_3-\\tau_1$, which correspond to _b[/cut2] - _b[/cut1] and _b[/cut3] - _b[/cut1]: lincom _b[/cut2] - _b[/cut1] // cutpoint 2 ( 1) - [/]cut1 + [/]cut2 = 0\r------------------------------------------------------------------------------\rclass | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r(1) | 3.06 0.06 53.29 0.000 2.94 3.17\r------------------------------------------------------------------------------\rlincom _b[/cut3] - _b[/cut1] // cutpoint 3 ( 1) - [/]cut1 + [/]cut3 = 0\r------------------------------------------------------------------------------\rclass | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r(1) | 7.08 0.11 64.44 0.000 6.86 7.29\r------------------------------------------------------------------------------\rThe estimate of $\\tau_3-\\tau_1$ is, of course, 0. These estimates would match those from a program using the alternative parameterization. ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:5:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.6 The parallel regression assumption 平行回归假设指的是在有序回归模型中的一个重要假设，即每个自变量对于每个结果类别的影响是平行的。简单来说，就是当自变量的取值变化时，不同结果类别之间的关系是保持一致的，只是存在一定的偏移或平移。 具体来说，在有序回归模型中，我们考虑多个有序的结果类别，比如低、中、高三个阶段。平行回归假设表明，自变量对于每个阶段的影响是一致的，只是在不同的阶段之间存在一定的平移或偏移。换句话说，自变量的影响随着结果类别的增加是保持不变的。 这个假设在统计建模中起着重要作用，因为它简化了模型的解释和推断过程。然而，在实际应用中，这个假设并不总是成立，因此需要进行相应的检验和处理。如果平行回归假设被拒绝，就需要考虑使用其他不需要这种假设的模型。 假设我们对一种药物治疗心脏病的效果进行研究，我们将患者的病情分为轻、中、重三个等级。我们想知道药物治疗时间对于病情的改善有何影响。 根据平行回归假设，药物治疗时间对于不同等级病情的改善效果应该是一致的，只是存在一定的平移。举个例子，假设我们发现增加药物治疗时间可以显著降低每个等级病情的严重程度，但是降低的幅度可能不同。比如，增加一周的药物治疗时间可能对于轻病情的改善效果是30%，对于中等病情是25%，对于重病情是20%。 这就是平行回归假设的含义，即自变量（药物治疗时间）对于不同等级的病情改善效果是平行的，只是存在一定的偏移。也就是说，药物治疗时间对于病情的改善效果随着病情等级的增加是保持一致的，只是改善的幅度有所不同。 然而，如果平行回归假设不成立，就意味着药物治疗时间对于不同等级的病情改善效果并不是平行的，可能存在交叉效应或者其他的差异。在这种情况下，我们需要考虑使用其他模型来更准确地描述药物治疗时间与病情之间的关系。 Before discussing interpretation, it is important to understand an assumption that is implicit in the ORM, known both as the parallel regression assumption and, for the ordinal logit model, the proportional-odds assumption. Using Equation (7.1), the ORM with $J$ outcome categories can be written as: $$\\Pr\\left(y=1\\mid\\mathbf{x}\\right)=F\\left(\\tau_1-\\mathbf{x}\\boldsymbol{\\beta}\\right)$$ $$\\Pr\\left(y=m\\mid\\mathbf{x}\\right)=F\\left(\\tau_{m}-\\mathbf{x}\\boldsymbol{\\beta}\\right)-F\\left(\\tau_{m-1}-\\mathbf{x}\\boldsymbol{\\beta}\\right)\\mathrm{~for~}m=2\\mathrm{~to~}J-1$$ $$\\Pr\\left(y=J\\mid\\mathbf{x}\\right)=1-F\\left(\\tau_{J-1}-\\mathbf{x}\\boldsymbol{\\beta}\\right)$$ Using these equations, the cumulative probabilities have the simple form $$\\Pr(y\\leq m\\mid\\mathbf{x})=F(\\tau_m-\\mathbf{x}\\boldsymbol{\\beta})\\quad\\mathrm{for~}m=1\\mathrm{~to~}J-1$$ Notice that $\\beta$ does not have a subscript $m$. Accordingly, this equation shows that the ORM is equivalent to $J - 1$ binary regressions with the critical assumption that the slope coefficients are identical in each binary regression. For example, with four outcomes and one independent variable, the cumulative probability equations are: $$\\begin{aligned}\\Pr\\left(y\\leq1\\mid\\mathbf{x}\\right)\u0026=F\\left(\\tau_1-\\beta x\\right)\\\\Pr\\left(y\\leq2\\mid\\mathbf{x}\\right)\u0026=F\\left(\\tau_2-\\beta x\\right)\\\\Pr\\left(y\\leq3\\mid\\mathbf{x}\\right)\u0026=F\\left(\\tau_3-\\beta x\\right)\\end{aligned}$$ Recall that the intercept $a$ is not in the equation because it was assumed to equal 0 to identify the model. These equations lead to the following figure: 第一部分：介绍 在这段文字中，作者讨论了有序回归模型（ORM）的一个重要假设，即“平行回归假设”或者在有序logit模型中叫做“比例几率假设”。 这个假设告诉我们，在这个模型中，无论有多少个结果类别，每一次增加一个类别，自变量对结果的影响都是一致的。换句话说，无论是第一个结果还是最后一个结果，自变量的影响都是一样的。 第二部分：方程表示 具体来说，我们用一系列的方程来表示每个结果类别的发生概率。比如，对于第一个结果类别，概率是根据一个函数 $ F $，结合一些参数 $ \\tau_1 $ 和 $ \\beta $ 计算得到的。这个方程是： $ \\Pr(y=1|\\mathbf{x}) = F(\\tau_1 - \\mathbf{x}\\boldsymbol{\\beta}) $ 第三部分：解释阈值参数 而对于其它类别，概率的计算与前面的类别有关，通过减去前一个类别的概率来得到。这些方程中包含了一些参数 $ \\tau_1, \\tau_2, …, \\tau_{J-1} $，它们是用来决定在哪些点概率发生变化的。 这些阈值参数告诉我们，在自变量的不同取值下，从一个结果类别跳到另一个结果类别的临界点在哪里。 第四部分：斜率参数的解释 还有一个参数 $ \\boldsymbol{\\beta} $，表示自变量对结果的影响。在这些方程中，参数 $ \\boldsymbol{\\beta} $ 是相同的，不管是哪个结果类别。这就是为什么我们说这个模型遵循“平行回归假设”，因为每个类别之间的关系都是平行的，斜率参数都相同。 这意味着，自变量对于每个结果类别的影响是一致的，只是在不同结果类别间有一定的偏移。这就是为什么我们称之为“平行回归假设”。 第五部分：方程的简化形式 在这个模型中，我们可以进一步简化方程，得到一种更简单的形式来表示累积概率。这种形式告诉我们，在给定自变量 $ \\mathbf{x} $ 的情况下，结果小于等于某个特定类别的累积概率是多少。这个形式如下： $ \\Pr(y \\leq m|\\mathbf{x}) = F(\\tau_m - \\mathbf{x}\\boldsymbol{\\beta}) \\quad \\text{for } m=1 \\text{ to } J-1 $ 第六部分：参数的一致性 最后，需要注意的是，尽管方程中的参数 $ \\boldsymbol{\\beta} $ 在不同的结果类别中是相同的，但这并不意味着这个参数在整个模型中是固定的。事实上，在模型估计过程中，我们会通过拟合数据来估计参数的值，以使模型尽可能地拟合数据。 因此，即使在这个假设下，参数 $ \\boldsymbol{\\beta} $ 是相同的，但在实际应用中，我们仍然需要对它进行估计，以获得最优的模型拟合效果。 Each probability curve differs only in being shifted to the left or right. The curves are parallel as a consequence of the assumption that the $\\beta’s$ are equal for each equation. This figure suggests that the parallel regression assumption can be tested by comparing the estimates from $J-1$ binary regressions. $$\\mathrm{Pr}(y\\leq m\\mid\\mathbf{x})=F(\\tau_m-\\mathbf{x}\\boldsymbol{\\beta}_m)\\quad\\mathrm{for}m=1\\mathrm{to}J-1$$ where the $\\beta’s$ are allowed to differ across the equations. The model","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:6:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.6.1 Testing the parallel regression assumption using oparallel oparallel can be used after ologit to compute the omnibus tests described above. The ic option provides the statistics AIC and BIC comparing the generalized ordered logit model with the ordered logit model. ssc install oparallel ologit class i.female i.white i.year i.educ c.age##c.age income, nolog oparallel, ic Tests of the parallel regression assumption\r| Chi2 df P\u003eChi2\r-----------------+----------------------\rWolfe Gould | 127.1 16 0.000\rBrant | 137 16 0.000\rscore | 144.9 16 0.000\rlikelihood ratio | 126.3 16 0.000\rWald | 141.2 16 0.000\rInformation criteria\r| ologit gologit difference ------+-----------------------------------\rAIC | 10609.85 10515.55 94.29 BIC | 10682.82 10694.67 -11.85 The results labeled “likelihood ratio” and “Wald” are the LR and Wald tests based on the generalized ordered logit model. The line “Wolfe Gould” contains the approximate LR test, “Brant” refers to the Brant test, and “score” is the score test. All tests reject the null hypothesis with $p \u003c 0.001$. The score and Wald tests have similar values, while the two LR tests are larger. We find that these tests are often, perhaps usually, significant. The AIC and BIC statistics can be used to evaluate the trade-off between the better fit of the generalized model and the loss of parsimony from having $J - 1$ coefficients for each independent variable instead of just one. In this example, the smaller values of both the AIC and BIC statistics for gologit compared with ologit provide evidence against the ologit model compared with the model in which the parallel regression assumption is relaxed. It is common for the BIC statistic to prefer the ologit model even when the significance tests reject the parallel regression assumption, and this sometimes happens with AIC as well. Although oparallel can be used with ologit but not with oprobit, the approximate LR test presented as “Wolfe Gould” can be performed with oprobit by using the user-written command omodel. omodel, however, does not support factor-variable notation. ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:6:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.6.2 Testing the parallel regression assumption using brant The SPost command brant also computes the Brant test for the ORM. The advantage of our command, which is used by oparallel to make its computations, is that it provides separate tests for each of the independent variables in the model. After running ologit, you run brant, which has the following syntax: brant [,detail] The detail option provides a table of coefficients from each of the binary models. For example, brant, detail Estimated coefficients from binary logits\r-----------------------------------------------\rVariable | y_gt_1 y_gt_2 y_gt_3 -------------+---------------------------------\rfemale |\rfemale | -0.382 -0.027 -0.135 | -3.43 -0.46 -0.90 |\rwhite |\rwhite | 0.323 0.396 0.088 | 2.60 5.11 0.42 |\ryear |\r1996 | -0.328 -0.141 -0.110 | -2.10 -1.91 -0.57 2012 | -1.075 -0.406 -0.309 | -6.94 -4.98 -1.45 |\reduc |\rhs only | 0.841 0.497 -0.151 | 6.89 5.96 -0.59 college | 2.668 2.092 1.456 | 10.26 20.70 5.96 |\rage | -0.006 -0.015 0.051 | -0.36 -1.54 1.88 |\rc.age#c.age | 0.000 0.000 -0.000 | 0.37 3.85 -1.28 |\r_cons | 2.377 -1.210 -5.286 | 5.60 -5.02 -7.34 -----------------------------------------------\rLegend: b/t\rBrant test of parallel regression assumption\r| chi2 p\u003echi2 df\r-------------+------------------------------\rAll | 137.04 0.000 16\r-------------+------------------------------\r1.female | 10.02 0.007 2\r1.white | 2.34 0.310 2\r2.year | 1.45 0.483 2\r3.year | 18.38 0.000 2\r2.educ | 14.41 0.001 2\r3.educ | 12.05 0.002 2\rage | 5.87 0.053 2\rc.age#c.age | 10.13 0.006 2\rA significant test statistic provides evidence that the parallel\rregression assumption has been violated.\rThe largest violation is for income, which may indicate particular problems with the parallel regression assumption for this variable. Looking at the coefficients from the binary logits, we see that for income the estimates from the binary logit of lower class versus working/middle/upper class differ from the other two binary logits. This suggests that income differences matter more for whether people report themselves as lower class than it does for either of the other thresholds. If the focus of our project was the relationship between income and subjective class identification, this would serve as a substantively interesting finding that we would have missed had we not used brant. 这段话介绍了使用 brant 命令进行 Brant 测试的方法和结果解释： Brant 测试：Brant 测试用于测试平行回归假设是否成立。这个假设是有序回归模型中的一个重要假设，即每个自变量对于每个结果类别的影响是平行的。 使用方法：首先，使用 ologit 命令拟合有序 logit 模型。然后，运行 brant 命令来进行 Brant 测试。该命令的语法是 brant [,detail]。使用 detail 选项可以得到每个二元模型的系数表格。 结果解释：在 Brant 测试的结果中，对每个自变量进行了单独的平行回归假设检验。如果检验统计量显著，则意味着平行回归假设被违反。 系数表格：系数表格展示了每个自变量在不同结果类别下的系数估计值，以及对应的 t 值。此外，还提供了对平行回归假设检验的结果。如果检验统计量显著，则表示对应的自变量违反了平行回归假设。 解释：如果某个自变量的平行回归假设检验显著，说明该自变量在不同结果类别下的影响不是平行的，即不同结果类别之间的效应存在差异。在本例中，发现收入（income）变量的违反程度最大，这可能意味着该变量对于结果类别的影响存在特定的问题。系数表格也显示了收入变量在不同二元模型中的估计值存在差异，这表明了收入对于被调查者报告自己为下层阶级的影响与其他两个阈值相比更加显著。如果研究项目的重点是收入与主观阶级认同之间的关系，那么这将作为一个具有实质性意义的发现。 ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:6:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.6.3 Caveat regarding the parallel regression assumption In the majority of the real-world applications of the ORM that we have seen, the hypothesis of parallel regressions is rejected. Keep in mind, however, that the tests of the parallel regression assumption are sensitive to other types of misspecification. Further, we have seen examples where the parallel regression assumption is violated but the predictions from ologit are very similar to those from the generalized ordered logit model or the multinomial logit model. When the hypothesis is rejected, consider alternative models that do not impose the constraint of parallel regressions. As illustrated in chapter 8, fitting the multinomial logit model on a seemingly ordinal outcome can lead to quite different conclusions. The generalized ordered logit model is another alternative to consider. Violation of the parallel regression assumption is not, however, a rationale for using the LRM. The assumptions implied by the application of the LRM to ordinal data are even stronger. 这段话讲述了在实际应用中，我们经常会看到平行回归假设被拒绝的情况。然而，需要记住的是，对平行回归假设的检验对于其他类型的错误规范也很敏感。此外，我们也看到过违反平行回归假设的例子，但是来自 ologit 的预测结果与广义有序 logit 模型或多项式 logit 模型的预测结果非常相似。当假设被拒绝时，应考虑不需要平行回归约束的备选模型。正如在第 8 章中所示，对看似有序的结果进行多项式 logit 模型拟合可能会得出完全不同的结论。广义有序 logit 模型是另一个要考虑的备选方案。然而，违反平行回归假设并不是使用多项式回归模型的理由。将多项式回归模型应用于有序数据所隐含的假设甚至更加严格。 ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:6:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.7 Overview of interpretation Most of the rest of the chapter focuses on interpreting results from the ORM. First, we consider methods of interpretation that are based on transforming the coefficients. If the idea of a latent variable makes substantive sense, or if you are tempted to run a linear regression on an ordinal outcome, interpretations based on rescaling $y^*$ to compute standardized coefficients can be used just like coefficients for the LRM. Coefficients can also be exponentiated and interpreted as odds ratios in the ordered logit model. We examine these strategies of interpretation first because they follow most straightforwardly from the methods of interpretation many readers are already familiar with from linear regression, but we regard them also as having important limitations that we discuss. We then consider approaches to interpretation that use predicted probabilities, extending each of the methods for the LRM to multiple outcomes. We typically find these approaches far more informative. Because the ORM is nonlinear in the outcome probabilities, no approach can fully describe the relationship between a variable and the outcome probabilities. Consequently, you should consider each of these methods before deciding which approach is most effective in your application. As with models for binary outcomes, the basic command for interpretations based on predictions is margins, although our mtable, mchange, and mgen commands make things much simpler. Not only do these commands have the advantages illustrated for binary models in chapter 6, but when there are multiple outcome categories, margins can only compute predictions for one outcome at a time. Our commands will compute predictions for all categories and combine the results. ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:7:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.8 Interpreting transformed coefficients As with the binary response model (BRM), coefficients for the ordered logit model are about 1.7 times larger than those for the ordered probit model because of the arbitrary assumption about the variance of the error term. For this reason, neither ordered logit nor ordered probit coefficients offer a direct interpretation that is readily meaningful. There are two ways we can transform the coefficients into more meaningful quantities: standardization and odds ratios. In both cases, these interpretations are permissible only when the independent variable is not specified using polynomial or interaction terms. ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:8:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.8.1 Marginal change in y* In the ORM, $y*=\\mathbf{x}\\boldsymbol{\\beta}+\\varepsilon $ and the marginal change in $y^*$ with respect to $x_k$ is $$\\frac{\\partial y^*}{\\partial x_k}=\\beta_k$$ Because $y*$ is latent, its true metric is unknown. The value of $y*$depends on the identification assumption we make about the variance of the errors. As a result, the marginal change in $y*$cannot be interpreted without standardizing by the estimated standard deviation of $y*$,which is computed as $$\\widehat{\\sigma}_{y^{*}}^{2}=\\widehat{\\beta}^{\\prime}\\widehat{\\mathrm{Var}}\\left(\\mathbf{x}\\right)\\widehat{\\beta}+\\mathrm{Var}\\left(\\varepsilon\\right)$$ where ${\\widehat{\\mathrm{Var}}}\\left(\\mathbf{x}\\right)$ is the covariance matrix for the observed $x’s,\\widehat{\\beta}$ contains maximum likelihood estimates, and $\\mathrm{Var}(\\varepsilon)=1$ for ordered probit and $\\pi^{2}/3$ for ordered logit. Then the $y^*$ standardized coefficient for $x_k$ is $$\\beta_{k}^{Sy^* }=\\frac{\\beta_{k}}{\\sigma_{y^*}}$$ which can be interpreted as follows: For a unit increase in $x_k$,$y^* $ expected to increase by $\\beta_{k}^{\\mathbf{S}y^{*}}$ standard deviations, holding all other variables constant. The fully standardized coefficient is $$\\beta_{k}^{S}=\\frac{\\sigma_{k}\\beta_{k}}{\\sigma_{y^{*}}}$$ which can be interpreted as follows: For a standard deviation increase in $x_k$,$y^* $ is expected to inciease by $\\beta_{k}^{S}$ standard deviations, holding all other variables constant. $y^*$的定义和边际变化： 在有序反应模型（ORM）中，$y^*$表示一个未观测到的变量，可以用以下方程表示： $$ y^* = \\mathbf{x}\\boldsymbol{\\beta} + \\varepsilon $$ 这里，$\\mathbf{x}$是自变量向量，$\\boldsymbol{\\beta}$是参数向量，$\\varepsilon$是误差项。 对于特定自变量$x_k$，$y^*$关于$x_k$的边际变化等于与该变量相关的系数$\\beta_k$： $$ \\frac{\\partial y^*}{\\partial x_k} = \\beta_k $$ $y^*$的标准化和解释： 由于$y^*$的真实尺度未知，需要标准化才能解释其含义。标准化后的系数表示单位变化引起的标准偏差变化。 对于自变量$x_k$，其标准化系数为： $$\\beta_{k}^{Sy^* }=\\frac{\\beta_{k}}{\\sigma_{y^*}}$$ 其中，$\\sigma_{y^* }$是$y^*$的估计标准差。 完全标准化系数： 完全标准化系数考虑了自变量 $x_k$ 和 $y^* $ 的标准差，用于衡量单位标准差变化对$y^* $的影响。 对于自变量$x_k$，其完全标准化系数为： $$\\beta_{k}^{S}=\\frac{\\sigma_{k}\\beta_{k}}{\\sigma_{y^{*}}}$$ 使用listcoef命令计算： 这些系数是通过listcoef命令和std选项计算得出的。 让我们考虑一个学生的考试成绩预测模型的例子： 考试成绩模型： 我们正在研究一个模型，用于预测学生的考试成绩$y^*$。我们假设考试成绩受到学习时间和课外活动的影响。 考试成绩模型是一个线性模型，可以表示为： $$ y^* = \\beta_0 + \\beta_1 \\times \\text{学习时间} + \\beta_2 \\times \\text{课外活动} + \\varepsilon $$ 在这个方程中，$\\beta_0$是截距，$\\beta_1$和$\\beta_2$分别是学习时间和课外活动的系数，$\\varepsilon$是未观测到的因素对考试成绩的影响。 标准化系数的解释： 标准化系数用于比较不同变量对$y^* $的影响。假设我们估计出的$\\beta_1$为5，表示每增加一个单位的学习时间，$y^* $预期会增加5个单位。 但是，学习时间的单位可能与考试成绩的单位不同，因此我们需要将系数标准化。 标准化系数$\\beta_{1}^{Sy^* }$是系数$\\beta_1$除以$y^* $的标准差$\\sigma_{y^* }$。如果$\\beta_1$为5，而$\\sigma_{y^* }$为10，那么： $$ \\beta_{1}^{Sy^* } = \\frac{5}{10} = 0.5 $$ 这意味着每增加一个标准差的学习时间，$y^* $预期会增加0.5个标准差。 完全标准化系数的解释： 完全标准化系数考虑了自变量和$y^* $的标准差。假设我们还知道课外活动的标准差为3，而$y^* $的标准差仍然为10。 完全标准化系数$\\beta_{1}^{S}$是系数$\\beta_1$乘以自变量$x$的标准差，再除以$y^* $的标准差。如果$\\beta_1$为5，学习时间的标准差为2，那么： $$ \\beta_{1}^{S} = \\frac{2 \\times 5}{10} = 1 $$ 这意味着每增加一个标准差的学习时间，$y^*$预期会增加1个标准差。 These coefficients are computed using listcoef with the std option. For example, after fitting the ordered logit model, ologit class i.female i.white i.year i.educ c.age##c.age income, nolog listcoef, std help ologit (N=5620): Unstandardized and standardized estimates Observed SD: 0.6749\rLatent SD: 2.1437\r-------------------------------------------------------------------------------\r| b z P\u003e|z| bStdX bStdY bStdXY SDofX\r-------------+-----------------------------------------------------------------\rfemale |\rfemale | 0.0162 0.298 0.765 0.008 0.008 0.004 0.498\r|\rwhite |\rwhite | 0.2363 3.277 0.001 0.092 0.110 0.043 0.389\r|\ryear |\r1996 | -0.0799 -1.158 0.247 -0.040 -0.037 -0.019 0.498\r2012 | -0.5039 -6.594 0.000 -0.233 -0.235 -0.109 0.463\r|\reduc |\rhs only | 0.3705 4.730 0.000 0.183 0.173 0.085 0.493\rcollege | 1.5656 15.994 0.000 0.670 0.730 0.313 0.428\r|\rage | -0.0488 -5.308 0.000 -0.825 -0.023 -0.385 16.897\r|\rc.age#c.age | 0.0007 7.646 0.000 1.202 0.000 0.561 1695.148\r|\rincome | 0.0116 22.203 0.000 0.770 0.005 0.359 66.258\r-------","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:8:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.8.2 Odds ratios The ordinal logit model (but not the ordinal probit model) can also be interpreted using odds ratios. Equation (7.2) defined the ordered logit model as $$\\Omega_{\\leq m|\u003em}\\left(\\mathbf{x}\\right)=\\exp\\left(\\tau_m-\\mathbf{x}\\boldsymbol{\\beta}\\right)$$ For example, with four outcomes we would simultaneously estimate the three equations $$\\Omega_{\\leq1|\u003e1}\\left(\\mathbf{x}\\right)=\\exp\\left(\\tau_1-\\mathbf{x}\\boldsymbol{\\beta}\\right)$$ $$\\Omega_{\\leq2|\u003e2}\\left(\\mathbf{x}\\right)=\\exp\\left(\\tau_{2}-\\mathbf{x\\beta}\\right)$$ $$\\Omega_{\\leq3|\u003e3}\\left(\\mathbf{x}\\right)=\\exp\\left(\\tau_{3}-\\mathbf{x\\beta}\\right)$$ Using the same approach as shown for binary logit, the effect of a unit change in $x_k$ equals $$\\frac{\\Omega_{\\leq m|\u003em}\\left(\\mathbf{x},x_k+1\\right)}{\\Omega_{\\leq m|\u003em}\\left(\\mathbf{x},x_k\\right)}=e^{-\\beta_k}=\\frac{1}{e^{\\beta_k}}$$ The value of the odds ratio does not depend on the value of $m$, which is why the parallel regression assumption is also known as the proportional-odds assumption. We could interpret the odds ratio as follows: For a unit increase in $x_k$, the odds of a lower outcome compared with a higher outcome are changed by the factor $\\exp(-\\beta_k)$, holding all other variables constant. For a change in $x_k$ of $\\delta$, $$\\frac{\\Omega_{\\leq m|\u003em}\\left(\\mathrm{x},x_k+\\delta\\right)}{\\Omega_{\\leq m|\u003em}\\left(\\mathrm{x},x_k\\right)}=\\exp\\left(-\\delta\\times\\beta_k\\right)=\\frac{1}{\\exp\\left(\\delta\\times\\beta_k\\right)}$$ which we interpret as follows: For an increase of $\\delta$ in $x_k$， the odds of a lower outcome compared with a higher outcome change by a factor of exp$\\left(-\\delta\\times\\beta_{k}\\right)$ , holding all other variables constant. Notice that the odds ratio is derived by changing one variable, $x_k$, while holding all other variables constant. Accordingly, you do not want to compute the odds ratio for a variable that is included as a polynomial (for example, age and age-squared) or is included in an interaction term. 有序logit模型方程式（7.2）： $$ \\Omega_{\\leq m|\u003em}(\\mathbf{x}) = \\exp(\\tau_m - \\mathbf{x}\\boldsymbol{\\beta}) $$ 这个方程描述了一种统计模型，用于预测一系列有序结果的概率。$\\Omega_{\\leq m|\u003em}(\\mathbf{x})$表示在给定一组特征$\\mathbf{x}$的情况下，得到结果小于或等于$m$的概率与得到结果大于$m$的概率之比。$\\tau_m$是模型中的一个参数，$\\mathbf{x}$是包含特征值的向量，$\\boldsymbol{\\beta}$是与特征相关的参数向量。 各个等式的解释： 对于有四个结果的情况，我们需要同时估计三个方程： 第一个方程表示等于或小于1的结果的概率与大于1的结果的概率之比。 $$\\Omega_{\\leq1|\u003e1}\\left(\\mathbf{x}\\right)=\\exp\\left(\\tau_1-\\mathbf{x}\\boldsymbol{\\beta}\\right)$$ 第二个方程表示等于或小于2的结果的概率与大于2的结果的概率之比。 $$\\Omega_{\\leq2|\u003e2}\\left(\\mathbf{x}\\right)=\\exp\\left(\\tau_{2}-\\mathbf{x\\beta}\\right)$$ 第三个方程表示等于或小于3的结果的概率与大于3的结果的概率之比。 $$\\Omega_{\\leq3|\u003e3}\\left(\\mathbf{x}\\right)=\\exp\\left(\\tau_{3}-\\mathbf{x\\beta}\\right)$$ 赔率比的计算公式： $$ \\frac{\\Omega_{\\leq m|\u003em}(\\mathbf{x},x_k+1)}{\\Omega_{\\leq m|\u003em}(\\mathbf{x},x_k)} = e^{-\\beta_k} = \\frac{1}{e^{\\beta_k}} $$ 这个公式告诉我们，当一个特征值$x_k$增加一个单位时，结果小于或等于$m$的概率与结果大于$m$的概率之比会以$e^{-\\beta_k}$的因子改变，而其他特征保持不变。$\\beta_k$是与特征$x_k$相关的参数。 赔率比的解释： 赔率比的值不依赖于结果的具体取值，因此被称为平行回归假设或比例赔率假设。对赔率比的解释是，对于$x_k$的单位增加，结果较低与结果较高之间的赔率会以$\\exp(-\\beta_k)$的因子改变，而其他变量保持不变。 当然，让我们通过一个简单的例子来说明这些概念。 假设我们正在研究一个考试成绩的有序结果：不及格、及格、良好、优秀。我们想知道学生的学习时间（特征$x_k$）对于各个考试成绩的影响。 有序logit模型方程式： $$ \\Omega_{\\leq m|\u003em}(\\mathbf{x}) = \\exp(\\tau_m - \\mathbf{x}\\boldsymbol{\\beta}) $$ 在我们的例子中，$\\Omega_{\\leq m|\u003em}(\\mathbf{x})$表示在给定学习时间$\\mathbf{x}$的情况下，得到结果小于或等于某个等级$m$（比如“及格”）的概率与得到结果大于$m$的概率之比。$\\tau_m$是模型中的一个参数，表示不同等级之间的分界点，$\\mathbf{x}$是学习时间的值，$\\boldsymbol{\\beta}$是与学习时间相关的参数。 赔率比的计算公式： $$ \\frac{\\Omega_{\\leq m|\u003em}(\\mathbf{x},x_k+1)}{\\Omega_{\\leq m|\u003em}(\\mathbf{x},x_k)} = e^{-\\beta_k} = \\frac{1}{e^{\\beta_k}} $$ 这个公式告诉我们，当学习时间增加一个单位时，结果小于或等于某个等级$m$（比如“及格”）的概率与结果大于$m$的概率之比会以$e^{-\\beta_k}$的因子改变，而其他特征保持不变。$\\beta_k$是与学习时间$x_k$相关的参数。 假设我们的模型告诉我们，$\\beta_k = 0.5$。这意味着当学习时间增加一个单位时，结果小于或等于某个等级$m$的概率与结果大于$m$的概率之比会减少一半。 例如，如果学生平均每天学习3小时（$x_k = 3$），那么结果小于或等于“及格”的概率与结果大于“及格”的概率之比为$e^{-0.5} \\approx 0.606$。这意味着考试成绩达到及格的概率是考试成绩高于及格的概率的0.606倍。 当然，让我用更具体的数值来展示计算过程。 假设我们的模型给出的参数是 $\\beta_k = 0.5$，而学生平均每天学习3小时（$x_k = 3$）。现在我们来计算结果小于或等于“及格”的概率与结果大于“及格”的概","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:8:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.9 Interpretations based on predicted probabilities As noted, we usually prefer interpretations based on predicted probabilities. We find these interpretations to be both clearer for our own thinking and more effective with audiences. Probabilities can be estimated with the formula $$\\widehat{\\mathrm{Pr}}\\left(y=m\\mid\\mathbf{x}\\right)=F\\left(\\widehat{\\tau_m}-\\mathbf{x}\\widehat{\\beta}\\right)-F\\left(\\widehat{\\tau}_{m-1}-\\mathbf{x}\\widehat{\\beta}\\right)$$ Cumulative probabilities are computed as $$\\widehat{\\Pr}\\left(y\\leq m\\mid\\mathbf{x}\\right)=\\sum_{k\\leq m}\\widehat{\\Pr}\\left(y=k\\mid\\mathbf{x}\\right)=F\\left(\\widehat{\\tau}_m-\\mathbf{x}\\widehat{\\boldsymbol{\\beta}}\\right)$$ The values of x can be based on observations in the sample or can be hypothetical values of interest. 让我详细解释这两个公式的含义，并给出一个例子。 预测概率公式： $$ \\widehat{\\mathrm{Pr}}\\left(y=m\\mid\\mathbf{x}\\right) = F\\left(\\widehat{\\tau_m}-\\mathbf{x}\\widehat{\\beta}\\right)-F\\left(\\widehat{\\tau}_{m-1}-\\mathbf{x}\\widehat{\\beta}\\right) $$ 这个公式用于计算在给定特征$\\mathbf{x}$的情况下，观测到结果为$m$的概率。$\\widehat{\\mathrm{Pr}}\\left(y=m\\mid\\mathbf{x}\\right)$表示结果等于$m$的概率的估计值。$F$代表累积分布函数，通常是指标函数的累积分布函数，它将一个值映射到概率上。$\\widehat{\\tau_m}$是模型中的一个参数，表示结果为$m$的临界值，$\\widehat{\\beta}$是模型中与特征相关的参数。 累积概率公式： $$ \\widehat{\\Pr}\\left(y\\leq m\\mid\\mathbf{x}\\right) = \\sum_{k\\leq m}\\widehat{\\Pr}\\left(y=k\\mid\\mathbf{x}\\right) = F\\left(\\widehat{\\tau}_m-\\mathbf{x}\\widehat{\\boldsymbol{\\beta}}\\right) $$ 这个公式用于计算在给定特征$\\mathbf{x}$的情况下，观测到结果小于或等于$m$的概率。$\\widehat{\\Pr}\\left(y\\leq m\\mid\\mathbf{x}\\right)$表示结果小于或等于$m$的概率的估计值。这个概率是对从结果等于1到结果等于$m$的所有可能结果的概率进行求和得到的。$\\widehat{\\tau}_m$是模型中的一个参数，表示结果为$m$的临界值，$\\widehat{\\boldsymbol{\\beta}}$是模型中与特征相关的参数向量。 例子： 假设我们正在研究一个学生考试成绩的模型，特征$\\mathbf{x}$包括学习时间和家庭背景等因素。我们想要估计一个学生考试成绩为及格的概率，并计算学生考试成绩小于或等于良好的概率。 假设我们的模型给出以下估计值：$\\widehat{\\tau_1} = 0$，$\\widehat{\\tau_2} = 1$，$\\widehat{\\beta} = 0.5$。现在，我们有一个学生，他每天学习3小时。 首先，我们计算学生考试成绩为及格的概率： $$ \\begin{align*} \\widehat{\\mathrm{Pr}}(y=\\text{及格}|\\mathbf{x}) \u0026= F(\\widehat{\\tau_2} - \\mathbf{x}\\widehat{\\beta}) - F(\\widehat{\\tau_1} - \\mathbf{x}\\widehat{\\beta}) \\ \u0026= F(1 - 3 \\times 0.5) - F(0 - 3 \\times 0.5) \\ \u0026= F(1.5) - F(-1.5) \\end{align*} $$ 这个结果是一个估计值，表示学生考试成绩为及格的概率。 接下来，我们计算学生考试成绩小于或等于良好的概率： $$ \\widehat{\\Pr}(y\\leq \\text{良好}|\\mathbf{x}) = F(\\widehat{\\tau}_3 - \\mathbf{x}\\widehat{\\boldsymbol{\\beta}}) $$ 在这个例子中，我们假设良好的成绩对应的临界值是$\\widehat{\\tau}_3 = 2$。因此： $$ \\begin{align*} \\widehat{\\Pr}(y\\leq \\text{良好}|\\mathbf{x}) \u0026= F(\\widehat{\\tau}_3 - \\mathbf{x}\\widehat{\\boldsymbol{\\beta}}) \\ \u0026= F(2 - 3 \\times 0.5) \\ \u0026= F(0.5) \\end{align*} $$ 这个结果是一个估计值，表示学生考试成绩小于或等于良好的概率。 The following sections use predicted probabilities in a variety of ways. We begin by examining the distribution of predictions for each observation in the estimation sample as a first step in evaluating your model. Next, we show how marginal effects provide an overall assessment of the impact of each variable. To focus on particular types of respondents, we compute predictions for ideal types defined by substantively motivated characteristics for all independent variables. Extending methods from chapter 6, we show how to statistically test differences in the predictions between ideal types. For categorical predictors, tables of predictions computed as these variables change is an effective way to demonstrate the effects of these variables is to use tables of predictions computed as these variables change. An important challenge is to decide where to hold the values of other variables when making predictions. Finally, we plot predictions as a continuous independent variable changes. ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:9:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.10 Predicted probabilities with predict After fitting a model with ologit or oprobit, a useful first step for assessing your model is to compute the in-sample predictions with the command predict newvar1 [ newvar2 [newvar3··· ]] [if] [in] where you specify one new variable name for each category of the dependent variable. For instance, in the following example, predict specifies that the variables prlover, prworking, prmiddle, and prupper be created with predicted values for the four outcome categories: ologit class i.female i.white i.year i.educ c.age##c.age income, nolog predict prlower prworking prmiddle prupper Ordered logistic regression Number of obs = 5,620\rLR chi2(9) = 1453.95\rProb \u003e chi2 = 0.0000\rLog likelihood = -5016.2107 Pseudo R2 = 0.1266\r------------------------------------------------------------------------------\rclass | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rfemale |\rfemale | 0.02 0.05 0.30 0.765 -0.09 0.12\r|\rwhite |\rwhite | 0.24 0.07 3.28 0.001 0.09 0.38\r|\ryear |\r1996 | -0.08 0.07 -1.16 0.247 -0.22 0.06\r2012 | -0.50 0.08 -6.59 0.000 -0.65 -0.35\r|\reduc |\rhs only | 0.37 0.08 4.73 0.000 0.22 0.52\rcollege | 1.57 0.10 15.99 0.000 1.37 1.76\r|\rage | -0.05 0.01 -5.31 0.000 -0.07 -0.03\r|\rc.age#c.age | 0.00 0.00 7.65 0.000 0.00 0.00\r|\rincome | 0.01 0.00 22.20 0.000 0.01 0.01\r-------------+----------------------------------------------------------------\r/cut1 | -2.14 0.23 -2.59 -1.69\r/cut2 | 0.92 0.23 0.47 1.36\r/cut3 | 4.93 0.24 4.46 5.41\r------------------------------------------------------------------------------\r(option pr assumed; predicted probabilities) The message (option pr assumed; predicted probabilities) reflects that predict can compute many different quantities. Because we did not specify an option indicating which quantity to predict, the default option pr for predicted probabilities was assumed. Predictions in the sample are useful for getting a general sense of what is going on in your model and can be useful for uncovering problems in your data. For example, if there are observations where the predicted probability of being in prlower (or any other outcome) are noticeably larger or smaller than the other predictions, you might check whether there are data problems for those observations. The range of predictions can also give you a rough idea of how large marginal effects can be for a given outcome. If the range of probabilities is small within the estimation sample, the effects of the independent variables will also be small. If the distribution of predictions has multiple modes — let’s say, two — it suggests there could be a binary predictor that is important. Although sometimes the distribution of predictions leads to additional data checking or model revision, often it only assures you that the predictions are reasonable and that you are ready for the methods of interpretation that we consider in the remainder of this chapter. 好的，让我详细解释一下。 样本中的预测值：假设我们有一个模型，用来预测学生考试成绩的可能结果。对于每个学生，我们可以使用这个模型来预测他们可能取得的不同等级的考试成绩，比如不及格、及格、良好和优秀。这些预测值告诉我们，根据模型，每个学生将以多大的概率获得不同等级的成绩。 检测数据问题：对于第二条，我们讨论的是样本中的预测值可能帮助我们发现数据中存在的问题。 当我们观察到某些观测的预测概率明显高于或低于其他观测时，这可能表明数据中存在问题。例如，如果某些观测的预测概率远高于其他观测，这可能是因为这些观测包含了异常值，或者数据记录可能存在错误。 举个例子，假设我们正在研究一个模型，用来预测学生考试成绩的可能结果。我们发现，某些学生的预测概率特别高，远高于其他学生。这可能意味着这些学生的数据存在问题，比如他们的成绩可能被错误地记录在了高分段，或者他们的学习时间等特征值可能存在异常值。 通过观察这些异常的预测值，我们可以发现并修正数据中的问题，例如，我们可以重新检查这些学生的成绩记录，或者确认他们的特征值是否正确录入。这样，我们就可以提高模型的准确性，并确保我们的分析结果是可靠的。 边际效应的估计：当我们说预测值的范围可以帮助我们了解模型的边际效应时，我们是指模型对于不同情况的反应程度。边际效应是指独立变量的一个单位变化如何影响因变量的变化。如果模型的预测值范围很小，意味着模型对于不同情况的反应差异不大，这可能表明模型的边际效应较小。换句话说，即使独立变量有所变化，模型对于结果的预测也不会有太大变化。 举个例子来说明，假设我们正在研究一个模型，用来预测学生考试成绩的可能结果。我们发现，无论学生的学习时间是少还是多，模型对于他们的考试成绩的预测都差不多。这就意味着学生的学习时间对于考试成绩的影响并不显著，模型的边际效应较小。 而当预测值的范围很大时，意味着模型对于不同情况的反应差异较大，这可能表明模型的边际效应较大。换句话说，独立变量的变化会导致结果的较大变化。 检测重要预测变量：当我们观察到预测分布中存在多个峰值时，这可能意味着有一个重要的二元预测变量影响着结果。换句话说，这种现象可能暗示着某个二元变量对于结果的影响非常重要。通过观察这些预测分布的特征，我们可以更深入地了解模型中各个变量的影响，并相应地调整我们的分析方法。 总之，通过观察样本中的预测结果，我们可以更好地了","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:10:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.11 Marginal effects The marginal change in the probability of outcome m is computed as $$\\frac{\\partial\\Pr(y=m\\mid\\mathbf{x})}{\\partial x_k}=\\frac{\\partial F(\\tau_m-\\mathbf{x}\\boldsymbol{\\beta})}{\\partial x_k}-\\frac{\\partial F(\\tau_{m-1}-\\mathbf{x}\\boldsymbol{\\beta})}{\\partial x_k}$$ which is the slope of the curve relating $x_k$ to $\\Pr(y=m|\\mathbf{x})$, holding all other variables constant. The value of the marginal change depends on the value of $x_k$ where the change is evaluated, as well as the values of all other $x’s$ Because the marginal change can be misleading when the probability curve is changing rapidly, we usually prefer using discrete change. The discrete change is the change in the probability of $m$ for a change in $x_k$ from the start value $x_k^{\\mathrm{start}}$ to the end value $x_{k}^{\\mathrm{end}}$ (for example, a change from $x_k = 0$ to $x_k = 1$), holding all other $x$’s constant. Formally, $$\\frac{\\Delta\\Pr\\left(y=m\\mid\\mathbf{x}\\right)}{\\Delta x_k\\left(x_k^{\\mathrm{start}}\\to x_k^{\\mathrm{cod}}\\right)}=\\Pr\\left(y=m\\mid\\mathbf{x},x_k=x_k^{\\mathrm{cod}}\\right)-\\Pr\\left(y=m\\mid\\mathbf{x},x_k=x_k^{\\mathrm{start}}\\right)$$ where $\\Pr\\left(y=m\\mid\\mathbf{x},x_k\\right)$ is the probability that $y = m$ given $x$, noting a specific value for $X_k$.The change indicates that when $x_k$ changes from $x_{k}^{\\mathrm{start}}\\text{ to }x_{k}^{\\mathrm{end}}$ the probability of outcome m changes by $\\Delta\\Pr\\left(y=m\\mid\\mathbf{x}\\right)/\\Delta x_{k}$ holding all other variables at the specific values in $x$. The magnitude of the discrete change depends on the value at which $X_k$ starts, the amount of change in $x_k$, and the values of all other variables. For both marginal and discrete change, we can compute average marginal effects (AMEs), marginal effects at the mean (MEMs), or marginal effects at representative values other than the means. As with the BRM, we find AMEs to be the most useful summary of the effects, thus we consider AMEs for the ORM in this section. MEMs are considered briefly in section 7.15. Examining the distribution of effects over observations is also valuable. To save space, we do not illustrate this in the current chapter, but this can be done using the commands presented in section 8.8.1. 当然，以下是你要求的完整答案： 首先，让我们来详细解释这两个公式。 第一个公式： $$ \\frac{\\partial\\Pr(y=m\\mid\\mathbf{x})}{\\partial x_k} = \\frac{\\partial F(\\tau_m-\\mathbf{x}\\boldsymbol{\\beta})}{\\partial x_k} - \\frac{\\partial F(\\tau_{m-1}-\\mathbf{x}\\boldsymbol{\\beta})}{\\partial x_k} $$ 这个公式表示了因变量 $y$ 为 $m$ 的概率 $\\Pr(y=m\\mid\\mathbf{x})$ 对于自变量 $x_k$ 的边际变化。让我们逐步分解这个公式： $\\frac{\\partial\\Pr(y=m\\mid\\mathbf{x})}{\\partial x_k}$：表示因变量 $y$ 为 $m$ 的概率随着 $x_k$ 的变化而变化的速率。 $\\frac{\\partial F(\\tau_m-\\mathbf{x}\\boldsymbol{\\beta})}{\\partial x_k}$：表示 $x_k$ 的微小变化对应的概率值 $F(\\tau_m-\\mathbf{x}\\boldsymbol{\\beta})$ 的变化量。 $\\frac{\\partial F(\\tau_{m-1}-\\mathbf{x}\\boldsymbol{\\beta})}{\\partial x_k}$：类似地，表示 $x_k$ 的微小变化对应的概率值 $F(\\tau_{m-1}-\\mathbf{x}\\boldsymbol{\\beta})$ 的变化量。 换句话说，这个公式告诉我们，当自变量 $x_k$ 变化时，因变量 $y$ 为 $m$ 的概率的变化量，是由两部分组成的：一部分是 $y$ 大于 $m$ 的概率的变化量，另一部分是 $y$ 小于等于 $m$ 的概率的变化量。 第二个公式： $$ \\frac{\\Delta\\Pr\\left(y=m\\mid\\mathbf{x}\\right)}{\\Delta x_k\\left(x_k^{\\mathrm{start}}\\to x_k^{\\mathrm{end}}\\right)} = \\Pr\\left(y=m\\mid\\mathbf{x},x_k=x_k^{\\mathrm{end}}\\right) - \\Pr\\left(y=m\\mid\\mathbf{x},x_k=x_k^{\\mathrm{start}}\\right) $$ 这个公式表示，在自变量 $x_k$ 从起始值 $x_k^{\\mathrm{start}}$ 变化到结束值 $x_{k}^{\\mathrm{end}}$ 的情况下，因变量 $y$ 为 $m$ 的概率的变化量。让我们逐步分解这个公式： $\\frac{\\Delta\\Pr\\left(y=m\\mid\\mathbf{x}\\right)}{\\Delta x_k\\left(x_k^{\\mathrm{start}}\\to x_k^{\\mathrm{end}}\\right)}$：表示自变量 $x_k$ 从起始值到结束值的变化对应的因变量 $y$ 为 $m$ 的概率的变化量。 $\\Pr\\left(y=m\\mid\\mathbf{x},x_k=x_k^{\\mathrm{end}}\\right)$：表示当 $x_k$ 变为结束值时，$y$ 为 $m$ 的概率。 $\\Pr\\left(y=m\\mid\\mathbf{x},x_k=x_k^{\\mathrm{start}}\\right)$：表示当 $x_k$ 是起始值时，$y$ 为 $m$ 的概率。 换句话说，这个公式告诉我们，当自变量 $x_k$ 从起始值变化到结束值时，因变量 $y$ 为 $m$ 的概率的变化量，是结束值时的概率减去起始值时的概率。 现在，让我们来举个例子来说明这两个公式的用法。 假设我们有一个模型，可以预测学生的考试成绩。我们想知道的是，当学生花更多时间学习时，他们的考试成绩会如何变化。 首先，我们使用第一个公式来计算边际变化。我们假设学生学习时间是一个自变量 $x_k","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:11:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.11.1 Plotting marginal effects As we suggested for the BRM, the AME is a valuable tool for examining the effects of your variables, and we often compute these effects for an initial review of the results of a model. Without doubt, AMEs are far more informative than the parameter estimates or odds ratios. There is, however, a lot of information to be absorbed. By default, for each continuous variable, mchange computes the marginal change and discrete changes for a 1-unit and a standard deviation change in continuous variables; for factor variables, mchange computes a discrete change from 0 to 1. One way to limit the amount of information is to only look at discrete changes of a standard deviation for continuous variables. For example, 当我们谈论一个自变量的平均边际效应时，我们实际上是在讨论这个自变量对于因变量的影响，而不是仅仅讨论它们之间的相关性或关系强度。平均边际效应提供了一个量化的指标，告诉我们当自变量的值发生变化时，因变量的预期变化量。 在实际应用中，平均边际效应通常是在拟合了统计模型之后计算得出的。比如，在逻辑回归模型中，我们可以计算出每个自变量对于因变量的平均边际效应，从而了解它们对于结果的实际影响。这种影响可以是因变量的变化概率，也可以是其他感兴趣的结果指标。 举个例子，假设我们在研究人们购买某种产品的决策时，使用了一个逻辑回归模型，并且其中一个自变量是收入水平。通过计算收入水平的平均边际效应，我们可以得知当收入增加一个单位时，购买该产品的概率预计会增加多少。这种量化的解释有助于我们更深入地理解模型结果，并且为决策提供更具体的指导。 ologit class i.female i.white i.year i.educ c.age##c.age income, nolog mchange, amount(sd) brief ologit: Changes in Pr(y) | Number of obs = 5620\rExpression: Pr(class), predict(outcome())\r| lower working middle upper ------------------------+-------------------------------------------\rfemale | female vs male | -0.001 -0.002 0.003 0.000 p-value | 0.766 0.765 0.765 0.765 white | white vs nonwhite | -0.016 -0.031 0.041 0.006 p-value | 0.002 0.001 0.001 0.001 year | 1996 vs 1980 | 0.004 0.012 -0.014 -0.003 p-value | 0.243 0.249 0.246 0.253 2012 vs 1980 | 0.033 0.067 -0.086 -0.014 p-value | 0.000 0.000 0.000 0.000 2012 vs 1996 | 0.029 0.055 -0.073 -0.011 p-value | 0.000 0.000 0.000 0.000 educ | hs only vs not hs grad | -0.029 -0.047 0.070 0.006 p-value | 0.000 0.000 0.000 0.000 college vs not hs grad | -0.079 -0.252 0.286 0.046 p-value | 0.000 0.000 0.000 0.000 college vs hs only | -0.050 -0.205 0.216 0.039 p-value | 0.000 0.000 0.000 0.000 age | +SD | -0.018 -0.071 0.067 0.022 p-value | 0.000 0.000 0.000 0.000 income | +SD | -0.036 -0.119 0.126 0.030 p-value | 0.000 0.000 0.000 0.000 Even so, there are a lot of coefficients. Fortunately, they can be quickly understood by plotting them. To explain how to do this, we begin by examining the AMEs for a standard deviation change in income and age. Because the model includes age and age-squared, when age is increased by a standard deviation, we need to increase age-squared by the appropriate amount. This is done automatically by Stata because we entered age into the model with the factor-variable notation c.age##c.age. To compute the average discrete changes for a standard deviation increase, type mchange age income, amount(sd) brief ologit: Changes in Pr(y) | Number of obs = 5620\rExpression: Pr(class), predict(outcome())\r| lower working middle upper -------------+-------------------------------------------\rage | +SD | -0.018 -0.071 0.067 0.022 p-value | 0.000 0.000 0.000 0.000 income | +SD | -0.036 -0.119 0.126 0.030 p-value | 0.000 0.000 0.000 0.000 mchange leaves these results in memory, and they are used by our mchangeplot command to create the plot. The horizontal axis indicates the magnitude of the effect, with the letters within the graph marking the discrete change for each outcome. For example, the M in the row for income shows that, on average for a standard deviation change in income, the probability of identifying with the middle class increases by 0.126. Overall, it is apparent that the effects of income are larger than those for a standard deviation change in age. For both variables, the effects are in the same directions with the same relative magnitudes. (Before proceeding, you should make sure you see how the graph corresponds to the output from mchange above.) The plot was produced with the following command: mchangeplot age income, symbols(L W M U) /// min(-.15) max(.15) gap(.05) a","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:11:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.11.2 Marginal effects for a quick overview AMEs are a much better way to obtain a quick overview of the magnitudes of effects than are the estimated coefficients. After fitting your model, you can obtain a table of all effects by simply typing mchange, perhaps restricting effects to discrete changes of a standard deviation: ologit class i.female i.white i.year i.educ c.age##c.age income, nolog mchange, amount(sd) brief ologit: Changes in Pr(y) | Number of obs = 5620\rExpression: Pr(class), predict(outcome())\r| lower working middle upper ------------------------+-------------------------------------------\rfemale | female vs male | -0.001 -0.002 0.003 0.000 p-value | 0.766 0.765 0.765 0.765 white | white vs nonwhite | -0.016 -0.031 0.041 0.006 p-value | 0.002 0.001 0.001 0.001 year | 1996 vs 1980 | 0.004 0.012 -0.014 -0.003 p-value | 0.243 0.249 0.246 0.253 2012 vs 1980 | 0.033 0.067 -0.086 -0.014 p-value | 0.000 0.000 0.000 0.000 2012 vs 1996 | 0.029 0.055 -0.073 -0.011 p-value | 0.000 0.000 0.000 0.000 educ | hs only vs not hs grad | -0.029 -0.047 0.070 0.006 p-value | 0.000 0.000 0.000 0.000 college vs not hs grad | -0.079 -0.252 0.286 0.046 p-value | 0.000 0.000 0.000 0.000 college vs hs only | -0.050 -0.205 0.216 0.039 p-value | 0.000 0.000 0.000 0.000 age | +SD | -0.018 -0.071 0.067 0.022 p-value | 0.000 0.000 0.000 0.000 income | +SD | -0.036 -0.119 0.126 0.030 p-value | 0.000 0.000 0.000 0.000 With a simple command, you can plot the effects: mchangeplot, sig(.05) symbols(L W M U) leftmargin(5) A quick review highlights which variables we might, want to examine more closely. ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:11:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.12 Predicted probabilities for ideal types Ideal types define substantively interesting cases in the data by specifying values of the independent variables. Predicted probabilities for these types of individuals (or whatever the unit of analysis may be) can be computed with mtable or margins. Unlike marginal effects, by comparing two or more ideal types, you can compare probabilities as a whole set of independent variables vary, not just a change in a single variable. In our example, ideal types can be used to examine what more and less advantaged individuals look like and how they differ in their class identification. For instance, we might want to compare the following hypothetical individuals surveyed in 2012: A 25-year-old, nonwhite man without a high school diploma and with a household income of $30,000$ per year. A 60-year-old, white woman with a college degree and with a household income of $150,000$ per year. To compute the predictions, we begin by using margins before showing how mtable can simplify the work. We use a at() to specify values of the independent variables. If there are variables whose values are not specified with a at(), we can use the option atmeans to assign them to their means. Otherwise, by default, margins and mtable would compute the average predicted probability over the sample for the unspecified independent variables. We do not want to do this because ideal types should be thought of as hypothetical observations, so averaging predictions over observations for some independent variables muddles the interpretation. Using values we specified for our first ideal type, we run margins: ologit class i.female i.white i.year i.educ c.age##c.age income, nolog margins, at(female=0 white=0 year=3 educ=1 age=25 income=30) Adjusted predictions Number of obs = 5,620\rModel VCE: OIM\r1._predict: Pr(class==1), predict(pr outcome(1))\r2._predict: Pr(class==2), predict(pr outcome(2))\r3._predict: Pr(class==3), predict(pr outcome(3))\r4._predict: Pr(class==4), predict(pr outcome(4))\rAt: female = 0\rwhite = 0\ryear = 3\reduc = 1\rage = 25\rincome = 30\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_predict |\r1 | 0.23 0.02 11.35 0.000 0.19 0.27\r2 | 0.63 0.01 59.17 0.000 0.61 0.65\r3 | 0.13 0.01 10.15 0.000 0.11 0.16\r4 | 0.00 0.00 6.68 0.000 0.00 0.00\r------------------------------------------------------------------------------\rmargins can only compute a prediction for a single outcome. Because we did not specify which outcome, margins used the default prediction, which is described as Pr(class == l), predict(). This is the predicted probability for the first outcome. Hence, we find that the predicted probability of identifying as lower class for our first ideal type is 0.23. margins, at(female=0 white=0 year=3 educ=1 age=25 income=30) predict(outcome(1)) margins, at(female=0 white=0 year=3 educ=1 age=25 income=30) predict(outcome(2)) margins, at(female=0 white=0 year=3 educ=1 age=25 income=30) predict(outcome(3)) margins, at(female=0 white=0 year=3 educ=1 age=25 income=30) predict(outcome(4)) It is easier, however, to use mtable, which computes predictions for all outcome categories and combines them into a single table. The option ci indicates that we want the output to show the confidence interval. mtable, at(female=0 white=0 year=3 educ=1 age=25 income=30) ci Expression: Pr(class), predict(outcome())\r| lower working middle upper\r----------+---------------------------------------\rPr(y) | 0.230 0.634 0.133 0.003\rll | 0.190 0.613 0.108 0.002\rul | 0.270 0.655 0.159 0.004\rSpecified values of covariates\r| female white year educ age income\r----------+-----------------------------------------------------------\rCurrent | 0 0 3 1 25 30\rWe could also compute predicted probabilities for both ideal types at the same time: mtable, atright norownum width(7) /// at(","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:12:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.12.1 (Advanced) Testing differences between ideal types Although we regard this section as extremely useful, we mark it as advanced because it requires a firm grasp of using loops and local macros in Stata. If you are still getting used to these, you might want to skip this section until you are comfortable with both. 同We may want to know whether a difference between ideal types is statistically significant for the same reason that we may perform a significance test for a set of coefficients: we are considering a change that involves multiple variables, and we want to evaluate how likely it is that we would observe a difference this large just by chance. Although the differences between predictions for our two ideal types are almost certainly significant, we can test this by extending methods used for binary outcomes. 同To test differences in predictions, we need to overwrite the estimation results from ologit with the predictions generated by margins. An inherent limitation in margins is that posting can only be done for a single outcome. That is, we cannot post the predictions for our four outcomes at one time. (We hope this will be addressed in future versions of margins.) To deal with this inconvenience, we will use a forvalues loop to repeat the tests for each outcome. First, we list the model and store the estimates, because we will have to restore the model results after we post the predictions for a particular outcome: ologit class i.female i.white i.year i.educ c.age##c.age income, nolog estimates store olm Next, we compute tests for each of the four outcome categories by using the following commands: mlincom, clear forvalues iout = 1/4 { // start loop quietly { mtable, out(`iout') post /// at(female=0 white=0 year=3 ed=1 age=25 income=30) /// at(female=1 white=1 year=3 ed=3 age=60 income=150) mlincom 1 - 2, stats(est pvalue) rowname(outcome `iout') add estimates restore olm } } // end loop We start with mlincom, clear to erase previous results from mlincom before we accumulate the new results that we will use to make our table. The forvalues {...} loop runs the code between braces once for each outcome. We use quietly {...} so that the output from mtable and mlincom is not displayed. Instead, we will list results after the loop is completed. Within the loop, the mtable option post saves the estimates for outcome ‘iout’ to the matrix e(b) so that mlincom can test the difference between the predictions for the first and second ideal types. The mlincom command uses option add to collect the results to display later. After the loop, running mlincom without options displays the results we just computed. The column named lincom has the linear combination of the estimates—in this case, the difference in predictions—while column pvalue is the p-value for testing that the difference is 0: mlincom | lincom pvalue ————-+——————- outcome 1 | 0.222 0.000 outcome 2 | 0.496 0.000 outcome 3 | -0.626 0.000 outcome 4 | -0.092 0.000 ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:12:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.13 Tables of predicted probabilities When there are substantively important categorical predictors in the model, examining tables of predicted probabilities over values of these variables can be an effective way to interpret the results. In this example, we use mtable to look at predictions over the values of the year of the survey, which correspond to 1980, 1996, and 2012: ologit class i.female i.white i.year i.educ c.age##c.age income, nolog mtable, at(year=(1 2 3)) atmeans norownum Expression: Pr(class), predict(outcome())\ryear lower working middle upper\r------------------------------------------------\r1 0.049 0.473 0.462 0.016\r2 0.053 0.489 0.443 0.015\r3 0.078 0.565 0.347 0.010\rSpecified values of covariates\r| 1. 1. 2. 3. | female white educ educ age income\r----------+-----------------------------------------------------------\rCurrent | .549 .814 .582 .241 45.2 68.1\rThe atmeans option holds other variables at their means in the estimation sample. We conclude the following: Changing only the year of the survey, and with income measured in 2012 dollars for all survey years, the probability of a respondent identifying as working class increased from 0.47 in 1980 to 0.57 in 2012, while the probability of identifying as middle class declined from 0.46 to 0.35. To obtain confidence intervals for the predictions, we use the option stat(ci), which could be abbreviated simply as ci: mtable, at(year=(1 2 3)) atmeans stat(ci) Expression: Pr(class), predict(outcome())\r| year lower working middle upper\r----------+-------------------------------------------------\rPr(y) | 1 0.049 0.473 0.462 0.016\rll | 1 0.042 0.447 0.433 0.013\rul | 1 0.056 0.499 0.491 0.020\rPr(y) | 2 0.053 0.489 0.443 0.015\rll | 2 0.046 0.468 0.421 0.012\rul | 2 0.059 0.510 0.466 0.018\rPr(y) | 3 0.078 0.565 0.347 0.010\rll | 3 0.068 0.543 0.321 0.008\rul | 3 0.088 0.587 0.372 0.012\rSpecified values of covariates\r| 1. 1. 2. 3. | female white educ educ age income\r----------+-----------------------------------------------------------\rCurrent | .549 .814 .582 .241 45.2 68.1\rThe lower and upper bounds of the intervals print on separate rows beneath each prediction. For example: Holding independent variables at their sample means, respondents in 2012 had a 0.078 probability of identifying as lower class (95% CI: [0.068, 0.088]). We might also want to generate tables for a combination of categorical independent variables. For example, how does class affiliation vary by race for the three years of our survey? While we are considering the probabilities implied by having year and white in the model as separate independent variables, we could also fit a model in which the interaction term i.year#i.white is included. mtable, at(year=(1 2 3) white=(0 1)) atmeans norownum Expression: Pr(class), predict(outcome())\rwhite year lower working middle upper\r----------------------------------------------------------\r0 1 0.059 0.511 0.417 0.013\r0 2 0.063 0.526 0.399 0.012\r0 3 0.093 0.593 0.306 0.008\r1 1 0.047 0.464 0.472 0.017\r1 2 0.051 0.480 0.454 0.016\r1 3 0.075 0.558 0.356 0.010\rSpecified values of covariates\r| 1. 2. 3. | female educ educ age income\r----------+-------------------------------------------------\rCurrent | .549 .582 .241 45.2 68.1\rThe predictions vary by year within a given value of white. The way in which variables vary is determined by the order in which the variables are specified with ologit, not by the order of variables within the at() statement. The table might be clearer if predictions were arranged to vary by white within each value of year (in practice, we often have to try it both ways before deciding which is clearer for the purpose at hand). We can refit the ologit model with year listed before white, or we can specify the values of year within three separate at() statements: mtable, atmeans norownum /// at(year=1 white=(0 1)) /// 1980 at(year=2 white=(0 1)) /// 1996 at(year=3 white=(0 1)) // 2012 Expression: Pr(class), predict(outcome())\rwhite year lower working m","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:13:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.14 Plotting predicted probabilities Plotting predicted probabilities for each outcome can also be useful for the ORM. These plots illustrate how predicted probabilities change as a continuous, independent variable changes. With the BRM, we showed two approaches for making plots: directly with marginsplot or in two steps with margins and graph. Plotting multiple outcomes, however, can only be done using the latter technique because marginsplot is limited to plotting a single outcome. To illustrate graphing predictions, we consider how the probability of class affiliation changes as household income changes, holding all other variables at their sample means. Of course, the plot could also be constructed for other sets of characteristics.The option at(inc=0(25)250) tells mgen to generate predictions as income changes from 0 to 250 in increments of 25, leading to 11 sets of predictions. The option atmeans holds other variables to their means. We use stub(CL_) to add CL_ (indicating predictions for class) to the names of variables generated by mgen: ologit class i.female i.white i.year i.educ c.age##c.age income, nolog mgen, at(income=(0(25)250)) stub(CL_) atmeans Predictions from: margins, at(income=(0(25)250)) atmeans predict(outcome())\rVariable Obs Unique Mean Min Max Label\r--------------------------------------------------------------------------------------\rCL_pr1 11 11 .0439624 .0074601 .1207294 pr(y=lower) from margins\rCL_ll1 11 11 .0385464 .0057745 .1067216 95% lower limit\rCL_ul1 11 11 .0493784 .0091458 .1347373 95% upper limit\rCL_income 11 11 125 0 250 household income\rCL_Cpr1 11 11 .0439624 .0074601 .1207294 pr(y\u003c=lower)\rCL_pr2 11 11 .377087 .1301718 .6238775 pr(y=working) from margins\rCL_ll2 11 11 .3565982 .1081524 .6070259 95% lower limit\rCL_ul2 11 11 .3975758 .1521912 .6407292 95% upper limit\rCL_Cpr2 11 11 .4210494 .137632 .744607 pr(y\u003c=working)\rCL_pr3 11 11 .5424338 .2492696 .7612023 pr(y=middle) from margins\rCL_ll3 11 11 .5212693 .2296227 .7417022 95% lower limit\rCL_ul3 11 11 .5635983 .2689165 .7807024 95% upper limit\rCL_Cpr3 11 11 .9634833 .8988342 .9938766 pr(y\u003c=middle)\rCL_pr4 11 11 .0365167 .0061234 .1011657 pr(y=upper) from margins\rCL_ll4 11 11 .030147 .0047639 .0828273 95% lower limit\rCL_ul4 11 11 .0428865 .0074829 .1195042 95% upper limit\rCL_Cpr4 11 10 1 1 1 pr(y\u003c=upper)\r-------------------------------------------------------------------------------------\rSpecified values of covariates\r1. 1. 2. 3. 2. 3. female white year year educ educ age ---------------------------------------------------------------------------\r.5491103 .8140569 .4510676 .3099644 .5818505 .2414591 45.15712 Each variable has 11 observations corresponding to different values of income. Variables containing predicted probabilities are stored in variables named CL_pr#. For example, CL_pr2 is the predicted probability of identifying as working class, the second category of our outcome. Variables containing cumulative probabilities—that is, the probability of observing a given category or lower—are stored as variables CL_Cpr#. For example, CL_Cpr2 is the predicted probability of a respondent identifying as either lower class or working class. Although mgen assigns variable labels to the variables it generates, we can change these to improve the look of the plot that we are creating. Specifically, we use label var CL_pr1 \"Lower\" label var CL_pr2 \"Working\" label var CL_pr3 \"Middle\" label var CL_pr4 \"Upper\" label var CL_Cpr1 \"Lower\" label var CL_Cpr2 \"Lower/Working\" label var CL_Cpr3 \"Lower/Working/Middle\" Next, we plot the probabilities of individual outcomes by using graph. Here we plot the four probabilities against values of income. graph twoway connected CL_pr1 CL_pr2 CL_pr3 CL_pr4 CL_income, /// title(\"Panel A: Predicted Probabilities\") /// xtitle(\"Household income (2012 dollars)\") /// xlabel(0(50)250) ylabel(0(.25)1, grid gmin gmax) /// ytitle(\"\") name(tmpprob, replace) Standard options for graph are used to specify the axes and labels. The ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:14:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.15 Probability plots and marginal effects Having considered various methods of interpretation, we now show the link between marginal effects and plots of predicted probabilities to hopefully provide you with new insights on the nature of ordinal models. The following graph, based on section 7.14, shows how the probabilities of class affiliation change with income, holding all other variables at their means. The mean of income is indicated with a dashed, vertical line: The slope of each probability curve evaluated at the mean of income, indicated by where the probability curves intersect the vertical line, is the marginal change in the probability of a given class affiliation with respect to income, with all variables held at their means. We can compute these changes by using mchange, atmeans to estimate MEMs: ologit class i.female i.white i.year i.educ c.age##c.age income, nolog mchange income, atmeans amount(marginal) dec(4) ologit: Changes in Pr(y) | Number of obs = 5620\rExpression: Pr(class), predict(outcome())\r| lower working middle upper -------------+-------------------------------------------\rincome | Marginal | -0.0006 -0.0022 0.0027 0.0002 p-value | 0.0000 0.0000 0.0000 0.0000 Predictions at base value\r| lower working middle upper -------------+-------------------------------------------\rPr(y|base) | 0.0586 0.5107 0.4173 0.0134 Base values of regressors\r| 1. 1. 2. 3. 2. 3. | female white year year educ educ age income -------------+---------------------------------------------------------------------------------------\rat | .5491 .8141 .4511 .31 .5819 .2415 45.16 68.08 1: Estimates with margins option atmeans.\rThe marginal changes are in row Marginal, with the significance level for the change listed in row p-value. These changes correspond to the probability curves at the point of intersection with the vertical line. For the slope for middle class, shown with squares, is 0.0027. The magnitude of the marginal changes would differ if we computed the marginal effects at different values of the independent variables. For example, we can compute the effects with income equal to $250,000$, with all other variables still kept at their means: mchange income, at(income=250) atmeans amount(marginal) dec(4) ologit: Changes in Pr(y) | Number of obs = 5620\rExpression: Pr(class), predict(outcome())\r| lower working middle upper -------------+-------------------------------------------\rincome | Marginal | -0.0001 -0.0013 0.0003 0.0011 p-value | 0.0000 0.0000 0.0378 0.0000 Predictions at base value\r| lower working middle upper -------------+-------------------------------------------\rPr(y|base) | 0.0075 0.1302 0.7612 0.1012 Base values of regressors\r| 1. 1. 2. 3. 2. 3. | female white year year educ educ age income -------------+---------------------------------------------------------------------------------------\rat | .5491 .8141 .4511 .31 .5819 .2415 45.16 250 1: Estimates with margins option atmeans.\rThe marginal change for the probability of identifying with the middle class is much smaller, corresponding to the leveling off of the curve (shown with ♦’s) on the right side of the graph. In this example, the signs of the marginal effects for each outcome are the same throughout the range of income. This, however, does not need to be true. In the ORM, not only does the magnitude of the effect change as the values of the independent variables change, but even the sign can change. That is to say, the effect of a variable can be positive at one point and can be negative at other points, even if we have not included polynomial terms or interactions in the model. In a model without interaction or polynomials for a given independent variable, the sign of that variable’s regression coefficient will always be the same as the direction of changes in the probability of the highest outcome category as the independent variable increases. In our example of subjective social class, because the coefficient for income is positive, increases in inc","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:15:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.16 Less common models for ordinal outcomes Stata can also fit several less commonly used models for ordinal outcomes. In concluding this chapter, we describe these models briefly and note their commands for estimation. Long (Forthcoming) provides further details. SPost commands do not work with all these models, but our m* commands do work with the estimation commands that support margins. ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:16:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.16.1 The stereotype logistic model The stereotype logistic model, also referred to as the stereotype ORM, was proposed by Anderson (1984) in response to the restrictive assumption of parallel regressions in the ORM. The stereotype logistic model is a compromise between allowing the coefficients for each independent variable to vary by outcome category (as is the case with the multinomial logit model, considered in the next chapter) and restricting the coefficients to be identical across all outcomes (as was the case with the ordered logit model). The stereotype logistic model can be fit in Stata by using the slogit command (see [R] slogit). The one-dimensional version of the model is defined as 这段话是在讨论一种名为\"stereotype logistic model\"（刻板逻辑模型）的统计模型。它是对传统的有序Logit模型的改进，旨在克服其对于所有结果都采用相同系数的限制，同时又不像多项式Logit模型那样允许每个自变量的系数随着结果类别的变化而变化。该模型可以在Stata中使用**slogit**命令来拟合。 $$\\ln\\frac{\\Pr\\left(y=q\\mid\\mathbf{x}\\right)}{\\Pr\\left(y=r\\mid\\mathbf{x}\\right)}=\\left(\\theta_{q}-\\theta_{r}\\right)-\\left(\\phi_{q}-\\phi_{r}\\right)\\left(\\mathbf{x}\\boldsymbol{\\beta}\\right)$$ where $\\beta$ is a vector of coefficients associated with the independent variables, the $\\alpha$’s are intercepts, and the $\\gamma$’s are scale factors that mediate the effects of the x’s. This one-dimensional model is ordinal as defined in section 7.15 and often produces very similar predictions to the ORM. When additional dimensions are added, it is no longer an ordinal model. Indeed, with enough dimensions, it is equivalent to the multinomial logit model. Accordingly, we postpone further discussion until chapter 8. ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:16:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.16.2 The generalized ordered logit model The parallel regression assumption results from assuming the same coefficient vector ($ \\beta $) in the $ J - 1 $ logit equations $$\\ln\\Omega_{\\leq m|\u003em}\\left(\\mathbf{x}\\right)=\\tau_{m}-\\mathbf{x}\\boldsymbol{\\beta}$$ where $\\Omega_{\\leq m|\u003em}\\left(\\mathbf{x}\\right)=\\mathrm{Pr}\\left(y\\leq m\\mid\\mathbf{x}\\right)/\\mathrm{Pr}\\left(y\u003em\\mid\\mathbf{x}\\right)$ The generalized ordered logit model allows $ \\beta $ to differ for each of the $ J - 1 $ comparisons. That is $$\\ln\\Omega_{\\leq m|\u003em}\\left(\\mathrm{x}\\right)=\\tau_m-\\mathbf{x}\\beta_m\\quad\\mathrm{for~}j=1\\mathrm{~to~}J-1$$ where predicted probabilities are computed as $$\\Pr\\left(y=1\\mid\\mathbf{x}\\right)=\\frac{\\exp\\left(\\tau_1-\\mathbf{x\\beta}_1\\right)}{1+\\exp\\left(\\tau_1-\\mathbf{x\\beta}_1\\right)}$$ $$\\Pr\\left(y=j\\mid\\mathbf{x}\\right)=\\frac{\\exp\\left(\\tau_j-\\mathbf{x}\\beta_j\\right)}{1+\\exp\\left(\\tau_j-\\mathbf{x}\\boldsymbol{\\beta_j}\\right)}-\\frac{\\exp\\left(\\tau_{j-1}-\\mathbf{x}\\beta_{j-1}\\right)}{1+\\exp\\left(\\tau_{j-1}-\\mathbf{x}\\boldsymbol{\\beta}_{j-1}\\right)}\\quad\\mathrm{for~}j=2\\mathrm{~to~}J-1$$ $$\\Pr\\left(y=J\\mid\\mathbf{x}\\right)=1-\\frac{\\exp\\left(\\tau_{J-1}-\\mathbf{x\\beta_{J-1}}\\right)}{1+\\exp\\left(\\tau_{J-1}-\\mathbf{x\\beta}_{J-1}\\right)}$$ No formal constraint precludes negative predicted probabilities. Discussions of this model can be found in Clogg and Shihadeh (1994, 146-147), Fahrmeir and Tutz (1994, 91), and McCullagh and Nelder (1989, 155). A critical view of the model can be found in Greene and Hensher (2010, 189-192), who highlight that the model can predict negative “probabilities” and that it cannot be formulated in terms of a continuous latent dependent variable. Further, as noted by Long (Forthcoming), the generalized ordered logit model is not an ordinal regression model because, like the multinomial logit model, it does not necessarily make predictions that maintain the ordinality of the outcome. Parallel Regression Assumption: 这个假设源自于多项logit模型，其中假设不同类别之间的回归系数是相同的。具体地，在你提供的公式中，$\\boldsymbol{\\beta}$表示回归系数向量。这个假设认为在$J - 1$个logit方程中，这个系数向量是相同的。例如，如果我们用一个多项logit模型来预测学生通过不同课程的可能性，这个假设就是认为不同课程对学生通过考试的影响是相似的。 Generalized Ordered Logit Model: 广义有序logit模型允许不同类别之间的回归系数不同。在你提供的公式中，$\\beta_m$表示第$m$个比较的系数向量。这意味着在预测不同的类别之间，可以使用不同的回归系数。例如，假设我们要预测一家餐厅的顾客评价，有“不满意”、“一般”和“满意”三个等级，而这些等级之间的影响可能是不同的，比如食物质量对“满意”和“不满意”的影响可能是不同的。 Predicted Probabilities: 预测概率是在给定自变量的情况下，某个类别的发生概率。在你的公式中，$\\Pr\\left(y=j\\mid\\mathbf{x}\\right)$表示在给定自变量$\\mathbf{x}$的情况下，因变量$y$取值为$j$的概率。这个概率通过使用模型中的参数$\\tau_j$和系数向量$\\beta_j$进行计算。举个例子，假设我们要预测一名学生通过一门课程的可能性，$\\Pr\\left(y=1\\mid\\mathbf{x}\\right)$表示学生通过课程的概率，$\\Pr\\left(y=2\\mid\\mathbf{x}\\right)$表示学生可能通过但也可能不通过的概率，$\\Pr\\left(y=3\\mid\\mathbf{x}\\right)$表示学生不通过课程的概率。 Negative Predicted Probabilities: 负的预测概率是指在模型中计算出的某个类别的发生概率为负数。这在实际应用中是不合理的，因为概率应该在0到1之间。如果模型产生负的预测概率，可能表示模型存在问题，需要进行调整或者修正。例如，在学生考试成绩预测的例子中，负的预测概率可能表示模型对学生考试成绩的预测不准确，需要重新调整模型参数或者考虑其他因素。 首先，我们来看你给出的第一个公式： $$ \\ln\\Omega_{\\leq m|\u003em}\\left(\\mathbf{x}\\right)=\\tau_{m}-\\mathbf{x}\\boldsymbol{\\beta} $$ 这个公式是广义有序logit模型的一个基本方程。让我们来分解它： $\\ln\\Omega_{\\leq m|\u003em}\\left(\\mathbf{x}\\right)$表示的是两个概率的比值的自然对数。这个比值是$y\\leq m$的概率与$y\u003em$的概率之比，其中$y$是因变量，$m$是一个类别标记。 $\\tau_{m}$是一个阈值参数，用来划分不同的类别。对于不同的$m$，有不同的阈值。 $\\mathbf{x}$是自变量的向量。 $\\boldsymbol{\\beta}$是系数向量，表示自变量对因变量的影响。 这个方程的含义是：在给定自变量$\\mathbf{x}$的情况下，$y$属于不同类别的概率之比的对数，等于一个类别的阈值参数$\\tau_{m}$减去自变量$\\mathbf{x}$与系数向量$\\boldsymbol{\\beta}$的内积。 接下来，我们来看第二个公式： $$ \\Pr\\left(y=1\\mid\\mathbf{x}\\right)=\\frac{\\exp\\left(\\tau_1-\\mathbf{x\\beta}_1\\right)}{1+\\exp\\left(\\tau_1-\\mathbf{x\\beta}_1\\right)} $$ 这个公式是预测$y$等于第一个类别的概率的方程。让我们来解释它： $\\Pr\\left(y=1\\mid\\mathbf{x}\\right)$表示的是在给定自变量$\\mathbf{x}$的情况下，$y$等于第一个类别的概率。 $\\exp$是指数函数，用来计算参数$\\tau_1-\\mathbf{x\\beta}_1$的指数。 分母$1+\\exp\\left(\\tau_1-\\mathbf{x\\beta}_1\\right)$确保了概率的范围在0到1之间。 这个方程的含义是：在给定自变量$\\mathbf{x}$的情况下，$y$等于第一个类别的概率，是阈值参数$\\tau_1$和自变量$\\mathbf{x}$与系数向量$\\boldsymbol{\\beta}_1$的内积的指数函数，除以1加上这个指数函数。 第三个公式: $$ \\Pr\\left(y=j\\mid\\mathbf{x}\\right)=\\frac{\\exp\\left(\\tau_j-\\mathbf{x}\\beta_j\\right)}{1+\\exp\\left(\\tau_j-\\mathbf{x}\\boldsymbol{\\be","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:16:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"16.3 (Advanced) Predictions without using factor-variable notation Factor variables make it much simpler to make predictions when there are linked variables, such as age and age-squared. Because gologit2 does not support factor-variable notation, we use this model to illustrate how to make the correct predictions. The most important point for most readers is likely that you want to use factor variables whenever possible! If you use factor-variable notation in your models, you do not need to worry about the issues discussed in this section. However, you might still find the section useful to deepen your understanding of predictions in nonlinear models. We can compute predicted probabilities for given values of observations as we did with the ordered logit model and use the same approach to interpretation. For example, here are results using the same ideal types that we used in section 7.12. mtable, atright norownum width(7) /// at(female=0 white=0 year1996=0 year2012=1 educ_hs=0 educ_col=0 /// age=25 agesq=625 income=30) /// at(female=1 white=1 year1996=0 year2012=1 educ_hs=0 educ_col=1 /// age=60 agesq=3600 income=150) Expression: Pr(class), predict(outcome())\rlower working middle upper female white educ_col age agesq income\r-----------------------------------------------------------------------------------------\r0.155 0.701 0.136 0.008 0 0 0 25 625 30\r0.000 0.122 0.809 0.069 1 1 1 60 3600 150\rSpecified values of covariates\r| year1996 year2012 educ_hs\r----------+----------------------------\rCurrent | 0 1 0\rComparing the results from ologit that were computed earlier in the chapter, mtable, atright norownum width(7) /// at(female=0 white=0 year=3 ed=1 age=25 income=30) /// at(female=1 white=1 year=3 ed=3 age=60 income=150) Expression: Pr(class), predict(outcome())\rlower working middle upper female white educ age income\r-------------------------------------------------------------------------------\r0.230 0.634 0.133 0.003 0 0 1 25 30\r0.008 0.138 0.759 0.095 1 1 3 60 150\rSpecified values of covariates\r| year\r----------+--------\rCurrent | 3\rThe main difference between the generalized and the ordered logit models is that the predicted probabilities for the categories with the highest probabilities (working class for the first ideal type and middle class for the second) are about 0.06 higher in the generalized ordered logit model. We can also use mchange to obtain changes in the predicted probability for particular values of the independent variables, which provides an opportunity to illustrate how to deal with polynomial terms, such as age-squared, when you are not using factor-variable notation. Suppose that we want the discrete change for white, which is a binary variable. If factor-variable notation had been used, mchange would know that it is a binary variable. Because we are not using factor-variable notation, we must tell mchange to compute the change from 0 to 1 with the option amount (binary). It is tempting, but incorrect, to compute the change like this: mchange white, amount(binary) atmeans // incorrect method! gologit2: Changes in Pr(y) | Number of obs = 5620\rExpression: Pr(class), predict(outcome())\r| lower working middle upper -------------+-------------------------------------------\rwhite | 0 to 1 | -0.001 -0.066 0.072 -0.005 p-value | 0.403 0.001 0.000 0.336 Predictions at base value\r| lower working middle upper -------------+-------------------------------------------\rPr(y|base) | 0.011 0.503 0.466 0.020 Base values of regressors\r| female white year1996 year2012 educ_hs educ_col age agesq -------------+----------------------------------------------------------------------------------------\rat | .549 .814 .451 .31 .582 .241 45.2 2325 | income -------------+----------\rat | 68.1 Because the mean of age is 45.16, agesq should be held at $45.16 \\times 45.16 = 2039$, not 2325, which is the mean of agesq. The correct way to compute marginal effects is to specify the value of agesq in at(): mchange white, amount(binary) at","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:16:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.16.4 The sequential logit model Some ordinal outcomes represent the progress of events or stages in some process through which an individual can advance. For example, the outcome could be faculty rank, where the stages are assistant professor, associate professor, and full professor. The key characteristic of the process is th at an individual must pass through each stage. The outcome is thus the result of a sequence of potential transitions: an assistant professor may or may not make the transition to associate professor, and an associate professor may or may not make the transition to full professor The most straightforward way to model an outcome like this is as a series of BRMs. Consider the binary logit model from chapter 5: $$\\ln\\frac{\\Pr\\left(y=1\\mid\\mathbf{x}\\right)}{\\Pr\\left(y=0\\mid\\mathbf{x}\\right)}=\\alpha+\\mathbf{x}\\boldsymbol{\\beta}$$ where we have made the intercept explicit rather than including it in $ \\beta $. To extend this to multiple transitions, we estimate for each transition the log odds of having made the transition (y \u003e m) versus not having made the transition (y = m). For example, we estimate the log odds of being an associate or a full professor (y \u003e 1) versus being an assistant professor (y = 1). We allow separate coefficients ($ \\beta_m $) for each transition from $ y = m $: $$\\ln\\frac{\\Pr(y\u003em\\mid\\mathbf{x})}{\\Pr(y=m\\mid\\mathbf{x})}=\\alpha_m+\\mathbf{x}\\beta_m\\quad\\mathrm{for~}m=1\\mathrm{~to~}J-1$$ where J is the number of stages. This is an example of a broader group of models called sequential logit models (for example, Liao [1994, 26-28]). This model differs importantly from the generalized ordered logit model in that observations in which $ y \u003c m $ are not used in the estimation of $ \\beta_m $. For example, assistant professors are not used when modeling the transition from associate professor to full professor. To demonstrate how to fit this model, we use the variable educ in the gssclass4 dataset as our outcome. The three values of educ represent two transitions: students may or may not graduate from high school, and high school graduates may or may not graduate from college. To fit the model, we first use recode to create dummy variables representing whether or not respondents at each stage made the transition to the next. The variable educ has the distribution use gssclass4, clear tab educ, miss educational |\rattainment | Freq. Percent Cum.\r------------+-----------------------------------\rnot hs grad | 993 17.67 17.67\rhs only | 3,270 58.19 75.85\rcollege | 1,357 24.15 100.00\r------------+-----------------------------------\rTotal | 5,620 100.00\rWe create the variable gradcollege to indicate if someone with a high school diploma graduated from college, where those who did not graduate from high school (educ=1) are recoded as missing, not as 0. Those who did not graduate from high school are not included in the analysis of the transition to college graduation. recode educ (1=0) (2 3=1), gen(gradhs) label var gradhs \"Graduate high school?\" recode educ (1=.) (2=0) (3=1), gen(gradcollege) label var gradcollege \"Graduate college?\" Next, we use logit to fit separate models for each transition, using race and sex as independent variables. * HS degree vs not logit gradhs i.white i.female, or nolog Logistic regression Number of obs = 5,620\rLR chi2(2) = 25.40\rProb \u003e chi2 = 0.0000\rLog likelihood = -2608.1189 Pseudo R2 = 0.0048\r------------------------------------------------------------------------------\rgradhs | Odds ratio Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rwhite |\rwhite | 1.53 0.13 5.09 0.000 1.30 1.80\r|\rfemale |\rfemale | 0.97 0.07 -0.48 0.631 0.84 1.11\r_cons | 3.39 0.29 14.35 0.000 2.87 4.00\r------------------------------------------------------------------------------\rNote: _cons estimates baseline odds.\r* College degree vs HS degree logit gradcollege i.white i.female, or nolog Logistic regression Number of obs = 4,627","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:16:4","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"In th is chapter, we discuss methods for interpreting results from models for binary outcom es.","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"The challenge of interpreting results, then, is to find a summary of how changes in the independent variables are associated with changes in the outcome that best reflects critical substantive processes without overwhelming yourself or your readers with distracting detail. **Using odds ratios to interpret the logit model is very common but rarely is it sufficient for understanding the results of the model. Nonetheless, it is important to understand what odds ratios mean for several reasons.**For one, odds ratios are used a lot, and you need to understand what they can and cannot tell you. Also, odds ratios are useful for understanding the structure of the ordinal regression model in chapter 7 and the multinomial logit model in chapter 8. Interpretation based only on (y^*) parallels interpretation in the linear regression model, but it is not often used for binary outcomes. It is, however, sometimes useful for models for ordinal outcomes, considered in chapter 7. We begin in section 6.2 with marginal effects, which we find more informative than the more commonly used odds ratios as scalar measures to assess the magnitude of a variable’s effect. In section 6.3, we consider computing predictions based on substantively motivated profiles of values for the independent variables, also referred to as ideal types. Thinking about the types of individuals represented in your sample is a valuable way to gain an intuitive sense of which configurations of variables are substantively important. Tables of predictions, which are discussed in section 6.4, can effectively highlight the impact of categorical independent variables. We end our discussion of interpretation in section 6.6 by considering graphical methods to show how probabilities change as a continuous independent variable changes. When using logit or probit, or any nonlinear model, we suggest that you try a variety of methods of interpretation with the goal of finding an elegant way to present the results that does justice to the complexities of the nonlinear model and the substantive application. No one method works in all situations, and often the only way to determine which method is most effective is to try them all. Fortunately, the methods we consider in this chapter can be readily extended to models for ordinal, nominal, and count outcomes, which are considered in chapters 7-9. ","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:0:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1 Interpretation using regression coefficients Interpretation of regression models involves examining how a change in an independent variable is associated with a change in the outcome. In the nonlinear binary regression model (BRM), a regression coefficient indicates the direction of a variable’s effect. In our model of labor force participation, the coefficients for (k5) and (k618) are both negative, which implies that higher numbers of children are associated with a lower probability of being in the labor force. What is harder to interpret from the coefficient is the magnitude of the effect. The logit model, for example, can be written as $$\\ln\\Omega\\left(\\mathbf{x}\\right)=\\mathbf{x}\\beta $$ The ($\\beta$) coefficients indicate the effect of the independent variable on the log odds of the outcome, where the log odds is also known as the logit. We can interpret the ($\\beta$)’s as follows: For a unit change in ($x_k$), we expect the log of the odds of the outcome to change by ($\\beta_k$) units, holding all other variables constant. This interpretation does not depend on the level of ($X_k$) or the levels of the other variables in the model. In this regard, it is just like the linear regression model. The problem is that a change of ($\\beta_k$) in the log odds has little substantive meaning for most people. Consequently, tables of logit coefficients typically have little value for conveying the magnitude of effects. As an alternative, odds ratios can be used to explain the effects of independent variables on the odds, which we consider in the next section. ","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:1:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.1 Interpretation using odds ratios Effects for the logit model (but not the probit model) can be interpreted in terms of changes in the odds. For binary outcomes, we typically consider the odds of observing a positive outcome, coded 1, versus a negative outcome, coded 0: $$\\Omega\\left(\\mathbf{x}\\right)=\\frac{\\Pr\\left(y=1\\mid\\mathbf{x}\\right)}{\\Pr\\left(y=0\\mid\\mathbf{x}\\right)}=\\frac{\\Pr\\left(y=1\\mid\\mathbf{x}\\right)}{1-\\Pr\\left(y=1\\mid\\mathbf{x}\\right)}$$ In the logit model, the log odds are a linear combination of the (x)’s and $beta’s$. For example, consider a model with three independent variables: The problem with interpreting these ( \\beta )’s directly is that changes in log odds are not substantively meaningful to most audiences. To make the interpretation more meaningful, we can transform the log odds to the odds by taking the exponential of both sides of the equation. This leads to a model that is multiplicative instead of linear but in which the outcome is the odds: $$\\Omega\\left(\\mathbf{x},x_{3}\\right)=e^{\\beta_{0}}e^{\\beta_{1}x_{1}}e^{\\beta_{2}x_{2}}e^{\\beta_{3}x_{3}}$$ Our notation emphasizes the value of (X_3), which we want to increase by 1: \\begin{align*} \\Omega\\left(\\mathbf{x},x_3+1\\right) \u0026= e^{\\beta_0}e^{\\beta_1x_1}e^{\\beta_2x_2}e^{\\beta_3\\left(x_3+1\\right)} \\ \\end{align*} \\begin{align*} \\qquad\\qquad\\qquad= e^{\\beta_0}e^{\\beta_0}e^{\\beta_1x_1}e^{\\beta_2x_2}e^{\\beta_3x_3}e^{\\beta_3} \\end{align*} This leads to the odds ratio $$\\frac{\\Omega\\left(\\mathbf{x},x_{3}+1\\right)}{\\Omega\\left(\\mathbf{x},x_{3}\\right)}=\\frac{e^{\\beta_{0}}e^{\\beta_{1}x_{1}}e^{\\beta_{2}x_{2}}e^{\\beta_{3}x_{3}}e^{\\beta_{3}}}{e^{\\beta_{0}}e^{\\beta_{1}x_{1}}e^{\\beta_{2}x_{2}}e^{\\beta_{3}x_{3}}}=e^{\\beta_{3}}$$ Accordingly, we can interpret the exponential of the logit coefficient as follows: For a unit change in $x_k$, the odds are expected to change by a factor of $\\exp(\\beta_k)$, holding other variables constant. For $\\exp(\\beta_k) \u003e 1$, you could say that the odds are “exp($\\beta_k$) times larger,” and for $\\exp(\\beta_k) \u003c 1$, you could say that the odds are $\\exp(\\beta_k)$ times smaller. If $\\exp(\\beta_k) = 1$, then $X_k$ does not affect the odds. We can evaluate the effect of a standard deviation change in $x_k$ instead of a unit change: **The odds ratio is computed by changing one variable, while holding all other variables constant. This means that the formula in (6.1) cannot be used when the variable being changed is mathematically linked to another variable. For example, if $x_1$ is age and $x_2$ is age-squared, you cannot increase $x_1$ by 1 while holding $x_2$ constant.**In such cases, the odds ratio computed as $\\exp(\\beta_k)$ should not be interpreted. Although odds ratios are a common method of interpretation for logit models, it is essential to understand their limitations. Most importantly, they do not indicate the magnitude of the change in the probability of the outcome. We begin with an example from our model of labor force participation, followed by a few words of caution. The output from logit with the or option shows the odds ratios instead of the estimated $\\beta$’s: logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog Logistic regression Number of obs = 753\rLR chi2(8) = 124.30\rProb \u003e chi2 = 0.0000\rLog likelihood = -452.72367 Pseudo R2 = 0.1207\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5 | -1.39 0.19 -7.25 0.000 -1.77 -1.02\rk618 | -0.07 0.07 -0.96 0.336 -0.20 0.07\r|\ragecat |\r40-49 | -0.63 0.21 -3.00 0.003 -1.04 -0.22\r50+ | -1.28 0.26 -4.92 0.000 -1.79 -0.77\r|\rwc |\rcollege | 0.80 0.23 3.48 0.001 0.35 1.25\r|\rhc |\rcollege | 0.14 0.21 0.66 0.508 -0.27 0.54\rlwg | 0.61 0.15 4.04 0.000 0.31 0.91\rinc | -0.04 0.01 -4.24 0.000 -0.05 -0.02\r_cons | 1.01 0.29 3.54 0.000 0.45 1.57\r------------------------------------------------------------------------------\rHere ","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:1:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.2 (Advanced) Interpretation using y* Binary logit and probit models are rarely interpreted in terms of the latent variable $y^*$. Accordingly, this section is primarily useful to provide a deeper understanding of identification and why logit coefficients are generally larger than probit coefficients. As discussed in section 5.1.1, the logit and probit models can be derived from regression of a latent variable $y^*$. $$y^{*}=\\mathbf{x}\\beta+\\varepsilon $$ where $\\varepsilon$ is a random error. For the probit model, we assume $\\varepsilon$ is normal with $\\operatorname{Var}(\\varepsilon)=1.$ For logit, we assume $\\varepsilon$ is distributed logistically with $\\mathrm{Var}(\\varepsilon)=\\pi^2/3$ As with the linear regression model, the marginal change in $y^*$ with respect to $xk$ is $$\\frac{\\partial y^*}{\\partial x_k}=\\beta_k$$ However, because $y^*$ is latent, its true metric is unknown and depends on the identification assumption we make about the variance of the errors As we saw in section 5.2.2, the coefficients produced by logit and probit cannot be directly compared with one another. The logit coefficients will typically be about 1.7 times larger than the probit coefficients, simply as a result of the arbitrary assumption about the variance of the error. Consequently, the marginal change in y* cannot be interpreted without standardizing by the estimated standard deviation of y*, which is computed as $$\\widehat{\\sigma}_{y^{*}}^{2}=\\widehat{\\boldsymbol{\\beta}}^{\\prime}\\widehat{\\mathrm{Var}}\\left(\\mathbf{x}\\right)\\widehat{\\boldsymbol{\\beta}}+\\mathrm{Var}\\left(\\varepsilon\\right)$$ where $\\operatorname{Var}\\left(\\mathbf{x}\\right)$ is the covariance matrix for the observed $x’s$,$\\widehat\\beta$ contains maximum likelihood estimates, and $\\operatorname{Var}(\\varepsilon)=1$ for probit and $\\mathrm{Var}(\\varepsilon)=\\pi^2/3$ for logit. Then the $y^*$-standardized coefficient for $xk$ is which can be interpreted as follows: For a unit increase in $x_k$, $y*$ is expected to increase by $\\beta_{k}^{S}y^{*}$ standard deviations,holding all other variables constant. The fully standardized coefficient is $$\\beta_{k}^{S}=\\frac{\\sigma_{k}\\beta_{k}}{\\sigma_{y^{*}}}$$ which can be interpreted as follows: For each standard deviation increase in $x_k$ , $y*$ is expected to increase by $\\beta_{k}^{S}$ standard deviations, holding all other variables constant. These coefficients are computed by listcoef with the std option: logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog listcoef, std help logit (N=753): Unstandardized and standardized estimates Observed SD: 0.4956\rLatent SD: 2.0474\r-------------------------------------------------------------------------------\r| b z P\u003e|z| bStdX bStdY bStdXY SDofX\r-------------+-----------------------------------------------------------------\rk5 | -1.3916 -7.250 0.000 -0.729 -0.680 -0.356 0.524\rk618 | -0.0657 -0.961 0.336 -0.087 -0.032 -0.042 1.320\r|\ragecat |\r40-49 | -0.6268 -3.003 0.003 -0.305 -0.306 -0.149 0.487\r50+ | -1.2791 -4.924 0.000 -0.529 -0.625 -0.259 0.414\r|\rwc |\rcollege | 0.7977 3.481 0.001 0.359 0.390 0.175 0.450\r|\rhc |\rcollege | 0.1359 0.661 0.508 0.066 0.066 0.032 0.488\rlwg | 0.6099 4.045 0.000 0.358 0.298 0.175 0.588\rinc | -0.0351 -4.238 0.000 -0.408 -0.017 -0.199 11.635\rconstant | 1.0140 3.545 0.000 . . . .\r-------------------------------------------------------------------------------\rb = raw coefficient\rz = z-score for test of b=0\rP\u003e|z| = p-value for z-test\rbStdX = x-standardized coefficient\rbStdY = y-standardized coefficient\rbStdXY = fully standardized coefficient\rSDofX = standard deviation of X\rThe y*-standardized* coefficients are in the column labeled bStdY*, and the fully standardized coefficients are in the column bStdXY. We could interpret these coefficients as follows: For each additional young child, the propensity of a woman to join the labor force decreases by 0.68 standard deviations, holding all other variables constant. For every standard deviation increase in family income, ","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:1:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2 Marginal effects: Changes in probabilities A marginal effect measures the change in the probability of an outcome for a change in Xk, holding all other independent variables constant at specific values. The critical idea is that one variable is changing while the other variables are not. There are two varieties of marginal effects. A marginal change computes the effect of an instantaneous or infinitely small change in Xk. A discrete change computes the effect of a discrete or finite change in Xk. (See section 4.5 for an introductory discussion of marginal effects.) Marginal change and discrete change in the BRM\rA marginal change, shown by the tangent to the probability curve at x = 1 in figure 6.1, is the rate of change in the probability for an infinitely small change in $x_k$ holding other variables at specific values: $$\\frac{\\partial\\Pr(y=1\\mid\\mathbf{x}=\\mathbf{x}^*)}{\\partial x_k}$$ Because the effect is computed with a partial derivative, some authors refer to this as the partial change or partial effect. In this formula, x* contains specific values of the independent variables. For example, x* could equal x7 with the observed values for the ith observation, it could equal the means x of all variables, or it could equal any other values. When the meaning is clear, we will refer to x without specifying x*. The important thing is that the value of the marginal effect depends on the specific values of the xk s where the change is computed. In the BRM , the marginal change has the simple formula $$\\frac{\\partial\\Pr\\left(y_i=1\\mid\\mathbf{x}\\right)}{\\partial x_k}=f\\left(\\mathbf{x}\\beta\\right)\\beta_k$$ where $f$ is the normal probability distribution function (PDF) for probit and the logistic PDF for logit. In logit models, the marginal change has a particularly convenient form: $$\\frac{\\partial\\Pr\\left(y_i=1\\mid\\mathbf{x}\\right)}{\\partial x_k}=\\Pr\\left(y_i=1\\mid\\mathbf{x}\\right)\\left[1-\\Pr\\left(y_i=1\\mid\\mathbf{x}\\right)\\right]\\beta_k$$ From this formula, we see that the change must be greatest when $\\Pr\\left(y=1\\mid\\mathbf{x}\\right)=0.5$,where the marginal change is $(0.5)(0.5)\\beta_{k}=\\beta_{k}/4$.Accordingly, dividing a binary logit coefficient by 4 indicates the maximum marginal change in the probability (Cramer 1991, 8). As long as the model does not include power or interaction terms, the marginal change for xk has the same sign as $\\beita_k$ for all values of x because the PDF is always positive. (Computing marginal effects when powers and interactions are in the model is discussed in section 6.2.1.) The formula also shows that marginal changes for different independent variables differ by a scale factor. For example, the ratio of the marginal effect of $x_j$ to the effect of $x_k$ is $$\\frac{\\partial\\Pr\\left(y_i=1\\mid\\mathbf{x}\\right)/\\partial x_j}{\\partial\\Pr\\left(y_i=1\\mid\\mathbf{x}\\right)/\\partial x_k}=\\frac{f\\left(\\mathbf{x}\\boldsymbol{\\beta}\\right)\\beta_j}{f\\left(\\mathbf{x}\\boldsymbol{\\beta}\\right)\\beta_k}=\\frac{\\beta_j}{\\beta_k}$$ for all values of x. Consequently, while does not tell you the magnitude of $x_k’s$ effect, it can tell you how much larger or smaller it is than the effects of other variables. A discrete change, sometimes called a first difference, is the actual change in the predicted probability for a given change in $x_k$, holding other variables at specific values. For example, the discrete change for an increase in age from 30 to 40 is the change in the probability of being in the labor force as age increases from 30 to 40, holding other variables at specified values. Defining $x_k^{\\mathrm{start}}$ as the starting value of $x_k$ and $x_k^{\\mathrm{end}}$ as the ending value,the discrete change equals $$\\frac{\\Delta\\Pr\\left(y=1\\mid\\mathbf{x}\\right)}{\\Delta x_k\\left(x_k^{\\mathrm{start}}\\to x_k^{\\mathrm{ond}}\\right)}=\\Pr\\left(y=1\\mid\\mathbf{x},x_k=x_k^{\\mathrm{end}}\\right)-\\Pr\\left(y=1\\mid\\mathbf{x},x_k=x_k^{\\mathrm{start}}\\right)$$ For binary variables, such as having attended college, the obvi","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:2:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.1 Linked variables Fundamental to the concept of a marginal effect is the idea that only one variable changes while holding all other variables at specified values. An exception must be made for variables that are linked mathematically. For example, if $x_{\\text{age}}$ is age and $X_{\\text{agesq}} = x_{\\text{age}} \\times x_{\\text{age}}$, you cannot change $x_{\\text{age}}$ while holding $x_{\\text{agesq}}$ constant. The change in $x_{\\text{age}}$ must be matched by a corresponding change in $x_{\\text{agesq}}$. This is easy to illustrate with a discrete change in age from 20 to 30: $$\\begin{aligned}\\frac{\\Delta\\operatorname{Pr}\\left(y=1\\mid\\mathbf{x}\\right)}{\\Delta\\operatorname{gre}\\left(20\\to30\\right)}\u0026=\u0026\\operatorname{Pr}\\left(y=1\\mid\\mathbf{x},x_{\\mathrm{age}}=30,x_{\\mathrm{agesq}}=30^2\\right)\\end{aligned}$$ $$\\begin{aligned}\\quad\\quad\\qquad\\qquad\\qquad\u0026-\\operatorname{Pr}\\left(y=1\\mid\\mathbf{x},x_{\\mathrm{age}}=20,x_{\\mathrm{agesq}}=20^2\\right)\\end{aligned}$$ Linked variables must also be considered for the variables being held constant. For example, if we are computing the marginal effect of $x_k$ while holding age at its mean, we need to hold $x_{\\text{age}}$ at $\\text{mean}(x_{\\text{age}})$ and $x_{\\text{agesq}}$ at $[\\text{mean}(x_{\\text{age}}) × \\text{mean}(x_{\\text{age}})]$ not at $\\text{mean}(x_{\\text{agesq}})$. Similarly, if your model includes $X_{\\text{female}}$, $x_{\\text{age}}$, and the interaction $$x_{\\mathrm{female}\\times\\mathrm{age}}=x_{\\mathrm{female}}\\times x_{\\mathrm{age}},$, you cannot change $x_{\\mathrm{age}}$ while holding $x_{\\mathrm{female}\\times\\mathrm{age}}$ constant. Categorical regressors that enter a model as a set of indicators are also linked. Suppose that education has three categories: no high school degree, high school diploma as the highest degree, and college diploma as the highest degree. Let $x_{\\text{hs}} = 1$ if high school is the highest degree and equal 0 otherwise; and let $x_{\\text{College}} = 1$ if college is the highest degree and equal 0 otherwise. If $x_{\\text{hs}} = 1$, then $x_{\\text{College}} = 0$. You cannot increase $x_{\\text{College}}$ from 0 to 1 while holding $x_{\\text{hs}}$ at 1. Computing the effect of having college as the highest degree ($x_{\\text{hs}} = 0$, $x_{\\text{College}} = 1$) compared with high school as the highest degree ($x_{\\text{hs}} = 1$, $x_{\\text{College}} = 0$) involves changing two variables: When discussing marginal effects with linked variables, we will say “holding other variables constant” with the implicit understanding that appropriate adjustments for linked variables are being made. A major benefit of using factor-variable notation when specifying a regression model is that margins, mchange, mtable, and mgen keep track of which variables are linked, and compute predictions and marginal effects correctly. 边际效应概念： 边际效应是指当我们微小地改变一个变量时，观察这个变化对概率的影响有多大。这里强调了在计算边际效应时，其他变量都被保持在特定的值上，保证我们只观察一个变量的变化对概率的影响。 相关变量的例外情况： 文中提到，如果有数学上相关的变量，需要特别注意。比如，如果 $x_{\\text{age}}$ 表示年龄，而 $X_{\\text{agesq}} = x_{\\text{age}} \\times x_{\\text{age}}$ 表示年龄的平方，那么在改变 $x_{\\text{age}}$ 的同时，需要相应地调整 $x_{\\text{agesq}}$。这是因为它们是数学上相关的。 具体例子： 通过公式 $\\frac{\\Delta\\operatorname{Pr}\\left(y=1\\mid\\mathbf{x}\\right)}{\\Delta\\operatorname{gre}\\left(20\\to30\\right)}$ 的计算，我们可以看到对于年龄从20到30的变化，事件 $y=1$ 的概率变化是通过两个部分的减法来计算的。第一部分是在年龄为30岁和 $x_{\\text{agesq}}$ 为 $30^2$ 的情况下，事件发生的概率；第二部分是在年龄为20岁和 $x_{\\text{agesq}}$ 为 $20^2$ 的情况下，事件发生的概率。两者之差即为由于年龄变化引起的边际效应。 变量的数学关系和离散变化： 文中还提到，对于有数学关系的变量，需要适当调整。比如，如果模型中包含了交互项 $x_{\\text{female}\\times\\text{age}}$，在改变 $x_{\\text{age}}$ 时需要注意。类别型变量也需要特殊处理，确保在改变某一个变量时，与之相关的其他变量也得到相应调整。 ","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:2:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.2 Summary measures of change The marginal effect of a variable depends on the specific values of all independent variables. Because the effect of $x_k$ differs for each observation (unless, of course, multiple observations have identical values), there is a distribution of marginal effects in the sample. For interpretation, we seek a simple, informative summary of this distribution of effects. There are three basic approaches: Marginal effect at the mean (MEM). Compute the marginal effect of $x_k$ with all variables held at their means. Marginal effect at representative values (MER). Compute the marginal effect of $X_k$ with variables held at specific values that are selected for being especially instructive for the substantive questions being considered. The MEM is a special case of the MER. Average marginal effect (AME). Compute the marginal effect of $X_k$ for each observation at its observed values $x_i$, and then compute the average of these effects. We consider each measure before discussing how to decide which measure is appropriate for your application. MEMs and MERs The MEM is computed with all variables held at their means. For a marginal change, this is $$\\frac{\\partial\\Pr\\left(y=1\\mid\\overline{\\mathbf{x}},x_k=\\overline{x}_k\\right)}{\\partial x_k}$$ which can be interpreted as follows: For someone who is average on all characteristics, the marginal change of $x_k$ is The discrete change equals For someone who is average on all characteristics, increasing $x_k$ by 8 changes the probability by … The MER would replace “who is average” with a description of the values of the covariates. AMEs The AME is the mean of the marginal effect computed at the observed values for all observations in the estimation sample. For a marginal change, this is which can be interpreted as follows: The average marginal effect of $x_k$ is … The average discrete change equals which is interpreted as follows: On average, increasing $x_k$ by $\\delta $ increases the probability by … For factor variables or changes from one fixed value to another (for example, from the maximum to the maximum), we say On average, increasing $X_k$ from start-value to end-value increases the probability by… Standard errors of marginal effects For each of these measures of change, standard errors can be computed using the delta method (Agresti 2013, 72-75; Wooldridge 2010, 576-577; Xu and Long 2005; [r] margins). The standard errors allow you to test whether a marginal effect is 0, to add a confidence interval to the estimated effect, and to test such things as whether marginal effects are equal at different values of the independent variables. ","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:2:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.3 Should you use the AME, the MEM , or the MER? The popularity of the MEM is probably because of the ease of computation. Computing an AME, in principle, involves N times more computation than the corresponding MEM. With the rapid growth in computing power, this is a trivial issue compared with having readily available software that easily computes the AME. For example, with our prchange command in SPost9, computing MEMs was trivially easy. Although you could compute the AME with prchange, you needed to write your own program to collect and summarize the computations for each observation. Few people, ourselves included, bothered to do that. With Stata’s margins and our mchange, it is as easy to compute AMEs as MEMs. These computational advances do not, however, imply that the AME is always the best way to assess the effect of a variable. There are several issues to consider when deciding which measure of change to use. Does the marginal effect computed at the mean of all variables provide useful information about the overall effect of that variable? This is relevant not only in deciding what to do in current analyses but when evaluating past research that used the MEM. A common criticism of the MEM is that typically there is no actual case in the dataset for which all variables equal the mean. Most obviously, with binary independent variables, the mean does not correspond to a possible value of an observation. For example, a variable like “pregnant” is measured as 0 and 1 without it being possible to observe someone with a value equal to a sample mean of intermediate value. This issue alone leads some to disfavor the MEM (Hanmer and Kalkan 2013). We are not ourselves as concerned about this point because holding a binary variable at its mean is, roughly speaking, taking a weighted mean of effects for each group. If the groups are a focus of the analysis, you can compute MERs for each group by using group-specific means. Alternatively, effects can be computed at the modal values of the binary variables, but this ignores everyone who is in a less well-represented group. Sometimes, it is argued that the MEM is a reasonable approximation to the AME. Although Greene and Hensher (2010, 36) correctly observed that the AME and MEM are often similar, they incorrectly suggest that this is especially true in large samples. Although the two measures will often be similar, they can differ in substantively meaningful ways, and whether this is the case has little to do with whether a sample is bigger or smaller. Bartus (2005) and Verlinda (2006) explain more precisely when $MEM$ and $AME$ differ and which is larger. For the binary logit and probit models, the difference between the $AME$ and $MEM$ for depends on three things: the probability that $y = 1$ when all $\\beta$’s are held to their means, the variance of $x_k$, and the size of $\\beta_k$ (Bartus 2005; Hanmer and Kalkan 2013, SI). The sign of the difference between the $AME$ and $MEM$ depends on $Pr(y = 1 | x)$, with the $AME$ being larger at lower and higher probabilities. In the middle, the $MEM$ is larger, with the largest difference occurring when $Pr(y = 1 | x) = 0.5$. The $AME$ and $MEM$ will be equal when the probability is about 0.21 and 0.79 for the binary logit model, and about 0.15 and 0.85 for the binary probit model. The $AME, MEM,$ and $MER$ are each summary measures, and no single summary of effects is ideal for all situations. Broadly speaking, we believe that the $AME$ is the best summary of the effect of a variable. Because it averages the effects across all cases in the sample, it can be interpreted as the average size of the effect in the sample. The $MEM$ is computed at values of the independent variables that might not be representative of anyone in the sample. However, both $AME$ and $MEM$ are limited because they are based on averages. If the average value of each regressor is a substantively interesting location in the data, the $MEM$ is useful because it te","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:2:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.4 Examples of marginal effects In this section, we use our model of labor force participation to illustrate the computation and interpretation of marginal effects with mchange. The mchange command makes it simple to compute marginal effects for different amounts of changes, either averaging effects over the sample or computing them at fixed values. mchange uses margins to compute the effects, which are then collected into a compact table. For example, running mchange after fitting our baseline model creates a 30-line table that summarizes 500 lines of output from a dozen margins commands. If you want to learn more about margins, you can add the option details to mchange to see how to use margins output. Information on using margins to compute marginal effects is given in section 6.2.6. We begin by fitting our model and storing the estimates so that they can be restored later: use binlfp4, clear logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog Logistic regression Number of obs = 753\rLR chi2(8) = 124.30\rProb \u003e chi2 = 0.0000\rLog likelihood = -452.72367 Pseudo R2 = 0.1207\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5 | -1.39 0.19 -7.25 0.000 -1.77 -1.02\rk618 | -0.07 0.07 -0.96 0.336 -0.20 0.07\r|\ragecat |\r40-49 | -0.63 0.21 -3.00 0.003 -1.04 -0.22\r50+ | -1.28 0.26 -4.92 0.000 -1.79 -0.77\r|\rwc |\rcollege | 0.80 0.23 3.48 0.001 0.35 1.25\r|\rhc |\rcollege | 0.14 0.21 0.66 0.508 -0.27 0.54\rlwg | 0.61 0.15 4.04 0.000 0.31 0.91\rinc | -0.04 0.01 -4.24 0.000 -0.05 -0.02\r_cons | 1.01 0.29 3.54 0.000 0.45 1.57\r------------------------------------------------------------------------------\restimates store base estat summarize, labels Estimation sample logit Number of obs = 753\r---------------------------------------------------------------------------------------\rVariable | Mean Std. dev. Min Max Label\r-------------+-------------------------------------------------------------------------\rlfp | .5683931 .4956295 0 1 In paid labor force?\rk5 | .2377158 .523959 0 3 # kids \u003c 6\rk618 | 1.353254 1.319874 0 8 # kids 6-18\ragecat | Wife's age group\r40-49 | .3851262 .4869486 0 1\r50+ | .2191235 .4139274 0 1\rwc | Wife attended college?\rcollege | .2815405 .4500494 0 1\rhc | Husband attended college?\rcollege | .3917663 .4884694 0 1\rlwg | 1.097115 .5875564 -2.05412 3.21888 Log of wife's estimated wages\rinc | 20.12897 11.6348 -.029 96 Family income excluding wife's\r---------------------------------------------------------------------------------------\rWe will next show how to compute and interpret AMEs for continuous and factor variables, before examining the corresponding MEMs. Marginal effects in models with powers and interactions are then considered. Finally, we show how to compute the distribution of effects for observations in the estimation sample. 2.4.1 AMEs for continuous variables For continuous independent variables, mchange computes the average marginal change and average discrete change of 1 and a standard deviation. To assess the effects of income and wages, type: mchange inc lwg logit: Changes in Pr(y) | Number of obs = 753\rExpression: Pr(lfp), predict(pr)\r| Change p-value -------------+---------------------\rinc | +1 | -0.007 0.000 +SD | -0.086 0.000 Marginal | -0.007 0.000 lwg | +1 | 0.120 0.000 +SD | 0.072 0.000 Marginal | 0.127 0.000 Average predictions\r| not in LF in LF -------------+---------------------\rPr(y|base) | 0.432 0.568 The average predictions, listed below the table of changes, show that in the sample the average predicted probability of being in the labor force is 0.432. This is the same value you would obtain by first running predict and then computing the mean of the predictions. In later examples, we often suppress this result by adding the brief option. Summarizing the AMEs for a standard deviation change, we can say Holding other variables at their observ","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:2:4","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.5 The distribution of marginal effects The value of a marginal effect depends on the level of all variables in the model. Because each observation can have different values of the independent variables, there is a distribution of marginal effects within the sample where the AME is the mean of this distribution. Although the mean tells you where the center of the distribution is, it does not reflect variation within the distribution. Just as the means of the independent variables used to compute the MEM might not correspond even approximately to anyone in the sample, the AME might not correspond to the magnitude of the marginal effect for anyone in the sample. For this reason, we believe that examining the distribution of marginal effects provides valuable substantive insights. We consider two approaches for learning about the distribution of marginal effects. First, we compute effects for each observation and create a histogram of the effects. Although there is no Stata command for this, we provide simple programs that you can adapt to your needs. Second, we compute marginal effects at strategic locations in the data space by using MERs. This approach is presented in section 6.3. A third approach that we do not consider here estimates the quantiles of the effects in the population; see Firpo (2007) and Cattaneo (2010) for seminal papers, and see Cattaneo, Drukker, and Holland (2013) and Drukker (2014) for intuition, Stata commands, and extensions to survival data. For example, a training program that boosts the income of low-income participants and has no effect on higher-income participants could have the 0.25 quantile effect be significant and the 0.75 quantile effect be insignificant. These quantiles of effects provide the researcher with a more nuanced picture of the effect of a treatment than the one provided by the mean effect. The marginal change for the BRM, assuming no interactions or power terms, equals: $$\\frac{\\partial\\Pr\\left(y_i=1\\mid\\mathbf{x}_i\\right)}{\\partial x_k}=f\\left(\\mathbf{x}_i\\boldsymbol{\\beta}\\right)\\beta_k$$ The shape of the distribution of marginal changes for each observation is determined by $ f(x_i\\beta) $, where $ \\beta_k $ simply rescales $ f(x_i\\beta) $ to create the distribution of effects for $ x_k $. The distribution is more spread out if $ \\beta_k $ is larger in absolute value and is more condensed if $ \\beta_k $ is smaller. Although the shape of the distribution of discrete changes will be similar for different variables, they are not a simple rescaling of each other. 当我们谈论计算每个观察值的边际变化时，我们想知道当某个自变量微小变化时，因变量的变化量是多少。在 logistic 回归模型中，我们可以使用一个简单的公式来计算这个边际变化。 首先，我们有一个 logistic 回归模型，其预测某个事件发生的概率。假设我们有一个自变量 $ x_i $，我们想要知道当 $ x_i $ 发生微小变化时，事件发生的概率会如何变化。 公式中的 $ f(x_i\\boldsymbol{\\beta}) $ 是 logistic 函数，它描述了自变量 $ x_i $ 和事件发生的概率之间的关系。这个函数会给出一个在 0 到 1 之间的值，表示事件发生的概率。 $ (1 - f(x_i\\boldsymbol{\\beta})) $ 则表示事件不发生的概率，因为 logistic 函数的性质保证了事件发生和不发生的概率之和为 1。 因此，$ f(x_i\\boldsymbol{\\beta}) \\cdot (1 - f(x_i\\boldsymbol{\\beta})) $ 就是事件发生和不发生的联合概率密度函数，也可以看作是事件发生的概率密度函数（Probability Density Function, PDF）。 当我们计算出每个观察值的 $ \\text{Pr}(y_i = 1 | x_i) $（即事件发生的概率）后，我们可以将其代入上述公式中，得到对应的边际变化。这个边际变化告诉我们，当 $ x_i $ 发生微小变化时，事件发生的概率会如何变化。 举个例子来说明这个过程：假设我们有一个 logistic 回归模型，想要预测一个学生是否通过了考试，而自变量包括学习时间 $ x_1 $ 和学习资料数量 $ x_2 $。我们可以使用上述公式来计算每个学生通过考试的概率，并进一步分析当学习时间或学习资料数量微小变化时，通过考试的概率会如何变化。这样，我们就可以评估这些因素对考试结果的影响程度。 There are several ways to compute the marginal changes for each observation. For the logit model, the simplest approach is to use the formula where $ \\text{Pr}(y_i = 1 | x_i) 1 - \\text{Pr}(y_i = 1 | x_i) $ is the PDF for the logistic distribution. After predict computes $ \\text{Pr}(y_i = 1 | x_i) $ for each observation, it is easy to create a variable containing the marginal effects: 给定一个具有多个自变量 $ x_1, x_2, \\ldots, x_n $ 的模型，我们想要计算当某个自变量 $ x_k $ 微小变化时，因变量 $ y $ 的变化量。这个变化量称为边际变化（marginal change）。 边际变化可以使用以下公式计算： $$ \\frac{\\partial\\Pr(y_i=1|\\mathbf{x}_i)}{\\partial x_k} = f(\\mathbf{x}_i\\boldsymbol{\\beta}) \\cdot \\beta_k $$ 这里的 $ \\","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:2:5","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.6 (Advanced) Algorithm for computing the distribution of effects In this section, we use margins and slightly more advanced programming techniques to create a general algorithm for plotting the distribution of effects. Although the programming is more complicated, the code works with any model that is compatible with margins, even if your model includes interactions and product terms. We suggest you read this section after you have mastered other materials in this chapter. Instead of using generate to compute marginal effects based on the formula for a specific model, this algorithm uses margins to compute the effect for each observation. Although this is computationally slow, it works very generally for creating a histogram of any marginal effect that can be computed by margins or by predictions made by margins. We begin with a review of using margins to compute marginal effects (see section 4.5 for related information) 2.6.1 Using margins to compute marginal effects The option dydx (varname) tells margins to compute marginal effects. If varname is a factor variable, such as i.wc in our example, margins computes the discrete change as varname changes from 0 to 1. If varname is not a factor variable, margins computes the marginal change (that is, partial derivative) for varname. We begin by fitting the model and storing the results. We must store them because we will use the post option with margins, which replaces the regression estimates in memory with the results from margins. use binlfp4, clear logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog estimates store mymodel Next, we compute the marginal change with margins,dydx (inc), leaving in the return r(b). margins, dydx(wc) matlist r(b) Average marginal effects Number of obs = 753\rModel VCE: OIM\rExpression: Pr(lfp), predict()\rdy/dx wrt: 1.wc\r------------------------------------------------------------------------------\r| Delta-method\r| dy/dx std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rwc |\rcollege | 0.16 0.04 3.69 0.000 0.08 0.25\r------------------------------------------------------------------------------\rNote: dy/dx for factor levels is the discrete change from the base level.\rWe can also use margins to compute discrete changes for continuous variables, but this takes two steps. First, we make two predictions and post the results. Second, we use lincom or mlincom to compute the discrete change. For example, suppose that we want to compute the change in the probability of labor force participation as the number of young children increases from 0 to 3. We compute predictions with two atspecs, one for k5=0 and the other for k5=3: | 0b. 1.\r| wc wc -------------+---------------------\ry1 | 0 .1624037 margins, at(k5=0) at(k5=3) post Predictive margins Number of obs = 753\rModel VCE: OIM\rExpression: Pr(lfp), predict()\r1._at: k5 = 0\r2._at: k5 = 3\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_at |\r1 | 0.64 0.02 34.97 0.000 0.60 0.67\r2 | 0.04 0.02 2.09 0.036 0.00 0.07\r------------------------------------------------------------------------------\rBecause we used the post option, the predictions are saved to e(b), which allows us to use mlincom (or lincom) to compute the average discrete change: mlincom 2 - 1 | lincom pvalue ll ul -------------+---------------------------------------\r1 | -0.599 0.000 -0.656 -0.541 The linear combination in the column lincom is returned to r(est). We can also compute a change of a fixed amount from the observed values, for example, the average change as inc increases by 1 from its observed values. To do this, we will want to use a single margins command to produce two different predictions: one in which predictions are computed at the observed values and one in which inc is increased by 1. To get predic","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:2:6","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3 Ideal types An ideal type is a hypothetical observation with substantively illustrative values. A table of probabilities for ideal types of people, countries, cows, or whatever you are studying can quickly summarize the effects of key variables. In our example of labor force participation, we want to examine four ideal types of respondents: A young family with lower income, no college education, and young children. A young family with college education and young children. A middle-aged family with college education and teenage children. An older family with college education and adult children. We find ideal types to be particularly illustrative for interpretation when independent variables are substantially correlated. In the above example, we first consider the contrast between lower income and no college education and higher income and college education, because these indicators of SES covary strongly enough that it is easy to envision them as low- and high-SES prototypes. Across the latter three examples, we construct ideal types reflecting that the age of parents and their children change together. We use mtable to estimate the probabilities for each of these ideal types. To introduce the command and explain some options, we begin with an example that combines two sets of predictions. (See section 4.4 for an introduction to mtable.) We then illustrate two approaches for creating a table of ideal types. For our first ideal type, we define a young, lower-class family as having the values specified as at (agecat=1 k5=2 k618=0 inc=10 lwg=.75 hc=0 wc=0). lwg equals the log of the federal minimum wage for 1975, the year the data were collected. We use mtable to make predictions, using the rowname() option to label the results. The option ci, a synonym for statistics (ci), requests confidence intervals along with the predicted probability. Because this is the first step in constructing a table of predictions, we use the clear option to remove from memory any prior predictions saved by mtable. logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog mtable, rowname(1 Young low SES young kids) ci clear /// at(agecat=1 k5=2 k618=0 inc=10 lwg=.75 hc=0 wc=0) Expression: Pr(lfp), predict()\r| Pr(y) ll ul\r----------------------------+-----------------------------\r1 Young low SES young kids | 0.159 0.068 0.251\rSpecified values of covariates\r| k5 k618 agecat wc hc lwg inc\r----------+---------------------------------------------------------------------\rCurrent | 2 0 1 0 0 .75 10\rWe conclude the following: For a young, lower SES family with two young children, the estimated probability of being in the labor force is 0.16 with a 95% confidence interval from 0.07 to 0.25. For our next ideal type, we define a young, college-educated family with young children by using at(agecat=1 k5=2 k618=0 wc=1 hc=1), which specifies the values for all the independent variables except lwg and inc. Because we used the atmeans option, these variables are set to the means in the estimation sample. To place the new prediction below the prediction from the last mtable command, we use the below option. Below the table of predictions is a table showing the levels of the covariates when the predictions were made. Although you can suppress its display with the brief option, we find it useful for knowing exactly how the ideal types were defined. Set 1 refers to the first predictions in the table, which we numbered as 1. The row Current contains values of the at() variables from the current or most recent mtable command. We add two more ideal types that show what happens to the probability of being in the labor force as women and children get older. We use quietly to suppress output until the last mtable command, which displays the complete table. quietly mtable, rowname(3 Midage college with teens) ci below /// at(agecat==2 k5==0 k618==2 wc==1 hc==1) atmeans mtable, rowname(4 Older college with adult kids) ci below /// at(agecat==3 k5==0 k618==0 wc==1 hc==1) atmeans Expr","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:3:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3.1 Using local means with ideal types The last three rows of the table were constructed using the atmeans option to specify the values of inc and lwg to the sample means. We refer to means based on the entire estimation sample as global means. Although using global means for each ideal type is simple, it often is not realistic. For example, it is reasonable to assume that levels of income and wages would be higher for college-educated respondents than for those who have not attended college and that they would change with age, which is not reflected in the global means. To address this problem, we can use local means that are defined based on the characteristics specified in the at() statements. To do this, we create a selection variable that equals 1 if an observation is part of the group defined by the conditions of an atspec and equals 0 otherwise. In other words, a selection variable indicates whether an observation is part of the group defined for the ideal type. To create these variables, we use the generate command with if conditions that correspond to the atspecs used for an ideal type: capture drop _sel* gen _selYC = agecat==1 \u0026 k5==2 \u0026 k618==0 \u0026 wc==1 \u0026 hc==1 label var _selYC \"Select Young college young kids\" gen _selMC = agecat==2 \u0026 k5==0 \u0026 k618==2 \u0026 wc==1 \u0026 hc==1 label var _selMC \"Select Midage college with teens\" gen _selOC = agecat==3 \u0026 k5==0 \u0026 k618==0 \u0026 wc==1 \u0026 hc==1 label var _selOC \"Select Older college with adult kids\" Once these variables are created, we can make a table of predictions containing local means for variables not explicitly set by the atspec. The first row of the table is unchanged from before because all variables for that ideal type were explicitly specified in the at() option: quietly mtable, rowname(1 Young low SES young kids) ci clear /// at(agecat=1 k5=2 k618=0 inc=10 lwg=.75 hc=0 wc=0) In the next command, we add if _selYC==1 to the mtable command so that predictions are based only on observations defined by _selYC: quietly mtable if _selYC==1, rowname(2 Young college young kids) /// atmeans ci below The if condition selects observations where agecat==1 \u0026 k5==2 \u0026 k618==0 \u0026 wc==1 \u0026 hc==1, which define our ideal type. The means of these variables will equal their specified values (for example, agecat will equal 1 and k5 will equal 2), while those variables not used to define the selection variable will equal the local mean defined by selection variables. For example, lwg will equal the average log of wages for young families with college education. Accordingly, the if condition makes it easy to specify the values we wanted to define our ideal type. In the same way, we add the last two ideal types to the table: quietly mtable if _selMC==1, rowname(3 Midage college with teens) /// atmeans ci below mtable if _selOC==1, rowname(4 Older college with adult kids) /// atmeans ci below Expression: Pr(lfp), predict()\r| Pr(y) ll ul\r----------------------------+-----------------------------\r1 Young low SES young kids | 0.159 0.068 0.251\r2 Young college young kids | 0.394 0.234 0.554\r3 Midage college with teen | 0.739 0.659 0.820\r4 Older college with adult | 0.631 0.528 0.734\rSpecified values of covariates\r| k5 k618 agecat wc hc lwg inc\r----------+---------------------------------------------------------------------\rSet 1 | 2 0 1 0 0 .75 10\rSet 2 | 2 0 1 1 1 1.62 16.6\rSet 3 | 0 2 2 1 1 1.16 24.4\rCurrent | 0 0 3 1 1 1.38 27.9\rAn advantage of using local means with ideal types is that the values of variables not specified in the type are held to values more consistent with what is actually observed, so the ideal type more accurately resembles the actual cases in our dataset that share the key features of the ideal type. ","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:3:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3.2 Comparing ideal types with statistical tests The predicted probabilities of labor force participation vary among the four ideal types. Before concluding, for example, that the probability of being in the labor force is greater for a young, college-educated family with children than for a family with no college education, we need to test whether the predictions are significantly different. Essentially, this involves testing whether a discrete change is 0 when the starting values and ending values vary on multiple variables. To show how this is done, we compute two ideal types in the same mtable command and post the results so that we can evaluate them with mlincom. Because we are posting the results, we begin with estimates store so that we can later restore the estimation results from logit after they have been replaced by the posted predictions. logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog estimates store base mtable, atmeans post /// at(agecat=1 k5=2 k618=0 wc=0 hc=0 lwg=.75 inc=10) /// ideal type 1 at(agecat=1 k5=2 k618=0 wc=1 hc=1 lwg=1.62 inc=16.64) // ideal type 2 Expression: Pr(lfp), predict()\r| wc hc lwg inc Pr(y)\r----------+-------------------------------------------------\r1 | 0 0 .75 10 0.159\r2 | 1 1 1.62 16.6 0.394\rSpecified values of covariates\r| k5 k618 agecat\r----------+-----------------------------\rCurrent | 2 0 1\rNow, we estimate the difference in the predictions and end by restoring the estimation results from logit: mlincom 1 - 2 | lincom pvalue ll ul -------------+---------------------------------------\r1 | -0.234 0.000 -0.340 -0.129 We conclude the following: A wife from a young, lower SES family with young children is significantly less likely to be in the labor force than a wife from a young family with college education (p \u003c 0.001). ","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:3:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3.3 (Advanced) Using macros to test differences between ideal types In this section, we discuss using local macros and returns to automate the process of computing predictions at multiple fixed values of the at() variables. If you rarely test the equality of predictions, the methods from the last section should meet your needs. If you often test the equality of predictions, this section can save you time. It is tedious and error-prone to specify the atspecs for multiple ideal types to test the equality of predictions. To automate this process, we can use the returned results from mtable. When mtable is run with a single at(), it returns the local r(atspec) as a string that contains the specified values of the covariates. This is easiest to understand with an example: mtable, atmeans at(agecat=1 k5=2 k618=0 wc=0 hc=0 lwg=.75 inc=10) Expression: Pr(lfp), predict()\rPr(y)\r--------\r0.159\rSpecified values of covariates\r| k5 k618 agecat wc hc lwg inc\r----------+---------------------------------------------------------------------\rCurrent | 2 0 1 0 0 .75 10\rThe values shown in the Specified values of covariates table are saved in the return r(atspec): display \"`r(atspec)'\" . display \"`r(atspec)'\"\rk5=2 k618=0 1b.agecat=1 2.agecat=0 3.agecat=0 0b.wc=1 1.wc=0 0b.hc=1 1.hc=0 lwg=.75 inc=10 We create a local macro that is used to specify the atspec for mtable: local myatspec `r(atspec)' mtable, atmeans at(`myatspec') Expression: Pr(lfp), predict()\rPr(y)\r--------\r0.159\rSpecified values of covariates\r| k5 k618 agecat wc hc lwg inc\r----------+---------------------------------------------------------------------\rCurrent | 2 0 1 0 0 .75 10\rThe results match those we obtained earlier. Using this strategy and the selection variables created before (see page 273), we create local macros with the atspecs for our four ideal types: quietly mtable, atmeans at(agecat=1 k5=2 k618=0 inc=10 lwg=.75 hc=0 wc=0) local YngLow `r(atspec)' quietly mtable if _selYC == 1, atmeans local YngCol `r(atspec)' quietly mtable if _selMC == 1, atmeans local MidCol `r(atspec)' quietly mtable if _selOC == 1, atmeans local OldCol `r(atspec)' We use these locals to compute four predictions with a single mtable: mtable, at(`YngLow') at(`YngCol') at(`MidCol') at(`OldCol') post Expression: Pr(lfp), predict()\r| k5 k618 agecat wc hc lwg inc Pr(y)\r----------+-------------------------------------------------------------------------------\r1 | 2 0 1 0 0 .75 10 0.159\r2 | 2 0 1 1 1 1.62 16.6 0.394\r3 | 0 2 2 1 1 1.16 24.4 0.739\r4 | 0 0 3 1 1 1.38 27.9 0.631\rSpecified values where .n indicates no values specified with at()\r| No at()\r----------+---------\rCurrent | .n\rBecause the values of all independent variables were specified for each prediction, their values appear in the table of predictions rather than in a table of values of covariates below the predictions. Because there are no values to place in the table, .n is shown. Because the predictions were posted, we can use mlincom for each comparison: mlincom 1 - 2 mlincom 1 - 3 mlincom 1 - 4 . mlincom 1 - 2\r| lincom pvalue ll ul -------------+---------------------------------------\r1 | -0.235 0.000 -0.340 -0.129 . mlincom 1 - 3\r| lincom pvalue ll ul -------------+---------------------------------------\r1 | -0.580 0.000 -0.720 -0.440 . mlincom 1 - 4\r| lincom pvalue ll ul -------------+---------------------------------------\r1 | -0.471 0.000 -0.622 -0.320 Alternatively, we can take advantage of the pwcompare() option in margins, which is not available with the mtable command. We specify the values at which we want to make predictions, just like we did with mtable. We suppress the lengthy listing of the titlegend and request pairwise comparisons of the estimates: estimates restore base margins, at(`YngLow') at(`YngCol') at(`MidCol') at(`OldCol') /// noatlegend pwcompare(effects) Pairwise comparisons of adjusted predictions Number of obs = 753\rModel VCE: OIM\rExpression: Pr(lfp), predict()\r----------------------------------------------------","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:3:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3.4 Marginal effects for ideal types Given our cautions about relying on a single value to summarize marginal effects, ideal types are an excellent way to examine variation in the size of effects at different locations in the data space. Here we are taking different hypothetical observations and describing how the predicted probability changes as the value of one of the independent variables for that observation changes. To do this, we use local macros to specify the values at which the change is to be computed, just as we did with mtable. First, we com pute discrete changes for wc and k5 for a young, low SES family: mchange wc k5, atmeans amount(one) at(`YngLow') logit: Changes in Pr(y) | Number of obs = 753 Expression: Pr(lfp), predict(pr)\r| Change p-value ---------------+---------------------\rwc | college vs no | 0.137 0.008 k5 | +1 | -0.114 0.000 Predictions at base value\r| not in LF in LF -------------+---------------------\rPr(y|base) | 0.841 0.159 Base values of regressors\r| k5 k618 agecat wc hc lwg inc -------------+----------------------------------------------------------------------------\rat | 2 0 1 0 0 .75 10 1: Estimates with margins option atmeans.\rmatrix YngLow = r(table) mchange leaves the marginal effects in the r (table) matrix, which we copy to the matrix YngLow so that we can combine it with estimates of effects for other ideal types. mchange wc k5, atmeans amount(one) at(`YngCol') matrix YngCol = r(table) mchange wc k5, atmeans amount(one) at(`MidCol') matrix MidCol = r(table) mchange wc k5, atmeans amount(one) at(`OldCol') matrix OldCol = r(table) Next, we select the first column of each matrix, which contains the effects, and concatenate them into a single matrix we name me: matrix me = YngLow[1...,1], YngCol[1...,1], MidCol[1...,1], OldCol[1...,1] matrix colnames me = YngLow YngCol MidCol OldCol matlist me, format(%9.2f) twidth(15) | YngLow YngCol MidCol OldCol ----------------+-------------------------------------------\rwc | college vs no | 0.14 0.19 0.19 0.19 ----------------+-------------------------------------------\rk5 | +1 | -0.11 -0.32 -0.32 -0.32 The effects of the wife going to college are within 0.06 across the four ideal types. The effects of having one more young child in the family, however, increase in magnitude from -0.11 for young families without college education to -0.33 for older families that attended college. The differences in discrete changes for k5 reflect the variation in the size of effects within the sample. ","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:3:4","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4 Tables of predicted probabilities When you are interested in the effects of one or more categorical independent variables, a table of predictions can be very effective. For example, our analysis thus far highlights the importance of attending college and having young children. To see how these variables jointly affect the probability of being in the labor force, we can use a simple mtable command: use binlfp4, clear logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog estimates store base mtable, at(wc=(0 1) k5=(0 1 2 3)) atmeans Expression: Pr(lfp), predict()\r| k5 wc Pr(y)\r----------+-----------------------------\r1 | 0 0 0.604\r2 | 0 1 0.772\r3 | 1 0 0.275\r4 | 1 1 0.457\r5 | 2 0 0.086\r6 | 2 1 0.173\r7 | 3 0 0.023\r8 | 3 1 0.049\rSpecified values of covariates\r| 2. 3. 1. | k618 agecat agecat hc lwg inc\r----------+-------------------------------------------------------------\rCurrent | 1.35 .385 .219 .392 1.1 20.1\rAlthough this is the information we want, it is not an effective table. We can improve it by using two at()’s along with atvars(wc k5) to list values of wc in the first column followed by values of k5. The option names(columns) removes the row numbers (see help matlist for details on the names() option). mtable, at(wc=0 k5=(0 1 2 3)) at(wc=1 k5=(0 1 2 3)) atmeans /// atvars(wc k5) names(columns) Expression: Pr(lfp), predict()\r1. wc k5 Pr(y)\r----------------------------\r0 0 0.604\r0 1 0.275\r0 2 0.086\r0 3 0.023\r1 0 0.772\r1 1 0.457\r1 2 0.173\r1 3 0.049\rSpecified values of covariates\r| 2. 3. 1. | k618 agecat agecat hc lwg inc\r----------+-------------------------------------------------------------\rCurrent | 1.35 .385 .219 .392 1.1 20.1\rThe table shows the strong effect of education and how the size of the effects differ by the number of young children, but the information still is not presented well. Our next step is to compute the discrete change for college education conditional on the number of young children: $$\\frac{\\Delta\\Pr\\left(y=1\\mid\\mathbf{x},\\mathbf{k}5\\right)}{\\Delta\\text{wc}\\left(0\\to1\\right)}$$ Because wc was entered into the model as a factor variable, we can compute the discrete change by using dydx(wc). In the process, let’s create an even more effective table that gets close to what we might include in a paper. First, we compute the predictions for wc=0: quietly mtable, estname(NoCol) at(wc=0 k5=(0 1 2 3)) atmeans brief Next, we make predictions for wc=1. We use the right option to place the predictions to the right of those from the prior mtable command, and we use atvars(_none) because we do not want the column with k5 included again. quietly mtable, estname(College) at(wc=1 k5=(0 1 2 3)) atmeans /// atvars(_none) right Now, we use the dydx(wc) option to compute discrete changes. We place these along with the p-value for testing whether the change is 0 to the right: mtable, estname(Change) dydx(wc) at(k5=(0 1 2 3)) atmeans /// atvars(_none) right stats(estimate p) names(columns) brief Expression: Pr(lfp), predict()\rk5 NoCol NoCol College Change p\r----------------------------------------------------------\r0 0.772 0.604 0.772 0.168 0.000\r1 0.457 0.275 0.457 0.182 0.001\r2 0.173 0.086 0.173 0.087 0.013\r3 0.049 0.023 0.049 0.027 0.085\rWe can summarize these findings: For someone who is average on all characteristics and has no young children, having attended college significantly increases the predicted probability of being in the labor force by 0.17. The size of the effect decreases with the number of young children. For example, for someone with two young children, the increase is only 0.09, which is significant at the 0.01 level. Although this table shows clearly how education and children affect labor force participation, it assumes that it is reasonable to change wc and k5 while holding other variables at their global means. This is unrealistic. For example, it is likely that women with three young children will be in the youngest age group, while few people with three young children will be over 50. Each cel","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:4:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"5 Second differences comparing marginal effects We can compute AMEs based on a subset of observations. For example, suppose that we are interested in ways in which the wife’s and the husband’s educations interact to affect labor force participation. Because we are focusing on the joint effects of these two variables, we fit a new model that includes the interaction between wc and hc: logit lfp k5 k618 i.agecat wc##hc lwg inc, nolog estimates store base Logistic regression Number of obs = 753\rLR chi2(9) = 125.57\rProb \u003e chi2 = 0.0000\rLog likelihood = -452.08908 Pseudo R2 = 0.1219\r----------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-----------------+----------------------------------------------------------------\rk5 | -1.40 0.19 -7.27 0.000 -1.78 -1.02\rk618 | -0.07 0.07 -0.96 0.337 -0.20 0.07\r|\ragecat |\r40-49 | -0.62 0.21 -2.97 0.003 -1.03 -0.21\r50+ | -1.27 0.26 -4.88 0.000 -1.78 -0.76\r|\rwc |\rcollege | 1.19 0.43 2.75 0.006 0.34 2.05\r|\rhc |\rcollege | 0.25 0.23 1.08 0.281 -0.20 0.69\r|\rwc#hc |\rcollege#college | -0.56 0.51 -1.11 0.269 -1.55 0.43\r|\rlwg | 0.61 0.15 4.05 0.000 0.31 0.90\rinc | -0.03 0.01 -4.19 0.000 -0.05 -0.02\r_cons | 0.98 0.29 3.41 0.001 0.42 1.54\r----------------------------------------------------------------------------------\rWe want to know whether the effect of a women going to college is the same for a women whose husband did go to college as for a woman whose husband did not go to college: $$H_0{:\\frac{\\Delta\\Pr\\left(y=1\\mid\\mathrm{x,hc}=0\\right)}{\\Delta\\mathrm{wc}}}=\\frac{\\Delta\\Pr\\left(y=1\\mid\\mathrm{x,hc}=1\\right)}{\\Delta\\mathrm{wc}}$$ To test this hypothesis, we compute the AME of wc averaging over only those cases where hc is 0 and compare it with the AME for those cases where hc is 1. Although we could compute these discrete changes by using mchange wc if hc==1 and mchange wc if hc==0, this will not allow us to test whether the effects are equal because the estimates cannot be posted for testing with mlincom. To test the hypothesis, we use mtable with the dydx(wc) option to compute the discrete change for wc and the over(hc) option to request the changes be computed with the subgroups defined by hc: mtable, dydx(wc) over(hc) post Expression: Pr(lfp), predict()\r| d Pr(y)\r----------+---------\rno | 0.233\rcollege | 0.128\rSpecified values where .n indicates no values specified with at()\r| No at()\r----------+---------\rCurrent | .n\rThe row labeled no contains the discrete change of wc for those women whose husbands did not attend college (no is the value label for hc= 0), and the row college contains the change for those whose husbands attended college. The post option saves the estimates to e(b), which allows us to use mlincom to test whether the marginal effects are equal: test _b[1.wc:0.hc]=_b[1.wc:1.hc] mlincom 1 - 2 ( 1) [1.wc]0bn.hc - [1.wc]1.hc = 0\rchi2( 1) = 1.42\rProb \u003e chi2 = 0.2329\r. mlincom 1 - 2\r| lincom pvalue ll ul -------------+---------------------------------------\r1 | 0.105 0.233 -0.068 0.279 We conclude the following: Although the average effect of the wife going to college is 0.10 larger when the husband did not go to college than when he did, this difference is not significant (p \u003e 0.10). ","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:5:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"6 Graphing predicted probabilities With a continuous independent variable, you can plot the predicted probabilities over the range of the variable. For example, to examine the effects of inc, we might plot the predicted probability of labor force participation as income changes, holding other variables at fixed values. We offer two approaches for making such graphs. First, Stata’s marginsplot command uses predictions from margins to create plots. As you will see, it quickly produces effective graphs. The second approach uses our mgen command to generate variables with the values to be plotted, which are then plotted with graph. This is essentially what marginsplot does behind the scenes. Although marginsplot is simpler, mgen is more flexible in ways that often justify the greater effort that it requires. The advantage will be particularly apparent in subsequent chapters when we create plots for multiple outcomes that cannot be created with marginsplot. We begin by showing you how to create plots where one variable changes while all other variables are held constant. ","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:6:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"6.1 Using marginsplot The first step is to use margins to compute predicted probabilities as income increases from 0 to 100, while holding other variables at their means: use binlfp4, clear logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog estimates store base margins, at(inc=(0(10)100)) atmeans Adjusted predictions Number of obs = 753\rModel VCE: OIM\rExpression: Pr(lfp), predict()\r1._at: k5 = .2377158 (mean)\rk618 = 1.353254 (mean)\r1.agecat = .3957503 (mean)\r2.agecat = .3851262 (mean)\r3.agecat = .2191235 (mean)\r0.wc = .7184595 (mean)\r1.wc = .2815405 (mean)\r0.hc = .6082337 (mean)\r1.hc = .3917663 (mean)\rlwg = 1.097115 (mean)\rinc = 0\r11._at: k5 = .2377158 (mean)\rk618 = 1.353254 (mean)\r1.agecat = .3957503 (mean)\r2.agecat = .3851262 (mean)\r3.agecat = .2191235 (mean)\r0.wc = .7184595 (mean)\r1.wc = .2815405 (mean)\r0.hc = .6082337 (mean)\r1.hc = .3917663 (mean)\rlwg = 1.097115 (mean)\rinc = 100\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_at |\r1 | 0.73 0.04 20.36 0.000 0.66 0.81\r2 | 0.66 0.03 25.32 0.000 0.61 0.71\r3 | 0.58 0.02 29.40 0.000 0.54 0.62\r4 | 0.49 0.03 17.17 0.000 0.44 0.55\r5 | 0.41 0.04 9.20 0.000 0.32 0.49\r6 | 0.32 0.06 5.70 0.000 0.21 0.44\r7 | 0.25 0.06 3.94 0.000 0.13 0.38\r8 | 0.19 0.07 2.95 0.003 0.06 0.32\r9 | 0.14 0.06 2.33 0.020 0.02 0.26\r10 | 0.11 0.06 1.92 0.055 -0.00 0.21\r11 | 0.08 0.05 1.63 0.103 -0.02 0.17\r------------------------------------------------------------------------------\rThe atlegend shows the values of the independent variables for each of the 11 predictions in the table, which are automatically saved in the matrix r(b). Because the atlegend can be quite long, we often use noatlegend to suppress it. Then, we use margins for a more compact summary. margins, at(inc=(0(10)100)) atmeans noatlegend Adjusted predictions Number of obs = 753\rModel VCE: OIM\rExpression: Pr(lfp), predict()\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_at |\r1 | 0.73 0.04 20.36 0.000 0.66 0.81\r2 | 0.66 0.03 25.32 0.000 0.61 0.71\r3 | 0.58 0.02 29.40 0.000 0.54 0.62\r4 | 0.49 0.03 17.17 0.000 0.44 0.55\r5 | 0.41 0.04 9.20 0.000 0.32 0.49\r6 | 0.32 0.06 5.70 0.000 0.21 0.44\r7 | 0.25 0.06 3.94 0.000 0.13 0.38\r8 | 0.19 0.07 2.95 0.003 0.06 0.32\r9 | 0.14 0.06 2.33 0.020 0.02 0.26\r10 | 0.11 0.06 1.92 0.055 -0.00 0.21\r11 | 0.08 0.05 1.63 0.103 -0.02 0.17\r------------------------------------------------------------------------------\rmlistat at() values held constant\r2. 3. 1. 1. k5 k618 agecat agecat wc hc lwg --------------------------------------------------------------------\r.238 1.35 .385 .219 .282 .392 1.1 at() values vary\r_at | inc -------+---------\r1 | 0 2 | 10 3 | 20 4 | 30 5 | 40 6 | 50 7 | 60 8 | 70 9 | 80 10 | 90 11 | 100 Either way, marginsplot uses the predictions in r(b) along with other returns from margins, and it graphs the predictions including the 95% confidence intervals. marginsplot The graph shows how the probability of being in the labor force decreases with family income. It also shows that the confidence intervals are smaller near the center of the data (the mean of inc is 20.1) and increase as we move to the extremes. Although marginsplot does an excellent job of creating the graph without requiring options, you can fully customize the graph. Use help marginsplot for full details. For example, to suppress the confidence interval, type marginsplot, noci. To use shading to show the confidence interval (illustrated below), type marginsplot, recast(line) recastci(rarea). If you are only interested in plotting a single type of prediction from one model, there is little reason to use anything but marginsplot. But, if you want to plot multiple outcomes, such as for multinomial l","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:6:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"6.2 Using mgen with the graph command To create the same graph as above by using mgen, our first step is to generate variables for plotting: mgen, atmeans at(inc=(0(10)100)) stub(PLT) predlabel(Pr(LFP)) Predictions from: margins, atmeans at(inc=(0(10)100)) predict(pr)\rVariable Obs Unique Mean Min Max Label\r-------------------------------------------------------------------------------------------\rPLTpr1 11 11 .3608011 .0768617 .7349035 Pr(LFP)\rPLTll1 11 11 .2708139 -.0156624 .6641427 95% lower limit\rPLTul1 11 11 .4507883 .1693859 .8056643 95% upper limit\rPLTinc 11 11 50 0 100 Family income excluding wife's\r-------------------------------------------------------------------------------------------\rSpecified values of covariates\r1. 3. 1. 1. k5 k618 agecat agecat wc hc lwg ---------------------------------------------------------------------------\r.2377158 1.353254 .3851262 .2191235 .2815405 .3917663 1.097115 The option stub() specifies the prefix for variables that are generated. If stub() is not specified, the default stub(_) is used. If you want to replace existing plot variables (perhaps while debugging your do-file), add the option replace. The option predlabel() customizes the variable label for PLTprl, which is handy because by default graph uses this label for the y axis. If we list the values for the first 13 observations, we see the variables created by mgen: list PLTinc PLTpr PLTll PLTul lfp in 1/13, clean PLTinc PLTpr1 PLTll1 PLTul1 lfp 1. 0 .7349035 .6641427 .8056643 not in LF 2. 10 .6613024 .6101217 .7124832 not in LF 3. 20 .5789738 .5403737 .6175739 not in LF 4. 30 .4920058 .4358374 .5481742 not in LF 5. 40 .405519 .3191012 .4919367 not in LF 6. 50 .324523 .2129492 .4360968 not in LF 7. 60 .2528245 .1272066 .3784425 not in LF 8. 70 .1924535 .0644926 .3204144 not in LF 9. 80 .1437253 .0227563 .2646942 not in LF 10. 90 .1057196 -.0023663 .2138055 not in LF 11. 100 .0768617 -.0156624 .1693859 not in LF 12. . . . . not in LF 13. . . . . not in LF Column 1 contains the 11 values of income from variable PLTinc that will define the x coordinates. The next column contains predicted probabilities computed at the values of income with other variables held at their means. The negative effect of income is shown by the increasingly small probabilities. The next two columns contain the upper and lower bounds of the confidence intervals for the predictions. The first four variables have missing values beginning in rows 12 and 13 because our atspec requested only 11 predictions. The last column shows the observed variable lfp, which does not have missing values. This is being shown to remind you that the variables created for graphing are added to the dataset used for estimation. You can also create a basic graph w ithout confidence intervals: scatter PLTpr PLTinc Next, we want to add the 95% confidence interval around the predictions. This requires more complicated graph options. To explain these, let’s start by looking at the graph we want to create: twoway /// (rarea PLTul PLTll PLTinc, color(gs12)) /// (connected PLTpr PLTinc, msymbol(i)) /// , title(\"Adjusted Predictions\") /// caption(\"Other variables held at their means\") /// ytitle(Pr(LFP)) ylabel(0(.25)1, grid gmin gmax) legend(off) Here is the twoway command that we will explain: The first thing to realize is that the twoway command includes two plots: a plot and a connected plot. These are overlaid to make a single graph. First, the shaded confidence intervals are created with a rarea plot where the area on the y axis is shaded between the values of PLTul for the upper level or bound and PLTll for the lower bound. We chose color(gs12) to make the shading grayscale level 12, a matter of personal preference. Second, the line with predicted probabilities is created with a connected plot, where msymbol(i) specifies that the symbols (shown as solid circles in our prior graph) that are connected should be invisible—that is, draw the line without symbols. We defined the rarea p","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:6:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"6.3 Graphing multiple predictions An effective way to show the effects of two variables is to graph predictions at various levels of one variable as the other variable changes. This can be done with either marginsplot or mgen. 6.3.1 Using marginsplot We can plot the effects of income for each of the age groups. First, we compute the predictions with margins, where margins agecat indicates that we want predictions for each level of the factor variable agecat. at(inc = (0(10)100))atmeans specifies predictions as income increases from 0 to 100 by 10s, with all variables except agecat at their means: margins agecat, at(inc=(0(10)100)) atmeans noatlegend Adjusted predictions Number of obs = 753\rModel VCE: OIM\rExpression: Pr(lfp), predict()\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_at#agecat |\r1#30-39 | 0.82 0.03 26.36 0.000 0.76 0.88\r1#40-49 | 0.71 0.05 15.79 0.000 0.63 0.80\r1#50+ | 0.57 0.06 9.20 0.000 0.44 0.69\r2#30-39 | 0.77 0.03 25.73 0.000 0.71 0.83\r2#40-49 | 0.64 0.04 17.04 0.000 0.56 0.71\r2#50+ | 0.48 0.05 9.42 0.000 0.38 0.58\r3#30-39 | 0.70 0.03 21.88 0.000 0.64 0.76\r3#40-49 | 0.55 0.03 17.13 0.000 0.49 0.62\r3#50+ | 0.39 0.04 8.95 0.000 0.31 0.48\r4#30-39 | 0.62 0.04 14.75 0.000 0.54 0.70\r4#40-49 | 0.47 0.04 12.72 0.000 0.39 0.54\r4#50+ | 0.31 0.04 7.27 0.000 0.23 0.40\r5#30-39 | 0.53 0.06 9.22 0.000 0.42 0.65\r5#40-49 | 0.38 0.05 8.08 0.000 0.29 0.47\r5#50+ | 0.24 0.04 5.40 0.000 0.15 0.33\r6#30-39 | 0.45 0.07 6.01 0.000 0.30 0.59\r6#40-49 | 0.30 0.06 5.34 0.000 0.19 0.41\r6#50+ | 0.18 0.05 4.01 0.000 0.09 0.27\r7#30-39 | 0.36 0.09 4.19 0.000 0.19 0.53\r7#40-49 | 0.23 0.06 3.81 0.000 0.11 0.35\r7#50+ | 0.14 0.04 3.09 0.002 0.05 0.22\r8#30-39 | 0.29 0.09 3.10 0.002 0.11 0.47\r8#40-49 | 0.18 0.06 2.89 0.004 0.06 0.30\r8#50+ | 0.10 0.04 2.48 0.013 0.02 0.18\r9#30-39 | 0.22 0.09 2.42 0.016 0.04 0.40\r9#40-49 | 0.13 0.06 2.30 0.021 0.02 0.24\r9#50+ | 0.07 0.04 2.05 0.040 0.00 0.14\r10#30-39 | 0.17 0.08 1.96 0.049 0.00 0.33\r10#40-49 | 0.10 0.05 1.91 0.056 -0.00 0.20\r10#50+ | 0.05 0.03 1.75 0.080 -0.01 0.11\r11#30-39 | 0.12 0.07 1.65 0.099 -0.02 0.27\r11#40-49 | 0.07 0.04 1.63 0.104 -0.01 0.15\r11#50+ | 0.04 0.02 1.52 0.128 -0.01 0.09\r------------------------------------------------------------------------------\rmlistat k5 k618 agecat agecat wc hc lwg --------------------------------------------------------------------\r.238 1.35 .385 .219 .282 .392 1.1 at() values vary\r_at | inc -------+---------\r1 | 0 2 | 10 3 | 20 4 | 30 5 | 40 6 | 50 7 | 60 8 | 70 9 | 80 10 | 90 11 | 100 The labeling of the predictions from margins can be confusing. The left column of the prediction table is labeled _at#agecat, which indicates that the information in this column begins with a number corresponding to the 11 values of inc used for making predictions; these are referred to as the _at values. For example, 1 is the prediction with inc=0 while 11 is the prediction with inc=100. After that, the value or value label for agecat is listed. For example, the row labeled l#30-39 contains the predictions when all variables except agecat are held at the first _at value, with agecat=1 as indicated by the value label 30-39. The command marginsplot, noci automatically understands what these predictions are and creates the plot we want. marginsplot, noci legend(cols(3)) This example shows that marginsplot can plot multiple curves for the same outcome from the same model. Unfortunately, it cannot plot curves for multiple outcomes (for example, the probability of categories 1, 2, and 3 in an ordinal model) or predictions from different models (for example, showing how the predictions differ from two specifications of the model). For this, you need to use mgen. 6.3.2 Using mgen with graph We can create the same graph as in the previous section by running mgen once for each level of agecat: mgen, atmeans at(in","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:6:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"6.3 Overlapping confidence intervals We find that researchers sometimes conclude that estimates are significantly different only if confidence intervals for two estimates do not overlap. That is, if the confidence intervals overlap, the hypothesis that the estimates are equal is accepted. Although this might have been a useful approximation when computation was very expensive, it often leads to incorrect conclusions because it ignores the covariances of the estimators that need to be taken into account when testing equality. To illustrate the problem, as well as show how discrete changes and marginal changes can be graphed, we use the techniques above to plot the probability of labor force participation by income for women who attended college and those who did not. We start with the graph before showing how we created it. We use one mgen command for each level of wc: mgen, atmeans at(inc=(0(5)100) wc=0) stub(PLTWC0) predlab(NoCollege) Predictions from: margins, atmeans at(inc=(0(5)100) wc=0) predict(pr)\rVariable Obs Unique Mean Min Max Label\r-------------------------------------------------------------------------------------\rPLTWC0pr1 21 21 .3177494 .0623648 .6889161 NoCollege\rPLTWC0ll1 21 21 .2309727 -.0151898 .6107005 95% lower limit\rPLTWC0ul1 21 21 .4045261 .1399194 .7671317 95% upper limit\rPLTWC0inc 21 21 50 0 100 Family income excluding wife's\r-------------------------------------------------------------------------------------\rSpecified values of covariates\r1. 3. 1. k5 k618 agecat agecat wc hc lwg ---------------------------------------------------------------------------\r.2377158 1.353254 .3851262 .2191235 0 .3917663 1.097115 mgen, atmeans at(inc=(0(5)100) wc=1) stub(PLTWC1) predlab(College) twoway /// (rarea PLTWC0ul PLTWC0ll PLTWC0inc, col(gs12)) /// (rarea PLTWC1ul PLTWC1ll PLTWC0inc, col(gs12)) /// (connected PLTWC0pr PLTWC1pr PLTWC1inc, msym(i i)) /// , ytitle(Pr(In Labor Force)) legend(order(4 3)) Judging by the overlap of confidence intervals, we might mistakenly conclude that the probability of labor force participation was significantly higher for women who attended college when family income was between $5,000$ and $40,000$ but not at other incomes. To see how poorly this “approximation” works, we compute the discrete change conditional on income with mgen. The option dydx(wc) specifies that we want to predict the marginal effect of wc. Because wc was entered into the model as the factor variable i.wc, mgen computes a discrete change. mgen, dydx(wc) atmeans at(inc=(0(5)100)) stub(PLTWCDC) /// predlab(Discrete change in LFP by attending college) Predictions from: margins, dydx(wc) atmeans at(inc=(0(5)100)) predict(pr)\rVariable Obs Unique Mean Min Max Label\r------------------------------------------------------------------------------------------------------\rPLTWCDCd_pr1 21 21 .1507267 .0663191 .1967745 Discrete change in LFP by attending college\rPLTWCDCll1 21 21 .0556941 -.0111785 .0895455 95% lower limit\rPLTWCDCul1 21 21 .2457593 .1438166 .3049388 95% upper limit\rPLTWCDCinc 21 21 50 0 100 Family income excluding wife's\r------------------------------------------------------------------------------------------------------\rSpecified values of covariates\r1. 3. 1. 1. k5 k618 agecat agecat wc hc lwg ---------------------------------------------------------------------------\r.2377158 1.353254 .3851262 .2191235 .2815405 .3917663 1.097115 Plotting the results along with those for the probabilities leads to figure 6.2, which shows that women who attended college have significantly higher probabilities of labor force participation over almost the entire income distribution, excepting only incomes above $95,000$ (where there are very few cases). What is remarkable about margins is that it allows you to test just about anything you might want to say about your predictions! twoway /// (rarea PLTWC0ul PLTWC0ll PLTWC0inc, col(gs12)) /// (rarea PLTWC1ul PLTWC1ll PLTWC0inc, col(gs12)) /// (connected PLTWC0pr PLTWC1pr PLTWC1in","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:6:4","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"6.5 Adding power terms and plotting predictions As shown in section 6.2.1, squared terms can be included in models by using factor-variable notation. For example, income and income-squared can be included in the model by adding the term c.inc##c.inc. Although you can obtain the same parameter estimates by generating a new variable for income-squared, margins or our m* commands will not compute predictions correctly. With factor-variable notation, however, power terms and interaction terms do not pose any special problems. When mgen makes predictions, it automatically increases income-squared appropriately as income changes. To illustrate how this works, we compare predictions from a model that is linear in income with a model that adds the squared term c.inc#c.inc. First, we fit the model that includes income (but not income-squared) and make predictions: logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog mgen, predlabel(linear) atmeans at(inc=(0(10)100)) stub(_lin) Predictions from: margins, atmeans at(inc=(0(10)100)) predict(pr)\rVariable Obs Unique Mean Min Max Label\r------------------------------------------------------------------------------------------\r_linpr1 11 11 .3608011 .0768617 .7349035 linear\r_linll1 11 11 .2708139 -.0156624 .6641427 95% lower limit\r_linul1 11 11 .4507883 .1693859 .8056643 95% upper limit\r_lininc 11 11 50 0 100 Family income excluding wife's\r------------------------------------------------------------------------------------------\rSpecified values of covariates\r1. 3. 1. 1. k5 k618 agecat agecat wc hc lwg ---------------------------------------------------------------------------\r.2377158 1.353254 .3851262 .2191235 .2815405 .3917663 1.097115 Next, we fit th e model that adds income-squared and make predictions: logit lfp k5 k618 i.agecat i.wc i.hc lwg c.inc##c.inc, nolog mgen, predlabel(quadratic) atmeans at(inc=(0(10)100)) stub(_quad) Predictions from: margins, atmeans at(inc=(0(10)100)) predict(pr)\rVariable Obs Unique Mean Min Max Label\r-------------------------------------------------------------------------------------\r_quadpr1 11 11 .4410442 .2887324 .8078035 quadratic\r_quadll1 11 11 .2613509 -.1501807 .7207593 95% lower limit\r_quadul1 11 11 .6207375 .4265932 .9690264 95% upper limit\r_quadinc 11 11 50 0 100 Family income excluding wife's\r-------------------------------------------------------------------------------------\rSpecified values of covariates\r1. 3. 1. 1. k5 k618 agecat agecat wc hc lwg ---------------------------------------------------------------------------\r.2377158 1.353254 .3851262 .2191235 .2815405 .3917663 1.097115 Then, we plot the predictions: graph twoway connected _linpr _quadpr _lininc, /// title(\"Comparing income specifications\") /// caption(\"Other variables at their means\") /// msym(Oh Dh) msiz(*1.5 *1) mcol(black black) lpat(solid dash) /// ytitle(\"Pr(In Labor Force)\") /// ylabel(0(.25)1, grid gmin gmax) Overlapping confidence intervals compared with discrete change\rAlthough the differences at higher incomes are suggestive and dramatic, the evidence for preferring the quadratic model is mixed. BIC provides positive support for the linear model, while AIC supports the quadratic model. The coefficient for income-squared is significant at the 0.046 level. The confidence intervals around the predictions at high income levels (not shown) are wide. Based on these results, we are not convinced to abandon our baseline model. ","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:6:5","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"6.6 (Advanced) Graphs with local means When plotting predictions over the range of a variable, you must decide where to hold the values of other variables. With the atmeans option in mgen, as the plotted variable changes, the other variables stay at the same global means. Following our previous discussion of local means, in this section we show you how to allow the values of the other variables to change as the variable being plotted changes. This requires using mtable with the over() option and moving predictions from the matrix that mtable returns. These steps require more data management than other parts of the book, but they can provide valuable insights into how robust your plot and conclusions are to assumptions about the levels of other variables. When demonstrating tables of predictions, we suggested caution before holding other variables at their global means because changing one variable while holding all other variables at the same values might not be realistic. For example, suppose that we included age in our model as a continuous variable ranging from 20 to 90. Plotting predictions as age changes while holding the number of young children constant is unrealistic because older respondents are unlikely to have any young children in the family. Note that we have the same problem if we used a subscribed instead of atmeans here; in that case, we would be including in our average predictions those cases for which the hypothetical value of age is implausible given the observed numbers of children. One alternative approach, which we will not explore further here, is to forgo using global means in favor of a set of representative values that are substantively plausible for all values of age (that is, a family with no children). In any event, if you are plotting predictions in regions of your data where it is impossible or very unlikely that observations will exist, the predictions might be misleading. You can determine whether global means are reasonable by exploring how other values affect the results. We will consider what happens when we use local means instead of global in generating the plot. To illustrate how this is done, we start with the example used above, where we plotted labor force participation by income. mgen, at(inc=(0(10)100)) atmeans stub(GLOBAL) predlabel(Global means) Predictions from: margins, at(inc=(0(10)100)) atmeans predict(pr)\rVariable Obs Unique Mean Min Max Label\r-------------------------------------------------------------------------------------\rGLOBALpr1 11 11 .4410442 .2887324 .8078035 Global means\rGLOBALll1 11 11 .2613509 -.1501807 .7207593 95% lower limit\rGLOBALul1 11 11 .6207375 .4265932 .9690264 95% upper limit\rGLOBALinc 11 11 50 0 100 Family income excluding wife's\r-------------------------------------------------------------------------------------\rSpecified values of covariates\r1. 3. 1. 1. k5 k618 agecat agecat wc hc lwg ---------------------------------------------------------------------------\r.2377158 1.353254 .3851262 .2191235 .2815405 .3917663 1.097115 Plotting the predictions produces th e following plot: These predictions were made by increasing family incomes from $0$ to $100,000$, holding wc, he, lwg, and other variables at their global means. This implies that those with no income have the same education and wages as those with $100,000$. As noted, before accepting this graph as a reasonable summary of the effect of family income on labor force participation, we want to determine how sensitive the predictions are to the values at which we held the other variables. In particular, what would happen if we held the other variables at levels more typical of those with a given income? Because inc is continuous, we cannot compute means for the non-income variables conditional on a single value of income, because these means might be based on very few observations. Instead, we begin by generating the variable inc10k, which divides income into groups of $10,000$: gen inc10k = trunc(i","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:6:6","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"This chapter shows how to fit the binary regression model, how to test hypotheses, how to compute residuals and influence statistics, and how to calculate scalar measures of model fit. ","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"This chapter focuses on the two most often used models for binary outcomes, the binary logit and binary probit models, referred to jointly as the binary regression model (BRM). The BRM allows a researcher to explore how each explanatory variable affects the probability of the event occurring. The BRM is also the foundation from which more complex models for ordinal, nominal, and count models are derived. Ordinal and nominal regression models are equivalent to simultaneously fitting a set of BRMs. Although the link is less direct in count models, the Poisson distribution can be derived as the outcome of many binary trials. Consequently, the principles of fitting, testing, and interpreting binary models provide essential tools that are used in later chapters. Although each chapter of the book is largely self-contained, the two chapters on binary outcomes provide more detailed explanations than later chapters. **We begin the chapter by reviewing the mathematical structure of the binary regression model. We then examine statistical testing and fit. These discussions are brief, and much of it is intended either as a simple overview or as a review for those who are familiar with the models.**For a complete discussion, see Agresti (2013), Hosmer, Lemeshow, and Sturdivant (2013), or Long (1997). Although the material in this chapter is fundamental to working with these models, we anticipate that the more important contribution of this book will be in helping you interpret and present results. The issues involved in effective interpretation are extensive enough that we devote a chapter of its own to the topic, to which this chapter might be considered the prelude. ","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:0:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1 The statistical model There are three ways to derive the BRM, with each method leading to the same statistical model. First, a latent variable can be hypothesized along with a measurement model relating the latent variable to the observed binary outcome. Second, the model can be constructed as a probability model. Third, the model can be generated as a random utility or discrete choice model. This last approach is not considered in our review; see Long (1997, 155-156) for an introduction and Train (2009) for a detailed discussion. ","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:1:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.1 A latent-variable model Assume a latent or unobserved variable $y*$ ranging from $-\\infty $ to $\\infty $ that is related to the observed independent variables by the structural model $$y_i^*=\\mathbf{x}_i\\boldsymbol{\\beta}+\\varepsilon_i$$ where $i$ indicates the observation and $\\varepsilon$ is a random error. For a single independent variable, we can simplify the notation to $$y_{i}^{*}=\\alpha+\\beta x_{i}+\\varepsilon_{i}$$ These equations are identical to those for the linear regression model except —— and this is a big exception —— that the dependent variable is unobserved. The observed binary dependent variable has two values, typically coded as 0 for a negative outcome (that is, the event did not occur) and 1 for a positive outcome (that is, the event did occur). A measurement equation defines the link between the binary observed variable $y$ and the continuous latent variable $y*$. Cases with positive values of $y*$ are observed as $y$ = 1, while cases with negative or 0 values of $y*$ are observed as $y$ = 0. To give a concrete example, imagine a survey item that asks respondents if they agree or disagree with the proposition that “a working mother can establish just as warm and secure a relationship with her children as a mother who does not work”. Obviously, respondents will vary greatly in their opinions. Some people adamantly agree with the proposition, some adamantly disagree, and still others have weak opinions one way or the other. *Imagine an underlying continuum $y$ of feelings about this item, with each respondent having a specific value on the continuum.*Those respondents with positive values for y will answer “agree” to the survey question (y = 1) and those with negative values will “disagree” (y = 0). A shift in a respondent’s opinion might move her from agreeing strongly with the position to agreeing weakly with the position, which would not change the response we observe. Or, the respondent might move from weakly agreeing to weakly disagreeing, in which case, we would observe a change from y = 1 to y = 0. Consider a second example, which we use throughout this chapter. Let y = 1 if a woman is in the paid labor force and let y = 0 if she is not. The independent variables include age, number of children, education, family income, and expected wages. Not all women in the labor force (y = 1) are there with the same certainty. One woman might be close to leaving the labor force, whereas another woman could be firm in her decision.to work. In both cases, we observe y = 1. The idea of a latent y* is that an underlying propensity to work generates the observed state. Although we cannot directly observe the propensity, at some point a change in y* results in a change in what we observe, namely, whether the woman is in the labor force. Relationship between latent variable y* and P r(y = 1) for the BRM\rThe latent-variable model for a binary outcome with a single independent variable is shown in figure above. For a given value of $x$ $$\\Pr(y=1\\mid x)=\\Pr(y^{*}\u003e0\\mid x)$$ Substituting the structural model and rearranging terms $$\\Pr(y=1\\mid x)=\\Pr(\\varepsilon\u003e-[\\alpha+\\beta x]\\mid x)$$ which shows how the probability depends on the distribution of the error $\\varepsilon$. Two distributions of $\\varepsilon$ are commonly used, both with an assumed mean of 0. First, $\\varepsilon$ is assumed to be normal with $\\mathrm{Var}(\\varepsilon)=1$. This leads to the binary probit model in which (5.1) becomes $$\\Pr(y=1\\mid x)=\\int_{-\\infty}^{\\alpha+\\beta x}\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{t^{2}}{2}\\right)dt$$ Alternatively, $\\varepsilon$ is assumed to be distributed logistically with $\\mathrm{Var}(\\varepsilon)=\\pi^2/3$, leading to the binary logit model with the simpler equation The peculiar value assumed for $\\mathrm{Var}(\\varepsilon)$ in the logit model illustrates a basic point about the identification of models with latent outcomes. In the linear regression model, $\\mathrm{Var}(\\varepsilon)$ can be estimated because y is","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:1:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.2 A nonlinear probability model Can all binary dependent variables be conceptualized as observed manifestations of some underlying latent propensity? Although philosophically interesting, perhaps, the question is of little practical importance, because the BRM can also be derived without appealing to a latent variable. This is done by specifying a nonlinear model relating the $x’s$ to the probability of an event. Following Theil (1970), the logit model can be derived by constructing a model in which the predicted $\\Pr\\left(y=1\\mid\\mathbf{x}\\right)$ is forced to be within the range 0 to 1. For example, in the linear probability model. $$\\Pr\\left(y=1\\mid\\mathbf{x}\\right)=\\mathbf{x}\\boldsymbol{\\beta}+\\varepsilon $$ the predicted probabilities can be greater than 1 and less than 0. To constrain the predictions to the range 0 to 1, we first transform the probability into the odds, $$\\Omega\\left(\\mathbf{x}\\right)=\\frac{\\Pr\\left(y=1\\mid\\mathbf{x}\\right)}{\\Pr\\left(y=0\\mid\\mathbf{x}\\right)}=\\frac{\\Pr\\left(y=1\\mid\\mathbf{x}\\right)}{1-\\Pr\\left(y=1\\mid\\mathbf{x}\\right)}$$ which indicate how often something happens $(y = 1)$ relative to how often it does not happen $(y = 0)$. The odds range from 0 when $\\mathrm{Pr}\\left(y=1\\mid\\mathbf{x}\\right)=0$ to ∞ when $\\mathrm{Pr}\\left(y=1\\mid\\mathbf{x}\\right)=1$. The log of the odds, often referred to as the logit, ranges from $-∞$ to $∞$. This range suggests a model that is linear in the logit: $$\\ln\\Omega\\left(\\mathbf{x}\\right)=\\mathbf{x}\\beta $$ This equation is equivalent to the logit model (5.2). Interpretation of this form of the model often focuses on factor changes in the odds, which are discussed below. Other binary regression models are created by choosing functions of $x\\beta$ that range from 0 to 1. Cumulative distribution functions (CDFs) have this property and readily provide several examples. For example, the CDF for the standard normal distribution results in the probit model. ","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:1:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2 Estimation using logit and probit commands Logit and probit models can be fit with the following commands and their basic options: logit depvar [indepvars] [if] [in] [weight] [, noconstant asis or vce(vcetype)] probit depvar [indepvars] [if] [in] [weight] [, noconstant asis vce(vcetype)] Variable lists depvar is the dependent variable, indepvars is a list of independent variables. If indepvars is not included, Stata fits a model with only an intercept. Warning about dependent variable. In binary models, all nonmissing, nonzero values of depvar are classified as positive outcomes, traditionally referred to as successes. Only zero values are considered negative outcomes, which are referred to as failures. Because negative values are nonzero, they are considered to be positive outcomes. To avoid possible confusion, we recommend that you explicitly create a 0/1 variable for use as depvar. Specifying the estimation sample if and in qualifiers. can be used to restrict the estimation sample. For example, if you want to fit a logit model for only women who went to college, as indicated by the variable wc, you could specify logit lfp k5 k618 age he lwg if wc==1. Listwise deletion. Stata excludes cases in which there are missing values for any of the variables in the model. Accordingly, if two models are fit using the same dataset but have different independent variables, the models may have different samples. We recommend that you use mark and markout (discussed in section 3.1.6) to explicitly remove cases with missing data. Weights and complex samples Both logit and probit can be used with fweight, pweight, and iweight. Survey estimation can be done using svy: logit or svy: probit. See section 3.1.7 for details. Options noconstant specifies that the model not have a constant term asis specifies that estimates for variables that have perfect prediction should be included in the results table. For details, see page 197. or (for logit only) reports the odds ratios defined as $\\mathrm{exp}(\\widehat{\\beta})$. Standard errors and confidence intervals are similarly transformed. Alternatively, our listcoef command can be used. vce(vcetype) specifies the type of standard errors to be computed. See section 3.1.9 for details. ","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:2:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.1 Example of logit model Our example is from Mroz’s (1987) study of the labor force participation of women, using data from the 1976 Panel Study of Income Dynamics. The sample consists of 753 white, married women between the ages of 30 and 60 years. The dependent variable lfp equals 1 if a woman is in the labor force and equals 0 otherwise. We use codebook, compact to list information about the variables we plan to include in our model: use binlfp4, clear codebook lfp k5 k618 agecat wc hc lwg inc, compact Variable Obs Unique Mean Min Max Label\r-------------------------------------------------------------------------------------------\rlfp 753 2 .5683931 0 1 In paid labor force?\rk5 753 4 .2377158 0 3 # kids \u003c 6\rk618 753 9 1.353254 0 8 # kids 6-18\ragecat 753 3 1.823373 1 3 Wife's age group\rwc 753 2 .2815405 0 1 Wife attended college?\rhc 753 2 .3917663 0 1 Husband attended college?\rlwg 753 676 1.097115 -2.054124 3.218876 Log of wife's estimated wages\rinc 753 621 20.12897 -.0290001 96 Family income excluding wife's\r-------------------------------------------------------------------------------------------\rAlthough the meaning of most of the variables is clear from the label, lwg is the log of an estimate of what the wife’s wages would be if she was employed, given her other characteristics. Because the outcome is labor force participation, it is important to include what the wife might be expected to earn if she was employed. Following the same reasoning, inc is family income excluding whatever the wife earns; this is, therefore, a measure of what the family income would be if the wife was not employed. We consider interpretation later, but it may also help bearing in mind that the data are from 1976. In the United States, prices have risen by just over a factor of 4 between 1976 and 2014, so a change in income of $5,000$ in 1974 is similar to a change in income of $20,000$ in 2014. Because agecat is ordinal, we use tabulate to examine the distribution among the age groups: tabulate agecat, missing Next, we want to fit the logit model. To be consistent with the naming practice Stata will use, we use 2.agecat and 3.agecat to refer to dummy variables indicating whether agecat==2 and whether agecat==3, respectively. By fitting the logit model, $$\\begin{aligned}\\Pr(1\\mathbf{f}\\mathbf{p}=1)\u0026=F(\\beta_0+\\beta_\\mathbf{kf}\\mathbf{k}\\mathbf{5}+\\beta_\\text{kG19}\\mathbf{k}\\text{618}+\\beta_\\text{2 agecat}2.\\text{agecat}\\\u0026+\\beta_\\text{3.agecat}3.\\text{agecat}+\\beta_\\text{we}\\mathbf{w}\\mathbf{c}+\\beta_\\text{he}\\mathbf{hc}+\\beta_\\text{lwg}1\\text{wg}+\\beta_\\text{lac}\\mathbf{inc})\\end{aligned}$$ we obtain the following results: logit lfp k5 k618 i.agecat i.wc i.hc lwg inc Iteration 0: Log likelihood = -514.8732 Iteration 1: Log likelihood = -453.10297 Iteration 2: Log likelihood = -452.72408 Iteration 3: Log likelihood = -452.72367 Iteration 4: Log likelihood = -452.72367 Logistic regression Number of obs = 753\rLR chi2(8) = 124.30\rProb \u003e chi2 = 0.0000\rLog likelihood = -452.72367 Pseudo R2 = 0.1207\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5 | -1.39 0.19 -7.25 0.000 -1.77 -1.02\rk618 | -0.07 0.07 -0.96 0.336 -0.20 0.07\r|\ragecat |\r40-49 | -0.63 0.21 -3.00 0.003 -1.04 -0.22\r50+ | -1.28 0.26 -4.92 0.000 -1.79 -0.77\r|\rwc |\rcollege | 0.80 0.23 3.48 0.001 0.35 1.25\r|\rhc |\rcollege | 0.14 0.21 0.66 0.508 -0.27 0.54\rlwg | 0.61 0.15 4.04 0.000 0.31 0.91\rinc | -0.04 0.01 -4.24 0.000 -0.05 -0.02\r_cons | 1.01 0.29 3.54 0.000 0.45 1.57\r------------------------------------------------------------------------------\rThe information in the header and table of coefficients is in the same form as discussed in chapter 3. The iteration log begins with Iteration 0: log likelihood = -514.8732 and ends with Iteration 4: log likelihood = -452.72367, with the intermediate iterations showing the steps take","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:2:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.2 comparing logit and probit Above, we fit the model with logit, but we could have used probit instead. An easy way to show how the results would differ is to put them side by side in a single table. We can do this by using estimates table (see [R] estimates table), which is more generally useful for combining results from multiple models into one table. After fitting the logit model, we use estimates store to save the estimates with the name Mlogit: logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog estimates store Mlogit We then fit a probit model and store the results: probit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog estimates store Mprobit Next, we combine the results with estimates table. Option b() sets the format for displaying the coefficients. b(%9.3f) lists the estimates in nine columns with five decimal places. Option t requests test statistics for individual coefficients—either z tests or f tests depending on the model that was fit. Varlable labels uses variable labels rather than variable names to label coefficients (the option was named label before Stata 13), with varwidth 0 indicating how many columns should be used for the labels. Variable names and value labels are used with factor variables. estimates table Mlogit Mprobit, b(%9.3f) t varlabel varwidth(30) The estimates table output labels the test statistic as t regardless of whether z tests or t are used. --------------------------------------------------------\rVariable | Mlogit Mprobit -------------------------------+------------------------\r# kids \u003c 6 | -1.392 -0.840 | -7.25 -7.50 # kids 6-18 | -0.066 -0.041 | -0.96 -1.01 | Wife's age group | 40-49 | -0.627 -0.382 | -3.00 -3.06 50+ | -1.279 -0.780 | -4.92 -5.00 | Wife attended college? | college | 0.798 0.482 | 3.48 3.55 | Husband attended college? | college | 0.136 0.074 | 0.66 0.60 Log of wife's estimated wages | 0.610 0.371 | 4.04 4.21 Family income excluding wife's | -0.035 -0.021 | -4.24 -4.37 Constant | 1.014 0.622 | 3.54 3.69 --------------------------------------------------------\rLegend: b/t\rComparing results, the estimated logit coefficients are about 1.7 times larger than the probit estimates. For example, the ratios for k5 and inc are 1.66. This illustrates how the magnitudes of the coefficients are affected by the assumed $\\operatorname{Var}(\\varepsilon)$. The ratio of estimates for hc is larger because of the large standard errors for these estimates. Values of the z tests for logit and probit are quite similar because they are not affected by the assumed $\\operatorname{Var}(\\varepsilon)$, but they are not exactly the same because the models assume different distributions of the errors. ","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:2:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.3 (Advanced) Observations predicted perfectly We mark this section as advanced because if you work with large samples where your outcome variable is not rare, you may never encounter perfect prediction. If you have smaller samples with binary predictors, you may encounter it regularly. Maximum likelihood estimation is not possible when the dependent variable does not vary within one of the categories of an independent variable. This is referred to as perfect prediction or quasiincomplete separation. To illustrate this, suppose that we are treating k5 as categorical rather than continuous in our model of labor force participation. To do this, we regress lfp on indicator variables for the number of children.’* Variable k5_l equals 1 if a person had one young child and equals 0 otherwise, and so on for k5_2 and k5_3. Only three respondents had three young children, and none of these women were in the paid labor force: tab lfp k5 In paid |\rlabor | # kids \u003c 6\rforce? | 0 1 2 3 | Total\r-----------+--------------------------------------------+----------\rnot in LF | 231 72 19 3 | 325 in LF | 375 46 7 0 | 428 -----------+--------------------------------------------+----------\rTotal | 606 118 26 3 | 753 We find that lfp is 0 every time k5_3 is 0. A logit model predicting lfp with the binary variables k5_l, k5_2, and k5_3 (with no children being the excluded category) cannot be estimated because the observed coefficient for k5_3 is effectively infinite. Think of it this way: The observed odds of being in the labor force for those with no children is 375/231 = 1.62, while the observed odds for those with three young children is 0/3 = 0. The odds ratio is 0/1.62 = 0. For the odds ratios to be 0, $\\widehat{\\beta}{k5.3}$ must be negative infinity. As the likelihood is maximized, estimates of $\\beta{k5.3}$ get more and more negative until Stata realizes that the parameter cannot be estimated and reports the following: logit lfp k5_1 k5_2 k5_3, or nolog note: k5_3 != 0 predicts failure perfectly;\rk5_3 omitted and 3 obs not used.\rLogistic regression Number of obs = 750\rLR chi2(2) = 31.05\rProb \u003e chi2 = 0.0000\rLog likelihood = -496.82164 Pseudo R2 = 0.0303\r------------------------------------------------------------------------------\rlfp | Odds ratio Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5_1 | 0.39 0.08 -4.52 0.000 0.26 0.59\rk5_2 | 0.23 0.10 -3.30 0.001 0.09 0.55\rk5_3 | 1.00 (omitted)\r_cons | 1.62 0.14 5.79 0.000 1.38 1.91\r------------------------------------------------------------------------------\rNote: _cons estimates baseline odds.\rnote: k5_3 != 0 predicts failure perfectly;\rk5_3 omitted and 3 obs not used.\rThe message can be interpreted as follows. If someone in the sample has three young children (that is, if k5_3 is not equal to 0), then she is never in the labor force (that is, lfp equals 0), which is considered a “failure” in the terminology of the model. At this point, Stata removes the three cases where k5_3 is 0 and also removes k5_3 from the model. In the output, the coefficient for k5_3 is shown as 1 followed by (omitted). If you use the asis option, Stata keeps k5_3 and the observations in the model and shows the estimate at convergence. logit lfp k5_1 k5_2 k5_3, or asis nolog Logistic regression Number of obs = 753\rLR chi2(2) = 36.10\rProb \u003e chi2 = 0.0000\rLog likelihood = -496.82164 Pseudo R2 = 0.0351\r------------------------------------------------------------------------------\rlfp | Odds ratio Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5_1 | 0.39 0.08 -4.52 0.000 0.26 0.59\rk5_2 | 0.23 0.10 -3.30 0.001 0.09 0.55\rk5_3 | 0.00 . . . . .\r_cons | 1.62 0.14 5.79 0.000 1.38 1.91\r------------------------------------------------------------------------------\rNote: _cons estimates baseline odds.\rNote: 3 failures and 0 successes completely determined.\rThe estimated odds ratio of 4.43e-","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:2:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3 Hypothesis testing Hypothesis tests of regression coefficients can be conducted with the $z$ statistics from the estimation output, using the test command for Wald tests of simple and complex hypotheses, and with the lrtest command for the corresponding likelihood-ratio (LR) tests. We discuss using each to test hypotheses involving a single coefficient and then show how test and lrtest can be used for hypotheses involving multiple coefficients. See section 3.2 for general information on hypothesis testing using Stata. While often in this book, we show how to conduct both Wald and LR tests of the same hypothesis, in practice, you would want to test a hypothesis with only one type of test. ","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:3:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3.1 Testing individual coefficients Most often, we are interested in testing the hypothesis $H_{0}\\colon\\beta_{k}=0$, which corresponds to results in column z in the output from logit and probit. For example, consider the results for variables k5 and wc from the logit output generated in section 5.2.1: logit lfp k5 i.wc i.hc k618 i.agecat lwg inc, nolog Logistic regression Number of obs = 753\rLR chi2(8) = 124.30\rProb \u003e chi2 = 0.0000\rLog likelihood = -452.72367 Pseudo R2 = 0.1207\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5 | -1.39 0.19 -7.25 0.000 -1.77 -1.02\r|\rwc |\rcollege | 0.80 0.23 3.48 0.001 0.35 1.25\r|\rhc |\rcollege | 0.14 0.21 0.66 0.508 -0.27 0.54\rk618 | -0.07 0.07 -0.96 0.336 -0.20 0.07\r|\ragecat |\r40-49 | -0.63 0.21 -3.00 0.003 -1.04 -0.22\r50+ | -1.28 0.26 -4.92 0.000 -1.79 -0.77\r|\rlwg | 0.61 0.15 4.04 0.000 0.31 0.91\rinc | -0.04 0.01 -4.24 0.000 -0.05 -0.02\r_cons | 1.01 0.29 3.54 0.000 0.45 1.57\r------------------------------------------------------------------------------\rWe conclude the following: Having young children has a significant effect on the probability of being in the labor force $z = -7.25, p \u003c 0.01$ for a two-tailed test. The effect of the wife attending college is significant at the 0.01 level 3.1.1 Testing single coefficients using test The $z$ test included in the output of estimation commands is a Wald test, which can also be computed as a chi-squared test by using test. For example, to test $H_{0}\\colon\\beta_{\\mathrm{k5}}=0$ test k5 ( 1) [lfp]k5 = 0\rchi2( 1) = 52.57\rProb \u003e chi2 = 0.0000\rStata refers to the coefficient for k5 as [lfp ]k5 because the dependent variable is lfp . We conclude the following: The effect of having young children on entering the labor force is significant at the 0.01 level $(\\chi^{2}=52.57,\\mathrm{~df}=1,p\u003c0.01)$ The value of the $z$ test is identical to the square root of the corresponding chi-squared test with 1 degree of freedom. For example, using display as a calculator display sqrt(52.57) This corresponds to —7.25 from the logit output shown above. Aside: Using returns . Using returned results is a better way to show this. When you use the test command, the chi-squared statistic is returned as the scalar r(chi2). The command display sqrt(r(chi2)) then provides the same result more elegantly and with slightly more accuracy. 3.1.2 Testing single coefficients using Irtest An LR test is computed by comparing the log likelihood from a full model with that of a restricted model. To test a single coefficient, we begin by fitting the full model and storing the results: logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog estimates store Mfull Then, we fit the model without k5 and store the results: logit lfp k618 i.agecat i.wc i.hc lwg inc, nolog estimates store Mnok5 Next, we run Irtest : lrtest Mfull Mnok5 Likelihood-ratio test\rAssumption: Mnok5 nested within Mfull\rLR chi2(1) = 62.55\rProb \u003e chi2 = 0.0000\rThe LR test shows the following: The effect of having young children is significant at the 0.01 level $(\\operatorname{LR}\\chi^2=62.55,\\mathrm{~df}=1,p\u003c0.01)$. If you want to run an LR test comparing a model stored by using “estimates store” with the last model fit, you can use a single period to represent the last model. For example, instead of “lrtest Mfull Mnok5,” you could use “lrtest Mfull .,” where “.” represents the last model. ","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:3:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3.2 Testing mutiple coefficiencts You might want to test complex hypotheses that involve more than one coefficient. For example, we have two variables that reflect education in the family: “he” and “wc.” The conclusion that education has (or does not have) a significant effect on labor force participation cannot be based on separate tests of single coefficients. A joint hypothesis can also be tested using either “test” or “lrtest.” Similarly, to test the effect of “agecat,” requires testing the coefficients of all indicator variables. 3.2.1 Testing multiple coefficients using test To test that the effect of the wife attending college and of the husband attending college on labor force participation are simultaneously equal to 0 (that is, $H_{0}\\colon\\beta_{\\mathbf{wc}}=\\beta_{\\mathbf{hc}}=0$), we fit the full model and test the two coefficients. We must use “i.he” and “i.wc,” not “hc” and “wc.” estimates restore Mfull test 1.hc 1.wc ( 1) [lfp]1.hc = 0\r( 2) [lfp]1.wc = 0\rchi2( 2) = 17.83\rProb \u003e chi2 = 0.0001\rWe reject the hypothesis that the effects of the husband’s and the wife’s education are simultaneously equal to 0 $(\\chi^{2}=17.83,\\mathrm{df}=2,p\u003c0.01)$. test can also be used to test the equality of coefficients. For example, to test that the effect of the wife attending college on labor force participation is equal to the effect of the husband attending college (that is, Ha: βVC = βhc), we type: test 1.hc = 1.wc ( 1) - [lfp]1.wc + [lfp]1.hc = 0\rchi2( 1) = 3.24\rProb \u003e chi2 = 0.0719\rHere, the test translated $\\beta_{\\mathbf{uc}}=\\beta_{\\mathbf{hc}}$ into the equivalent expression $-\\beta_{\\mathbf{wc}}+\\beta_{\\mathbf{hc}}=0$. The null hypothesis that the effects of husband’s and wife’s education are equal is marginally significant. We might conclude the following: There is weak evidence that the effects of husband’s and wife’s education are equal $(\\chi^{2}=3.24,\\mathrm{df}=1,p=0.072)$ We can test that the effect of agecat is 0 by specifying the two indicator variables that were created from the factor variable i.agecat: test 2.agecat 3.agecat ( 1) [lfp]2.agecat = 0\r( 2) [lfp]3.agecat = 0\rchi2( 2) = 24.27\rProb \u003e chi2 = 0.0000\rTo avoid having to specify each of the automatically created indicators, we can use testparm: testparm i.agecat ( 1) [lfp]2.agecat = 0\r( 2) [lfp]3.agecat = 0\rchi2( 2) = 24.27\rProb \u003e chi2 = 0.0000\rThe advantage of “testparm” is that it works no matter how many indicator variables have been created by “i.catvar.” 3.2.2 Testing multiple coefficients using Irtest To compute an LR test of multiple coefficients, we start by fitting the full model and saving the results with “estimates store estname.” To test the hypothesis that the effect of the wife attending college and of the husband attending college on labor force participation are both equal to 0 (that is, $H_{0}\\colon\\beta_{\\mathbf{vc}}=\\beta_{\\mathbf{hc}}=0)$, we fit the model that excludes these two variables and then run lrtest: logit lfp k5 k618 i.agecat lwg inc, nolog estimates store Mnowchc lrtest Mfull Mnowchc Likelihood-ratio test\rAssumption: Mnowchc nested within Mfull\rLR chi2(2) = 18.68\rProb \u003e chi2 = 0.0001\rWe conclude the following: The hypothesis that the effects of the husband’s and the wife’s education are simultaneously equal to 0 can be rejected at the 0.01 level. $(\\mathrm{LR}\\chi^{2}=18.68,\\mathrm{df}=2,p\u003c0.01)$ This logic can be extended to exclude other variables. Say that we wish to test the hypothesis that the effects of all the independent variables are simultaneously 0. We do not need to fit the full model again because the results are still saved from our use of “estimates store Mfull” above. We fit the model with no independent variables and then run “lrtest”: logit lfp, nolog estimates store Mconstant lrtest Mfull Mconstant Logistic regression Number of obs = 753\rLR chi2(0) = 0.00\rProb \u003e chi2 = .\rLog likelihood = -514.8732 Pseudo R2 = 0.0000\r------------------------------------------------------------------------------\rlfp |","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:3:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3.3 Comparing LR and Wald tests Although the LR and Wald tests are asymptotically equivalent, their values differ in finite samples. In our example, Statistical theory is unclear on whether the LR or Wald test is to be preferred in models for categorical outcomes, although many statisticians, ourselves included, prefer the LR test. The choice of which test to use is often determined by convenience, personal preference, and convention within an area of research. Recall from chapter 3 that if robust standard errors or svy estimation is used, only Wald tests are available. For Wald tests of a single coefficient, some disciplines prefer to use chi-squared tests, while others prefer the corresponding $z$ test. ","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:3:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4 Predicted probabilities, residuals, and influential observations For a given set of values of the independent variables, the predicted probability can be computed from the estimated coefficients. $$\\mathrm{Logit:~}\\widehat{\\Pr}\\left(y=1\\mid\\mathbf{x}\\right)=\\Lambda\\left(\\mathbf{x}\\widehat{\\beta}\\right)\\quad\\mathrm{Probit:~}\\widehat{\\Pr}\\left(y=1\\mid\\mathbf{x}\\right)=\\Phi\\left(\\mathbf{x}\\widehat{\\beta}\\right)$$ where $\\Lambda$ is the CDF for the logistic distribution with variance $π^2/3$, and $\\Phi$ is the CDF for the normal distribution with variance 1. For any set of values of the independent variables, whether occurring in the sample or not, the predicted probability can be computed. In this section, we consider predictions for each observation in the dataset along with residuals and measures of influence based on these predictions. In sections 6.2-6.6, we use predicted probabilities for interpretation. ","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:4:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4.1 Predicted probabilities using predict After running logit or probit, predict newvar [if] [in] computes the predicted probability of a positive outcome for each observation, given the observed values on the independent variables, and saves them in the new variable newvar. Predictions are computed for all cases in memory that do not have missing values for any variables in the model, regardless of whether “if” and “in” were used to restrict the estimation sample. For example, if you fit “logit lfp k5 i.age catif wc==l,” only 212 cases are used when fitting the model. But “predict newvar” computes predictions for all 753 cases in the dataset. If you want predictions only for the estimation sample, you can use the command “predict newvar if e(sample)==1,” where “e(sample)” is the variable created by “logit” or “probit” to indicate whether a case was used when fitting the model. We can use predict to examine the range of predicted probabilities from our model. For example, we start by computing the predictions: logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog estimates store logit predict prlogit predict prlogit\r(option pr assumed; Pr(lfp))\rBecause we did not specify which quantity to predict, the default option “pr” for the probability of a positive outcome was assumed, and the new variable “prlogit” was given the default variable label “Pr(lfp).” In general, and especially when fitting multiple models, we suggest adding your own variable label to the prediction to avoid having multiple variables with the same label. Here we add a variable label and compute summary statistics: label var prlogit \"Logit: Pr(lfp | X)\" codebook prlogit, compact Variable Obs Unique Mean Min Max Label\r----------------------------------------------------------------------------\rprlogit 753 753 .5683931 .0135618 .9512301 Logit: Pr(lfp | X)\r----------------------------------------------------------------------------\rThe predicted probabilities range from 0.014 to 0.951 with a mean of 0.568. We use “dotplot” to examine the distribution of predictions. dotplot prlogit, ylabel(0(.2)1, grid gmin gmax) In this example of “dotplot,” the option “ylabel(0(.2)1, grid gmin gmax)” sets the range of the axis from 0 to 1 with grid lines in increments of 0.2, where the “gmin” and “gmax” suboptions add lines at the minimum and maximum values. Even if the actual range of the predictions is smaller than 0 to 1, we find it useful to see the distribution relative to the entire potential range of probabilities. Examining the distribution of predictions is a valuable first step after fitting your model to get a general sense of your data and possibly detect problems. Our plot shows that there are individuals with predicted probabilities that span almost the entire range from 0 to 1, with roughly two-thirds of the observations between 0.40 and 0.80. The large range reflects that our sample contains individuals with both very large and very small probabilities of labor force participation. Examining the characteristics of these individuals could be useful for guiding later analysis. If the distribution was bimodal, it would suggest the importance of a binary independent variable or the possibility of two types of individuals, perhaps with shared characteristics on many variables. 4.1.1 Comparing logit to probit predictions predict can also be used to show that the predictions from logit and probit models are nearly identical. Although the two models make different assumptions about the distribution of $\\varepsilon,$, these differences are absorbed in the relative magnitudes of the estimated coefficients. To see this, we begin by fitting comparable logit and probit models and computing their predicted probabilities. First, we fit the logit model, store the estimates, and compute predictions: use binlfp4, clear logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog estimates store logit predict prlogit Next, we fit the probit model: probit lfp k5 k618 i.agecat i.wc i.hc lwg in","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:4:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4.2 Residuals and influential observations using predict After you have fit your baseline model, we suggest that you examine residuals and look for influential observations before beginning postestimation analyses for interpretation. Residuals and influential observations can help you discover problems with your data and sometimes suggest problems in your model specification. Residuals are the difference between a model’s predicted and observed outcomes for each observation in the sample. Cases that have large residuals are known as outliers. When an observation has a large effect on the estimated parameters, it is said to be influential. We illustrate these ideas with the linear regression model in figure below. The distinction between an outlier and an influential observation\rNot all outliers are influential, as the figure shows by using simulated data for the linear regression of y on x. The residual for an observation is its vertical distance from the regression line. In the top panel, the observation highlighted with a solid circle has a large residual and is considered an outlier. Even so, it is not very influential on the slope of the regression line. That is, the slope of the regression line is very close to what it would be if the highlighted observation was dropped from the sample and the model was fit again. In the bottom panel, the only observation whose value has changed is the highlighted observation marked with a square. The residual for this observation is small, but the observation is very influential; its presence is entirely responsible for the slope of the new regression line being positive instead of negative. Building on the analysis of residuals and influence in the linear regression model (see Fox [2008] and Weisberg [2005, chap. 5]), Pregibon (1981) extended these ideas to the BRM. 4.2.1 Residuals The predicted probability for a given set of independent variables is $$\\pi_{i}=\\Pr\\left(y_{i}=1\\mid\\mathbf{x}_{i}\\right)$$ The deviations $y_i-\\pi_i$ are heteroskedastic because the variance depends on the probability $\\pi_i$ of a positive outcome: $$\\mathrm{Var}\\left(y_{i}-\\pi_{i}\\mid\\mathbf{x}{i}\\right)=\\pi{i}\\left(1-\\pi_{i}\\right)$$ The variance is greatest when $π_i = 0.5$ and decreases as $π_i$ approaches 0 or 1. That is, a fair coin is the most unpredictable, with a variance of $0.5 (1 - 0.5) = 0.25$. A coin that has a very small probability of landing head up (or tail up) has a small variance, for example, $0.01(1 — 0.01) = 0.0099$. The Pearson residual divides the residual $y-\\widehat{\\pi}$ by its standard deviation: $$r_i=\\frac{y_i-\\widehat{\\pi}_i}{\\sqrt{\\widehat{\\pi}_i\\left(1-\\widehat{\\pi}_i\\right)}}$$ Large values of $r$ suggest a failure of the model to fit a given observation Pregibon (1981) showed that the variance of $r$ is not 1 because $\\operatorname{Var}(y_{i}-\\hat{\\pi}_{i})$ is not exactly equal to the estimate $\\widehat{\\pi}_i\\left(1-\\widehat{\\pi}_i\\right)$ He proposed the standardized Pearson residual $$r_{i}^{\\mathrm{std}}=\\frac{r_{i}}{\\sqrt{1-h_{\\mathrm{ii}}}}$$ where Although $r^{\\mathrm{std}}$ is preferred over $r$ because of its constant variance, we find that the two residuals are often similar in practice. However, because $r$ is simple to compute in Stata, you should use this measure. 4.2.2 Example An index plot is an easy way to examine residuals by plotting them against the observation number. Standardized residuals are computed by specifying the rstandard option with predict. For example, logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog predict rstd, rstandard label var rstd \"Standardized Residual\" sort inc, stable generate index = _n label var index \"Observation Number\" After computing the standardized residuals that are saved in the new variable rstd, we sorted the cases by inc so that observations are ordered from lowest to highest income in the plot that follows. The next line creates the variable index equal to the observation’s row number in the dataset, where _n on t","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:4:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4.3 Least likely observations The command leastlikely (Freese 2002) will list the least likely observations. For example, for a binary model, leastlikely will list both the observations with the smallest $\\widehat{\\Pr}\\left(y=0\\mid\\mathbf{x}\\right)$ when y$ = 0$ and the observations with smallest $\\widehat{\\Pr}\\left(y=1\\mid\\mathbf{x}\\right)$ when $y = 1$. In addition to being used after logit and probit, leastlikely can be used after most binary models in which the option pr for predict generates the predicted probabilities of a positive outcome (for example, cloglog, scobit, and hetprobit) and after many models for ordinal or nominal outcomes in which the option outcome(#) for predict generates the predicted probability of outcome# (for example, ologit, oprobit, mlogit, mprobit, and slogit). leastlikely is not appropriate for models in which the probabilities produced by predict are probabilities within groups or panels (for example, such as clogit, nlogit, and asmprobit). Syntax The syntax for leastlikely is as follows: leastlikely [varlist] [if] [in] [, n(#) generate(varname) [nodisplay | display] nolabel noobs] where varlist contains any variables whose values are to be listed in addition to the observation numbers and probabilities. Options n(#) specifies the number of observations to be listed for each level of the outcome variable. The default is n(5). For multiple observations with identical predicted probabilities, all observations will be listed. generate(varname) specifies that the probabilities of observing the outcome that was observed be stored in varname. Options controlling the list of values [no]display forces the format into display or tabular (nodisplay) format. If you do not specify one of these options, Stata chooses the one it decides will be most readable. nolabel causes numeric values rather than labels to be displayed noobs suppresses printing of the observation numbers Example We can use leastlikely to identify the least likely observations from our model of labor force participation and to list the values of the variables k5, k618, and wc for these observations. Based on our model logit lfp k5 k618 i.age cat i.wc i.hc lwg inc, use binlfp4, clear logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog leastlikely k5 k618 wc agecat Outcome: 0 (not in LF)\r+------------------------------------------+\r| Prob k5 k618 wc agecat |\r|------------------------------------------|\r60. | .14033509 0 1 college 30-39 |\r172. | .1291429 0 2 college 30-39 |\r221. | .17652958 0 2 college 30-39 |\r252. | .11741232 0 0 college 30-39 |\r262. | .16508454 0 3 college 30-39 |\r+------------------------------------------+\rOutcome: 1 (in LF)\r+------------------------------------------+\r| Prob k5 k618 wc agecat |\r|------------------------------------------|\r427. | .17668873 0 5 no 50+ |\r496. | .18095707 1 0 no 50+ |\r534. | .10392637 1 2 no 30-39 |\r635. | .11522446 1 3 college 30-39 |\r662. | .11338183 2 0 no 30-39 |\r+------------------------------------------+\rAmong women not in the labor force, we find that the lowest predicted probability of not being in the labor force occurs for those who have young children, attended college, and are younger. For women in the labor force with the lowest probabilities of being in the labor force, all but one individual have young children, most have more than one older child, and one attends college. This suggests further consideration of how labor force participation is affected by having children in the family. ","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:4:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"5 Measures of fit As discussed in chapter 3, a scalar measure of fit can be useful when comparing competing models. Information criteria such as the Bayesian information criterion (BIC) and Akaike’s information criterion (AIC) can be used to select among models and are often very useful. There are many pseudo-R^2 statistics that are inspired by the coefficient of determination R^2 in the linear regression model, but we find them less informative, even though they are often used. Finally, the Hosmer Lemeshow statistic is a popular way to assess the overall fit of the model, but we do not recommend it for reasons we explain below. ","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:5:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"5.1 Information criteria To illustrate scalar measures of fit and information criteria, we consider two models. Model 1 (M1) contains our original specification of independent variables k5, k618, agecat, wc, he, lwg, and inc. Model 2 (M2) drops variables k618, he, and lwg, and adds income-squared with c. in c##c. The models are fit, and estimates are stored. quietly logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog estimates store model1 quietly logit lfp k5 i.agecat i.wc c.inc##c.inc, nolog estimates store model2 We can list the estimates by using the option stats(N BIC AIC R2_p) to include the sample size, BIC, AIC, and pseudo-R^2 that are normally included in models fit with maximum likelihood. Recall that the formulas for these statistics are given in section 3.3.2. estimates table model1 model2, b(%9.3f) p(%9.3f) stats(N bic aic r2_p) --------------------------------------\rVariable | model1 model2 -------------+------------------------\rk5 | -1.392 -1.369 | 0.000 0.000 k618 | -0.066 | 0.336 |\ragecat |\r40-49 | -0.627 -0.512 | 0.003 0.011 50+ | -1.279 -1.137 | 0.000 0.000 |\rwc |\rcollege | 0.798 1.119 | 0.001 0.000 |\rhc |\r1 | 0.136 | 0.508 |\rlwg | 0.610 | 0.000 inc | -0.035 -0.060 | 0.000 0.001 |\rc.inc#c.inc | 0.000 | 0.083 |\r_cons | 1.014 1.743 | 0.000 0.000 -------------+------------------------\rN | 753 753 bic | 965.064 968.574 aic | 923.447 936.206 r2_p | 0.121 0.104 --------------------------------------\rLegend: b/p\rModel 2 (M2) modifies Model 1 (M1) by removing one statistically significant variable and two non-significant variables from M1, while adding a variable that is significant at the 0.10 level. Since the models are not nested, they cannot be compared with a likelihood ratio (LR) test, but we can use the BIC and AIC statistics. In this example, both the BIC and AIC statistics are smaller for M1, providing support for that model. Following Raftery’s (1995) guidelines, we can say that there is positive (neither weak nor strong) support for M1. You can obtain information criteria in two other ways. You can use the ic option to fitstat, which shows multiple versions of the AIC and BIC measures (see section 3.3 for the formula for these measures): estimates restore model1 quietly fitstat, save ic estimates restore model2 fitstat, diff ic | Current Saved Difference -------------------------+--------------------------------------\rAIC | AIC | 936.206 923.447 12.759 (divided by N) | 1.243 1.226 0.017 -------------------------+--------------------------------------\rBIC | BIC(df=7/9/-2) | 968.574 965.064 3.511 BIC (based on deviance) | -4019.347 -4022.857 3.511 BIC' (based on LRX2) | -67.796 -71.307 3.511 Difference of 3.511 in BIC provides positive support for saved model.\rThe results match those from the estimates table output and even tell you the strength of support for the preferred model. You can also use the estat ic command: logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog estat ic Akaike's information criterion and Bayesian information criterion\r-----------------------------------------------------------------------------\rModel | N ll(null) ll(model) df AIC BIC\r-------------+---------------------------------------------------------------\r. | 753 -514.8732 -452.7237 9 923.4473 965.0639\r-----------------------------------------------------------------------------\rNote: BIC uses N = number of observations. See [R] IC note.\rlogit lfp k5 i.agecat i.wc c.inc##c.inc, nolog estat ic Akaike's information criterion and Bayesian information criterion\r-----------------------------------------------------------------------------\rModel | N ll(null) ll(model) df AIC BIC\r-------------+---------------------------------------------------------------\r. | 753 -514.8732 -461.103 7 936.206 968.5745\r-----------------------------------------------------------------------------\rNote: BIC uses N = number of observations. See [R] IC note.\rThese results match those from fitstat ","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:5:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"5.2 $Pseudo-R^2’s$ Within a substantive area, $pseudo-R^2s$ might provide a rough index of whether a model is adequate. For example, if prior models of labor force participation routinely have values of 0.4 for a particular $pseudo-R^2$, you would expect that new analyses with a different sample or with revised measures of the variables would result in a similar value for that measure. But there is no convincing evidence that selecting a model that maximizes the value of a $pseudo-R^2$ results in a model that is optimal in any sense other than the model has a larger value of that measure. We use the same models estimated in the last section and use fitstat to compute the scalar measures of fit (see section 3.3 for the formula for these measures): logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog quietly fitstat, save logit lfp k5 i.agecat i.wc c.inc##c.inc, nolog fitstat, diff | Current Saved Difference -------------------------+--------------------------------------\rLog-likelihood | Model | -461.103 -452.724 -8.379 Intercept-only | -514.873 -514.873 0.000 -------------------------+--------------------------------------\rChi-square | D(df=746/744/2) | 922.206 905.447 16.759 LR(df=6/8/-2) | 107.540 124.299 -16.759 p-value | 0.000 0.000 0.000 -------------------------+--------------------------------------\rR2 | McFadden | 0.104 0.121 -0.016 McFadden(adjusted) | 0.091 0.103 -0.012 McKelvey \u0026 Zavoina | 0.183 0.215 -0.032 Cox-Snell/ML | 0.133 0.152 -0.019 Cragg-Uhler/Nagelkerke | 0.179 0.204 -0.026 Efron | 0.135 0.153 -0.018 Tjur's D | 0.135 0.153 -0.018 Count | 0.672 0.676 -0.004 Count(adjusted) | 0.240 0.249 -0.009 -------------------------+--------------------------------------\rIC | AIC | 936.206 923.447 12.759 AIC divided by N | 1.243 1.226 0.017 BIC(df=7/9/-2) | 968.574 965.064 3.511 -------------------------+--------------------------------------\rVariance of | e | 3.290 3.290 0.000 y-star | 4.026 4.192 -0.165 Note: Likelihood-ratio test assumes current model nested in saved model.\rDifference of 3.511 in BIC provides positive support for saved model.\rAfter fitting our first model with logit, we used quietly to suppress the output from fitstat with the save option to retain the results in memory. After fitting the second model, fitstat, diff displays the fit statistics for both models, fitstat, diff computes differences between all measures, shown in the column labeled Difference, even if the models are not nested. As with the lrtest command, you must determine if it makes sense to interpret the computed difference. What do the fit statistics show? The values of the $pseudo-R^2s$ are slightly larger for M1, which is labeled “Saved logit” in the table. If you take the $pseudo-R^2s$ as evidence for the best model, which we do not, there is some evidence preferring M1. ","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:5:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"5.3 (Advanced) Hosmer-Lemeshow statistic We only recommend reading this section if you are considering using the Hosmer-Lemeshow statistic. After reviewing how the measure is computed, we illustrate that the statistic is highly dependent upon an arbitrary decision on the number of groups used. As a result, we do not recommend this measure. The idea of the Hosmer-Lemeshow (HL) test is to compare predicted probabilities with the observed data (Hosmer and Lemeshow 1980; Lemeshow and Hosmer 1982). This popular test can be computed using the estat gof command after fitting a logit or probit model. Unlike the measures presented above, this command also works with models fit by using complex survey data with the svy prefix. To explain how the test works, we review the steps that are used to compute it. Fit the regression model Compute the predicted probabilities $\\widehat{\\pi}_i$ Sort the data from the smallest $\\widehat{\\pi}_i$ to the largest. Divide the observations into G groups, where 10 groups are often used. Each group will have $n_{g}\\approx\\frac{N}{G}$ The first group will have the $n_1$ smallest values of $\\widehat{\\pi}_i$, and so on. If $G$ does not divide equally into $N$, the group sizes will differ slightly. WTithin each group, compute the mean prediction: $$\\overline{\\pi_g}=\\sum_\\text{i in group g}{ \\widehat { \\pi }_i/n_g}$$ Also compute th e mean number of observed cases where $y = 1$: $$\\overline{y_g}=\\sum_{i\\text{ in group g}} y _ i / n _ g$$ The test statistic is $$HL=\\sum_{g=1}^{G}\\frac{\\left(n_g\\overline{y}_g-n_g\\overline{\\pi}_g\\right)^2}{n_g\\overline{\\pi}_g\\left(1-\\overline{\\pi}_g\\right)}$$ Hosmer, Lemeshow, and Sturdivant (2013) ran simulations that suggest that HL is distributed approximately as $χ^2$ with $G – 2$ degrees of freedom if the model is correctly specified. If the p-value is large, the model is considered to fit the data. To give an example, we fit the model we have used as a running example, and we use estat gof to compute the HL statistic: logit lfp k5 k618 i.agecat i.wc i.hc lwg c.inc, nolog estat gof, group(10) note: obs collapsed on 10 quantiles of estimated probabilities.\rGoodness-of-fit test after logistic model\rVariable: lfp\rNumber of observations = 753\rNumber of groups = 10\rHosmer–Lemeshow chi2(8) = 13.76\rProb \u003e chi2 = 0.0881\rThe p-value is greater than 0.05, indicating that the model fits based on the criterion provided for the HL. Unfortunately, we do not find conclusions from the HL test to be convincing. First, as Hosmer and Lemeshow point out, the HL test is not a substitute for examining individual predictions and residuals as discussed in the last section. Second, Allison (2012b. 67) raised concerns that the test is not very powerful. In a simple simulation with 500 cases, the HL test failed to reject an incorrect model 75% of the time. Third, and most critically, the choice of the number of groups is arbitrary, even though 10 is most often used. The results of the Hosmer-Lemeshow test are sensitive to the arbitrary choice of the number of groups. In our experience, this is often the case, and for this reason, we do not recommend the test. We can illustrate the sensitivity of the Hosmer-Lemeshow test by varying the number of groups used to compute the test from 5 to 15 in the model fit above: quietly logit lfp k5 k618 i.agecat i.wc i.hc lwg c.inc, nolog capture matrix drop hl forvalues numgroups = 5(1)15 { quietly estat gof, group(`numgroups') local rm = r(m) matrix r = r(chi2), r(df), chi2tail(r(df),r(chi2)) matrix rownames r = \"`rm' groups\" matrix hl = nullmat(hl) \\ r } matrix colnames hl = chi2 df prob matlist hl, format(%8.3f) | chi2 df prob -------------+-----------------------------\r5 groups | 4.043 3.000 0.257 6 groups | 8.762 4.000 0.067 7 groups | 10.424 5.000 0.064 8 groups | 13.831 6.000 0.032 9 groups | 15.503 7.000 0.030 10 groups | 13.763 8.000 0.088 11 groups | 17.980 9.000 0.035 12 groups | 24.055 10.000 0.007 13 groups | 15.230 11.000 0.172 14 groups | 19.360 12.000 ","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:5:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"6 Other commands for binary outcomes Logit and probit models are the most commonly used models for binary outcomes and are the only ones that we consider in this book, but other models exist that can be fit in Stata. Among them are the following: cloglog assumes a complementary log-log distribution for the errors instead of a logistic or normal distribution. scobit fits a logit model that relaxes the assumption that the marginal change in the probability is greatest when Pr($y$ = 1) = 0.5. hetprobit allows the assumed variance of the errors in the probit model to vary as a function of the independent variables, which is one approach to comparing logit and probit coefficients across groups (Williams 2009). ivprobit fits a probit model where one or more of the regressors are endogenously determined. biprobit simultaneously fits two binary probits and can be used when errors are correlated with each other, as in the estimation of seemingly unrelated regression models for continuous dependent variables. mvprobit (Cappellari and Jenkins 2003) extends this idea to more than two binary probits. Binary outcomes that reflect an event that is expected to happen eventually for all cases are often handled using survival analysis, which is not covered in this book. Cleves et al. (2010) provides a detailed introduction to survival analysis in Stata focusing on the st* commands. Likewise, we do not consider Stata’s extensive commands for working with panel and multilevel data, including Stata’s xt* and me* commands, but these are discussed extensively in Rabe-Hesketh and Skrondal (2012) and Cameron and Trivedi (2010). ","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:6:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"introduce methods of interpretation that will be used throughout the rest of the book. ","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"introduce methods of interpretation that will be used throughout the rest of the book. Models for categorical outcomes are nonlinear, and this nonlinearity is th e fundamental challenge th at must be addressed for effective interpretation. Most simply, this means th at you cannot effectively interpret your model by presenting a list of the estimated parameters. Instead, we believe that the most effective way to interpret these models is by first fitting the model and then computing and examining postestimation predictions of the outcomes. ","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:0:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1 Comparing linear and nonlinear models ","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:1:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.1 linear models Consider a linear regression model where $y$ is the dependent variable, $x$ is a continuous independent variable, and $d$ is a binary independent variable. The model is $$y=\\alpha+\\beta x+\\delta d+\\varepsilon $$ Given the usual assumption th at $E(\\varepsilon\\mid x,d)=0$, it follows that $$E(y|x,d)=\\alpha+\\beta x+\\delta d$$ which is graphed in figure 4.1. The solid line plots $E(\\varepsilon\\mid x,d)$ as $x$ changes, holding $d$ = 0;that is, $E(y|x,d)=\\alpha+\\beta x$ ,The dashed line plots $E(\\varepsilon\\mid x,d)$ as $x$ changes when $d$ = 1, which has the effect of changing the intercept :$E(y|x,d)=\\alpha+\\beta x+\\delta1=(\\alpha+\\delta)+\\beta x.$ A simple linear model\rThe effect of $x$ on $y$ can be computed as the partial derivative of $E(\\varepsilon\\mid x,d)$ with respect to $x$. This is sometimes called the marginal change: $$\\frac{\\partial E(y|x,d)}{\\partial x}=\\frac{\\partial\\left(\\alpha+\\beta x+\\delta d\\right)}{\\partial x}=\\beta $$ The marginal change is the ratio of the change in the expected value of y to the change in x , when the change in x is infinitely small, holding d constant. In linear models, the marginal change equals the discrete change in $E(\\varepsilon\\mid x,d)$ as x changes by one unit, holding other variables constant. In our notation, we indicate that x is changing by a discrete amount with $\\Delta x$ using $(x\\rightarrow x+1)$ to indicate that x changes from its current value to be 1 larger (for example, from 10 to 11 or from 9.3 to 10.3): $$\\frac{\\Delta E(y|x,d)}{\\Delta x(x\\to x+1)}={\\alpha+\\beta(x+1)+\\delta d}-(\\alpha+\\beta x+\\delta d)=\\beta $$ When x increases by 1, $E(\\varepsilon\\mid x,d)$ increases by $\\beta$ regardless of the values for $x$ and $d$ at the point where change is measured. This is shown by the four small triangles in figure above with bases of length 1 and heights of $\\beta$. The effect of d cannot be computed as a partial derivative because $d$ is discrete. Instead, we measure the change in $E(y\\mid x,d)$ with a discrete change from 0 to 1 indicated as $\\Delta d\\left(0\\rightarrow1\\right)$: $$\\frac{\\Delta E(y\\mid x,d)}{\\Delta d(0\\rightarrow1)}=(\\begin{array}{c}\\alpha+\\beta x+\\delta1\\end{array})-(\\begin{array}{c}\\alpha+\\beta x+\\delta0\\end{array})=\\delta $$ When d changes from 0 to 1,$E(\\varepsilon\\mid x,d)$ changes by $\\delta$ units regardless of the level of $x$. This is shown by the two arrows labeled $\\delta$ in figure above marking the distance between the solid and dashed lines. The distinguishing feature for interpretation in linear models is th at the effect of a given change in an independent variable is the same regardless of the value of that variable at the start of its change and regardless of the level of the other variables in the model. Interpretation only needs to specify which variable is changing, by how much, and that other variables are being held constant. Given the simple structure of linear models, such as regress, most interpretations require only reporting the estimates. There are, however, important exceptions. In our discussion, we assumed that the model does not include polynomial terms such as $x^2$ or interactions such as $xd$. When such terms are included, the linear model becomes nonlinear in the sense we consider in the next section. ","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:1:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.2 Nonlinear models We use a logit model to illustrate the idea of nonlinearity. Let $y = 1$ if the outcome occurs, say, if a person is in the labor force, and otherwise y = 0. T he curves are from the logit equation $$\\Pr(y=1|x,d)=\\frac{\\exp{(\\alpha+\\beta x+\\delta d)}}{1+\\exp{(\\alpha+\\beta x+\\delta d)}}$$ where the $\\alpha$,$\\beta$, and $\\delta$ parameters in this equation are unrelated to those for the linear model. Once again, $x$ is continuous and $d$ is binary. The model is shown in figure A simple nonlinear model\rThe nonlinearity of the model makes it more difficult to interpret the effects of x and d on the probability of y occurring. For example, neither the marginal change $\\partial\\Pr\\left(y=1|x,d\\right)/\\partial x$ nor the discrete change $\\Delta\\Pr\\left(y=1\\mid x,d\\right)/\\Delta d(0\\rightarrow1)$ are constant, but instead depend on the values of $ x$ and $d$ . Consider the effect of changing d from 0 to 1 for a given value of $x$. This effect is the distance between the solid curve for d = 0 and the dashed curve for $d$ = 1. Because the curves are not parallel, the magnitude of the difference in the predicted probability at d = 1 compared with d = 0 depends on the value of x where the difference is computed. Accordingly, $\\Delta_{d1}\\neq\\Delta_{d2}$ Similarly, the magnitude of the effect of $ x$ depends on the values of $x$ and d where the effect is evaluated so that $\\Delta_{x1}\\neq\\Delta_{x2}\\neq\\Delta_{x3}\\neq\\Delta_{x4}$ . In nonlinear models, the effect of a change in a variable depends on the values of all variables in the model and is no longer simply equal to a parameter of the model. Accordingly, the methods of interpretation that we recommend for nonlinear models are largely based on the use of predictions, which we consider in the next section. ","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:1:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2 Approaches to interpretation The primary methods of interpretation presented in this book are based on predictions from the model. The model is fit and the estimated parameters are used to make predictions at values of the independent variables that are (hopefully) useful for understanding the substantive implications of the nonlinear model. These methods depend critically on Stata’s predict and margins commands, which are the foundation for the SPost commands m table, mchange, and mgen (referred to collectively as the m* commands). Although the basic use of these commands is straightforward, they have many—sometimes subtle—features that are valuable for fully interpreting your model. This chapter provides an overview of general principles and syntax for these commands. Details on why you would use each feature are explained fully when the commands are used in later chapters to interpret specific models. ","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:2:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.1 Method of interpretation based on predictions We use predictions in four basic ways Predictions for each observation. Most fundamentally, predictions can be computed for each observation by using predict. Predictions include the probabilities of outcomes as well as rates for count models. We often start our analysis by examining the distribution of predictions in the estimation sample. Predictions at specified values. Predicted values at specific values of the independent variables can be computed using the commands margins and mtable. These commands can compute predictions at substantively interesting combinations of values of the independent variables, which we refer to as profiles or ideal types. In some cases, tables of predictions are arranged by the level of one or more explanatory variables and can succinctly summarize processes affecting the outcomes Marginal effects. An important way to examine the effects of a variable is to compute how changes in the variable are associated with changes in the outcomes, holding other variables constant. These changes, known as marginal effects, can be computed as a marginal change when a regressor changes by an infinitely small amount or as a discrete change when a regressor changes by a fixed amount. Marginal effects are computed by margins, mtable. and mchange. which can easily compute average marginal effects and marginal effects at the mean. **Graphs of predictions.**For continuous independent variables, graphs often effectively summarize effects. Stata’s m arginsplot elegantly plots a single outcome category based on predictions from margins. Just as m argins can only compute predictions for one outcome at a time, marginsplot does not allow you to plot multiple outcomes. Because this is essential for models with nominal and ordinal outcomes, we wrote mgen to generate variables with predictions for all outcomes. These variables containing predictions can be plotted using Stata’s graph command ","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:2:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.2 Method of interpretation using parameters Although the predictions used for each of these methods are computed using the model’s estimated param eters, in some cases the parameters themselves can be used for interpretation. Examples include odds ratios for binary models, standardized coefficients for latent outcomes, and factor changes in rates for count models. These are considered in detail in later chapters. ","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:2:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.3 Stata and SPost commands for interpretation The most fundamentally important command for sophisticated interpretation using predictions is Stata’s ‘margins’ command. This command is incredibly powerful, flexible, and general. As a consequence, it can be rather intimidating to use. To make ‘margins’ simpler to use, we wrote a series of “wrappers” that use ‘margins’ for their computations; they simplify the process of specifying the predictions you want and produce output that is easier to interpret. Nonetheless, there are times when you might need to use ‘margins,’ either because our commands did not anticipate something you want to do or because we encountered technical issues that made using ‘margins’ the only option. Accordingly, even if our ’m*’ commands seem to do everything you want, you should have some familiarity with what ‘margins’ does and how it works. This will also give you a better understanding of what our commands are doing. ","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:2:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3 Predictions for each observation The ‘predict’ command computes predicted values for each observation in the current dataset. ‘predict’ has many options that depend on the model that was fit. Here we consider only the options that provide information we use regularly in later chapters. If you type ‘help estimation-command’ (for example, ‘help logit’), you can click on the ‘Also See’ tab in the upper-right corner of the window and then select the postestimation entry for the command (for example, ‘[R] logit postestimation’); the postestimation entry includes details on how ‘predict’ works for that estimation command. The simplest syntax for predict is predict newvarlist where newvarlist contains the name or names of the variables that are generated to hold the predictions. How many variables and what is predicted depends on the model. The defaults for estimation commands used in this book are listed in the following table. As an example, we compute predicted probabilities for a logit model of women’s labor force participation. Below, ‘predict’ generates the variable ‘prob’ (a name we chose) containing the probabilities of a woman being in the labor force. use binlfp4, clear logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog Logistic regression Number of obs = 753\rLR chi2(8) = 124.30\rProb \u003e chi2 = 0.0000\rLog likelihood = -452.72367 Pseudo R2 = 0.1207\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5 | -1.39 0.19 -7.25 0.000 -1.77 -1.02\rk618 | -0.07 0.07 -0.96 0.336 -0.20 0.07\r|\ragecat |\r40-49 | -0.63 0.21 -3.00 0.003 -1.04 -0.22\r50+ | -1.28 0.26 -4.92 0.000 -1.79 -0.77\r|\rwc |\rcollege | 0.80 0.23 3.48 0.001 0.35 1.25\r|\rhc |\rcollege | 0.14 0.21 0.66 0.508 -0.27 0.54\rlwg | 0.61 0.15 4.04 0.000 0.31 0.91\rinc | -0.04 0.01 -4.24 0.000 -0.05 -0.02\r_cons | 1.01 0.29 3.54 0.000 0.45 1.57\r------------------------------------------------------------------------------\rpredict prob summarize prob Variable | Obs Mean Std. dev. Min Max\r-------------+---------------------------------------------------------\rprob | 753 .5683931 .1945282 .0135618 .9512301\rThe summary statistics show that in the sample of 753, the probabilities range from 0.014 to 0.951 with an average of 0.568. A detailed discussion of predicted probabilities for binary models is provided in chapter 6 For models with ordinal or nominal outcomes (chapters 7 and 8), ‘predict’ computes the predicted probability of an observation falling into each of the outcome categories. So, instead of providing a single variable name for predictions, you specify as many names as there are categories. For example, after fitting a model for a nominal dependent variable with four categories, you can use ‘predict prob1 prob2 prob3 prob4’. The new variables contain the predicted probabilities of being in the first, second, third, and fourth categories. For count models, by default predict computes the rate or expected count. Alternatively, predict newvarname, pr(#) computes the predicted probabilities of the specified counts. For example, predict prob0, pr(0) generates the variable ‘prob0’ containing estimates of Pr(y = 0 | x). Also, ‘predict pr(#iow \u003e #high)’ computes probabilities of contiguous counts. For example, predict prob1to3, pr(1,3) generates the variable ‘prob1to3’ containing estimates of $Pr(1 \u003c y \u003c 3 | x).$ ","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:3:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4 Predictions at specified values Interpreting predictions at substantively interesting values of the regressors is an essential method of interpretation. Such predictions can be made with m argins and with the m* commands we have written that are based upon margins. We focus on several aspects of these commands: Specifying values of the independent variables. Explaining how factor variables are handled. Using a $numlist$ for predictions at multiple values. Making predictions by the levels of a variable defining groups. Predicting quantities th at are not the default for margins We will explain how to use the margins command to make predictions, and then we will show how the same things (and more) can be done using the m* command mtable. ","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:4:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4.1 Why use the m* commands instead of margins? Our m* commands mtable, mchange, and mgen are “wrappers” for margins. By wrapper, we mean that the m commands translate your specification into a series of margins commands that actually do the computations.* Although we think margins is an extraordinary command, it can be difficult to use, and the output can be difficult to interpret. It does difficult things with amazing ease and also makes you work hard to do some simple things. In some ways, frankly, margins is more suited for a programmer than for a data analyst. For example, if you are fitting models with ordinal, nominal, or count outcomes, you have to run margins once for each outcome. Then, you face the tedious and error-prone task of combining the output from several margins commands. The learning curve for margins can also be steep. Our commands make it easier—sometimes much easier. The output is more compact, and if you want to plot the predictions, variables are automatically generated. ","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:4:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4.2 Using margins for predictions The margins command allows you to predict many quantities and compute summary measures of your predictions. To begin, it is helpful to see how margins is related to predict. Consider the example in section 4.3, where predict computed the probability of labor force participation for each observation in the sample. Using summarize to analyze the variable generated by predict, we found that the mean probability was 0.568. We can obtain exactly the same mean prediction with margins: logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog margins Logistic regression Number of obs = 753\rLR chi2(8) = 124.30\rProb \u003e chi2 = 0.0000\rLog likelihood = -452.72367 Pseudo R2 = 0.1207\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5 | -1.39 0.19 -7.25 0.000 -1.77 -1.02\rk618 | -0.07 0.07 -0.96 0.336 -0.20 0.07\r|\ragecat |\r40-49 | -0.63 0.21 -3.00 0.003 -1.04 -0.22\r50+ | -1.28 0.26 -4.92 0.000 -1.79 -0.77\r|\rwc |\rcollege | 0.80 0.23 3.48 0.001 0.35 1.25\r|\rhc |\rcollege | 0.14 0.21 0.66 0.508 -0.27 0.54\rlwg | 0.61 0.15 4.04 0.000 0.31 0.91\rinc | -0.04 0.01 -4.24 0.000 -0.05 -0.02\r_cons | 1.01 0.29 3.54 0.000 0.45 1.57\r------------------------------------------------------------------------------\rPredictive margins Number of obs = 753\rModel VCE: OIM\rExpression: Pr(lfp), predict()\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_cons | 0.57 0.02 34.24 0.000 0.54 0.60\r------------------------------------------------------------------------------\rWhen no options are specified, margins calculates the mean of the default quantity computed by predict for the estimation command. Earlier, we used predict to generate a variable with the probabilities of $y = 1$ for each observation, and we used summarize to compute the mean probability. Behind the scenes, this is what margins does. An advantage of using margins to compute the average predicted probability is that it provides the 95% confidence interval along with a test of the null hypothesis that the average prediction is 0. Stata does this using the delta method to compute standard errors (see [R] margins or Agresti [2013, 72]). In this example, testing that the mean prediction is 0 is not useful; but, when we later use margins to compute marginal effects, testing whether estimates differ from 0 is very useful. In addition to computing average predictions over the sample, margins allows us to compute predictions at specified values of the independent variables, whether those values occur in the sample or not. The most common example of this is computing the prediction with all variables at their mean by using the atmeans option: margins, atmeans Adjusted predictions Number of obs = 753\rModel VCE: OIM\rExpression: Pr(lfp), predict()\rAt: k5 = .2377158 (mean)\rk618 = 1.353254 (mean)\r1.agecat = .3957503 (mean)\r2.agecat = .3851262 (mean)\r3.agecat = .2191235 (mean)\r0.wc = .7184595 (mean)\r1.wc = .2815405 (mean)\r0.hc = .6082337 (mean)\r1.hc = .3917663 (mean)\rlwg = 1.097115 (mean)\rinc = 20.12897 (mean)\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_cons | 0.58 0.02 29.33 0.000 0.54 0.62\r------------------------------------------------------------------------------\rThe output, begins by listing the values of the independent variables at which the prediction was calculated, called the atlegend, where (mean) lets you know that these values are the means. 4.2.1 The at() option for specifying values The at() option allows us to set specific values of the independent variables at which pre","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:4:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4.3 (Advanced) Nondefault predictions using margins Although the section heading seems esoteric, this is an important topic. The default predictions computed by margins or the m* commands are often the predictions you want, which is why they are the default. But you might want to predict some other quantity. Using the options described in this section, you can predict arbitrarily complex functions of any quantity computed by predict. Several useful applications of this powerful feature of margins are illustrated in later chapters. By default, margins predicts whatever predict would predict by default for the last estimation command. For instance, the default prediction for regress is the predicted value $\\widehat{E(y|\\mathbf{x})}$, whereas for logit the default prediction is $\\widehat{\\mathrm{Pr}}(y=1|\\mathbf{x})$. For most estimation commands, you can predict other quantities by adding an option to predict. For example, after logit, the command predict myxb, xb generates the variable myxb with the linear combination of the log-odds. To determine the default prediction and what other types of predictions are available for a given estimation command, type help estimation-command postestimation (for example, help logit postestimation). The margins command can estimate any of the quantities computed by predict, as well as arbitrarily complex functions of these quantities with the predict() and expression() options. 4.3.1 The predict() option With the predict(statistic) option, the margins command makes predictions for any statistic that can be computed by predict. For example, in the ordered logit model considered in chapter 7, the default prediction is the probability of the first outcome. Suppose that our outcome categories are numbered 1, 2, 3, and 4. Running margins without predict() computes the average of $\\widehat{\\mathrm{Pr}}(y_{i}=1|\\mathbf{x}_{i})$. Because predict prob2, outcome(2) generates the variable prob2 containing $\\widehat{\\Pr}(y_i=2|\\mathbf{x}_i)$, to estimate the average probability that y equals 2, we use margins, predict(outcome(2)): use gssclass4, clear ologit class i.fem i.white i.year i.ed age inc, nolog margins, predict(outcome(2)) Predictive margins Number of obs = 5,620\rModel VCE: OIM\rExpression: Pr(class==2), predict(outcome(2))\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_cons | 0.46 0.01 73.93 0.000 0.45 0.47\r------------------------------------------------------------------------------\rIn the output, Expression: Pr(class==2), predict(outcome(2)) indicates the quantity being estimated. Because margins can only predict one outcome at a time, we must either run margins once for each outcome or use mtable to automate this process, as we describe shortly. 4.3.2 The expression() option The expression() option lets you estimate transformations of what is computed by predict(). To show how this works, imagine that after fitting an ordered logit model on an outcome with four categories, we want the predicted probability that y is 2, 3, or 4. That is, we want to compute $\\begin{aligned}\\Pr(y\\ne1|\\mathbf{x})=1-\\Pr(y=1|\\mathbf{x})\\end{aligned}$. The option is expression(1 - predict(outcome(1))): margins, expression(1-predict(outcome(1))) Predictive margins Number of obs = 5,620\rModel VCE: OIM\rExpression: 1-predict(outcome(1))\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_cons | 0.93 0.00 274.89 0.000 0.92 0.94\r------------------------------------------------------------------------------\rA similar application is computing the probability of a 0 after fitting a binary model. We cannot obtain this prediction with the predict() option because the predict command doe","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:4:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4.4 Tables of predictions using mtable mtable makes tables from the predictions computed by margins. You do not need to run margins because mtable does this for you, using most of the options for margins that we just considered. In addition, mtable has options to customize how the results appear by adding labels, selecting statistics, and combining results from multiple mtable commands. There are, however, some features in margins that will not work with mtable. Most notably, perhaps, margins allows a varlist with factor variables, but mtable does not. But as we showed on page 151, results that can be computed with a varlist can be computed using at(), so this does not limit what you can do with mtable. To explain how m table works, we start by creating a table of predicted probabilities that vary by wc and he from a logit model. We will talk at length about how to interpret these predictions in chapter 6 use binlfp4, clear logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog mtable, at(wc=(0 1) hc=(0 1)) atmeans Expression: Pr(lfp), predict()\r| wc hc Pr(y)\r----------+-----------------------------\r1 | 0 0 0.509\r2 | 0 1 0.543\r3 | 1 0 0.697\r4 | 1 1 0.725\rSpecified values of covariates\r| 2. 3. | k5 k618 agecat agecat lwg inc\r----------+-------------------------------------------------------------\rCurrent | .238 1.35 .385 .219 1.1 20.1\rIn the header, Expression echoes the description that margins uses to describe the predictions it is making. The column Pr(y) contains predicted probabilities that lfp is 1. The first row of the prediction table, numbered 1, shows that the probability of being in the labor force is 0.509 for a woman who did not go to college (wc=0) and whose husband did not go to college (hc=0), holding other variables at their means as specified with the atmeans option. Rows 2, 3, and 4 show predictions for other combinations of hc and wc. Values of the independent variables that are held constant are displayed below the predictions. To convince you (we hope) of the advantages of mtable. let’s look at the corresponding output from margins. We show all the output because if you use noatlegend, you risk not knowing which predictions correspond to which values of the variables that vary. margins, at(wc=(0 1) hc=(0 1)) atmeans Adjusted predictions Number of obs = 753\rModel VCE: OIM\rExpression: Pr(lfp), predict()\r1._at: k5 = .2377158 (mean)\rk618 = 1.353254 (mean)\r1.agecat = .3957503 (mean)\r2.agecat = .3851262 (mean)\r3.agecat = .2191235 (mean)\rwc = 0\rhc = 0\rlwg = 1.097115 (mean)\rinc = 20.12897 (mean)\r2._at: k5 = .2377158 (mean)\rk618 = 1.353254 (mean)\r1.agecat = .3957503 (mean)\r2.agecat = .3851262 (mean)\r3.agecat = .2191235 (mean)\rwc = 0\rhc = 1\rlwg = 1.097115 (mean)\rinc = 20.12897 (mean)\r3._at: k5 = .2377158 (mean)\rk618 = 1.353254 (mean)\r1.agecat = .3957503 (mean)\r2.agecat = .3851262 (mean)\r3.agecat = .2191235 (mean)\rwc = 1\rhc = 0\rlwg = 1.097115 (mean)\rinc = 20.12897 (mean)\r4._at: k5 = .2377158 (mean)\rk618 = 1.353254 (mean)\r1.agecat = .3957503 (mean)\r2.agecat = .3851262 (mean)\r3.agecat = .2191235 (mean)\rwc = 1\rhc = 1\rlwg = 1.097115 (mean)\rinc = 20.12897 (mean)\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_at |\r1 | 0.51 0.03 18.54 0.000 0.46 0.56\r2 | 0.54 0.04 12.21 0.000 0.46 0.63\r3 | 0.70 0.05 14.23 0.000 0.60 0.79\r4 | 0.73 0.04 19.92 0.000 0.65 0.80\r------------------------------------------------------------------------------\rThe margins output has additional information about the predictions, such as the confidence interval, that was missing from the mtable output. We can include the confidence interval in the mtable output by adding the options statistics(ci). At the same time, we show how to customize the label for predictions by using estname(): mtable, at(wc=(0 1) hc=(0 1)) atmeans estname(Pr_LFP) statistics(ci) Expression: Pr(lfp), predict()","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:4:4","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4.5 Marginal effects: Changes in predictions Marginal effects are estimates of the change in an outcome for a change in one independent variable, holding all other variables constant. Here we provide an overview of the commands and basic concepts for computing marginal effects. A detailed discussion of marginal effects, along with substantive applications of alternative measures of change, is given in later chapters, especially chapter 6. We begin by discussing margins, which computes marginal effects with the dydx() option, and we then show how mtable can do the same thing. Because marginal effects are such a useful summary of effects in nonlinear models, we created mchange to easily compute many types of changes and present them in a compact table. 4.5.1 Marginal effects using margins margins can calculate the change in a predicted quantity as an independent variable changes, holding other variables constant. The prediction can be anything that margins can estimate. **The variables for which changes are calculated are specified using the dydx(varlist) option, where dydx(*) indicates that changes for all independent variables are to be computed.**For example, use binlfp4, clear logit lfp k5 i.agecat i.wc inc, nolog margins, dydx(*) Average marginal effects Number of obs = 753\rModel VCE: OIM\rExpression: Pr(lfp), predict()\rdy/dx wrt: k5 2.agecat 3.agecat 1.wc inc\r------------------------------------------------------------------------------\r| Delta-method\r| dy/dx std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5 | -0.29 0.03 -8.35 0.000 -0.36 -0.22\r|\ragecat |\r40-49 | -0.11 0.04 -2.72 0.007 -0.19 -0.03\r50+ | -0.24 0.05 -5.30 0.000 -0.33 -0.15\r|\rwc |\rcollege | 0.22 0.04 6.16 0.000 0.15 0.29\rinc | -0.01 0.00 -4.30 0.000 -0.01 -0.00\r------------------------------------------------------------------------------\rNote: dy/dx for factor levels is the discrete change from the base level.\rThe amount of change in a regressor that is used to calculate the change in the prediction depends on whether the variable is a continuous or a factor variable, where Stata assumes variables are continuous unless specified as factor variables with the i. notation. In our example, k5 and inc are continuous while agecat and wc are factor variables. For a continuous variable, margins estimates the marginal change, which is the partial derivative or instantaneous rate of change in the estimated quantity with respect to a given variable, holding other variables constant. For factor variables, margins calculates the discrete change, which is the difference in the prediction when the factor variable is 1 compared with the prediction when the variable is 0. For the binary variable wc, this is the change in the probability of being in the labor force if the wife attended college compared with if she did not attend college. For multiple-category factor variables, the change is from the base category to the value listed in column dy/dx. For i.agecat in this example, the row labeled 40-49 is the change in the probability for a change in agecat from the excluded base category 30-39 to the category 40-49. It bears repeating that margins only calculates the discrete change for variables specified with the i. factor-variable notation. For example (using underlining to highlight the differences between the two commands), although logit lfp k5 i.agecat wc inc and logit lfp k5 i.agecat i.wc inc yield the same estimates of the regression coefficients, the values of dydx() computed by margins will differ. In the first specification, wc is not a factor variable, so margins computes the partial derivative with respect to wc; in the second specification, i.wc is a factor variable, so margins computes the discrete change. Almost certainly in this context, you want the discrete change, and so factor-variable notation must be used when fitting the model. 4.5.2 Marginal effects using mtable Showing how mtable c","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:4:5","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4.6 Plotting predictions For continuous variables, graphs can effectively summarize effects. The Stata command marginsplot plots the predictions from the most recently run margins. Our mgen command can also be used to plot results from margins. The major difference between the commands is that marginsplot creates plots, while mgen generates variables that can be used with Stata’s graphing commands. The former approach is convenient, but ultimately limited because it allows you to plot only a single outcome category from a single model in a graph. 4.6.1 Plotting predictions with marginsplot marginsplot uses results from the preceding margins command. For example, here we plot the predicted probabilities of labor force participation over the ages 20 to 80 for women who attended college and those who did not: use binlfp4, clear logit lfp k5 k618 age i.wc i.hc lwg inc, nolog margins, at(age=(20(10)80) wc=(0 1)) atmeans marginsplot 4.6.2 Plotting predictions with marginsplot The mgen command generates variables that can be plotted using Stata’s graph commands. Like mtable and mchange, mgen runs margins for you and accepts most of the options that can be used with margins. The most important options for graphing are at(), which is used to specify the range of the variable on the x-axis and the levels of other variables, and atmeans, if you want to hold other variables at the mean. Here is a simple example that uses mgen to create a variable containing predictions as income increases from $0$ to $100,000$ in increments of $10,000$: use binlfp4, clear logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog mgen, at(inc=(0(10)100)) stub(A) atmeans Predictions from: margins, at(inc=(0(10)100)) atmeans predict(pr)\rVariable Obs Unique Mean Min Max Label\r-----------------------------------------------------------------------------------------------------\rApr1 11 11 .3608011 .0768617 .7349035 pr(y=in LF) from margins\rAll1 11 11 .2708139 -.0156624 .6641427 95% lower limit\rAul1 11 11 .4507883 .1693859 .8056643 95% upper limit\rAinc 11 11 50 0 100 Family income excluding wife's\r-----------------------------------------------------------------------------------------------------\rSpecified values of covariates\r1. 3. 1. 1. k5 k618 agecat agecat wc hc lwg ---------------------------------------------------------------------------\r.2377158 1.353254 .3851262 .2191235 .2815405 .3917663 1.097115 The option stub(stubname) provides the first letters to be used in the names of the variables that are generated. We recommend a stub that differs from the starting letters of variables in the dataset; then, afterward, the variables can be easily deleted by typing drop stubname*. If the variable names in your dataset are all lowercase, an uppercase stub works well for this purpose. If you do not use the stub() option, the default stub is an underscore, leading to variable names such as _pr1. If you want to overwrite existing variables, perhaps while debugging the command, you can include the option replace. In our example, mgen generated four variables: Aprl with the predicted probabilities, Alll and Aull with the lower and upper bounds of the confidence interval for the prediction, and Ainc with values of inc for each prediction. The values of Ainc are determined by the at() option. The summary statistics for generated variables show that inc ranges from 0 to 100, with predicted probabilities ranging from 0.08 to 0.73. We can list these values: list Apr Ainc in 1/12, clean graph twoway connected Apr Ainc Apr1 Ainc 1. .7349035 0 2. .6613024 10 3. .5789738 20 4. .4920058 30 5. .405519 40 6. .324523 50 7. .2528245 60 8. .1924535 70 9. .1437253 80 10. .1057196 90 11. .0768617 100 12. . . We can run mgen multiple times to generate variables with predictions at different levels of variables that are not varying. Here we use quietly to suppress the output from mgen, and we create variables with predictions at each level of agecat. The results are then plotted using a sin","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:4:6","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4.7 Interpretation of parameters Although the primary methods of interpretation in this book are based on predictions from the model, some methods of interpretation involve simple transformations of the model’s parameters. For some estimation commands, there are options to list transformations of the estimates, such as the or option to list odds ratios for logit or the beta option to list standardized coefficients for regress. Although Stata is commendably clear in explaining the meaning of the estimated parameters, in some models it is easy to be confused about proper interpretations. For example, the zip model (discussed in chapter 9) simultaneously fits a binary and a count model, and it is easy to be confused regarding the direction of the effects. For the estimation commands considered in this book, plus some not considered here, our listcoef command lists estimated coefficients in ways that facilitate interpretation. You can list coefficients selected by name or by significance level, list transformations of the coefficients, and request help on interpretation. In fact, often you will not need the normal output from the estimation. You could suppress this output with the prefix quietly (for example, quietly logit lf pk5 wche) and then use the listcoef command. 4.7.1 The listcoef command The listcoef command listcoef [varlist] [, [factor|percent|std] adjacent gt lt negative positive pvalue (#) nolabel constant off help] **where varlist indicates that coefficients for only these variables are to be listed. If no varlist is given, then coefficients for all variables are listed. The varlist should not use factor-variable notation.**For example, for the model logit lfp i.age cat i.wc lwg, the command listcoef agecat will show the coefficients for 2.agecat and 3.agecat. If agecat##c.lw g was in the model, estimates for all coefficients that include agecat would be listed. 4.7.1.1 Options for types of coefficients Depending on the model and the specified options, **listcoef computes standardized coefficients, factor changes in the odds or expected counts, or percentage changes in the odds or expected counts.**More information on these types of coefficients is provided below, as well as in the chapters that deal with specific types of outcomes. factor requests factor change coefficients indicating how many times larger or smaller the outcome is. In some cases, these coefficients are odds ratios. percent requests percentage change coefficients indicating the percentage change in the outcome. std requests that coefficients be standardized to a unit variance for the independent variables or the dependent variable. For models that can be derived from a latent-dependent variable (for example, the binary logit model), the variance of the latent outcome is estimated. The following options (details on these options are given below) are available for each estimation command. If an option is the default, it does not need to be specified. 4.7.1.2 Options for mlogit, m probit, and slogit For the mlogit, mprobit, and slogit commands discussed in chapter 8, listcoef can show the coefficients for each pair of outcome categories. When these models are used with ordered outcomes, it is helpful to look at a subset of these coefficients. The following options are for this purpose: adjacent specifies that only the coefficients from comparisons in which the two category values are adjacent will be printed (for example, comparing outcome 1 versus 2, and 2 versus 1, but not 1 versus 3). This option can be combined with gt or lt. gt specifies that only the coefficients from comparisons in which the first category has a larger value than the second will be printed (for example, comparing outcome 2 versus 1, but not 1 versus 2). lt specifies that only the coefficients from comparisons in which the first category has a smaller value than the second will be printed (for example, comparing outcome 1 versus 2, but not 2 versus 1). negative specifies that onl","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:4:7","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"Reviews Stata commands for fitting models,testing hypotheses,and computing measures of model fit","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"Reviews Stata commands for fitting models,testing hypotheses,and computing measures of model fit* Our book deals with what we think are the most fundamental and useful cross-sectional regression models for categorical and count outcomes: binary logit and probit, ordinal logit and probit, multinomial logit, Poisson regression, and negative binomial regression. We also explore several less common models, such as the stereotype logistic regression model and the zero-inflated and zero-truncated count models. Although these models differ in many respects, they generally share common features: Each model is fit by maximum likelihood, and many can be fit when data is collected using a complex sample survey design. Hypotheses about the parameters can be tested with Wald and likelihood-ratio tests. Measures of fit can be computed. The models can be interpreted by examining predicted values of the outcomes, a topic that is considered in chapter 4. Because of these similarities, the same principles and many of the same commands can be applied to each model. ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:0:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1 Estimation Each of the models we consider is fit using maximum likelihood (ML). ML estimates are the values of the parameters that have the greatest likelihood of generating the observed sample of data if the assumptions of the model are true. To obtain the ML estimates, a likelihood function calculates how likely it is that w e would observe the set of outcome values we actually observed if a given set of parameter estimates were the true parameters. If we imagine a surface in which the range of possible values of a makes up one axis and the range of $\\beta$ makes up another axis, the resulting graph of the likelihood function would look like a hill; the ML estimates would be the parameter values corresponding to the top of this hill. The variance of the estim ates corresponds roughly to how quickly the slope is changing near the top of the hill. ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:1:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.1 Stata’s output for ML estimation The process of iteration is reflected in the initial lines of Stata’s output. Below are the first lines of the output from the logit model of labor force participation th at we use as an example in chapters 5 and 6: use binlfp4, clear logit lfp k5 k618 agecat wc hc lwg inc Iteration 0: Log likelihood = -514.8732 Iteration 1: Log likelihood = -453.09301 Iteration 2: Log likelihood = -452.72688 Iteration 3: Log likelihood = -452.72649 Iteration 4: Log likelihood = -452.72649 Logistic regression Number of obs = 753\rLR chi2(7) = 124.29\rProb \u003e chi2 = 0.0000\rLog likelihood = -452.72649 Pseudo R2 = 0.1207\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5 | -1.39 0.19 -7.39 0.000 -1.76 -1.02\rk618 | -0.07 0.07 -0.96 0.338 -0.20 0.07\ragecat | -0.64 0.13 -4.92 0.000 -0.89 -0.38\rwc | 0.80 0.23 3.48 0.001 0.35 1.25\rhc | 0.14 0.21 0.66 0.510 -0.27 0.54\rlwg | 0.61 0.15 4.06 0.000 0.32 0.91\rinc | -0.03 0.01 -4.25 0.000 -0.05 -0.02\r_cons | 1.66 0.37 4.45 0.000 0.93 2.39\r------------------------------------------------------------------------------\rThe results begin with the iteration log, where the first, line, iteration 0, reports the value of the log likelihood at the start value. Whereas earlier we talked about maximizing the likelihood function, in practice, programs maximize the log of the likelihood, which simplifies the computations and yields the same result. Here the log likelihood at the start is —514.8732. The next four lines show the progress in maximizing the log likelihood, converging to the value of —452.72649. The rest of the output, which is omitted here, is discussed later in this section. ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:1:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.2 ML and sample size Under the usual assumptions, the ML estimator is consistent, efficient, and asymptotically normal. These properties hold as the sample size approaches infinity (see Cameron and Trivedi [2005]; Cramer [1986]; and Eliason [1993] for details). Although ML estimators are not necessarily bad estimators in small samples, the small-sample behavior of ML estim ators for the models we consider is largely unknown. Except for the logit and Poisson regression, which can be fit using exact perm utation methods with exlogistic or expoisson, alternative estimators with known small-sample properties are generally not available. With this in mind, Long (1997, 54) proposed the following guidelines for the use of ML in small samples: It is risky to use ML with samples smaller than 100, while samples over 500 seem adequate. These values should be raised depending on characteristics of the model and the data. First, if there are many parameters, more observations are needed A rule of at least 10 observations per parameter seems reasonable This does not imply that a minimum of 100 is not needed if you have only two parameters. Second, if the data are ill-conditioned (for example, independent variables are highly collinear) or if there is little variation in the dependent variable (for example, nearly all the outcomes are 1), a larger sample is required. Third, some models seem to require more observations (such as the ordinal regression model or the zero-inflated count models). ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:1:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.3 Problems in obtaining ML estimates Although the numerical methods used by S tata to fit models with ML are highly refined and generally work extremely well, you can encounter problems. If your sample size is adequate, but you cannot get a solution or you get estimates th at appear to not make substantive sense, one common cause is th at the data have not been properly “cleaned”. In addition to mistakes in constructing variables and selecting observations, the scaling of variables can cause problems. The larger the ratio between the largest and smallest standard deviations among variables in the model, the more problems you are likely to encounter with numerical m ethods due to rounding. For example, if income is measured in units of $1$, income is likely to have a very large standard deviation relative to other variables. Recoding income to units of $1,000$ can solve the problem. For a detailed technical discussion of maximum likelihood estim ation in Stata, see Gould, Pitblado, and Poi (2010) Overall, however, numerical methods for ML estimation work well when your model is appropriate for your data. Still, Cramer’s (1986, 10) advice about the need for care in estimation should be taken seriously: Check the data, check their transfer into the computer, check the actual computations (preferably by repeating at least a sample by a rival program), and always remain suspicious of the results, regardless of the appeal. ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:1:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.4 Syntax of estimation commands All single-equation estimation commands that we consider in this book have the same syntax: command depvar [ indepvars ] [if] [ in ] [ weight ] [ , options ] Here are a few examples for a lo g it model with lfp as the dependent variable: logit lfp k5 k618 age wc lwg logit lfp k5 k618 age wc lwg if hc == 1 logit lfp k5 k618 age wc lwg if hc == 1, level(90) The syntax diagram here uses 1) variable lists, 2) i f and in conditions, 3) weights, and 4) options. ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:1:4","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.5 Variable lists Stata automatically corrects some mistakes you may make when specifying independent variables. For example, if you include wc as an independent variable when th e sample is restricted to a single value of wc, such as logit lfp k5 age he wc if wc==l. then Stata drops wc from the model. Or suppose that you recode a k-category variable into a set of k indicator variables. Recall that one of the indicator variables must be excluded to avoid perfect collinearity. If you included all k indicator variables in indepvars, Stata automatically excludes one of them. Using factor-variable notation in the variable list In Stata 11 and later, you can specify a k-category variable as a set of indicator variables using Stata’s factor-variable notation. Prefixing a variable name with i.tells Stata to do this. In our previous example, suppose th at instead of age being measured in years, it was measured using three age groups with the variable agecat: tabulate agecat, missing Wife's age |\rgroup | Freq. Percent Cum.\r------------+-----------------------------------\r30-39 | 298 39.58 39.58\r40-49 | 290 38.51 78.09\r50+ | 165 21.91 100.00\r------------+-----------------------------------\rTotal | 753 100.00\rVariable agecat equals 1 for ages 30- 39, 2 for 40 49, and 3 for 50 or older. If we were not using factor variables, we could recode the three categories of ag ecat to generate three dummy variables: generate age3039 = (agecat==1) if agecat \u003c . label var age3039 \"Age 30 to 39?\" generate age4049 = (agecat==2) if agecat \u003c . label var age4049 \"Age 40 to 49?\" generate age50plus = (agecat==3) \u0026 agecat \u003c . label var age50plus \"Age 50 or older?\" Next, we fit a model using these variables, where age3039 is the excluded base category: logit lfp k5 k618 age4049 age50plus wc hc lwg inc, nolog Using factor-variable notation, we can fit the exact same model but let S tata automatically create the indicator variables: logit lfp k5 k618 i.agecat wc hc lwg inc, nolog By default, Stata uses the lowest value of the source variable as the base or omitted category in the model. Accordingly, logit lfp k5 k618 i.agecat wc hc lwg inc, nolog If you want a different base category, specify the base with the prefix ib#. , where # is the value of the base category. In our example, to treat women ages 50 or older as our base category instead of women ages 30-39, we would specify the model as follows: logit lfp k5 k618 ib3.agecat wc hc lwg inc, nolog By default, any variable not specified with i. is treated as a continuous variable. For example, in the specification logit lfp k5 k618 agecat wc hc lwg inc, nolog More on factor-variable notation sometimes factor variables can be confusing. First, the name of the variables automatically generated by Stata when fitting a model using factor variables might not be obvious. This is important because some postestimation commands, such as test and lincom, require the exact, symbolic name of the variable associated with a coefficient. To obtain the names associated with each coefficient, referred to as symbolic names, one can simply type the name of the last estimation command with the option coeflegend, such as logit, coefleg end.The model is not fit again, but the names associated with the estimates are listed. For example, fitting a model where factor-variable notation creates indicator variables and interactions produces output like this: logit lfp i.agecat c.age##c.age, nolog Logistic regression Number of obs = 753\rLR chi2(4) = 11.17\rProb \u003e chi2 = 0.0248\rLog likelihood = -509.29034 Pseudo R2 = 0.0108\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\ragecat |\r40-49 | -0.37 0.35 -1.05 0.296 -1.05 0.32\r50+ | -0.56 0.56 -1.00 0.319 -1.66 0.54\r|\rage | 0.26 0.14 1.77 0.077 -0.03 0.54\r|\rc.age#c.age | -0.00 0.00 -1.74 0.082 -0.01 0.00\r|\r_cons | -4.90 3.03 -1.62 ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:1:5","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.6 Specifying the estimation sample if and in restrictions can be used to define the sample of observations used to fit the model, referred to as the estimation sample, where the syntax for if and in conditions follows the guidelines in chapter 2, page 45. 1.6.1 Missing data Estimation commands use listwise deletion to exclude cases in which there are missing values for any of the variables in the model. Accordingly, if two models are fit using the same dataset but have different sets of independent variables, it is possible to have different samples. Suppose th at among the 753 cases in the sample, 23 have missing data for at least one variable. If we fit a model using all variables, we would obtain use binlfp4-missing, clear logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog Logistic regression Number of obs = 730\rLR chi2(8) = 113.28\rProb \u003e chi2 = 0.0000\rLog likelihood = -441.33862 Pseudo R2 = 0.1137\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5 | -1.40 0.19 -7.22 0.000 -1.78 -1.02\rk618 | -0.07 0.07 -0.95 0.340 -0.20 0.07\r|\ragecat |\r40-49 | -0.66 0.21 -3.11 0.002 -1.07 -0.24\r50+ | -1.26 0.26 -4.76 0.000 -1.77 -0.74\r|\rwc |\rcollege | 0.74 0.23 3.23 0.001 0.29 1.20\r|\rhc |\rcollege | 0.16 0.21 0.77 0.439 -0.25 0.57\rlwg | 0.59 0.15 3.88 0.000 0.29 0.88\rinc | -0.03 0.01 -3.82 0.000 -0.05 -0.02\r_cons | 1.02 0.29 3.51 0.000 0.45 1.60\r------------------------------------------------------------------------------\rSuppose that seven of the missing cases were missing only for k618 and that we fit a second model excluding k618: logit lfp k5 i.agecat i.wc i.hc lwg inc, nolog Logistic regression Number of obs = 737\rLR chi2(7) = 116.53\rProb \u003e chi2 = 0.0000\rLog likelihood = -444.78964 Pseudo R2 = 0.1158\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5 | -1.38 0.19 -7.23 0.000 -1.75 -1.01\r|\ragecat |\r40-49 | -0.60 0.21 -2.92 0.003 -1.00 -0.20\r50+ | -1.15 0.24 -4.87 0.000 -1.62 -0.69\r|\rwc |\rcollege | 0.78 0.23 3.38 0.001 0.33 1.23\r|\rhc |\rcollege | 0.14 0.21 0.67 0.505 -0.27 0.54\rlwg | 0.61 0.15 4.03 0.000 0.31 0.90\rinc | -0.03 0.01 -3.89 0.000 -0.05 -0.02\r_cons | 0.87 0.25 3.48 0.000 0.38 1.36\r------------------------------------------------------------------------------\rThus we cannot, use a likelihood ratio test or information criteria to compare the two models (see sections 3.2 and 3.3 for details), because changes in the estimates could be due either to changes in the model specification or to the use of different samples to fit the models. When you compare coefficients across models, you want the samples to be the same. Although Stata uses listwise deletion when fitting models, this is rarely the best way to handle missing data. We recommend that you make explicit decisions about which cases to include in your analyses rather than let cases be dropped implicitly. Indeed, we would prefer th at Stata issue an error rather than automatically drop cases. The mark and markout commands make it simple to explicitly exclude missing data, mark markvar generates the new variable markvar that equals 1 for all cases, markout mnrkvar varlist changes the values of markvar to 0 for any cases in which values of any of the variables in varlist are missing. The following example, where we have artificially created the missing data, illustrates how this works: use binlfp4-missing, clear mark nomiss markout nomiss lfp k5 k618 agecat wc hc lwg inc tab nomiss nomiss | Freq. Percent Cum.\r------------+-----------------------------------\r0 | 23 3.05 3.05\r1 | 730 96.95 100.00\r------------+-----------------------------------\rTotal | 753 100.00\rBecause nomiss is 1 for cases where none of the variables in our models is missing, to u","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:1:6","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.7 Weights and survey data Weights indicate that some observations should be given more weight than others when computing estimates. The syntax for specifying weights is [type=vamame] , where the square brackets are part of the command, type is the type of weight to be used, and vamame is the variable containing the weights. Stata recognizes four types of weights: fweights (frequency weights) indicate that an observation represents multiple observations with identical values. For example, if an observation has an fweight of 5, this is equivalent to having five identical, duplicate observations. If you do not include a weight modifier in your estim ation command, this is equivalent to specifying [fweight= l]. pweights (sampling weights) denote the inverse of the probability that the observation is included because of the sampling design. For example, if a case has a pweight of 1,200, that case had a 1 in 1,200 chance of being selected into the sample and in th at sense represents 1,200 observations in the population. aweights (analytic weights) are inversely proportional to the variance of an observation. The variance of the jth observation is assumed to be $\\frac{\\sigma^2}{\\omega_{j}}$, where $\\omega_{j}$ is the analytic weight. Analytic weights are used most often when observations are averages and the weights are the num ber of elements that gave rise to the average. For example, if each observation is the cell mean from a larger dataset, the data are heteroskedastic because the variance of the means decreases as the number of observations used to calculate them increases. iw eights (importance weights) have no formal statistical definition. They are used by programmers to facilitate certain types of computations Frequency weights differ notably from the other types because a dataset th at includes an fw eight variable can be used to create a new dataset that yields equivalent results without frequency weights by simply repeating observations with duplicate values. As a result, frequency weights pose no issues for various techniques we consider in this book The use of weights is a complex topic, and it is easy to apply weights incorrectly. If you need to use weights, we encourage you to read the discussions in [u] 11.1.6 weight and [u] 20.23 Weighted estimation . Winship and Radbill (1994) have an accessible introduction to weights in the linear regression model. Heeringa, West, and Berglund (2010) provide an in-depth treatment along with examples using Stata in their excellent book on complex survey design, a topic we consider next. 1.7.1 complex survey desighs Complex survey designs have three major features. First, samples can be divided into strata within which observations are selected separately. For example, a sample might be stratified by region of the country so th at the researchers can achieve precisely the number of respondents they want from each region. Second, samples can use clustering in which higher levels of aggregation, called primary sampling units, are selected first and then individuals are sampled from within these clusters. A survey of adolescents might use schools as its prim ary sampling unit and then sample students from within each school. Observations within clusters often share similarities leading to violations of the assumption of independent observations. Accordingly, when there is clustering, the usual standard errors will be incorrect because they do not adjust for the lack of independence. Third, individuals can have different probabilities of selection. For example, the design might oversample minority populations. Such oversampling allows more precise estimates of subgroup characteristics, but probability weights must be used to obtain accurate estimates for the population as a whole. Stata’s svy commands for samples with complex survey designs (see the Stata Survey Data Manual for details) provide estimates where the standard errors are adjusted for stratification, clustering, and wei","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:1:7","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.8 Options for regression models The following options apply to most regression models. Unique options for specific models are considered in later chapters. noconstant constrains the intercept to equal 0. For example, in a linear regression, the command regress y xl x2, noconstant would fit the model $y = \\beta_{1} X_{1} + \\beta_{2} X_{2} + \\varepsilon$ nolog suppresses the iteration history, which shortens the output. If you use this option, which we often do, and have problems obtaining estimates, it is a good idea to refit the model w ithout this option and with the trace option. trace lets you see the values of the parameters for each step of th e iteration. This can be useful for determining which variables may be causing a problem if your model has difficulty converging. level(#) specifies the level of the confidence interval. By default, Stata provides 95% confidence intervals for estimated coefficients, meaning that the interval around the estimated $\\hat{\\beta}$ would capture the true value of $\\beta$ 95% of the time if repeated samples were drawn, level () allows you to specify other intervals. For example, level (90) specifies a 90% interval. You can also change the default level with the command set level. For example, set level 90 specifies 90% confidence intervals. vce (cluster cluster-variable) specifies that the observations are independent across the clusters th at are defined by unique values of cluster-variable but are not necessarily independent within clusters. Specifying this option leads to robust standard errors, as discussed below, with additional corrections for the effects of clustered data. See Hosmer, Lemeshow, and Sturdivant (2013, chap. 9) for a detailed discussion of logit models with clustered data. Using vce ( cluster cluster-variable) does not affect the coefficient estimates but can have a large impact on the standard errors. vce(vcetype) specifies the type of standard errors th at are reported, vce (robust) replaces traditional standard errors with robust standard errors, which are also known as Huber, White, or sandwich standard errors. These are discussed further next, in section 3.1.9. Gould, Pitblado, and Poi (2010) provide details on how robust standard errors are computed in Stata. Robust standard errors are automatically used if the vce ( cluster cluster-variable) option is specified, if probability weights are used, or if a model is fit using svy. In earlier versions of Stata, this option was simply robust. Option vce (bootstrap) estimates the variance-covariance matrix by bootstrap, which involves repeated reestimation on samples drawn with replacement from the original estimation sample. Option vce(jackknife) uses the jackknife method, which involves refitting the model N times, each time leaving out a single observation. Type help vce option for further details. vsquish eliminates the blank lines in output that are inserted when factor-variable notation is used. We sometimes use nolog and vsquish in this book to save space. ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:1:8","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.9 Robust standard errors Robust standard errors, which are computed by Stata when the robust option is specified, go by a variety of names, including Huber-Eicker-White, clustered, White, heteroskedasticity-consistent, HCCM, and sandwich standard errors. In the last decade, their use has become increasingly common. For example, King and Roberts (2014) conducted a survey of articles in the American Political Science Review and found that 66% of the articles using regression models reported robust standard errors. Robust standard errors are considered robust in the sense that they are correct in the presence of some types of violations of the assumptions of the model. For example, if the correct model is a binary logit model but a binary probit model is fit, the model has been misspecified. The estimates obtained by fitting a logit model cannot be maximum likelihood estimates because an incorrect likelihood function is being used (that is, a logistic probability density is used instead of the correct normal density). When a model is misspecified in this way, the usual standard errors are incorrect (White 1982). For this reason, Arminger (1995) argues that robust standard errors should be broadly used. He writes: “If one keeps in mind that most researchers misspecify the model …, it is obvious that their estimated parameters can usually be interpreted only as minimum ignorance estimators and that the standard errors and test statistics may be far away from the correct asymptotic values, depending on the discrepancy between the assumed density and the actual density that generated the data.” In some cases, robust standard errors are likely to work quite well. If violations of the underlying model are minor, as we would argue is the case if the true model is logit and you fit a probit model, then the robust standard errors are preferred, but the differences are likely to be quite small. In our informal simulations, they are trivially different. On the other hand, if you fit a Poisson regression model (see chapter 9) in the presence of overdispersion, Cameron and Trivedi (2013, 72 80) provide convincing evidence that robust standard errors provide a more accurate assessment of statistical significance. If there is clustering in the data, robust standard errors should be used, ideally by specifying vce(cluster cluster-variablc) or by using svy estimation. Arguments for robust standard errors are compelling. Some argue they should be used nearly always in practice. At the same time, robust standard errors are not a general solution to problems of misspecification, and they have important limitations. Kauermann and Carroll (2001) show that even when the model is correct, robust standard errors have more sampling variability, and sometimes far more, than the usual standard errors. This is “the price that one pays to obtain consistency”. These theoretical results are consistent with simulations by Long and Ervin (2000), who found that in the linear regression model robust standard errors often did worse than the usual standard errors in samples smaller than 500. They recommended using small-sample versions that can be computed in Stata for regress with the options hc2 or hc3. Among nonlinear models, Kauermann and Carroll (2001) consider the Poisson regression model and the logit model. They showed that the loss of efficiency when using robust standard errors can be worse than th at occurring in normal models. However, we are unaware of small-sample versions of robust standard errors for nonlinear models There is a second and potentially very serious problem. If robust standard errors are used because a model is misspecified, it is im portant to consider what other implications misspecification may have. Freedman (2006) is dismissive of robust standard errors for many of the models discussed in this book for this reason, writing pointedly: “It remains unclear why applied workers should care about the variance of an estimator for the wro","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:1:9","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.10 Reading the estimation output Because we have already discussed the iteration log, in the following example we suppress it with the nolog option and consider other parts of the output from estimation commands. Although the sample output is from logit, our discussion applies generally to other regression models fit by maximum likelihood. We comment briefly below on changes to the estimation output for svy estimation. The following output from logit illustrates how Stata displays results from regression commands: use binlfp4, clear logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog Logistic regression Number of obs = 753\rLR chi2(8) = 124.30\rProb \u003e chi2 = 0.0000\rLog likelihood = -452.72367 Pseudo R2 = 0.1207\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5 | -1.39 0.19 -7.25 0.000 -1.77 -1.02\rk618 | -0.07 0.07 -0.96 0.336 -0.20 0.07\r|\ragecat |\r40-49 | -0.63 0.21 -3.00 0.003 -1.04 -0.22\r50+ | -1.28 0.26 -4.92 0.000 -1.79 -0.77\r|\rwc |\rcollege | 0.80 0.23 3.48 0.001 0.35 1.25\r|\rhc |\rcollege | 0.14 0.21 0.66 0.508 -0.27 0.54\rlwg | 0.61 0.15 4.04 0.000 0.31 0.91\rinc | -0.04 0.01 -4.24 0.000 -0.05 -0.02\r_cons | 1.01 0.29 3.54 0.000 0.45 1.57\r------------------------------------------------------------------------------\r1.10.1 Header Log likelihood = -452.72367 is the value of the log likelihood at convergence. Number of obs is the number of observations, excluding those with missing values and those excluded with if and in conditions. LR chi2(8) is the value of a likelihood-ratio chi-squared for the test of the null hypothesis that all the coefficients associated with independent variables are simultaneously equal to 0 (see page 119 for details). The number in parentheses is the degrees of freedom for the test. When robust standard errors or probability weights are used, results from a Wald test of the same null hypothesis are shown instead. Prob \u003e chi2 indicates the p-value. Pseudo R2 is the measure of fit also known as McFadden’s (1974) R^2. Details on how this measure is computed are given on page 126. 1.10.2 Estimates and standard errors The leftmost column lists the variables in the model, with the dependent variable at the top. The independent variables are in the same order as they were typed on the command line. The constant, labeled _cons, is last. With Stata 13 and later, factor variables are labeled with their value labels. For example, the indicator variable for agecat==2 is labeled as agecat followed by 40-49, which is the value label for category 2. In Stata 11 and 12, or with the nofvlabel option in Stata 13 and later, the indicator variable for agecat==2 is labeled as agecat followed on the next line by 2. Retyping the estim ation command followed by coef legend will list the symbolic names of each regression parameter. Column Coef . contains estimates of the regression coefficients Column Std. Err . contains the standard errors of the estimates. With the vce(robust) option, these are labeled Robust Std . Err. Column z contains the $z$ test equal to the estimate divided by its standard error. Column P\u003e|z| is the two-tailed significance level. A significance level listed as 0.000 means that p \u003c 0.001. For example, p = 0.00049 is rounded to 0.000. Column [95% Conf. Interval] contains the confidence interval for each estimate. Instead of testing a specific hypothesis (for example, $H_0: (\\beta = 0$), we can use a confidence interval that contains the true parameter with a chosen probability, known as the confidence level. For a given confidence level, the estimated upper and lower bounds define the confidence interval. 1.10.3 Differences in output for svy estimation With svy estimation, the output differs, reflecting that the estimates are no longer ML estimates. In addition to the sample size, an estim ate of the population size is shown. The likelih","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:1:10","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.11 Storing estimation results Stata considers the results of a model that has just been fit to be the active estimates.After fitting a model, you can typee return list to see a summary of the informationth at Stata stores about the active estimates. Postestimation commands are based onth e active estimates. When a new model is fit, its results become the active estimates,replacing the previous model’s estimates. The estimates store and estimates save commands preserve the active estimation results so that they can be retrieved and used even after a new model is fit.estimates store saves the active estimates to memory, while estimates save savesthem to a file. Storing estimation results is extremely useful for several reasons. For one,commands like lrtest and estimates table use results from more than one model.Because only one set of estimates can be active at a time, stored estimates are the way we can refer to multiple sets of estimates. Additionally, the margins command, usedextensively in later chapters, makes predictions based on estimates from a model thathas already been fit, meaning the active estimates. For some applications, however, we will need to overwrite the active estimates from our regression model with estimatesfrom margins by using the post option. Once this is done, the estimates from the regression model are no longer active and need to be restored (discussed below) as theactive estimates if you want to do additional postestimation analysis of the model. After running any estimation command, the syntax is estimates store name For example, to store the estimation results with the name logit 1. type use binlfp4, clear logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog estimates store logit1 After running estimates restore , the estimation results in memory are the same as if we had just fit the model, even though we may have fit other models in the interim. Of course, we need to be careful about changes made to the data after fitting the model, but the same caveat about not changing the data between fitting a model and postestimation analysis applies even when estimates store and estimates restore are not used. 1.11.1 (Advanced) Saving estimates to a file if you are fitting a model that takes hours to estimate, you may want to save the result to a file so you can use them later without refitting the model. Because estimates store holds the estimates in memory, estimates stored in one Statasession are not available in the next. Even within a S tata session, the command clear all erases stored estimates. To use estimates in a later session or after clearing memory,you can use estimates save to save results to a disk file: estimates save filename, replace For example, estimates save model1, replace will create the file model1.ster. We can load previously saved estimation results with estim ates use: estimates use filename estimates use restores the estimates almost as if we had just fit the model, and the“almost” here is very important. **As described earlier, when we fit a model, Stata creates the variable e(sample) to indicate which observations were used when fitting the model. Some postestimation commands need e (sample) to produce proper results.However, estimates use does not require that the data in memory are the data used to estimate the saved results. You can even run estimates use without data in memory.**Accordingly, estima tes use does not restore the e(sample) variable. Although this prevents some postestimation commands from working, this is better than having them give wrong answers because the wrong dataset is in memory. To deal with this issue, you can reset e(sample). When doing this, you are responsible for making sure that the data loaded to memory are the same as the data when the model was originally fit. Assuming the proper data are in memory, you use the estimates esample command to respecify the outcome and independent variables, thei if and in conditions, and the weights that were ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:1:11","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.12 Reformatting output with estimates table estimates table reformats the results from an estimation command to look more like the tables that are seen in articles and books. estimates table also makes it easier to move estimation results into a word processor or spreadsheet to make presentation-quality tables. We strongly recommend using this command or some other automated procedure rather than retyping results to make tables. Not only is this less tedious, but it diminishes the possibility of errors. Also, if you revise your model and used estimates table in your do-file, then you automatically have the corrected tables. The syntax is estimates table [ model-name1 [model-name2 … ] ] [ , options ] where model-name# is the name of a model whose results were stored using estim ates store . If model-name# is not specified, the estim ation results in memory are used. Here is a simple example that lets us compare estimates from similarly specified logit and probit models, a topic considered in detail in chapter 5. We start by fitting the two models and using estimates store to save the estimates: use binlfp4, clear logit lfp k5 i.agecat i.wc, nolog estimates store logit_model Logistic regression Number of obs = 753\rLR chi2(4) = 85.93\rProb \u003e chi2 = 0.0000\rLog likelihood = -471.9082 Pseudo R2 = 0.0834\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5 | -1.35 0.19 -7.27 0.000 -1.72 -0.99\r|\ragecat |\r40-49 | -0.62 0.20 -3.18 0.001 -1.01 -0.24\r50+ | -1.19 0.23 -5.27 0.000 -1.63 -0.75\r|\rwc |\rcollege | 0.83 0.18 4.53 0.000 0.47 1.19\r_cons | 0.89 0.16 5.40 0.000 0.57 1.21\r------------------------------------------------------------------------------\rprobit lfp k5 i.agecat i.wc, nolog estimates store probit_model Probit regression Number of obs = 753\rLR chi2(4) = 85.57\rProb \u003e chi2 = 0.0000\rLog likelihood = -472.08881 Pseudo R2 = 0.0831\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5 | -0.82 0.11 -7.56 0.000 -1.03 -0.61\r|\ragecat |\r40-49 | -0.37 0.12 -3.17 0.001 -0.61 -0.14\r50+ | -0.72 0.14 -5.29 0.000 -0.99 -0.46\r|\rwc |\rcollege | 0.50 0.11 4.57 0.000 0.29 0.71\r_cons | 0.54 0.10 5.50 0.000 0.35 0.73\r------------------------------------------------------------------------------\rWe combine the estimates by using estimates table : estimates table logit_model probit_model, b(%12.3f) t varlabel --------------------------------------------------------\rVariable | logit_model probit_model -------------------------+------------------------------\r# kids \u003c 6 | -1.351 -0.820 | -7.27 -7.56 | Wife's age group | 40-49 | -0.624 -0.374 | -3.18 -3.17 50+ | -1.190 -0.723 | -5.27 -5.29 | Wife attended college? | college | 0.832 0.500 | 4.53 4.57 Constant | 0.889 0.540 | 5.40 5.50 --------------------------------------------------------\rLegend: b/t\restimates table provides great flexibility for what you include in your table. Although you should check the Stata Base Reference Manual or type help estimates table for complete information, here are some of the most basic and helpful options: b(format) specifies the format used to print the coefficients. For example, b(%9.3f) indicates the estimates are to be in a column nine characters wide with three decimal places. For more information on formats, see help format or the Stata User’s Guide. varwidth (#) specifies the width of the column th at includes variable names and labels on the left side of the table. This is often needed when variable labels are used. keep (varlist) or drop (varlist) specify which of the independent variables to include in or exclude from the table. se[(format)], t[(format)], and p[C(format)] request standard errors, t or z statistics, and p-values, respect","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:1:12","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2 Testing If the assumptions of the model hold, ML estimators are distributed asymptotically normally: $$\\hat{\\beta_k}\\stackrel{a}{\\sim}N\\left(\\beta_{k},\\sigma_{\\stackrel{\\lambda}{\\beta_{k}}}^{2}\\right)$$ 当我们谈论最大似然估计（MLE）时，我们实际上是在说一种通过观察到的数据来猜测模型参数的方法。最大似然估计的目标是找到一组参数值，使得观察到的数据在这组参数下出现的可能性最大。 让我们用一个更具体的例子来解释： 想象你有一个魔术硬币，但你不知道它是如何工作的。你想知道抛一次硬币，它正面朝上的概率是多少。我们用一个字母 (p) 来表示这个概率。现在，你开始做实验，抛硬币多次，记录每次是正面还是反面。 我们把所有实验的结果称为观察到的数据，比如说你连续抛了10次硬币，结果是 7 次正面（H）和 3 次反面（T）。这组数据就是 (D)。 现在，MLE 的核心思想是：我们要找到一个 (p) 的值，使得在这个 (p) 下，观察到这组数据的概率最大。 在硬币的例子中，我们可以用二项分布来表示抛硬币的概率。假设硬币正面朝上的概率是 (p)，那么观察到 7 次正面和 3 次反面的概率可以用下面的公式表示： $ P(D|p) = p^7 \\cdot (1-p)^3 $ 这里，(P(D|p)) 表示在给定 (p) 的情况下，观察到数据 (D) 的概率。 然后，MLE 就是要找到使这个概率最大的 (p)。你可以把它想象成在 (p) 的可能取值范围内找到一个使得实验结果最有可能出现的 (p)。 数学上，我们可以通过求解导数为零的方程或者使用计算工具找到最大值。最终，我们得到了一个估计值 $ \\hat{p} $，它是使得观察到这组数据的概率最大的 (p)。 简而言之，MLE 就是通过数学方法找到一个最有可能解释观察到的数据的参数值。在硬币的例子中，它告诉我们硬币正面朝上的概率可能是多少，以最好地解释我们实验的结果。 The hypothesis $H_0 :{\\beta}_k = \\beta^*$ can be tested with the $ z $ statistic: z\r=\rβ\r^\rk\r−\rβ\r∗\rσ\r^\rβ\r^\rk\rIf Ho is true, then 2: is distributed approximately normally with a mean of 0 and a variance of 1 for large samples. The sampling distribution is shown in the following figure, where the shading shows the rejection region for a two-tailed test at the 0.05 level For some estimators, such as linear regression implemented by regress and with survey estimation, the estimators have at distribution rather than a normal distribution. The general principles of testing are, however, the same. ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:2:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.1 One-tailed and two-tailed tests The probability levels in the Stata output for estim ation commands are for two-tailed tests. When past research or theory suggests the sign of the coefficient, a one-tailed test might be used, and $H_0$ is rejected only when $t$ or $z$ is in the expected tail. You should divide P \u003e |t| (or P\u003e|z| ) by 2 only when the estimated coefficient is in the expected direction. Disciplines vary in their preferences for using one-tailed or two-tailed tests. Consequently, it is im portant to be explicit about whether p-values are for one-tailed or two-tailed tests. Unless stated otherwise, all the p-values we report in this book are for two-tailed tests. ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:3:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.2 Wald and likelihood-ratio tests For models fit by ML, hypotheses can be tested with Wald tests by using test and with likelihood-ratio (LR) tests by using lrtest. Only Wald tests are available for coefficients estimated using survey estimation. For both types of tests, there is a null hypothesis $H_0$ that implies constraints on the model’s parameters. For example, $H_0: \\beta_vc — \\beta_hc = 0$ hypothesizes that two of the parameters are 0 in the population. The Wald test assesses $H_0$ by considering two pieces of information. First, all else being equal, the greater the distance between the estimated coefficients and the hypothesized values, the less support we have for $H_0$. Second, the greater the curvature of the log-likelihood function, the more certainty we have about our estimates. This means that smaller differences between the estimates and hypothesized values are required to reject $H_0$. The LR test assesses $H_0$ by comparing the log likelihood from th e full model that does not include the constraints implied by $H_0$ with a restricted model th at does impose those constraints. If the constraints significantly reduce the log likelihood, then $H_0$ is rejected. Thus the LR test requires fitting two models. Although the LR and Wald tests are asymptotically equivalent, they have different values in finite samples, particularly in small samples. In general, it is unclear which test is to be preferred. Cameron and Trivedi (2005, 238) review the literature and conclude that neither test is uniformly superior. Nonetheless, many statisticians prefer the LR when both are suitable. We do recommend com puting only one test or the other; that is, we see no reason why you would want to compute or report both tests for a given hypothesis ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:4:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.3 Wald tests with test and testparm test computes Wald tests for linear hypotheses about parameters from the last model that was fit. Here we consider the most useful features of this powerful command. Features for multiple-equation models, such as mlogit, zip, and zinb, are discussed in chapters 8 and 9. Use help test for more features and help testnl for Wald tests of nonlinear hypotheses.” test varlist [ , accumulate ] The first syntax for test allows you to test that one or more coefficients from the last model are simultaneously equal to 0: where varlist contains names of independent variables from the last estimation. Some examples of test after fitting the model logit lfp k5 k618 i.agecat i.wc i.hclwg inc should make this first syntax clear. With one variable listed—here, k5—we are testing $H0: \\beta_k5 = 0$. use binlfp4, clear logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog Logistic regression Number of obs = 753\rLR chi2(8) = 124.30\rProb \u003e chi2 = 0.0000\rLog likelihood = -452.72367 Pseudo R2 = 0.1207\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5 | -1.39 0.19 -7.25 0.000 -1.77 -1.02\rk618 | -0.07 0.07 -0.96 0.336 -0.20 0.07\r|\ragecat |\r40-49 | -0.63 0.21 -3.00 0.003 -1.04 -0.22\r50+ | -1.28 0.26 -4.92 0.000 -1.79 -0.77\r|\rwc |\rcollege | 0.80 0.23 3.48 0.001 0.35 1.25\r|\rhc |\rcollege | 0.14 0.21 0.66 0.508 -0.27 0.54\rlwg | 0.61 0.15 4.04 0.000 0.31 0.91\rinc | -0.04 0.01 -4.24 0.000 -0.05 -0.02\r_cons | 1.01 0.29 3.54 0.000 0.45 1.57\r------------------------------------------------------------------------------\r* Wald tests test k5 ( 1) [lfp]k5 = 0\rchi2( 1) = 52.57\rProb \u003e chi2 = 0.0000\rThe resulting chi-squared test with 1 degree of freedom equals the square of the $z$ test statistic in the logit output. The results indicate that we can reject the null hypothesis. If we list all the regressors in the model, we can test that all the coefficients except the constant are simultaneously equal to 0. When factor-variable notation is used, variables must be specified with the value.variable-name syntax, such as 2.agecat. Recall that if you are not sure what name to use, you can replay the results by using the coef legend option (for example, logit, coef legend). test k5 k618 2.agecat 3.agecat 1.wc 1.hc lwg inc ( 1) [lfp]k5 = 0\r( 2) [lfp]k618 = 0\r( 3) [lfp]2.agecat = 0\r( 4) [lfp]3.agecat = 0\r( 5) [lfp]1.wc = 0\r( 6) [lfp]1.hc = 0\r( 7) [lfp]lwg = 0\r( 8) [lfp]inc = 0\rchi2( 8) = 95.90\rProb \u003e chi2 = 0.0000\rAs noted above, an LR test of the same hypothesis is part of the standard output ol estim ation commands, labeled as LR chi2 in the header ot the estimation output. To test all the coefficients associated with a factor variable with more than two categories, you can use testparm . For example, to test that all the coefficients for agecat are 0, we can use test: test 2.agecat 3.agecat ( 1) [lfp]2.agecat = 0\r( 2) [lfp]3.agecat = 0\rchi2( 2) = 24.27\rProb \u003e chi2 = 0.0000\rThe same results are obtained with testparm : ( 1) [lfp]2.agecat = 0\r( 2) [lfp]3.agecat = 0\rchi2( 2) = 24.27\rProb \u003e chi2 = 0.0000\rBecause agecat has only two categories, the advantage of testpaxm is not great. But when there are many categories, it is much simpler to use. The second syntax for test allows you to test hypotheses about linear combinations of coefficients: test [exp = exp] [ , accumulate ] For example, to test th at two coefficients are equal— say, $H_0:\\beta = \\beta_k618$: test k5=k618 ( 1) [lfp]k5 - [lfp]k618 = 0\rchi2( 1) = 45.07\rProb \u003e chi2 = 0.0000\rBecause the test statistic is significant, we reject the null hypothesis that the effect of having young children on labor force participation is equal to the effect of having older children. As before, testing hypotheses involving indicator variables requires us to specify both the value and the variable. For example, to test that the coeff","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:5:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.3.1 The accumulate option The accumulate option allows you to build more complex hypotheses based 011 the prior test command. For example, we begin with a test of $H_0:\\beta_k5 = \\beta_k618$: test k5=k618 ( 1) [lfp]k5 - [lfp]k618 = 0\rchi2( 1) = 45.07\rProb \u003e chi2 = 0.0000\rNext, add the constraint that $\\beta_wc = \\beta_hc$ test 1.wc=1.hc, accumulate ( 1) [lfp]k5 - [lfp]k618 = 0\r( 2) [lfp]1.wc - [lfp]1.hc = 0\rchi2( 2) = 47.63\rProb \u003e chi2 = 0.0000\rThis results in a test of $H_0:\\beta_k5 = \\beta_k618$, $\\beta_wc = \\beta_hc$. Instead of using the accumulate option, we could have used a single test command with multiple restrictions: test (k5=k618) ( 1.wc = 1.hc ). ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:5:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.4 LR tests with Irtest lrtest compares nested models by using an LR test. The syntax is where model-one and model-two are the names of estimation results stored by estimates store. When model-two is not specified, the most recent estimation results are used in its place. Typically, we begin by fitting the full or unconstrained model, and then we store the results. For example, Irtest model-one [ model-two] logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog estimates store full Logistic regression Number of obs = 753\rLR chi2(8) = 124.30\rProb \u003e chi2 = 0.0000\rLog likelihood = -452.72367 Pseudo R2 = 0.1207\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5 | -1.39 0.19 -7.25 0.000 -1.77 -1.02\rk618 | -0.07 0.07 -0.96 0.336 -0.20 0.07\r|\ragecat |\r40-49 | -0.63 0.21 -3.00 0.003 -1.04 -0.22\r50+ | -1.28 0.26 -4.92 0.000 -1.79 -0.77\r|\rwc |\rcollege | 0.80 0.23 3.48 0.001 0.35 1.25\r|\rhc |\rcollege | 0.14 0.21 0.66 0.508 -0.27 0.54\rlwg | 0.61 0.15 4.04 0.000 0.31 0.91\rinc | -0.04 0.01 -4.24 0.000 -0.05 -0.02\r_cons | 1.01 0.29 3.54 0.000 0.45 1.57\r------------------------------------------------------------------------------\rwhere full is the name we chose for the estimation results from the full model.9 After we store the results, we fit a model that is nested in the full model. A nested model is one that can be created by imposing constraints on the coefficients in the prior model. Most commonly, some of the variables from the full model are excluded, which in effect constrains the coefficients for these variables to be 0. For example, if we drop k5 and k618 from the last model, this produces logit lfp i.agecat i.wc i.hc lwg inc, nolog estimates store nokidvars Logistic regression Number of obs = 753\rLR chi2(6) = 61.59\rProb \u003e chi2 = 0.0000\rLog likelihood = -484.07589 Pseudo R2 = 0.0598\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\ragecat |\r40-49 | -0.02 0.18 -0.10 0.922 -0.36 0.33\r50+ | -0.48 0.21 -2.36 0.018 -0.89 -0.08\r|\rwc |\rcollege | 0.66 0.22 3.06 0.002 0.24 1.09\r|\rhc |\rcollege | 0.03 0.20 0.18 0.859 -0.35 0.42\rlwg | 0.61 0.15 4.18 0.000 0.32 0.89\rinc | -0.03 0.01 -4.37 0.000 -0.05 -0.02\r_cons | 0.22 0.22 1.01 0.312 -0.21 0.65\r------------------------------------------------------------------------------\rWe stored the results for the nested models as nokidvars. Next, we com pute the test: lrtest full nokidvars Likelihood-ratio test\rAssumption: nokidvars nested within full\rLR chi2(2) = 62.70\rProb \u003e chi2 = 0.0000\rThe output indicates that the LR test assumes that nokidvars is nested in full. It is up to the user to ensure that the models are nested. Because our models are nested, the result is an LR test of the hypothesis Ho: $H_0:\\beta_k5 = \\beta_k618 = 0$. The significant chi-squared statistic means that we reject the null hypothesis that these two coefficients are simultaneously equal to 0. Although we fit the full model first followed by the constrained model, lrtest allows the constrained model to be fit first followed by the full model. The output for all models fit by maximum likelihood includes an LR test that all the coefficients except the intercept(s) are 0. For our full model above, this is listed as LR chi2(8) = 124.30. The results can be computed with lrtest as follows: logit lfp, nolog Logistic regression Number of obs = 753\rLR chi2(0) = 0.00\rProb \u003e chi2 = .\rLog likelihood = -514.8732 Pseudo R2 = 0.0000\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_cons | 0.28 0.07 3.74 0.000 0.13 0.42\r--------------------------------------","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:6:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.4.1 Avoiding invalid LR tests lrtest does not always prevent you from computing an invalid test. There are two things that you must check: that the two models are nested and that the two models were fit using the same sample. In general, if either of these conditions is violated, the results of lrtest are meaningless. Although lrtest exits with an error message if the number of observations differs in the two models, this check does not catch those cases in which the number of observations is the same but the samples are different. One exception to the requirement of equal sample sizes is when perfect prediction removes some observations. In such a case, the apparent sample sizes for nested models differ, but an LR test is still appropriate (see section 5.2.3 for details). When this occurs, the force option can be used to force lrtest to compute the seemingly invalid test. For details on ensuring the same sample size, see our discussion of mark and markout in section 3.1.6. ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:6:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3 Measures of fit Assessing fit involves both the analysis of the fit of individual observations and the evaluation of scalar measures of fit for the model as a whole. Many scalar measures have been developed to summarize the overall goodness of fit of regression models. A scalar measure can in some cases be useful in comparing competing models and. ultimately, in selecting a final model. W ithin a substantive area, measures of fit might provide a rough index of whether a model is adequate. However, there is no convincing evidence that selecting a model th a t maximizes the value of a given measure results in a model th at is optimal in any sense other than the m odel’s having a larger (or, in some instances, smaller) value of th a t measure. Measures of fit provide some information, but it is partial information th at must be assessed within the context of the theory motivating the analysis, past research, and the estim ated parameters of the model being considered. ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:7:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3.1 Syntax of fitstat The SPost fitstat command calculates many fit statistics for the estimation commands in this book. We should mention again that we often find these measures of limited utility in our own research, with the exception of the information criteria BIC and AIC. When we do use these measures, we find it helpful to compare multiple measures, fitstat makes this simple. The options diff, saving (), and using () facilitate the comparison of measures across two models. Although fitstat duplicates some measures computed by other Stata commands (for example, the pseudo-R² in standard Stata output and the information criteria from estat ic), fitstat adds many more measures and makes it convenient to compare measures across models. The syntax is fitstat[, saving(name) using(name) ic force diff] fitstat terminates with an error if the last estimation command does not return a value for the log-likelihood function for a model with only an intercept (that is, if e(ll_0) is missing). This occurs, for example, if the noconstant option is used to fit a model. Although fitstat can be used when models are fit with weighted data, there are two limitations. First, some measures cannot be computed with some types of weights and none can be computed after svy estimation. Second, when pweights or robust standard errors are used to fit the model, fitstat uses the “pseudolikelihood” rather than the likelihood to compute measures of fit. Given the heuristic nature of the various measures of fit, we see no reason why the resulting measures would be inappropriate. Options fitstat[, saving(name)] saves the computed measures in a matrix, _fitstat_name, for later comparisons. When the saving() option is not used, fitstat saves results to the matrix _fitstat_0. using (name) compares the measures for the model in memory, referred to in the output as the current model, with those of the model saved as name. diff compares the current model to the prior model. ic presents only the Bayesian information criterion (BIC) and Akaike’s information criterion (AlC). When comparing two models, fitstat reports Raftery’s (1995) guidelines for assessing the strength of one model over another with BIC. force is required to compare information criteria when the number of observations or the estimation m ethod varies between the two models, or to conduct a likelihood ratio test under circumstances in which Stata’s lrtest command would require the force option. ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:7:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3.2 Methods and formulas used by fitstat In this section, we provide brief descriptions of each measure computed by fitstat. Full details for most measures along with citations to original sources are in Long (1997). We begin with formulas for several quantities that are used in the computation of other measures. We then consider the information criteria BIC and AIC. Again, these are the measures that we find most useful in practice. We then review the coefficient of determination $R^2$ for the linear regression model followed by numerous pseudo-R²s. 3.2.1 Quantities used in other measures Log-likelihood based measures. Stata begins maximum likelihood iterations by computing the log likelihood of the model with all parameters but the intercept constrained to 0, referred to as $\\ln L\\left(M_{\\text {Intercept }}\\right)$ The log likelihood upon convergence, referred to as $\\ln L\\left(M_{\\text {Full }}\\right)$, is also listed. This information is presented in the iteration log and in the head er for the estim ation results. LR chi-square test of all coefficients. An LR test of the hypothesis that all coefficients except the intercepts are 0 can be computed by comparing the log likelihoods: $LR = 2 \\ln L\\left(M_{\\text {Full }}\\right)-2 \\ln L\\left(M_{\\text {Intercept }}\\right)$LR is reported by Stata as $ LR chi2 (df) = #$, where the degrees of freedom in parentheses are the number of constrained parameters. For the zip and zinb models discussed in chapter 9, LR tests that the coefficients in the count portion (not the binary portion) of the model are 0. Deviance. The deviance compares the given model with a model that has one parameter for each observation so th at the model reproduces the observed d ata perfectly. The deviance is defined as $D=-2 \\ln L\\left(M_{\\text {Full }}\\right)$, where the degrees of freedom equals $N$ m inus the number of param eters. $D$ does not have a. chi-squared distribution. 3.2.2 Information criteria Information measures can be used to compare both nested and nonnested models. AIC. The formula for Akaike’s information criterion (1973) used by fitstat and Stata’s estat ic command is $$ \\mathrm{AIC}=-2 \\ln \\widehat{L}\\left(M_k\\right)+2 P_k $$ where $\\widehat{L}\\left(M_k\\right)$ is the likelihood of model $ M_k$ and $P_k$ is the number of parameters in the model (for example, $K$ + 1 in the binary regression model, where $K$ is the number of regressors). All else being equal, the model with the smaller AIC is considered the better-fitting model. Another definition of AIC is equal to the value in (3.1) divided by $N$ . We include this quantity in the $fitstat$ output as AIC divided by N. BIC. The Bayesian information criterion (BIC) was proposed by Raftery (1995) and others as a means to compare nested and nonnested models. Because BIC imposes a greater penalty for the number of parameters in a model, it favors a simpler mode, compared with the AIC measure. The BIC statistic is defined in at least three ways. Although this can be confusing the choice of which version to use is not important, as we show after presenting thvarious definitions. Stata defines the BIC for model $M_k$ as $$\\mathrm{BlC_k} = -2:\\ln\\hat{L}(M_{k})+\\mathrm{df}_{k}\\ \\ln N$$ where $df_{k}$ is the number of param eters in $M_k$, including auxiliary parameters such as a in the negative binomial regression model. As with AIC, the smaller or more negativ the BIC, the better the fit. A second definition of BIC is computed using the deviance $$\\mathrm{BIC_k}^{D}=D(M_{k})+\\mathrm{df}_{k}^{D}\\ \\ln N$$ where $df_{k}$ is the degrees of freedom associated with the deviance, fitstat labels this as BIC (based on deviance). The third version, sometimes denoted as BIC’, uses the LR chi-squared with $df_{k} equal to the number of regressors (not parameters) in the model. $$\\mathrm{BIC_k}^{\\prime}=-G^{2}(M_{k})+\\mathrm{df}_{k}^{\\prime}\\ \\ln N$$ The difference in the BICs from two models indicates which model is preferred. Because $\\mathrm{BIC_1}-\\mathr","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:7:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3.3 Example of fitstat To examine all the measures of fit, we repeat our example for information criteria, but this time we use fitstat without the ic option. We fit our base model and save the fitstat results with the name basemodel: use binlfp4, clear logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog fitstat, ic saving(basemodel) Next, we fit a model that includes the variable kid s, which is the sum of k5 and k618. and drops k5 and k618. fitstat compares this model with the saved model: generate kids = k5 + k618 label var kids \"Number of kids 18 or younger\" logit lfp kids i.agecat i.wc i.hc lwg inc, nolog fitstat,using(basemodel) | Current Saved Difference -------------------------+--------------------------------------\rLog-likelihood | Model | -478.684 -452.724 -25.960 Intercept-only | -514.873 -514.873 0.000 -------------------------+--------------------------------------\rChi-square | D(df=745/744/1) | 957.368 905.447 51.921 LR(df=7/8/-1) | 72.378 124.299 -51.921 p-value | 0.000 0.000 0.000 -------------------------+--------------------------------------\rR2 | McFadden | 0.070 0.121 -0.050 McFadden(adjusted) | 0.055 0.103 -0.048 McKelvey \u0026 Zavoina | 0.125 0.215 -0.090 Cox-Snell/ML | 0.092 0.152 -0.061 Cragg-Uhler/Nagelkerke | 0.123 0.204 -0.081 Efron | 0.090 0.153 -0.063 Tjur's D | 0.091 0.153 -0.063 Count | 0.633 0.676 -0.042 Count(adjusted) | 0.151 0.249 -0.098 -------------------------+--------------------------------------\rIC | AIC | 973.368 923.447 49.921 AIC divided by N | 1.293 1.226 0.066 BIC(df=8/9/-1) | 1010.361 965.064 45.297 -------------------------+--------------------------------------\rVariance of | e | 3.290 3.290 0.000 y-star | 3.761 4.192 -0.431 Note: Likelihood-ratio test assumes current model nested in saved model.\rDifference of 45.297 in BIC provides very strong support for saved model.\rIn this example, the two models are nested because the second model is in effect imposing the constraint $\\mathrm{t~}\\beta_{\\mathrm{k5}}=\\beta_{\\mathrm{k618}}$ on the first model. ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:7:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4 estat postestimation commands estat is a set of subcommands that provide different statistics about the model whose estimates are active. Each is invoked using estat subcommand. Here we provide an overview of some of the most useful subcommands, which we use in later chapters estat summarize estat summarize provides descriptive statistics for the variables in the model by using the estimation sample. For example, use binlfp4, clear logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog Logistic regression Number of obs = 753\rLR chi2(8) = 124.30\rProb \u003e chi2 = 0.0000\rLog likelihood = -452.72367 Pseudo R2 = 0.1207\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5 | -1.39 0.19 -7.25 0.000 -1.77 -1.02\rk618 | -0.07 0.07 -0.96 0.336 -0.20 0.07\r|\ragecat |\r40-49 | -0.63 0.21 -3.00 0.003 -1.04 -0.22\r50+ | -1.28 0.26 -4.92 0.000 -1.79 -0.77\r|\rwc |\rcollege | 0.80 0.23 3.48 0.001 0.35 1.25\r|\rhc |\rcollege | 0.14 0.21 0.66 0.508 -0.27 0.54\rlwg | 0.61 0.15 4.04 0.000 0.31 0.91\rinc | -0.04 0.01 -4.24 0.000 -0.05 -0.02\r_cons | 1.01 0.29 3.54 0.000 0.45 1.57\r------------------------------------------------------------------------------\restat summarize Estimation sample logit Number of obs = 753\r-------------------------------------------------------------------\rVariable | Mean Std. dev. Min Max\r-------------+-----------------------------------------------------\rlfp | .5683931 .4956295 0 1\rk5 | .2377158 .523959 0 3\rk618 | 1.353254 1.319874 0 8\r|\ragecat |\r40-49 | .3851262 .4869486 0 1\r50+ | .2191235 .4139274 0 1\r|\rwc |\rcollege | .2815405 .4500494 0 1\r|\rhc |\rcollege | .3917663 .4884694 0 1\rlwg | 1.097115 .5875564 -2.054124 3.218876\rinc | 20.12897 11.6348 -.0290001 96\r-------------------------------------------------------------------\rThe output is equivalent to the results from summarize modelvars if e(sample) == 1, where modelvars is the list of variables in your model. Several options are useful: labels displays variable labels rather than the names of the variables. noheader suppresses the header. noweights ignores the weights if they have been used in estimation. estat ic estat ic lists the inform ation criteria AIC and BIC for the last model. See page 123 for details estat vce estat vce lists the variance-covariance matrix for the coefficient estimates. For further details, see help estat vce ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:8:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"The chapter briefly illustrates the mechanics of using these commands in the context of a complex survey","date":"2024-01-16","objectID":"/19.chapter19complex-survey-data/","tags":["Interaction","stata"],"title":"Chapter19 ：Complex survey data","uri":"/19.chapter19complex-survey-data/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"The chapter briefly illustrates the mechanics of using these commands in the context of a complex survey The example dataset used in this chapter is the nhanes2.dta dataset. This is one of the Stata example datasets and is used via the Internet with the webuse command, shown below. webuse nhanes2 svyset The svyset command has already been used to declare the design for this survey, naming the primary sampling unit, the person weight, and the strata. Sampling weights: finalwgt\rVCE: linearized\rSingle unit: missing\rStrata 1: strata\rSampling unit 1: psu\rFPC 1: \u003czero\u003e\rLet’s now perform a regression analysis using this dataset. Let’s predict systolic blood pressure from the person’s age (in six age groups), sex, and weight. We use the svy prefix before the regress command to account for the survey design as specified by the svyset command. svy:regress bpsystol i.agegrp i.sex c.weight Survey: Linear regression\rNumber of strata = 31 Number of obs = 10,351\rNumber of PSUs = 62 Population size = 117,157,513\rDesign df = 31\rF(7, 25) = 328.16\rProb \u003e F = 0.0000\rR-squared = 0.3087\r------------------------------------------------------------------------------\r| Linearized\rbpsystol | Coefficient std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\ragegrp |\r30–39 | 1.20 0.57 2.11 0.043 0.04 2.37\r40–49 | 6.88 0.72 9.57 0.000 5.41 8.35\r50–59 | 16.04 0.71 22.44 0.000 14.58 17.49\r60–69 | 23.38 0.77 30.26 0.000 21.81 24.96\r70+ | 30.28 0.90 33.83 0.000 28.46 32.11\r|\rsex |\rFemale | -0.65 0.54 -1.20 0.239 -1.75 0.45\rweight | 0.43 0.02 24.85 0.000 0.39 0.46\r_cons | 88.07 1.32 66.49 0.000 85.36 90.77\r------------------------------------------------------------------------------\rWe can use the contrast, pwcompare, margins, and marginsplot commands to interpret these results. The use of these commands is briefly illustrated below. The contrast command can be used to make comparisons among the groups formed by a factor variable. The contrast command below tests the equality of the adjusted means for the six age groups. The test shows that the average systolic blood pressure is not equal among the six age groups. contrast agegrp Contrasts of marginal linear predictions\rDesign df = 31\rMargins: asbalanced\r------------------------------------------------\r| df F P\u003eF\r-------------+----------------------------------\ragegrp | 5 297.86 0.0000\rDesign | 31\r------------------------------------------------\rNote: F statistics are adjusted for the survey\rdesign.\rThe output of the contrast command indicates the $F$ test is adjusted for the survey design. If you wanted to omit the adjustment for the design degrees of freedom, youcould add the nosvyadjust option, as shown below. (See [R] contrast for more details about this option.) contrast agegrp,nosvyadjust Contrasts of marginal linear predictions\rDesign df = 31\rMargins: asbalanced\r------------------------------------------------\r| df F P\u003eF\r-------------+----------------------------------\ragegrp | 5 341.99 0.0000\r|\rDesign | 31\r------------------------------------------------\rThe pwcompare command can also be used to form pairwise comparisons among the different age groups. In the example below, the mcompare(sidak) option is included to adjust for multiple comparisons. pwcompare agegrp,pveffects mcompare(sidak) Pairwise comparisons of marginal linear predictions\rDesign df = 31\rMargins: asbalanced\r---------------------------\r| Number of\r| comparisons\r-------------+-------------\ragegrp | 15\r---------------------------\r--------------------------------------------------------\r| Sidak\r| Contrast Std. err. t P\u003e|t|\r----------------+---------------------------------------\ragegrp |\r30–39 vs 20–29 | 1.20 0.57 2.11 0.482\r40–49 vs 20–29 | 6.88 0.72 9.57 0.000\r50–59 vs 20–29 | 16.04 0.71 22.44 0.000\r60–69 vs 20–29 | 23.38 0.77 30.26 0.000\r70+ vs 20–29 | 30.28 0.90 33.83 0.000\r40–49 vs 30–39 | 5.68 0.65 8.76 0.000\r50–59 vs 30–39 | 14.83 0.76 19.51 0.000\r60–69 vs 30–39 | 22.","date":"2024-01-16","objectID":"/19.chapter19complex-survey-data/:0:0","tags":["Interaction","stata"],"title":"Chapter19 ：Complex survey data","uri":"/19.chapter19complex-survey-data/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"The chapter begins with a discussion of logistic regression models(most detailed)","date":"2024-01-15","objectID":"/18.chapter18nonlinear-models/","tags":["Interaction","stata"],"title":"Chapter18 ：Nonlinear models","uri":"/18.chapter18nonlinear-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"The chapter begins with a discussion of logistic regression models(most detailed) ","date":"2024-01-15","objectID":"/18.chapter18nonlinear-models/:0:0","tags":["Interaction","stata"],"title":"Chapter18 ：Nonlinear models","uri":"/18.chapter18nonlinear-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 Binary logistic regression ","date":"2024-01-15","objectID":"/18.chapter18nonlinear-models/:1:0","tags":["Interaction","stata"],"title":"Chapter18 ：Nonlinear models","uri":"/18.chapter18nonlinear-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.1 A logistic model with one categorical predictor Let’s consider a simple logistic regression model that predicts whether a person smokes (smoke) by the person’s self-reported social class (class). The variable class is a categorical variable that is coded: 1 = lower class, 2 = working class, 3 = middle class, and 4 = upper class. logit smoke i.class,nolog Logistic regression Number of obs = 15,464\rLR chi2(3) = 198.45\rProb \u003e chi2 = 0.0000\rLog likelihood = -9904.5707 Pseudo R2 = 0.0099\r--------------------------------------------------------------------------------\rsmoke | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r---------------+----------------------------------------------------------------\rclass |\rworking class | -0.32 0.07 -4.37 0.000 -0.46 -0.18\rmiddle class | -0.72 0.07 -9.80 0.000 -0.86 -0.58\rupper class | -0.92 0.12 -7.54 0.000 -1.16 -0.68\r|\r_cons | -0.13 0.07 -1.85 0.064 -0.26 0.01\r--------------------------------------------------------------------------------\rWe can interpret and visualize the results of this model using the contrast, pwcompare, margins, and marginsplot commands, as described in the following sections. 1.1.1 Using the contrast command If we want to test the overall equality of the four social class groups in terms of their log odds of smoking. logit smoke i.class,nolog Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------\r| df chi2 P\u003echi2\r-------------+----------------------------------\rclass | 3 196.71 0.0000\r------------------------------------------------\rWe can also apply contrast operators to form specific comparisons among the levels of the social class. contrast ar.class,nowald pveffects //(adjacent (previous) level) Contrasts of marginal linear predictions\rMargins: asbalanced\r-------------------------------------------------------------------------\r| Contrast Std. err. z P\u003e|z|\r---------------------------------+---------------------------------------\rclass |\r(working class vs lower class) | -0.32 0.07 -4.37 0.000\r(middle class vs working class) | -0.40 0.04 -11.26 0.000\r(upper class vs middle class) | -0.20 0.10 -1.92 0.055\r-------------------------------------------------------------------------\rThis test shows that the four social class groups are not all equal in terms of their log odds of smoking We can add the or option to the contrast command to display the results as odds ratios. contrast ar.class,nowald pveffects or Contrasts of marginal linear predictions\rMargins: asbalanced\r-------------------------------------------------------------------------\r| Odds ratio Std. err. z P\u003e|z|\r---------------------------------+---------------------------------------\rclass |\r(working class vs lower class) | 0.73 0.05 -4.37 0.000\r(middle class vs working class) | 0.67 0.02 -11.26 0.000\r(upper class vs middle class) | 0.82 0.09 -1.92 0.055\r-------------------------------------------------------------------------\rthe results can now be interpreted using odds ratios. For example, the odds of smoking for a person who identifies as middle class is 0.669 times the odds of smoking for someone who identifies as working class. 1.1.2 Using the pwcompare command We can also use the pwcompare command to form comparisons among the levels of the social class. Like the contrast command, these comparisons are made in the logodds metric. pwcompare class,pveffects Pairwise comparisons of marginal linear predictions\rMargins: asbalanced\r-----------------------------------------------------------------------\r| Unadjusted\r| Contrast Std. err. z P\u003e|z|\r-------------------------------+---------------------------------------\rsmoke |\rclass |\rworking class vs lower class | -0.32 0.07 -4.37 0.000\rmiddle class vs lower class | -0.72 0.07 -9.80 0.000\rupper class vs lower class | -0.92 0.12 -7.54 0.000\rmiddle class vs working class | -0.40 0.04 -11.26 0.000\rupper class vs working class | -0.60 0.10 -5.80 0.000\rupper class vs middle class | -0.20 0.10 -1.92 0.055\r----","date":"2024-01-15","objectID":"/18.chapter18nonlinear-models/:1:1","tags":["Interaction","stata"],"title":"Chapter18 ：Nonlinear models","uri":"/18.chapter18nonlinear-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.2 A logistic model with one continuous predictor Let’s now briefly consider a model with one continuous predictor, predicting whether a person smokes (smoke) from his or her education level. use gss_ivrm.dta logit smoke educ,nolog Logistic regression Number of obs = 16,332\rLR chi2(1) = 174.04\rProb \u003e chi2 = 0.0000\rLog likelihood = -10483.854 Pseudo R2 = 0.0082\r------------------------------------------------------------------------------\rsmoke | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\reduc | -0.07 0.01 -13.10 0.000 -0.08 -0.06\r_cons | 0.22 0.07 3.36 0.001 0.09 0.35\r------------------------------------------------------------------------------\rLet’s use the margins and marginsplot commands to visualize the relationship between education and the log odds of smoking. (Note the inclusion of the predict(xb) option on the margins command to specify the use of the log-odds metric.) margins,at(educ=(5(1)20))predict(xb) Adjusted predictions Number of obs = 16,332\rModel VCE: OIM\rExpression: Linear prediction (log odds), predict(xb)\r1._at: educ = 5\r2._at: educ = 6\r3._at: educ = 7\r4._at: educ = 8\r5._at: educ = 9\r6._at: educ = 10\r7._at: educ = 11\r8._at: educ = 12\r9._at: educ = 13\r10._at: educ = 14\r11._at: educ = 15\r12._at: educ = 16\r13._at: educ = 17\r14._at: educ = 18\r15._at: educ = 19\r16._at: educ = 20\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_at |\r1 | -0.12 0.04 -2.95 0.003 -0.20 -0.04\r2 | -0.19 0.04 -5.22 0.000 -0.26 -0.12\r3 | -0.26 0.03 -8.13 0.000 -0.32 -0.20\r4 | -0.33 0.03 -11.91 0.000 -0.38 -0.27\r5 | -0.40 0.02 -16.84 0.000 -0.44 -0.35\r6 | -0.46 0.02 -23.09 0.000 -0.50 -0.42\r7 | -0.53 0.02 -30.20 0.000 -0.57 -0.50\r8 | -0.60 0.02 -36.37 0.000 -0.63 -0.57\r9 | -0.67 0.02 -39.31 0.000 -0.70 -0.64\r10 | -0.74 0.02 -38.80 0.000 -0.78 -0.70\r11 | -0.81 0.02 -36.50 0.000 -0.85 -0.76\r12 | -0.88 0.03 -33.82 0.000 -0.93 -0.82\r13 | -0.94 0.03 -31.36 0.000 -1.00 -0.88\r14 | -1.01 0.03 -29.27 0.000 -1.08 -0.94\r15 | -1.08 0.04 -27.52 0.000 -1.16 -1.00\r16 | -1.15 0.04 -26.08 0.000 -1.24 -1.06\r------------------------------------------------------------------------------\rmarginsplot Note how the relationship between education and the log odds of smoking is linear. For every additional year of education, the log odds of smoking decreases by 0.07. Log odds of smoking by education level\rLet’s now visualize this relationship in terms of the probability of smoking. margins,at(educ=(5(1)20)) Adjusted predictions Number of obs = 16,332\rModel VCE: OIM\rExpression: Pr(smoke), predict()\r1._at: educ = 5\r2._at: educ = 6\r3._at: educ = 7\r4._at: educ = 8\r5._at: educ = 9\r6._at: educ = 10\r7._at: educ = 11\r8._at: educ = 12\r9._at: educ = 13\r10._at: educ = 14\r11._at: educ = 15\r12._at: educ = 16\r13._at: educ = 17\r14._at: educ = 18\r15._at: educ = 19\r16._at: educ = 20\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_at |\r1 | 0.47 0.01 45.90 0.000 0.45 0.49\r2 | 0.45 0.01 50.26 0.000 0.44 0.47\r3 | 0.44 0.01 55.77 0.000 0.42 0.45\r4 | 0.42 0.01 62.72 0.000 0.41 0.43\r5 | 0.40 0.01 71.30 0.000 0.39 0.41\r6 | 0.39 0.00 81.08 0.000 0.38 0.40\r7 | 0.37 0.00 90.01 0.000 0.36 0.38\r8 | 0.35 0.00 93.68 0.000 0.35 0.36\r9 | 0.34 0.00 88.76 0.000 0.33 0.35\r10 | 0.32 0.00 77.69 0.000 0.32 0.33\r11 | 0.31 0.00 65.44 0.000 0.30 0.32\r12 | 0.29 0.01 54.74 0.000 0.28 0.30\r13 | 0.28 0.01 46.16 0.000 0.27 0.29\r14 | 0.27 0.01 39.41 0.000 0.25 0.28\r15 | 0.25 0.01 34.10 0.000 0.24 0.27\r16 | 0.24 0.01 29.87 0.000 0.22 0.26\r------------------------------------------------------------------------------\rmarginsplot Predicted probability of smoking by education level\r","date":"2024-01-15","objectID":"/18.chapter18nonlinear-models/:1:2","tags":["Interaction","stata"],"title":"Chapter18 ：Nonlinear models","uri":"/18.chapter18nonlinear-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.3 A logistic model with covariates Let’s add some covariates to this model, predicting smoking from class as well as education, age, and year of interview logit smoke i.class educ age yrint,nolog Logistic regression Number of obs = 15,375\rLR chi2(6) = 742.33\rProb \u003e chi2 = 0.0000\rLog likelihood = -9580.0723 Pseudo R2 = 0.0373\r--------------------------------------------------------------------------------\rsmoke | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r---------------+----------------------------------------------------------------\rclass |\rworking class | -0.26 0.07 -3.47 0.001 -0.41 -0.11\rmiddle class | -0.46 0.08 -5.89 0.000 -0.61 -0.30\rupper class | -0.52 0.13 -4.11 0.000 -0.77 -0.27\r|\reduc | -0.09 0.01 -13.88 0.000 -0.10 -0.08\rage | -0.02 0.00 -18.12 0.000 -0.02 -0.02\ryrint | -0.03 0.00 -9.48 0.000 -0.04 -0.03\r_cons | 65.46 6.72 9.74 0.000 52.29 78.63\r--------------------------------------------------------------------------------\rcontrast class //test the overall effect of class Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------\r| df chi2 P\u003echi2\r-------------+----------------------------------\rclass | 3 49.68 0.0000\r------------------------------------------------\rWe can use the margins command to help us interpret this effect by computing the predictive margins of the probability of smoking by class, as shown below. margins class,nopvalues Predictive margins Number of obs = 15,375\rModel VCE: OIM\rExpression: Pr(smoke), predict()\r----------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r---------------+------------------------------------------------\rclass |\rlower class | 0.43 0.02 0.39 0.46\rworking class | 0.37 0.01 0.36 0.38\rmiddle class | 0.32 0.01 0.31 0.34\rupper class | 0.31 0.02 0.27 0.35\r----------------------------------------------------------------\rWe can use marginsplot command to graph these predictive margins marginsplot,xlabel(,angle(45)) The predictive marginal probability of smoking by class(adding covariance)\rIn the predicted probability metric, the size of the effect of a variable can (and will) vary as a function of the value of the covariates. By comparison, in the logit metric (like any linear model), the size of the effect of a variable remains constant regardless of the values of the covariate. Let’s explore this point by using the contrast command to estimate the effect of class. contrast ar.class,nowald pveffects //Compare each level of class with the previous level of class Contrasts of marginal linear predictions\rMargins: asbalanced\r-------------------------------------------------------------------------\r| Contrast Std. err. z P\u003e|z|\r---------------------------------+---------------------------------------\rclass |\r(working class vs lower class) | -0.26 0.07 -3.47 0.001\r(middle class vs working class) | -0.19 0.04 -5.14 0.000\r(upper class vs middle class) | -0.07 0.11 -0.64 0.523\r-------------------------------------------------------------------------\rThese differences are computed and expressed in the log-odds metric, the natural (linear) metric for the model. The magnitude of these group differences and their significance would remain constant at any level of the covariates. Let’s form these same comparisons but instead using the margins command, forming the comparisons using the predicted probability metric. margins ar.class,contrast(nowald pveffects) Contrasts of predictive margins Number of obs = 15,375\rModel VCE: OIM\rExpression: Pr(smoke), predict()\r-------------------------------------------------------------------------\r| Delta-method\r| Contrast std. err. z P\u003e|z|\r---------------------------------+---------------------------------------\rclass |\r(working class vs lower class) | -0.06 0.02 -3.41 0.001\r(middle class vs working class) | -0.04 0.01 -5.14 0.000\r(upper class vs middle class) | -0.01 0.02 -0.65 0.519\r--------------------------------------------------","date":"2024-01-15","objectID":"/18.chapter18nonlinear-models/:1:3","tags":["Interaction","stata"],"title":"Chapter18 ：Nonlinear models","uri":"/18.chapter18nonlinear-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2 Multinomial logistic regression Let’s now consider a multinomial logistic regression model, focusing on how the commands contrast, pwcompare, margins, and marginsplot can be used after fitting such a model. Let’s model this happiness rating as a function of gender, class, education, and year of interview. The mlogit command chooses the most frequent outcome (which was the second outcome, pretty happy) as the base outcome. use gss_ivrm.dta mlogit haprate i.gender i.class educ yrint,nolog Multinomial logistic regression Number of obs = 48,409\rLR chi2(12) = 2076.07\rProb \u003e chi2 = 0.0000\rLog likelihood = -44799.143 Pseudo R2 = 0.0226\r--------------------------------------------------------------------------------\rhaprate | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r---------------+----------------------------------------------------------------\rnot_too_happy |\rgender |\rFemale | 0.02 0.03 0.67 0.502 -0.04 0.08\r|\rclass |\rworking class | -0.99 0.05 -20.60 0.000 -1.08 -0.90\rmiddle class | -1.19 0.05 -23.33 0.000 -1.29 -1.09\rupper class | -0.89 0.10 -8.65 0.000 -1.09 -0.69\r|\reduc | -0.08 0.00 -16.13 0.000 -0.09 -0.07\ryrint | 0.00 0.00 2.55 0.011 0.00 0.01\r_cons | -6.38 2.65 -2.41 0.016 -11.59 -1.18\r---------------+----------------------------------------------------------------\rpretty_happy | (base outcome)\r---------------+----------------------------------------------------------------\rvery_happy |\rgender |\rFemale | 0.07 0.02 3.47 0.001 0.03 0.11\r|\rclass |\rworking class | 0.33 0.06 5.82 0.000 0.22 0.44\rmiddle class | 0.74 0.06 12.91 0.000 0.63 0.85\rupper class | 1.18 0.08 15.25 0.000 1.03 1.33\r|\reduc | -0.00 0.00 -0.04 0.966 -0.01 0.01\ryrint | -0.01 0.00 -6.59 0.000 -0.01 -0.00\r_cons | 11.27 1.88 5.99 0.000 7.58 14.96\r--------------------------------------------------------------------------------\rI find it hard to interpret the model using the coefficients. I find it far easier to interpret the results using the contrast, pwcompare, margins, and marginsplot commands. We can use the contrast command to obtain the overall effect of class. By default, this test is performed for the first equation (that is, for outcome 1, not too happy). This test is significant. contrast class Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------\r| df chi2 P\u003echi2\r-------------+----------------------------------\rnot_too_ha~y |\rclass | 3 558.71 0.0000\r------------------------------------------------\rAdding the equation(3) option performs the contrast with respect to the third outcome (that is, very happy). This test is also significant. contrast class,equation(3) Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------\r| df chi2 P\u003echi2\r-------------+----------------------------------\rvery_happy |\rclass | 3 564.26 0.0000\r------------------------------------------------\rThe atequations option can be used to apply the contrast command with respect to all the equations. The output of this command matches what we saw in the previous two contrast commands. contrast class,atequations Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------\r| df chi2 P\u003echi2\r-------------+----------------------------------\rnot_too_ha~y |\rclass | 3 558.71 0.0000\r-------------+----------------------------------\rpretty_happy |\rclass | (omitted)\r-------------+----------------------------------\rvery_happy |\rclass | 3 564.26 0.0000\r------------------------------------------------\rWe can use contrast operators with the contrast command to make specific comparisons among groups. In the example below, the ar. contrast operator is used to compare adjacent levels of class for the third equation. I also included the rrr option to interpret the results in terms of relative-risk ratios. Each of these contrasts is significant. contrast ar.class,equation(3) nowald pveffects rrr Contrasts of marginal linear predictions\rMargins: a","date":"2024-01-15","objectID":"/18.chapter18nonlinear-models/:2:0","tags":["Interaction","stata"],"title":"Chapter18 ：Nonlinear models","uri":"/18.chapter18nonlinear-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3 Ordinal logistic regression Let’s use the variable haprate from the previous section as the outcome but now model it using an ordinal logistic regression. Let’s use the ologit command to model the three-level variable haprate (1 = not too happy, 2 = pretty happy, and 3 = very happy) as a function of gender, class, education, and year of interview. use gss_ivrm.dta ologit haprate i.gender i.class educ yrint,nolog Ordered logistic regression Number of obs = 48,409\rLR chi2(6) = 1799.46\rProb \u003e chi2 = 0.0000\rLog likelihood = -44937.445 Pseudo R2 = 0.0196\r--------------------------------------------------------------------------------\rhaprate | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r---------------+----------------------------------------------------------------\rgender |\rFemale | 0.06 0.02 3.34 0.001 0.02 0.10\r|\rclass |\rworking class | 0.96 0.04 23.15 0.000 0.88 1.04\rmiddle class | 1.36 0.04 32.12 0.000 1.28 1.44\rupper class | 1.66 0.06 25.75 0.000 1.54 1.79\r|\reduc | 0.03 0.00 10.55 0.000 0.03 0.04\ryrint | -0.01 0.00 -7.84 0.000 -0.01 -0.00\r---------------+----------------------------------------------------------------\r/cut1 | -13.41 1.65 -16.64 -10.19\r/cut2 | -10.60 1.65 -13.82 -7.37\r--------------------------------------------------------------------------------\rI will bypass interpreting the coefficients and briefly illustrate the use of the contrast, pwcompare, margins, and marginsplot commands. First, let’s consider the contrast command. The contrast command below tests the overall effect of class. contrast class //test the overall effect of class Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------\r| df chi2 P\u003echi2\r-------------+----------------------------------\rhaprate |\rclass | 3 1286.31 0.0000\r------------------------------------------------\rWe can further dissect the overall effect of class through the use of contrast operators. The contrast command below uses the ar. contrast operator to compare each level of class with the previous level. contrast ar.class,nowald pveffects eform //Compare each level of class with the previous level. Contrasts of marginal linear predictions\rMargins: asbalanced\r-------------------------------------------------------------------------\r| exp(b) Std. err. z P\u003e|z|\r---------------------------------+---------------------------------------\rhaprate |\rclass |\r(working class vs lower class) | 2.61 0.11 23.15 0.000\r(middle class vs working class) | 1.50 0.03 20.95 0.000\r(upper class vs middle class) | 1.35 0.07 5.86 0.000\r-------------------------------------------------------------------------\rThe exponentiated coefficients can be very abstract. Instead, let’s compute the predictive marginal probability of being very happy (the third response) as a function of self-identified social class. The predictive marginal probability of rating oneself as very happy was 43.7% for those identifying themselves as upper class. margins class,predict(pr outcome(3)) Predictive margins Number of obs = 48,409\rModel VCE: OIM\rExpression: Pr(haprate==3), predict(pr outcome(3))\r--------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. z P\u003e|z| [95% conf. interval]\r---------------+----------------------------------------------------------------\rclass |\rlower class | 0.13 0.00 28.33 0.000 0.12 0.14\rworking class | 0.28 0.00 97.08 0.000 0.27 0.28\rmiddle class | 0.37 0.00 115.21 0.000 0.36 0.37\rupper class | 0.44 0.01 35.61 0.000 0.41 0.46\r--------------------------------------------------------------------------------\rBy applying the ar. contrast operator, we can obtain comparisons among the adjacent levels of social class. margins ar.class,predict(pr outcome(3)) contrast(pveffects nowald) //Comparisons among the adjacent levels of social class. Contrasts of predictive margins Number of obs = 48,409\rModel VCE: OIM\rExpression: Pr(haprate==3), predict(pr outcome(3))\r---------------------------------","date":"2024-01-15","objectID":"/18.chapter18nonlinear-models/:3:0","tags":["Interaction","stata"],"title":"Chapter18 ：Nonlinear models","uri":"/18.chapter18nonlinear-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4 Poisson regression Let’s now briefly consider a Poisson model, showing the use of the contrast, pwcompare, margins, and marginsplot commands following the use of the poisson command. Let’s fit a model predicting the number of children a person has from gender, class, education, and year of interview use gss_ivrm.dta poisson children i.gender i.class educ yrint Iteration 0: Log likelihood = -95914.71 Iteration 1: Log likelihood = -95914.709 Poisson regression Number of obs = 51,417\rLR chi2(6) = 5773.21\rProb \u003e chi2 = 0.0000\rLog likelihood = -95914.709 Pseudo R2 = 0.0292\r--------------------------------------------------------------------------------\rchildren | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r---------------+----------------------------------------------------------------\rgender |\rFemale | 0.13 0.01 20.47 0.000 0.12 0.14\r|\rclass |\rworking class | -0.09 0.01 -7.18 0.000 -0.12 -0.07\rmiddle class | -0.05 0.01 -3.58 0.000 -0.07 -0.02\rupper class | 0.06 0.02 2.87 0.004 0.02 0.10\r|\reduc | -0.07 0.00 -68.73 0.000 -0.07 -0.07\ryrint | -0.00 0.00 -3.75 0.000 -0.00 -0.00\r_cons | 3.68 0.58 6.38 0.000 2.55 4.82\r--------------------------------------------------------------------------------\rAs we have seen before, the contrast command can be used to test the overall effect of class. Contrasts of marginal linear predictions Margins: asbalanced\r------------------------------------------------\r| df chi2 P\u003echi2\r-------------+----------------------------------\rclass | 3 129.31 0.0000\r------------------------------------------------\rThe ar. contrast operator is used to compare adjacent levels of class, comparing each class with the previous class. contrast ar.class,nowald pveffects //compare the adjacent levels of class,comparing each class with the previous class Contrasts of marginal linear predictions\rMargins: asbalanced\r-------------------------------------------------------------------------\r| Contrast Std. err. z P\u003e|z|\r---------------------------------+---------------------------------------\rclass |\r(working class vs lower class) | -0.09 0.01 -7.18 0.000\r(middle class vs working class) | 0.05 0.01 6.69 0.000\r(upper class vs middle class) | 0.11 0.02 6.00 0.000\r-------------------------------------------------------------------------\rWe can use the pwcompare command to form pairwise comparisons among the four class groups. pwcompare class,pveffects Pairwise comparisons of marginal linear predictions\rMargins: asbalanced\r-----------------------------------------------------------------------\r| Unadjusted\r| Contrast Std. err. z P\u003e|z|\r-------------------------------+---------------------------------------\rchildren |\rclass |\rworking class vs lower class | -0.09 0.01 -7.18 0.000\rmiddle class vs lower class | -0.05 0.01 -3.58 0.000\rupper class vs lower class | 0.06 0.02 2.87 0.004\rmiddle class vs working class | 0.05 0.01 6.69 0.000\rupper class vs working class | 0.15 0.02 8.48 0.000\rupper class vs middle class | 0.11 0.02 6.00 0.000\r-----------------------------------------------------------------------\rWhen using the margins command, the default is to compute the predicted number of events. The margins command below computes the predicted number of children by class. margins class,nopvalues //compute the predicted number of children class Predictive margins Number of obs = 51,417\rModel VCE: OIM\rExpression: Predicted number of events, predict()\r----------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r---------------+------------------------------------------------\rclass |\rlower class | 2.08 0.03 2.03 2.12\rworking class | 1.89 0.01 1.87 1.91\rmiddle class | 1.98 0.01 1.96 2.00\rupper class | 2.21 0.04 2.13 2.28\r----------------------------------------------------------------\rThe margins command is used to compute the predicted number of children for those with 5 to 18 years of education. margins,at(educ=(5(1)18)) marginsplot Predictive margins Number of obs = 51,417\rMod","date":"2024-01-15","objectID":"/18.chapter18nonlinear-models/:4:0","tags":["Interaction","stata"],"title":"Chapter18 ：Nonlinear models","uri":"/18.chapter18nonlinear-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"5 More applications of nonlinear models ","date":"2024-01-15","objectID":"/18.chapter18nonlinear-models/:5:0","tags":["Interaction","stata"],"title":"Chapter18 ：Nonlinear models","uri":"/18.chapter18nonlinear-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"5.1 Categorical by categorical interaction This section illustrates a categorical by categorical interaction using a logistic regression. For this example, let’s use the variable fepol as the outcome variable. The respondent was asked if they believed that women are not suited for politics. The variable fepol is coded: 1 = yes and 0 = no. Thus using fepol as our outcome, we will model endorsement of the statement as a function of two categorical predictors: gender (gender) and a three-level measure of education (educ3). This analysis is restricted to interviews conducted between 1972 and 1980. use gss_ivrm keep if yrint \u003c= 1980 logit fepol i.educ3##gender age,nolog Logistic regression Number of obs = 5,014\rLR chi2(6) = 310.52\rProb \u003e chi2 = 0.0000\rLog likelihood = -3313.7435 Pseudo R2 = 0.0448\r------------------------------------------------------------------------------\rfepol | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\reduc3 |\rHS | -0.23 0.10 -2.26 0.024 -0.43 -0.03\rColl | -0.65 0.13 -4.99 0.000 -0.91 -0.40\r|\rgender |\rFemale | 0.19 0.10 1.94 0.052 -0.00 0.39\r|\reduc3#gender |\rHS#Female | -0.14 0.13 -1.06 0.287 -0.39 0.12\rColl#Female | -0.57 0.19 -2.97 0.003 -0.95 -0.20\r|\rage | 0.02 0.00 11.48 0.000 0.02 0.02\r_cons | -0.84 0.12 -6.99 0.000 -1.07 -0.60\r------------------------------------------------------------------------------\rThe contrast command is used to test the overall interaction of educ3 by gender. contrast educ3#gender //test the overall interaction of educ3 by gender Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------\r| df chi2 P\u003echi2\r-------------+----------------------------------\reduc3#gender | 2 8.85 0.0120\r------------------------------------------------\rTo help understand this interaction, let’s use the margins command to estimate the log odds of believing that women are not suited for politics by educ3 and gender. Then, let’s make a graph of these predicted logits using the marginsplot command. margins educ3#gender,nopvalues predict(xb) Predictive margins Number of obs = 5,014\rModel VCE: OIM\rExpression: Linear prediction (log odds), predict(xb)\r----------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r---------------+------------------------------------------------\reduc3#gender |\rnot hs#Male | 0.07 0.08 -0.08 0.22\rnot hs#Female | 0.27 0.07 0.14 0.40\rHS#Male | -0.16 0.06 -0.28 -0.03\rHS#Female | -0.10 0.05 -0.20 0.00\rColl#Male | -0.58 0.11 -0.79 -0.37\rColl#Female | -0.96 0.13 -1.21 -0.71\r----------------------------------------------------------------\rmarginsplot,legend(subtitle(Gender)) Predicted log odds of believing women are not suited for politics by gender and education\rAlthough the log-odds metric is not easy to interpret, we can still glean the trends implied by the gender by education interaction. The graph suggests that the log odds of agreeing with this statement declines with increasing education, and that this decline appears to be stronger for females than for males. test by interaction gender with comparisons of adjacent education levels，test partial interaction by applying the ar.contrast operator to educ3 and interacting that with gender. contrast ar.educ3#gender Contrasts of marginal linear predictions\rMargins: asbalanced\r-----------------------------------------------------------\r| df chi2 P\u003echi2\r------------------------+----------------------------------\reduc3#gender |\r(HS vs not hs) (joint) | 1 1.13 0.2874\r(Coll vs HS) (joint) | 1 5.60 0.0180\rJoint | 2 8.85 0.0120\r-----------------------------------------------------------\rLet’s now assess the gender difference at each level of education. We can do this by testing the simple effect of gender at each level of education using the contrast command below. contrast gender@educ3 Contrasts of marginal linear predictions\rMargins: asbalanced\r----","date":"2024-01-15","objectID":"/18.chapter18nonlinear-models/:5:1","tags":["Interaction","stata"],"title":"Chapter18 ：Nonlinear models","uri":"/18.chapter18nonlinear-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"5.2 Categorical by continuous interaction This section illustrates a categorical by continuous interaction using a binary logistic regression. The outcome for this example is the variable fepres, which is coded: 1 = would vote for a woman president and 0 = would not vote for a woman president. Let’s model this as a function of time (that is, year of interview) and education to see if the linear change in this attitude over time differed by education level. With respect to the year of interview, this question was asked in 17 different years ranging from 1972 to 1998, then it was asked again in 2008 and 2010. Because of the large 10-year gap between 1998 and 2008, we will omit the data for 2008 onward. With respect to education, let’s use the three-level categorical variable educ3, which is coded: 1 = non–high school graduate, 2 = high school graduate, and 3 = college graduate. use gss_ivrm.dta drop if yrint\u003e=2008 logit fepres i.yrint72##educ3 age gender Let’s begin by assessing the trend in the log odds of the outcome across years.we first fit a model using fepres as the outcome predicted by i.yrint72##educ3 as well as age and gender. logit fepres i.yrint72##educ3 age gender Iteration 0: Log likelihood = -9680.7297 Iteration 1: Log likelihood = -8870.722 Iteration 2: Log likelihood = -8766.7906 Iteration 3: Log likelihood = -8765.9168 Iteration 4: Log likelihood = -8765.9147 Iteration 5: Log likelihood = -8765.9147 Logistic regression Number of obs = 23,926\rLR chi2(52) = 1829.63\rProb \u003e chi2 = 0.0000\rLog likelihood = -8765.9147 Pseudo R2 = 0.0945\r-------------------------------------------------------------------------------\rfepres | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r--------------+----------------------------------------------------------------\ryrint72 |\r2 | 0.17 0.14 1.25 0.211 -0.10 0.43\r3 | 0.13 0.13 0.95 0.340 -0.13 0.39\r5 | 0.09 0.13 0.70 0.486 -0.17 0.35\r6 | 0.13 0.14 0.96 0.339 -0.14 0.40\r10 | 0.54 0.14 3.99 0.000 0.28 0.81\r11 | 0.72 0.15 4.67 0.000 0.42 1.02\r13 | 0.23 0.14 1.58 0.114 -0.05 0.51\r14 | 0.60 0.15 3.94 0.000 0.30 0.89\r16 | 0.68 0.18 3.75 0.000 0.33 1.04\r17 | 0.31 0.18 1.70 0.089 -0.05 0.67\r18 | 0.51 0.20 2.50 0.012 0.11 0.91\r19 | 0.89 0.20 4.35 0.000 0.49 1.29\r21 | 0.98 0.21 4.58 0.000 0.56 1.39\r22 | 1.17 0.18 6.36 0.000 0.81 1.52\r24 | 1.10 0.19 5.92 0.000 0.73 1.46\r26 | 1.16 0.19 6.02 0.000 0.78 1.54\r|\reduc3 |\rHS | 0.05 0.13 0.43 0.666 -0.19 0.30\rColl | 0.85 0.24 3.60 0.000 0.39 1.31\r|\ryrint72#educ3 |\r2#HS | 0.44 0.19 2.27 0.023 0.06 0.81\r2#Coll | -0.05 0.33 -0.15 0.884 -0.70 0.60\r3#HS | 0.46 0.19 2.41 0.016 0.08 0.83\r3#Coll | 0.22 0.35 0.63 0.529 -0.47 0.91\r5#HS | 0.36 0.19 1.95 0.051 -0.00 0.73\r5#Coll | 0.53 0.36 1.47 0.142 -0.18 1.23\r6#HS | 0.59 0.19 3.08 0.002 0.22 0.97\r6#Coll | 0.19 0.34 0.55 0.582 -0.48 0.86\r10#HS | 0.61 0.19 3.12 0.002 0.23 0.99\r10#Coll | 0.06 0.35 0.16 0.870 -0.62 0.74\r11#HS | 0.20 0.21 0.97 0.330 -0.20 0.61\r11#Coll | -0.16 0.34 -0.47 0.641 -0.83 0.51\r13#HS | 0.48 0.20 2.45 0.014 0.10 0.86\r13#Coll | 0.31 0.34 0.91 0.365 -0.36 0.99\r14#HS | 0.34 0.21 1.63 0.104 -0.07 0.75\r14#Coll | 0.54 0.39 1.39 0.163 -0.22 1.31\r16#HS | 0.45 0.25 1.78 0.075 -0.04 0.93\r16#Coll | 0.39 0.43 0.89 0.372 -0.46 1.23\r17#HS | 0.83 0.25 3.36 0.001 0.34 1.31\r17#Coll | 0.41 0.41 1.02 0.309 -0.38 1.21\r18#HS | 0.88 0.27 3.20 0.001 0.34 1.41\r18#Coll | 0.78 0.44 1.76 0.078 -0.09 1.64\r19#HS | 0.54 0.27 2.00 0.045 0.01 1.08\r19#Coll | 0.51 0.47 1.09 0.274 -0.41 1.43\r21#HS | 0.39 0.27 1.42 0.156 -0.15 0.92\r21#Coll | 0.31 0.43 0.72 0.470 -0.54 1.16\r22#HS | 0.40 0.23 1.70 0.089 -0.06 0.86\r22#Coll | 0.04 0.35 0.11 0.909 -0.65 0.73\r24#HS | 0.61 0.24 2.53 0.011 0.14 1.08\r24#Coll | 0.58 0.39 1.49 0.137 -0.18 1.34\r26#HS | 0.70 0.25 2.77 0.006 0.21 1.19\r26#Coll | 0.53 0.40 1.32 0.186 -0.25 1.31\r|\rage | -0.03 0.00 -23.33 0.000 -0.03 -0.02\rgender | -0.02 0.04 -0.50 0.617 -0.10 0.06\r_cons | 2.19 0.12 17.85 0.000 1.95 2.43\r-------------------------------------------------------------------------------\rWe then use t","date":"2024-01-15","objectID":"/18.chapter18nonlinear-models/:5:2","tags":["Interaction","stata"],"title":"Chapter18 ：Nonlinear models","uri":"/18.chapter18nonlinear-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"5.3 Piecewise modeling This section illustrates modeling a continuous predictor fit using piecewise modeling in the context of a logistic regression model. Let’s begin by inspecting the nature of the relationship between education and smoking status. we first fit a model predicting smoking status from education, treating education as a categorical variable. Age is also included in the model as a covariate. The output of the logit command is omitted to save space. use gss_ivrm.dta logit smoke i.educ age // treating education as a categorical variable. Iteration 0: Log likelihood = -10539.7 Iteration 1: Log likelihood = -10121.92 Iteration 2: Log likelihood = -10117.862 Iteration 3: Log likelihood = -10117.858 Iteration 4: Log likelihood = -10117.858 Logistic regression Number of obs = 16,274\rLR chi2(21) = 843.69\rProb \u003e chi2 = 0.0000\rLog likelihood = -10117.858 Pseudo R2 = 0.0400\r------------------------------------------------------------------------------\rsmoke | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\reduc |\r1 | 0.05 0.66 0.08 0.940 -1.24 1.34\r2 | -0.33 0.57 -0.58 0.563 -1.44 0.78\r3 | 0.63 0.42 1.49 0.136 -0.20 1.46\r4 | -0.05 0.41 -0.11 0.912 -0.85 0.76\r5 | 0.16 0.40 0.39 0.694 -0.63 0.94\r6 | -0.08 0.39 -0.22 0.828 -0.84 0.67\r7 | 0.05 0.38 0.12 0.901 -0.69 0.79\r8 | -0.01 0.36 -0.03 0.975 -0.73 0.70\r9 | 0.35 0.37 0.95 0.342 -0.37 1.07\r10 | 0.44 0.37 1.21 0.227 -0.27 1.16\r11 | 0.35 0.36 0.97 0.333 -0.36 1.07\r12 | -0.17 0.36 -0.47 0.639 -0.88 0.54\r13 | -0.36 0.36 -0.97 0.330 -1.07 0.36\r14 | -0.40 0.36 -1.09 0.276 -1.11 0.32\r15 | -0.43 0.37 -1.15 0.248 -1.15 0.30\r16 | -0.84 0.36 -2.31 0.021 -1.56 -0.13\r17 | -1.16 0.38 -3.06 0.002 -1.91 -0.42\r18 | -1.14 0.38 -3.00 0.003 -1.88 -0.39\r19 | -0.86 0.40 -2.15 0.032 -1.65 -0.08\r20 | -0.93 0.39 -2.36 0.018 -1.70 -0.16\r|\rage | -0.02 0.00 -18.58 0.000 -0.02 -0.02\r_cons | 0.49 0.37 1.33 0.185 -0.23 1.20\r------------------------------------------------------------------------------\rLet’s now make a graph that shows the predicted logit of smoking as a function of education, adjusting for age. margins educ,predict(xb) marginsplot,xline(12 16) Log odds of smoking by education\rThe graph in figure above includes vertical lines at 12 and 16 years of education. These junctures seem like excellent candidates for the placement of knots where there is a change in slope and a change in intercept. Let’s fit such a model below mkspline edprehsm 12 edhsm 16 edcom = educ,marginal logit smoke c.edprehsm c.edhsm c.edcom hsgrad cograd age,nolog Logistic regression Number of obs = 16,274\rLR chi2(6) = 806.95\rProb \u003e chi2 = 0.0000\rLog likelihood = -10136.225 Pseudo R2 = 0.0383\r------------------------------------------------------------------------------\rsmoke | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\redprehsm | 0.05 0.01 3.68 0.000 0.03 0.08\redhsm | -0.16 0.03 -5.83 0.000 -0.21 -0.10\redcom | 0.06 0.04 1.40 0.161 -0.02 0.14\rhsgrad | -0.60 0.06 -9.60 0.000 -0.72 -0.48\rcograd | -0.30 0.10 -3.18 0.001 -0.49 -0.12\rage | -0.02 0.00 -19.10 0.000 -0.02 -0.02\r_cons | 0.27 0.15 1.76 0.078 -0.03 0.57\r------------------------------------------------------------------------------\rBefore interpreting the results, let’s create a graph of the predicted log odds of smoking as a function of education. margins,at(edprehsm=0 edhsm=0 edcom=0 hsgrad=0 cograd=0) /// at(edprehsm=12 edhsm=0 edcom=0 hsgrad=0 cograd=0) /// at(edprehsm=12 edhsm=0 edcom=0 hsgrad=1 cograd=0) /// at(edprehsm=16 edhsm=4 edcom=0 hsgrad=1 cograd=0) /// at(edprehsm=16 edhsm=4 edcom=0 hsgrad=1 cograd=1) /// at(edprehsm=20 edhsm=8 edcom=4 hsgrad=1 cograd=1) /// predict(xb) noatlegend mat yhat = r(b)' mat educ = (0 \\ 12 \\ 12 \\ 16 \\ 16\\ 20) svmat yhat svmat educ graph twoway line yhat1 educ1,xline(12 16) title(\"Piecewise Model\") Predicted log odds of smoking from education fit using a piecew","date":"2024-01-15","objectID":"/18.chapter18nonlinear-models/:5:3","tags":["Interaction","stata"],"title":"Chapter18 ：Nonlinear models","uri":"/18.chapter18nonlinear-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter considers models where time is treated as a categorical variable","date":"2024-01-14","objectID":"/17.chapter17time-as-a-categorical-predictor/","tags":["Multilevel","Interaction","stata"],"title":"Chapter17 ：Time as a categorical predictor","uri":"/17.chapter17time-as-a-categorical-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter considers models where time is treated as a categorical variable The models presented in this chapter will use the mixed command to fit a model that is a hybrid of a traditional repeated-measures analysis of variance (ANOVA) and a mixed model. Like the repeated-measures ANOVA, there will be one fixed intercept (rather than having random intercepts that we commonly see when using the mixed command). By specifying the noconstant option in the random-effects portion of the mixed command, a fixed intercept will be estimated. To account for the nonindependence of residuals across time points, we will use the residuals() option within the random-effects portion of the mixed command. This allows us to model the structure of the residual covariances across time points. The following examples will use an unstructured residual covariance, which estimates a separate residual variance for each time point and a separate residual correlation among each pair of the time points. ","date":"2024-01-14","objectID":"/17.chapter17time-as-a-categorical-predictor/:0:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter17 ：Time as a categorical predictor","uri":"/17.chapter17time-as-a-categorical-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 Example 1: Time treated as a categorical variable Sleep was measured at three time points, the first a control (baseline) condition and the second and third measurements while taking a sleep medication. here are two aims of this study. The first aim is to assess the initial impact of sleep medication on duration of sleep.So the second aim is to the assess the sustained effectiveness of the medication on sleep, comparing the amount of sleep in the second and third months use sleep_cat3.dta summarize Note! Wide and long datasets Data for this kind of study might be stored with one observation per person and three variables representing the different time points. Sometimes, this is called a multivariate format, and Stata would call this a wide format. If your dataset is in that kind of form, you can use the reshape command to convert it to a long format. Let’s now use the mixed command to predict sleep from month, treating month as a categorical variable. Specifying || id: introduces the random-effects part of the model and indicates that the observations are nested within id. Next, the noconstant option is specified in the random-effects options so only one fixed intercept is fit. Furthermore, the residuals() option is included to specify covariance structure of the residuals between months (within each level of the person’s ID). The residuals() option specifies an unstructured residual covariance among the different months within each person. This accounts for the nonindependence of the observations among time points for each person. mixed sleep i.month || id:,noconstant residuals(unstructured,t(month))nolog Mixed-effects ML regression Number of obs = 300\rGroup variable: id Number of groups = 100\rObs per group:\rmin = 3\ravg = 3.0\rmax = 3\rWald chi2(2) = 35.94\rLog likelihood = -1437.3712 Prob \u003e chi2 = 0.0000\r------------------------------------------------------------------------------\rsleep | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rmonth |\r2 | 21.07 3.83 5.50 0.000 13.56 28.58\r3 | 18.43 3.81 4.84 0.000 10.97 25.89\r|\r_cons | 348.24 3.14 110.73 0.000 342.08 354.40\r------------------------------------------------------------------------------\r------------------------------------------------------------------------------\rRandom-effects parameters | Estimate Std. err. [95% conf. interval]\r-----------------------------+------------------------------------------------\rid: (empty) |\r-----------------------------+------------------------------------------------\rResidual: Unstructured |\rvar(e1) | 989.04 139.87 749.61 1304.95\rvar(e2) | 928.55 131.32 703.77 1225.14\rvar(e3) | 731.36 103.43 554.31 964.96\rcov(e1,e2) | 224.17 98.42 31.27 417.06\rcov(e1,e3) | 135.43 86.12 -33.37 304.22\rcov(e2,e3) | 107.72 83.11 -55.17 270.61\r------------------------------------------------------------------------------\rLR test vs. linear model: chi2(5) = 11.61 Prob \u003e chi2 = 0.0405\rNote: The reported degrees of freedom assumes the null hypothesis is not on the boundary of the parameter\rspace. If this is not true, then the reported test is conservative.\rBefore we interpret the coefficients from this model, let’s use the margins command to compute the predicted mean of sleep for each level of month, as shown below. margins month,nopvalues Adjusted predictions Number of obs = 300\rExpression: Linear prediction, fixed portion, predict()\r--------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r-------------+------------------------------------------------\rmonth |\r1 | 348.24 3.14 342.08 354.40\r2 | 369.31 3.05 363.34 375.28\r3 | 366.67 2.70 361.37 371.97\r--------------------------------------------------------------\rWe can use the marginsplot command to create a graph showing the predicted mean of sleep across the three months, as shown below. marginsplot Estimated minutes of sleep at night by month\rBefore making those spec","date":"2024-01-14","objectID":"/17.chapter17time-as-a-categorical-predictor/:1:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter17 ：Time as a categorical predictor","uri":"/17.chapter17time-as-a-categorical-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2 Example 2: Time (categorical) by two groups This study includes 100 participants in the treatment group and 100 participants in the control group. The main predictors for this example are group (a two-level categorical variable) and month (a three-level categorical variable). use sleep_catcat23.dta list in 1/6,sepby(id) summarize mixed sleep i.group##i.month || id:,noconstant residuals(un,t(month))nolog Mixed-effects ML regression Number of obs = 600\rGroup variable: id Number of groups = 200\rObs per group:\rmin = 3\ravg = 3.0\rmax = 3\rWald chi2(5) = 68.47\rLog likelihood = -2879.78 Prob \u003e chi2 = 0.0000\r-------------------------------------------------------------------------------\rsleep | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r--------------+----------------------------------------------------------------\rgroup |\rMedication | -7.08 4.38 -1.62 0.106 -15.66 1.50\r|\rmonth |\r2 | -3.73 3.75 -1.00 0.319 -11.07 3.61\r3 | -0.57 4.26 -0.13 0.893 -8.91 7.77\r|\rgroup#month |\rMedication#2 | 28.06 5.30 5.30 0.000 17.67 38.45\rMedication#3 | 24.94 6.02 4.14 0.000 13.14 36.74\r|\r_cons | 350.63 3.10 113.24 0.000 344.56 356.70\r-------------------------------------------------------------------------------\r------------------------------------------------------------------------------\rRandom-effects parameters | Estimate Std. err. [95% conf. interval]\r-----------------------------+------------------------------------------------\rid: (empty) |\r-----------------------------+------------------------------------------------\rResidual: Unstructured |\rvar(e1) | 958.68 95.87 788.05 1166.26\rvar(e2) | 721.30 72.13 592.92 877.48\rvar(e3) | 980.26 98.03 805.78 1192.50\rcov(e1,e2) | 138.07 59.61 21.25 254.90\rcov(e1,e3) | 63.24 68.69 -71.40 197.87\rcov(e2,e3) | 119.72 60.06 2.01 237.43\r------------------------------------------------------------------------------\rLR test vs. linear model: chi2(5) = 15.70 Prob \u003e chi2 = 0.0078\rNote: The reported degrees of freedom assumes the null hypothesis is not on the boundary of the parameter\rspace. If this is not true, then the reported test is conservative.\rWe can estimate the mean sleep by group and month using the margins command below. margins month#group,nopvalues marginsplot,noci Adjusted predictions Number of obs = 600\rExpression: Linear prediction, fixed portion, predict()\r---------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r--------------+------------------------------------------------\rmonth#group |\r1#Control | 350.63 3.10 344.56 356.70\r1#Medication | 343.55 3.10 337.48 349.62\r2#Control | 346.90 2.69 341.64 352.16\r2#Medication | 367.88 2.69 362.62 373.14\r3#Control | 350.06 3.13 343.92 356.20\r3#Medication | 367.92 3.13 361.78 374.06\r---------------------------------------------------------------\rEstimated sleep by month and treatment group\rBefore testing our two main questions of interest, let’s assess the overall interaction of group by month using the contrast command below. The overall interaction is significant. contrast group#month Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------\r| df chi2 P\u003echi2\r-------------+----------------------------------\rsleep |\rgroup#month | 2 30.21 0.0000\r------------------------------------------------\rNow to test our questions of interest regarding the initial effect of medication and the sustained effect of medication, we can apply the ar. contrast operator to month and interact that with group, as shown below. contrast ar.month#group,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r-----------------------------------------------------------------------\r| Contrast Std. err. z P\u003e|z|\r-------------------------------+---------------------------------------\rsleep |\rmonth#group |\r(2 vs 1) (Medication vs base) | 28.06 5.30 5.30 0.000\r(3 vs 2) (Medication vs base) | -3.12 5.41 -0.58 0.564\r--------------------------------------------","date":"2024-01-14","objectID":"/17.chapter17time-as-a-categorical-predictor/:2:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter17 ：Time as a categorical predictor","uri":"/17.chapter17time-as-a-categorical-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3 Example 3: Time (categorical) by three groups This study includes 300 participants, 100 assigned to each of the three treatment groups. The main variables of interest in this study are treatment group (with three levels) and month (with three levels). use sleep_catcat33.dta list in 1/6,sepby(id) summarize mixed sleep i.group##i.month ||id:,noconstant residuals(un,t(month)) nolog Mixed-effects ML regression Number of obs = 900\rGroup variable: id Number of groups = 300\rObs per group:\rmin = 3\ravg = 3.0\rmax = 3\rWald chi2(8) = 284.84\rLog likelihood = -4318.4068 Prob \u003e chi2 = 0.0000\r-------------------------------------------------------------------------------\rsleep | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r--------------+----------------------------------------------------------------\rgroup |\rMedication | 2.60 4.25 0.61 0.540 -5.72 10.92\rEducation | 3.17 4.25 0.75 0.455 -5.15 11.49\r|\rmonth |\r2 | 8.06 3.72 2.17 0.030 0.77 15.35\r3 | 13.35 4.06 3.29 0.001 5.39 21.31\r|\rgroup#month |\rMedication#2 | 21.88 5.26 4.16 0.000 11.57 32.19\rMedication#3 | 18.22 5.75 3.17 0.002 6.96 29.48\rEducation#2 | 7.11 5.26 1.35 0.177 -3.20 17.42\rEducation#3 | 34.49 5.75 6.00 0.000 23.23 45.75\r|\r_cons | 344.70 3.00 114.80 0.000 338.82 350.58\r-------------------------------------------------------------------------------\r------------------------------------------------------------------------------\rRandom-effects parameters | Estimate Std. err. [95% conf. interval]\r-----------------------------+------------------------------------------------\rid: (empty) |\r-----------------------------+------------------------------------------------\rResidual: Unstructured |\rvar(e1) | 901.52 73.61 768.20 1057.98\rvar(e2) | 836.25 68.28 712.59 981.38\rvar(e3) | 918.42 74.99 782.60 1077.81\rcov(e1,e2) | 177.08 51.16 76.80 277.35\rcov(e1,e3) | 84.56 52.76 -18.85 187.97\rcov(e2,e3) | 160.48 51.44 59.66 261.30\r------------------------------------------------------------------------------\rLR test vs. linear model: chi2(5) = 24.71 Prob \u003e chi2 = 0.0002\rNote: The reported degrees of freedom assumes the null hypothesis is not on the boundary of the parameter\rspace. If this is not true, then the reported test is conservative.\rThe margins and marginsplot commands are used below to estimate the predicted mean of sleep by group and month and to graph the results. margins month#group,nopvalues marginsplot,noci Adjusted predictions Number of obs = 900\rExpression: Linear prediction, fixed portion, predict()\r---------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r--------------+------------------------------------------------\rmonth#group |\r1#Control | 344.70 3.00 338.82 350.58\r1#Medication | 347.30 3.00 341.42 353.18\r1#Education | 347.87 3.00 341.99 353.75\r2#Control | 352.76 2.89 347.09 358.43\r2#Medication | 377.24 2.89 371.57 382.91\r2#Education | 363.04 2.89 357.37 368.71\r3#Control | 358.05 3.03 352.11 363.99\r3#Medication | 378.87 3.03 372.93 384.81\r3#Education | 395.71 3.03 389.77 401.65\r---------------------------------------------------------------\rSleep by month and treatment group\rLet’s now use the contrast command to test the group by month interaction. This overall interaction is significant. contrast group#month Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------\r| df chi2 P\u003echi2\r-------------+----------------------------------\rsleep |\rgroup#month | 4 62.05 0.0000\r------------------------------------------------\rLet’s form an interaction contrast in which we apply reference group contrasts to treatment group (r.group) and reverse adjacent group contrasts to month (ar.month). contrast ar.month#r.group,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r--------------------------------------------------------------------------\r| Contrast Std. err. z P\u003e|z|\r----------------------------------+---------------------------------------\rslee","date":"2024-01-14","objectID":"/17.chapter17time-as-a-categorical-predictor/:3:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter17 ：Time as a categorical predictor","uri":"/17.chapter17time-as-a-categorical-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4 Comparing models with different residual covariance structures The selection of the covariance structure impacts the estimates of the standard errors of the coefficients, but not the point estimates of the coefficients. When you have only three time points, an unstructured covariance can be a good choice. However, as the number of time points increases, the number of variances and covariances estimated by an unstructured covariance matrix increases dramatically. For example, if you have five time points, an unstructured covariance estimates five variances and 10 covariances (a total of 15 parameters). In such cases, you might consider more parsimonious covariance structures, such as the exchangeable, ar (autoregressive), or banded residual types. This leads to the question of how to choose among models using different covariance structures. My first recommendation would be to select a residual covariance structure that is grounded in theory or suggested by previous research. However, such information may be scarce or nonexistent. In such cases, you can fit different covariance structures seeking the residual covariance structure that combines the fewest parameters with the best measure of fit—using, for example, Akaike information criterion (AIC) or Bayesian information criterion (BIC). Stata makes this process easy, as illustrated below use sleep_cat3.dta mixed sleep i.month || id:,noconstant residuals(unstructured,t(month)) estimate store m_un mixed sleep i.month || id:,noconstant residuals(exchangeable,t(month)) estimate store m_ex mixed sleep i.month || id:,noconstant residuals(ar 1,t(month)) estimate store m_ar1 Using the dataset from example 1, models are fit using three different covariance structures: unstructured, exchangeable, and ar 1. After fitting each model, the estimates store command is used to store the estimates from the respective model. estimate stats m_un m_ex m_ar1 Akaike's information criterion and Bayesian information criterion\r-----------------------------------------------------------------------------\rModel | N ll(null) ll(model) df AIC BIC\r-------------+---------------------------------------------------------------\rm_un | 900 . -4367.631 9 8753.262 8796.484\rm_ex | 900 . -4372.205 5 8754.409 8778.421\rm_ar1 | 900 . -4370.91 5 8751.82 8775.832\r-----------------------------------------------------------------------------\rNote: BIC uses N = number of observations. See [R] IC note.\rRemember that when it comes to AIC and BIC, smaller is better. The ar 1 and exchangeable models have smaller AIC values than the unstructured model. Likewise, the ar 1 and exchangeable models also have smaller BIC values than the unstructured model. The ar 1 and exchangeable models also have the added benefit of including four fewer residual covariance parameters (5 versus 9). The ar 1 and exchangeable covariance structure appear to provide a fairly similar quality of fit, and both fit better than the unstructured covariance structure. Warning! Likelihood-ratio test It is tempting to ask whether the difference in covariance structures is significantly different and to want to use a command like lrtest to test whether one covariance structure fits significantly better than another. A key assumption of a likelihood-ratio test is that one model is nested within another model, where one model can be created from the other by omitting one or more parameters. In many (or perhaps most) cases, the models formed by comparing two different residual covariance structures are not nested within each other and the likelihood-ratio test is not valid. However, the AIC and BIC indices can be used even when models are not nested within each other. See [ME] mixed for a list of all available covariance structures you can choose within the residuals() option. Furthermore, chapter 7 of Singer and Willett (2003) provides additional descriptions of these residual covariance structures, including information to help you choose among the different st","date":"2024-01-14","objectID":"/17.chapter17time-as-a-categorical-predictor/:4:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter17 ：Time as a categorical predictor","uri":"/17.chapter17time-as-a-categorical-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"5 Analyses with small samples The mixed command uses large-sample methods. Applying such methods with smallsample sizes may lead to overly liberal statistical tests (that is, greater type I error rates). The mixed command allows you to specify the dfmethod() option to select testing methods that are more appropriate for small-sample sizes. Specifying the dfmethod() option is easy—knowing the best method to select is not so easy. For an introduction to the different small-sample methods you can choose from, I encourage you to see help mixed, especially the section in the PDF documentation titled Small-sample inference for fixed effects. That section describes five different small-sample adjustment methods: residual, repeated, anova, satterthwaite, and kroger. This leaves two remaining methods to consider: dfmethod(kroger) and dfmethod(satterthwaite). The dfmethod(kroger) option (described in Kenward and Roger (1997)) and dfmethod(satterthwaite) option (described in Satterthwaite (1946)) offer methods that are more applicable when making inferences with longitudinal models with small-sample sizes. There will likely be continued research in this area, and the potential for new techniques and options to arise. For now, it seems that the Kenward–Roger method is probably the most generally useful method available, although sensitivity analyses considering the Satterthwaite method would seem prudent. use sleep_catcat23small.dta count tab group month sort id month list in 1/20,sepby(id) summarize use large-sample statistical methods.(note how the significance of each parameter is tested using z-test) mixed sleep i.group##i.month || id:,noconstant residuals(un,t(month)) Performing gradient-based optimization: Iteration 0: Log likelihood = -364.19065 Iteration 1: Log likelihood = -363.52715 Iteration 2: Log likelihood = -363.12776 Iteration 3: Log likelihood = -363.1244 Iteration 4: Log likelihood = -363.1244 Computing standard errors ...\rMixed-effects ML regression Number of obs = 80\rGroup variable: id Number of groups = 30\rObs per group:\rmin = 1\ravg = 2.7\rmax = 3\rWald chi2(5) = 27.47\rLog likelihood = -363.1244 Prob \u003e chi2 = 0.0000\r-------------------------------------------------------------------------------\rsleep | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r--------------+----------------------------------------------------------------\rgroup |\rMedication | -19.76 9.34 -2.12 0.034 -38.06 -1.47\r|\rmonth |\r2 | -6.20 8.58 -0.72 0.470 -23.01 10.62\r3 | -20.99 10.16 -2.07 0.039 -40.91 -1.07\r|\rgroup#month |\rMedication#2 | 41.98 11.93 3.52 0.000 18.58 65.37\rMedication#3 | 48.40 14.15 3.42 0.001 20.68 76.13\r|\r_cons | 361.38 6.60 54.75 0.000 348.45 374.32\r-------------------------------------------------------------------------------\r------------------------------------------------------------------------------\rRandom-effects parameters | Estimate Std. err. [95% conf. interval]\r-----------------------------+------------------------------------------------\rid: (empty) |\r-----------------------------+------------------------------------------------\rResidual: Unstructured |\rvar(e1) | 612.04 163.59 362.46 1033.48\rvar(e2) | 459.72 122.77 272.38 775.92\rvar(e3) | 505.46 145.96 287.00 890.21\rcov(e1,e2) | 40.15 105.05 -165.75 246.05\rcov(e1,e3) | -111.18 115.76 -338.07 115.71\rcov(e2,e3) | 60.67 110.88 -156.66 277.99\r------------------------------------------------------------------------------\rLR test vs. linear model: chi2(5) = 2.13 Prob \u003e chi2 = 0.8305\rNote: The reported degrees of freedom assumes the null hypothesis is not on the boundary of the parameter\rspace. If this is not true, then the reported test is conservative.\rThe contrast command is used below to compare the change in sleep across months between the medication group versus the control group. contrast ar.month#group,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r-----------------------------------------------------------------------\r| Contrast Std. er","date":"2024-01-14","objectID":"/17.chapter17time-as-a-categorical-predictor/:5:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter17 ：Time as a categorical predictor","uri":"/17.chapter17time-as-a-categorical-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter considers models that involve the interaction of two categorical predictors with a linear continuous predictor.","date":"2024-01-13","objectID":"/14.chapter14continuous-by-categorical-by-categorical-interactions/","tags":["Interaction","stata"],"title":"Chapter14 ：Continuous by continuous by categorical interactions","uri":"/14.chapter14continuous-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter considers models that involve the interaction of two categorical predictors with a linear continuous predictor. ","date":"2024-01-13","objectID":"/14.chapter14continuous-by-categorical-by-categorical-interactions/:0:0","tags":["Interaction","stata"],"title":"Chapter14 ：Continuous by continuous by categorical interactions","uri":"/14.chapter14continuous-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 Chapter overview This chapter blends these two modeling techniques by exploring how the slope of the continuous variable varies as a function of the interaction of the two categorical variables. Let’s consider a hypothetical example of a model with income as the outcome variable. The predictors include gender (a two-level categorical variable), education (treated as a three-level categorical variable), and age (a continuous variable). Income can be modeled as a function of each of the predictors, as well as the interactions of all the predictors. A three-way interaction of age by gender by education would imply that the effect of age interacts with gender by education. One way to visualize such an interaction would be to graph age on the $x$ axis, with separate lines for the levels of education and separate graphs for gender. Fitted values of income as a function of age, education, and gender\rThe age slope by level of education and gender\rBut consider the differences in the age slopes between females and males at each level of education. This difference is $-250 (150-400)$ for non–high school graduates, whereas this difference is $-350 (250-600)$ for high school graduates, and the difference is $-700 (600-1300)$ for college graduates. The difference in the age slopes between females and males seems to be much larger for college graduates than for high school graduates and non–high school graduates. Let’s explore this in more detail with an example using the GSS dataset. To focus on the linear effect of age, we will keep those who are 22 to 55 years old. use gss_ivrm.dta keep if (age\u003e=22 \u0026 age\u003c=55) reg realrinc i.gender##i.educ3##c.age i.race,vce(robust) noci Linear regression Number of obs = 25,718\rF(13, 25704) = 411.30\rProb \u003e F = 0.0000\rR-squared = 0.1839\rRoot MSE = 23556\r-----------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t|\r-------------------+---------------------------------------\rgender |\rFemale | 1337.13 1693.69 0.79 0.430\r|\reduc3 |\rHS | 550.48 1782.19 0.31 0.757\rColl | -11156.10 2618.98 -4.26 0.000\r|\rgender#educ3 |\rFemale#HS | 783.10 2021.65 0.39 0.698\rFemale#Coll | 7657.91 3164.30 2.42 0.016\r|\rage | 413.87 45.62 9.07 0.000\r|\rgender#c.age |\rFemale | -264.98 50.66 -5.23 0.000\r|\reduc3#c.age |\rHS | 175.85 54.75 3.21 0.001\rColl | 897.33 77.47 11.58 0.000\r|\rgender#educ3#c.age |\rFemale#HS | -80.31 60.95 -1.32 0.188\rFemale#Coll | -414.66 93.27 -4.45 0.000\r|\rrace |\rblack | -2935.14 273.33 -10.74 0.000\rother | 185.40 956.34 0.19 0.846\r|\r_cons | 2691.23 1495.78 1.80 0.072\r-----------------------------------------------------------\rLet’s test the interaction of gender, education, and age using the contrast command below. The three-way interaction is significant. contrast i.gender#i.educ3#c.age Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------------\r| df F P\u003eF\r-------------------+----------------------------------\rgender#educ3#c.age | 2 10.17 0.0000\r|\rDenominator | 25704\r------------------------------------------------------\rTo begin the process of interpreting the three-way interaction, let’s create a graph of the adjusted means as a function of age, education, and gender. compute the adjusted means by gender and education for age 22 and 55 margins gender#educ3,at(age=(22 55)) Predictive margins Number of obs = 25,718\rModel VCE: Robust\rExpression: Linear prediction, predict()\r1._at: age = 22\r2._at: age = 55\r----------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r-----------------+----------------------------------------------------------------\r_at#gender#educ3 |\r1#Male#not hs | 11404.29 542.03 21.04 0.000 10341.88 12466.70\r1#Male#HS | 15823.46 349.34 45.29 0.000 15138.72 16508.20\r1#Male#Coll | 19989.51 916.25 21.82 0.000 18193.61 21785.41\r1#Female#not hs | 6911.76 353.90 19.53 0.000 6218.10 7605.42\r1#Female#HS | 10347.31 219.7","date":"2024-01-13","objectID":"/14.chapter14continuous-by-categorical-by-categorical-interactions/:1:0","tags":["Interaction","stata"],"title":"Chapter14 ：Continuous by continuous by categorical interactions","uri":"/14.chapter14continuous-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2 Simple effects of gender on the age slope We can use the contrast command to test the simple effect of gender on the age slope. This is illustrated below. contrast gender#c.age@educ3,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r-----------------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r-------------------------+---------------------------------------\rgender@educ3#c.age |\r(Female vs base) not hs | -264.98 50.66 -5.23 0.000\r(Female vs base) HS | -345.29 33.99 -10.16 0.000\r(Female vs base) Coll | -679.64 78.45 -8.66 0.000\r-----------------------------------------------------------------\rThe first test compares the age slope for females versus males among non–high school graduates. Referring to table 14.2, this test compares $\\beta\\tiny 1F$with$\\beta\\tiny 1M$ . The difference in these age slopes is $-264.98(148.89 - 413.87)$, and this difference is significant. The age slope for females who did not graduate high school is 264.98 units smaller than the age slope for males who did not graduate high school. The second test is similar to the first, except the comparison is made among high school graduates, comparing $\\beta\\tiny 2F$ with $\\beta\\tiny 2M$ from table 14.2. This test is also significant. The third test compares the age slope between females and males among college graduates (that is, comparing $\\beta\\tiny 3F$ with $\\beta\\tiny 3M$ ). This test is also significant. ","date":"2024-01-13","objectID":"/14.chapter14continuous-by-categorical-by-categorical-interactions/:2:0","tags":["Interaction","stata"],"title":"Chapter14 ：Continuous by continuous by categorical interactions","uri":"/14.chapter14continuous-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3 Simple effects of education on the age slope We can also look at the simple effects of education on the age slope at each level of gender. contrast educ3#c.age@gender Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------------\r| df F P\u003eF\r-------------------+----------------------------------\reduc3@gender#c.age |\rMale | 2 70.96 0.0000\rFemale | 2 43.37 0.0000\rJoint | 4 57.21 0.0000\r|\rDenominator | 25704\r------------------------------------------------------\rThe first test compares the age slope among the three levels of education for males. $H\\tiny 0$:$\\beta\\tiny 1M$ = $\\beta\\tiny 2M$ = $\\beta\\tiny 3M$ This test is significant. The age slope significantly differs as a function of education among males. The second test is like the first test, except that the comparisons are made for females. $H\\tiny 0$:$\\beta\\tiny 1F$ = $\\beta\\tiny 2F$ = $\\beta\\tiny 3F$ This test is also significant. Among females, the age slope significantly differs among the three levels of education. ","date":"2024-01-13","objectID":"/14.chapter14continuous-by-categorical-by-categorical-interactions/:3:0","tags":["Interaction","stata"],"title":"Chapter14 ：Continuous by continuous by categorical interactions","uri":"/14.chapter14continuous-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4 Simple contrasts on education for the age slope We can further dissect the simple effects tested above by applying contrast coefficients to the education factor.For example, say that we used the ar. contrast operator to form reverse adjacent group comparisons. contrast ar.educ3#c.age@gender,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r---------------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r-----------------------+---------------------------------------\reduc3@gender#c.age |\r(HS vs not hs) Male | 175.85 54.75 3.21 0.001\r(HS vs not hs) Female | 95.54 26.84 3.56 0.000\r(Coll vs HS) Male | 721.48 69.75 10.34 0.000\r(Coll vs HS) Female | 387.13 49.39 7.84 0.000\r---------------------------------------------------------------\r","date":"2024-01-13","objectID":"/14.chapter14continuous-by-categorical-by-categorical-interactions/:4:0","tags":["Interaction","stata"],"title":"Chapter14 ：Continuous by continuous by categorical interactions","uri":"/14.chapter14continuous-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"5 Partial interaction on education for the age slope The three-way interaction can be dissected by forming contrasts on the three-level categorical variable. Say that we use reverse adjacent group comparisons on education, which compares high school graduates with non–high school graduates and college graduates with high school graduates. contrast ar.educ3#r.gender#c.age,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r-------------------------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r---------------------------------+---------------------------------------\reduc3#gender#c.age |\r(HS vs not hs) (Female vs Male) | -80.31 60.95 -1.32 0.188\r(Coll vs HS) (Female vs Male) | -334.35 85.49 -3.91 0.000\r-------------------------------------------------------------------------\rThe first comparison tests the interaction of the contrast of high school graduates versus non–high school graduates by gender by age. The difference in the age slope between high school graduates and non–high school graduates for females is 244.43 minus 148.89 (95.54). For males, this difference is 589.72 minus 413.87 (175.85). The difference in these differences is $-80.31$, which is not significant (see the first comparison from the margins command).The difference in the age slope comparing high school graduates with non–high school graduates is not significantly different for males and females. The second test forms the same kind of comparison, but compares college graduates with high school graduates. The difference in the age slope comparing female college graduates with female high school graduates is 631.56 minus 244.43 (387.13). This difference for males is 1,311.20 minus 589.72 (721.48). The difference of these differences is and is statistically significant (see the second comparison from the margins output). The increase in the age slope comparing college graduates with high school graduates is greater for males than it is for females. ","date":"2024-01-13","objectID":"/14.chapter14continuous-by-categorical-by-categorical-interactions/:5:0","tags":["Interaction","stata"],"title":"Chapter14 ：Continuous by continuous by categorical interactions","uri":"/14.chapter14continuous-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"One of the unique features of multilevel models is the ability to study cross-level interactions—the interactions of a level-1 variable with a level-2 variable. Such interactions allow you to explore the extent to which the effect of a level-1 variable is moderated by a level-2 variable. ","date":"2024-01-13","objectID":"/15.chapter15multilevel-models/","tags":["Multilevel","Interaction","stata"],"title":"Chapter15 ：Multilevel models","uri":"/15.chapter15multilevel-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"One of the unique features of multilevel models is the ability to study cross-level interactions—the interactions of a level-1 variable with a level-2 variable. Such interactions allow you to explore the extent to which the effect of a level-1 variable is moderated by a level-2 variable. ","date":"2024-01-13","objectID":"/15.chapter15multilevel-models/:0:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter15 ：Multilevel models","uri":"/15.chapter15multilevel-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 Chapter overview This chapter contains four examples, all illustrating multilevel models where students are nested within schools. These four examples provide the opportunity to explore four kinds of crosslevel interactions: continuous by continuous (example 1) continuous by categorical (example 2) categorical by continuous (example 3) categorical by categorical (example 4) All of these examples are completely hypothetical and have been constructed to simplify the interpretation and visualization of the results. ","date":"2024-01-13","objectID":"/15.chapter15multilevel-models/:1:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter15 ：Multilevel models","uri":"/15.chapter15multilevel-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2 Example 1: Continuous by continuous interaction Consider a two-level multilevel model where students are nested within schools. One hundred schools were randomly sampled from a population of schools, and students were randomly sampled from each of the schools. Two student-level variables were measured: socioeconomic status (ses) and a standardized writing test score (write). Furthermore, a school-level variable was measured: the number students per computer within the school, stucomp use school_write.dta sum list in 1/5,abbreviate(30) Variable | Obs Mean Std. dev. Min Max\r-------------+---------------------------------------------------------\rschoolid | 3,026 50.71481 29.01184 1 100\rstuid | 3,026 17.08824 10.81501 1 52\rwrite | 3,026 542.1325 191.3663 0 1200\rses | 3,026 49.78352 10.18169 14.1897 85.06909\rstucomp | 3,026 5.857066 3.533295 1.149443 16.50701\r+------------------------------------------------+\r| schoolid stuid write ses stucomp |\r|------------------------------------------------|\r1. | 1 1 553 55.38129 3.350059 |\r2. | 1 2 530 61.13125 3.350059 |\r3. | 1 3 604 47.61407 3.350059 |\r4. | 1 4 433 48.26278 3.350059 |\r5. | 1 5 370 47.9762 3.350059 |\r+------------------------------------------------+\rThe aim of this hypothetical study is to determine if the greater availability of computers at a school reduces the strength of the relationship between socioeconomic status and writing test scores. In other words, the goal is to determine if there is a cross-level interaction of ses and stucomp in the prediction of write. The mixed command predicts write from c.ses, c.stucomp, and the interaction c.stucomp#c.ses. The random-effects portion of the model indicates that ses is a random effect across levels of schoolid. The covariance(un) (un is short for unstructured) option permits the random intercept and ses slope to be correlated. (In this and all subsequent examples in this chapter, the nolog and noheader options are used to save space.These options suppress the iteration log and the header information.) mixed write c.stucomp##c.ses || schoolid:ses,covariance(un)nolog noheader ---------------------------------------------------------------------------------\rwrite | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r----------------+----------------------------------------------------------------\rstucomp | -26.75 2.69 -9.95 0.000 -32.01 -21.48\rses | 0.36 0.68 0.53 0.597 -0.97 1.69\r|\rc.stucomp#c.ses | 0.24 0.10 2.28 0.023 0.03 0.44\r|\r_cons | 602.29 18.18 33.12 0.000 566.65 637.93\r---------------------------------------------------------------------------------\r------------------------------------------------------------------------------\rRandom-effects parameters | Estimate Std. err. [95% conf. interval]\r-----------------------------+------------------------------------------------\rschoolid: Unstructured |\rvar(ses) | 8.92 1.73 6.09 13.05\rvar(_cons) | 372.35 1229.73 0.58 241085.03\rcov(ses,_cons) | 19.20 35.82 -51.01 89.40\r-----------------------------+------------------------------------------------\rvar(Residual) | 9820.89 261.13 9322.20 10346.25\r------------------------------------------------------------------------------\rLR test vs. linear model: chi2(3) = 3253.82 Prob \u003e chi2 = 0.0000\rNote: LR test is conservative and provided only for reference.\rNote! Fixed and random effects The fixed effects are specified after the dependent variable and before the ||. The random effects are specified after the ||. As expected, the c.stucomp#c.ses interaction is significant. We can use themargins command below with the dydx(ses) option to compute the ses slope for schools that have between one and eight students per computer. margins,dydx(ses) at(stucomp=(1(1)8)) vsquish Average marginal effects Number of obs = 3,026\rExpression: Linear prediction, fixed portion, predict()\rdy/dx wrt: ses\r1._at: stucomp = 1\r2._at: stucomp = 2\r3._at: stucomp = 3\r4._at: stucomp = 4\r5._at: stucomp = 5\r6._at: stucomp = 6\r7._at: stucomp = 7\r8._at: stucomp = 8\r--","date":"2024-01-13","objectID":"/15.chapter15multilevel-models/:2:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter15 ：Multilevel models","uri":"/15.chapter15multilevel-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3 Example 2: Continuous by categorical interaction In this study, standardized reading scores were measured as well as the socioeconomic status of the student. Of the 100 schools, 35 were private (non-Catholic), 35 were public, and 30 were Catholic schools The goal of this hypothetical study is to examine the relationship between socioeconomic status (ses) and reading scores (read), and to determine if the strength of that relationship varies as a function of the type of school (private, public, or Catholic). This involves examining the cross-level interaction of ses and schtype. The mixed command for performing this analysis is shown below. The variable read is predicted from ses, schtype, and the interaction of these two variables. The variable ses is specified as a random coefficient that varies across schools. use school_read.dta sum list in 1/5,abbreviate(30) mixed read i.schtype##c.ses || schoolid:ses,covariance(un) nolog noheader --------------------------------------------------------------------------------\rread | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r---------------+----------------------------------------------------------------\rschtype |\rPublic | 41.97 23.27 1.80 0.071 -3.64 87.57\rCatholic | 152.48 23.79 6.41 0.000 105.86 199.10\r|\rses | 4.04 0.67 6.00 0.000 2.72 5.36\r|\rschtype#c.ses |\rPublic | -1.24 0.95 -1.30 0.192 -3.11 0.62\rCatholic | -3.37 0.99 -3.41 0.001 -5.30 -1.44\r|\r_cons | 519.28 16.57 31.33 0.000 486.80 551.77\r--------------------------------------------------------------------------------\r------------------------------------------------------------------------------\rRandom-effects parameters | Estimate Std. err. [95% conf. interval]\r-----------------------------+------------------------------------------------\rschoolid: Unstructured |\rvar(ses) | 12.13 2.12 8.61 17.10\rvar(_cons) | 4.90 82.77 0.00 1.14e+15\rcov(ses,_cons) | 6.75 31.24 -54.47 67.97\r-----------------------------+------------------------------------------------\rvar(Residual) | 10015.68 264.26 9510.90 10547.24\r------------------------------------------------------------------------------\rLR test vs. linear model: chi2(3) = 4052.81 Prob \u003e chi2 = 0.0000\rNote: LR test is conservative and provided only for reference.\rThe contrast command is used below to test the overall schtype#c.ses interaction. This tests the following null hypothesis: $H\\tiny 0$:$\\beta\\tiny 1$ = $\\beta\\tiny 2$ = $\\beta\\tiny 3$ $\\beta\\tiny 1$ is the average ses slope for private schools,$\\beta\\tiny 2$ is the average ses slope for public schools, and $\\beta\\tiny 3$ is the average ses slope for Catholic schools. This test is significant, indicating that the ses slopes differ by schtype. contrast schtype#c.ses Margins: asbalanced\r-------------------------------------------------\r| df chi2 P\u003echi2\r--------------+----------------------------------\rread |\rschtype#c.ses | 2 11.82 0.0027\r-------------------------------------------------\rLet’s create a graph that illustrates the ses slopes by schtype. We do this using the margins command to compute the adjusted means of reading scores as a function of ses and schtype, and then graphing these adjusted means using the marginsplot. margins schtype, at(ses=(20(5)80)) marginsplot,noci Reading score by socioeconomic status and school type\rLet’s now use the margins command combined with the dydx(ses) option to estimate the ses slope for each of the three different types of schools. margins,dydx(ses) over(schtype) vsquish Average marginal effects Number of obs = 2,973\rExpression: Linear prediction, fixed portion, predict()\rdy/dx wrt: ses\rOver: schtype\r------------------------------------------------------------------------------\r| Delta-method\r| dy/dx std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rses |\rschtype |\rPrivate | 4.04 0.67 6.00 0.000 2.72 5.36\rPublic | 2.80 0.67 4.17 0.000 1.48 4.12\rCatholic | 0.67 0.72 0.93 0.350 -0.74 2.09\r-----------------------------","date":"2024-01-13","objectID":"/15.chapter15multilevel-models/:3:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter15 ：Multilevel models","uri":"/15.chapter15multilevel-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4 Example 3: Categorical by continuous interaction The aim of this study is to look at gender differences in math performance and to determine if smaller class sizes are associated with smaller gender differences in math scores. In other words, the aim is to determine if there is a cross-level interaction between gender (a level-1 predictor) and class size (a level-2 predictor). use school_math.dta summarize list in 1/5 The mixed command is used to predict math from gender, clsize, and the interaction of these variables. The variable gender is specified as a random coefficient at the school level. mixed math i.gender##c.clsize ||schoolid:gender,covariance(un) nolog Mixed-effects ML regression Number of obs = 2,926\rGroup variable: schoolid Number of groups = 100\rObs per group:\rmin = 8\ravg = 29.3\rmax = 52\rWald chi2(3) = 59.38\rLog likelihood = -17691.732 Prob \u003e chi2 = 0.0000\r-----------------------------------------------------------------------------------\rmath | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r------------------+----------------------------------------------------------------\rgender |\rFemale | 13.83 12.41 1.11 0.265 -10.49 38.16\rclsize | -1.34 0.32 -4.15 0.000 -1.97 -0.71\r|\rgender#c.clsize |\rFemale | -1.10 0.48 -2.30 0.021 -2.04 -0.16\r|\r_cons | 451.94 8.34 54.18 0.000 435.59 468.29\r-----------------------------------------------------------------------------------\r------------------------------------------------------------------------------\rRandom-effects parameters | Estimate Std. err. [95% conf. interval]\r-----------------------------+------------------------------------------------\rschoolid: Unstructured |\rvar(gender) | 244.79 231.45 38.37 1561.80\rvar(_cons) | 35.46 96.33 0.17 7273.92\rcov(gender,_cons) | 78.98 115.00 -146.42 304.37\r-----------------------------+------------------------------------------------\rvar(Residual) | 10273.41 277.07 9744.46 10831.07\r------------------------------------------------------------------------------\rLR test vs. linear model: chi2(3) = 15.08 Prob \u003e chi2 = 0.0018\rNote: LR test is conservative and provided only for reference.\rTo help interpret this effect, we can graph the results using the margins and marginsplot commands margins gender, at(clsize=(15(5)40)) marginsplot,noci Math scores by gender and average class size\rLet’s assess the significance of the gender difference for class sizes ranging from 15 to 40 in five-student increments using the margins command below. margins r.gender,at(clsize=(15(5)40)) contrast(pveffects nowald) vsquish Contrasts of adjusted predictions Number of obs = 2,926\rExpression: Linear prediction, fixed portion, predict()\r1._at: clsize = 15\r2._at: clsize = 20\r3._at: clsize = 25\r4._at: clsize = 30\r5._at: clsize = 35\r6._at: clsize = 40\r------------------------------------------------------------\r| Delta-method\r| Contrast std. err. z P\u003e|z|\r--------------------+---------------------------------------\rgender@_at |\r(Female vs Male) 1 | -2.72 6.10 -0.45 0.656\r(Female vs Male) 2 | -8.24 4.61 -1.79 0.074\r(Female vs Male) 3 | -13.76 4.10 -3.35 0.001\r(Female vs Male) 4 | -19.27 4.88 -3.95 0.000\r(Female vs Male) 5 | -24.79 6.51 -3.81 0.000\r(Female vs Male) 6 | -30.31 8.51 -3.56 0.000\r------------------------------------------------------------\rThe margins command is repeated below for 15 to 40 students per class in onestudent increments (the output is omitted to save space). The marginsplot command is then used to visualize the gender differences (with a confidence interval) across the entire spectrum of class sizes margins r.gender,at(clsize=(15(1)40)) contrast(effects) marginsplot,recastci(rarea) ciopts(fcolor(%20)) yline(0) Gender difference in reading score by average class size\r","date":"2024-01-13","objectID":"/15.chapter15multilevel-models/:4:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter15 ：Multilevel models","uri":"/15.chapter15multilevel-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"5 Example 4: Categorical by continuous interaction This study focuses on gender differences in standardized science scores, and whether such differences vary by school size. In this study, each school is classified into one of three sizes: small, medium, or large. Thus, the focus of this study is on the cross-level interaction of gender by school size, where both gender and school size are categorical variables. use school_science.dta summarize list in 1/5 mixed science i.gender##i.schsize || schoolid: gender,covariance(un) nolog noheader -----------------------------------------------------------------------------------\rscience | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r------------------+----------------------------------------------------------------\rgender |\rFemale | -19.65 6.65 -2.96 0.003 -32.68 -6.62\r|\rschsize |\rMedium | 18.56 7.36 2.52 0.012 4.13 32.99\rLarge | 40.16 7.20 5.58 0.000 26.06 54.27\r|\rgender#schsize |\rFemale#Medium | 1.47 9.54 0.15 0.878 -17.23 20.17\rFemale#Large | 22.24 9.29 2.39 0.017 4.03 40.45\r|\r_cons | 394.97 5.14 76.89 0.000 384.90 405.04\r-----------------------------------------------------------------------------------\r------------------------------------------------------------------------------\rRandom-effects parameters | Estimate Std. err. [95% conf. interval]\r-----------------------------+------------------------------------------------\rschoolid: Unstructured |\rvar(gender) | 38.52 205.24 0.00 1.32e+06\rvar(_cons) | 155.78 115.88 36.25 669.43\rcov(gender,_cons) | 17.52 128.51 -234.35 269.38\r-----------------------------+------------------------------------------------\rvar(Residual) | 9772.30 271.33 9254.71 10318.83\r------------------------------------------------------------------------------\rLR test vs. linear model: chi2(3) = 12.15 Prob \u003e chi2 = 0.0069\rNote: LR test is conservative and provided only for reference.\rThe contrast command is used to test the overall gender#schsize interaction. This test is significant. contrast gender#schsize Contrasts of marginal linear predictions\rMargins: asbalanced\r--------------------------------------------------\r| df chi2 P\u003echi2\r---------------+----------------------------------\rscience |\rgender#schsize | 2 7.17 0.0278\r--------------------------------------------------\rcontrast gender#schsize To help interpret this interaction, we can use the margins command to display the adjusted mean of science scores as a function of gender and schsize margins schsize#gender,nopvalues vsquish marginsplot,noci Adjusted predictions Number of obs = 2,764\rExpression: Linear prediction, fixed portion, predict()\r----------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r---------------+------------------------------------------------\rschsize#gender |\rSmall#Male | 394.97 5.14 384.90 405.04\rSmall#Female | 375.32 5.46 364.62 386.03\rMedium#Male | 413.53 5.27 403.20 423.86\rMedium#Female | 395.36 5.56 384.45 406.26\rLarge#Male | 435.14 5.04 425.26 445.01\rLarge#Female | 437.73 5.36 427.23 448.23\r----------------------------------------------------------------\rScience scores by gender and school size\rOne way to further understand this interaction is by testing the simple effect of gender for each school size. This is performed using the contrast command, as shown below. contrast gender@schsize,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r-----------------------------------------------------------------\r| Contrast Std. err. z P\u003e|z|\r-------------------------+---------------------------------------\rscience |\rgender@schsize |\r(Female vs base) Small | -19.65 6.65 -2.96 0.003\r(Female vs base) Medium | -18.18 6.84 -2.66 0.008\r(Female vs base) Large | 2.59 6.49 0.40 0.690\r-----------------------------------------------------------------\rWe can also further probe the interaction by using partial interaction tests. Let’s apply the ar. contrast operator to school size (comparing each group with th","date":"2024-01-13","objectID":"/15.chapter15multilevel-models/:5:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter15 ：Multilevel models","uri":"/15.chapter15multilevel-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter considers models involving the analysis of longitudinal data.  ","date":"2024-01-13","objectID":"/16.chapter16time-as-a-continuous-predictor/","tags":["Multilevel","Interaction","stata"],"title":"Chapter16 ：Time as a continuous predictor","uri":"/16.chapter16time-as-a-continuous-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter considers models involving the analysis of longitudinal data. Such designs involve participants that are observed at more than one time point and time is generally treated as one of the important predictors in the model. Like any predictor, we need to ask ourselves how we want to model the relationship between time and the outcome. A key distinction is whether time will be treated as a continuous variable or as a categorical variable. There are several approaches that can be used for modeling longitudinal data, including repeated-measures analysis of variance (ANOVA), generalized estimating equations (GEE), and multilevel modeling. This chapter will focus on using multilevel modeling for analyzing longitudinal models where time is treated as level 1 and the person will be treated as level 2. In such a model, characteristics that change as a function of time are level-1 predictors and characteristics that are a property of the person that do not change over time are level-2 predictors. ","date":"2024-01-13","objectID":"/16.chapter16time-as-a-continuous-predictor/:0:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter16 ：Time as a continuous predictor","uri":"/16.chapter16time-as-a-continuous-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 Example 1: Linear effect of time Let’s begin by considering a simple model in which we look at the linear effect of time on the outcome variable. For example, let’s consider a study in which we are looking at the number of minutes people sleep at night over a seven-week period. For the sake of this example, assume that the people were selected for the study because they have recently experienced a stressful event in their life, and the purpose of the study is to understand the natural course of sleep change over the seven weeks after a stressful event. use sleep_conlin.dta list in 1/5 summarize +---------------------+\r| id obsday sleep |\r|---------------------|\r1. | 1 1 382 |\r2. | 1 6 382 |\r3. | 1 13 390 |\r4. | 1 21 378 |\r5. | 1 27 401 |\r+---------------------+\rThe dataset for this study is organized in a long format, with one observation per person per night of observation. Variable | Obs Mean Std. dev. Min Max\r-------------+---------------------------------------------------------\rid | 600 38 21.66677 1 75\robsday | 600 23.565 14.82854 1 52\rsleep | 600 360.785 48.13086 175 528\rInstead, we can fit a random-intercept model that accounts for the nonindependence of the residuals within each person. Such a model is fit below first using the xtset command to specify that id is the panel variable and obsday is the time variable. We can then use the xtreg command to fit a random-intercept model predicting sleep from obsday xtset id obsday Panel variable: id (weakly balanced)\rTime variable: obsday, 1 to 52, but with gaps\rDelta: 1 unit\rxtreg sleep obsday Random-effects GLS regression Number of obs = 600\rGroup variable: id Number of groups = 75\rR-squared: Obs per group:\rWithin = 0.1652 min = 8\rBetween = 0.0062 avg = 8.0\rOverall = 0.0218 max = 8\rWald chi2(1) = 103.56\rcorr(u_i, X) = 0 (assumed) Prob \u003e chi2 = 0.0000\r------------------------------------------------------------------------------\rsleep | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\robsday | 0.51 0.05 10.18 0.000 0.41 0.61\r_cons | 348.80 5.31 65.66 0.000 338.39 359.21\r-------------+----------------------------------------------------------------\rsigma_u | 44.412437\rsigma_e | 18.051717\rrho | .85821678 (fraction of variance due to u_i)\r------------------------------------------------------------------------------\rThe interpretation of the obsday coefficient is straightforward. For each additional day in the study, nightly minutes of sleep increased by, on average, by 0.51 minutes. In this model, the coefficient for obsday is treated as a fixed effect. The model recognizes that people randomly vary in terms of their average sleep time at the start of the study (represented by the random intercept). But perhaps people also vary individually in their trajectory of sleep times across the weeks of the study. By adding obsday as a random coefficient (that is, a random slope), the model can account for both individual differences in the average length of sleep at the start of the study (that is, a random intercept) and individual differences in the trajectory of sleep times across the weeks of the study (that is, a random slope for obsday). mixed sleep obsday || id:obsday, covariance(un) nolog Mixed-effects ML regression Number of obs = 600\rGroup variable: id Number of groups = 75\rObs per group:\rmin = 8\ravg = 8.0\rmax = 8\rWald chi2(1) = 20.54\rLog likelihood = -2488.5378 Prob \u003e chi2 = 0.0000\r-----------------------------------------------------------------------------------\rsleep | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r------------------+----------------------------------------------------------------\robsday | 0.51 0.11 4.53 0.000 0.29 0.73\r_cons | 348.82 3.31 105.43 0.000 342.34 355.31\r-----------------------------------------------------------------------------------\r------------------------------------------------------------------------------\rRandom-effects parameters | Estimate Std. e","date":"2024-01-13","objectID":"/16.chapter16time-as-a-continuous-predictor/:1:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter16 ：Time as a continuous predictor","uri":"/16.chapter16time-as-a-continuous-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2 Example 2: Linear effect of time by a categorical predictor Let’s consider another sleep study in which participants diagnosed with insomnia were randomly assigned to one of three different treatments to increase the number of minutes of sleep at night. The three different treatments were 1) control group (no treatment), 2) medication group (where a sleep medication is given), or 3) education group (where the participants receive education about how to sleep better and longer). This model involves a combination of a continuous predictor (time) and a threelevel categorical predictor (treatment group). use sleep_cat3conlin.dta list in 1/5 summarize We can fit a model that predicts sleep from the observation day, the group assignment, and the interaction of these two variables using the mixed command shown below. Note that the random-effects portion of the model specifies that obsday is a random effect. [Thinking in terms of a multilevel model, group#obsday is a crosslevel interaction of a level-2 variable (group) with a level-1 variable (obsday).] mixed sleep i.group##c.obsday || id:obsday,covariance(un)nolog Mixed-effects ML regression Number of obs = 600\rGroup variable: id Number of groups = 75\rObs per group:\rmin = 8\ravg = 8.0\rmax = 8\rWald chi2(5) = 66.55\rLog likelihood = -2482.6017 Prob \u003e chi2 = 0.0000\r-----------------------------------------------------------------------------------\rsleep | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r------------------+----------------------------------------------------------------\rgroup |\rMedication | 35.48 9.50 3.74 0.000 16.86 54.10\rEducation | 5.55 9.50 0.58 0.559 -13.07 24.17\r|\robsday | -0.07 0.18 -0.37 0.715 -0.42 0.29\r|\rgroup#c.obsday |\rMedication | 0.18 0.25 0.71 0.475 -0.32 0.68\rEducation | 0.83 0.25 3.27 0.001 0.33 1.33\r|\r_cons | 339.45 6.72 50.53 0.000 326.28 352.61\r-----------------------------------------------------------------------------------\r------------------------------------------------------------------------------\rRandom-effects parameters | Estimate Std. err. [95% conf. interval]\r-----------------------------+------------------------------------------------\rid: Unstructured |\rvar(obsday) | 0.72 0.13 0.50 1.03\rvar(_cons) | 1079.51 184.18 772.68 1508.17\rcov(obsday,_cons) | 22.52 4.23 14.22 30.82\r-----------------------------+------------------------------------------------\rvar(Residual) | 108.37 7.22 95.10 123.48\r------------------------------------------------------------------------------\rLR test vs. linear model: chi2(3) = 1425.42 Prob \u003e chi2 = 0.0000\rNote: LR test is conservative and provided only for reference.\rWe first use the margins command to estimate the predicted means by group at selected values of obsday. We follow that with the marginsplot command to graph the predicted means computed by the margins command. margins group,at(obsday=(0 45)) marginsplot,noci Adjusted predictions Number of obs = 600\rExpression: Linear prediction, fixed portion, predict()\r1._at: obsday = 0\r2._at: obsday = 45\r-------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. z P\u003e|z| [95% conf. interval]\r--------------+----------------------------------------------------------------\r_at#group |\r1#Control | 339.45 6.72 50.53 0.000 326.28 352.61\r1#Medication | 374.93 6.72 55.83 0.000 361.76 388.09\r1#Education | 345.00 6.72 51.36 0.000 331.83 358.16\r2#Control | 336.49 13.61 24.71 0.000 309.80 363.17\r2#Medication | 380.14 13.60 27.94 0.000 353.47 406.80\r2#Education | 379.42 13.61 27.88 0.000 352.75 406.10\r-------------------------------------------------------------------------------\rMinutes of sleep at night by time and treatment group\rThe slope appears to be slightly negative for the control group. In other words, their sleep durations appear to mildly decrease as a linear function of the observation day. For the medication group, the slope appears to be slightly positive. By contrast, sleep durations increase as a linear f","date":"2024-01-13","objectID":"/16.chapter16time-as-a-continuous-predictor/:2:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter16 ：Time as a continuous predictor","uri":"/16.chapter16time-as-a-continuous-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3 Example 3:Piecewise modeling of time Time can be modeled in a piecewise fashion by breaking up the days of observation into the baseline phase and the treatment phase. In this example, we can assess the slope of the relationship between sleep duration and time (obsday) during the baseline and treatment phases. We can also test for a sudden jump in sleep duration on the 31st day, corresponding to the start of the treatment phase. use sleep_conpw.dta list in 1/5,sepby(id) summarize We need to create some variables to prepare for the piecewise analysis. First, we use the mkspline command to create the variables named obsday1m and obsday2m, placing the knot at 31 days (corresponding to the start of the treatment phase). mkspline obsday1m 31 obsday2m = obsday,marginal Next, we create the variable trtphase that is coded 0 for the baseline phase (where obsday was 1 to 30) and coded 1 during the treatment phase (where obsday is 31 or more). generate trtphase = 0 if obsday \u003c= 30 replace trtphase = 1 if obsday \u003e=31 \u0026 !missing(obsday) We are now ready to run a piecewise model that predicts sleep from obsday1m, obsday2m, and trtphase. In this example, obsday1m, obsday2m, and trtphase are also included as random effects. mixed sleep obsday1m obsday2m trtphase || id: obsday1m obsday2m trtphase,covariance(un)nolog Mixed-effects ML regression Number of obs = 600\rGroup variable: id Number of groups = 75\rObs per group:\rmin = 8\ravg = 8.0\rmax = 8\rWald chi2(3) = 70.48\rLog likelihood = -2601.8889 Prob \u003e chi2 = 0.0000\r----------------------------------------------------------------------------------------\rsleep | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-----------------------+----------------------------------------------------------------\robsday1m | -0.01 0.12 -0.05 0.961 -0.24 0.23\robsday2m | 0.54 0.17 3.27 0.001 0.22 0.87\rtrtphase | 11.14 1.94 5.74 0.000 7.33 14.94\r_cons | 349.01 10.64 32.80 0.000 328.15 369.87\r----------------------------------------------------------------------------------------\r------------------------------------------------------------------------------\rRandom-effects parameters | Estimate Std. err. [95% conf. interval]\r-----------------------------+------------------------------------------------\rid: Unstructured |\rvar(obsday1m) | 0.83 0.18 0.55 1.25\rvar(obsday2m) | 0.69 0.33 0.27 1.78\rvar(trtphase) | 59.53 48.90 11.90 297.83\rvar(_cons) | 8425.42 1386.81 6102.18 11633.16\rcov(obsday1m,obsday2m) | -0.37 0.19 -0.73 -0.00\rcov(obsday1m,trtphase) | -1.54 2.14 -5.73 2.65\rcov(obsday1m,_cons) | 32.85 11.53 10.25 55.46\rcov(obsday2m,trtphase) | 2.84 2.78 -2.62 8.29\rcov(obsday2m,_cons) | -5.19 15.46 -35.50 25.12\rcov(trtphase,_cons) | -133.63 180.15 -486.71 219.45\r-----------------------------+------------------------------------------------\rvar(Residual) | 101.86 8.07 87.20 118.97\r------------------------------------------------------------------------------\rLR test vs. linear model: chi2(10) = 2057.12 Prob \u003e chi2 = 0.0000\rNote: LR test is conservative and provided only for reference.\rWe can then use the margins command to compute the predicted means for these key days. When obsday equals 31, we estimate the predicted mean assuming trtphase is 0 and 1, to estimate the jump in the fitted values due to the start of the treatment phase. (The noatlegend option is included to save space.) margins,at(obsday1m = 1 obsday2m = 0 trtphase=0) /// at(obsday1m = 31 obsday2m = 0 trtphase=0) /// at(obsday1m = 31 obsday2m = 0 trtphase=1) /// at(obsday1m = 49 obsday2m = 18 trtphase=1) nopvalues noatlegend Adjusted predictions Number of obs = 600\rExpression: Linear prediction, fixed portion, predict()\r--------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r-------------+------------------------------------------------\r_at |\r1 | 349.01 10.68 328.07 369.94\r2 | 348.83 12.30 324.72 372.94\r3 | 359.97 12.16 336.14 383.79\r4 | 369.65 13.11 343.95 395.34\r---------------------------------","date":"2024-01-13","objectID":"/16.chapter16time-as-a-continuous-predictor/:3:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter16 ：Time as a continuous predictor","uri":"/16.chapter16time-as-a-continuous-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4 Example 4: Piecewise effects of time by a categorical predictor Let’s consider an extension of the previous example that includes a baseline and treatment phase, but where participants are divided into different groups and receive different kinds of treatments during the treatment phase. The first 30 days of the study are a baseline period during which the sleep is observed, but no treatment is administered to any of the groups. Starting on the 31st day, the medication group receives sleep medication, the sleep education group receives education about how to lengthen their sleep, and the control group receives nothing. The first phase of the study (days 1 to 30) is called the baseline phase, and the second phase (day 31 until the end of the study) is called the treatment phase. We can study the slope of the relationship between sleep duration and time during each of these phases, as well as the change (jump or drop) in sleep that occurs at the transition from baseline to the treatment phase. Furthermore, we can investigate the impact of the treatment group assignment (control, medication, and education) on the slope in each phase, as well as the jump or drop in sleep due to the start of the treatment phase. use sleep_cat3pw.dta list in 1/5,sepby(id) summarize Imagine the educational periods (before high school graduation and after high school graduation) being replaced with the baseline and treatment phases. Also, imagine gender (male and female) being replaced with treatment group assignment (control, medication, and education). First, the mkspline command is used to create the variables obsday1m and obsday2m, specifying 31 as the knot. By including the marginal option, obsday1m will represent the slope for the baseline period and obsday2m will represent the change in the slope for the treatment period compared with the baseline period. mkspline obsday1m 31 obsday2m = obsday,marginal To account for the jump in sleep at the start of the treatment phase, we use the generate and replace commands to create the variable trtphase that is coded: 0 = baseline and 1 = treatment. generate trtphase = 0 if obsday \u003c 31 replace trtphase = 1 if obsday \u003e= 31 \u0026 !missing(obsday) label define trtlab 0 \"baseline\" 1 \"treatment\" label values trtphase trtlab The mixed command for fitting this model is shown below. Note the variables trtphase, obsday1m, and obsday2m are specified as random effects. The nolog, noheader, and noretable options are used to suppress the iteration log, header, and random-effects table to save space. mixed sleep i.group##(i.trtphase c.obsday1m c.obsday2m), || id:trtphase obsday1m obsday2m,covariance(un) nolog noheader noretable noci ---------------------------------------------------------------\rsleep | Coefficient Std. err. z P\u003e|z|\r-----------------------+---------------------------------------\rgroup |\rMedication | -2.07 3.41 -0.61 0.543\rEducation | 16.31 3.41 4.78 0.000\r|\rtrtphase |\rtreatment | -4.20 2.79 -1.51 0.132\robsday1m | -0.04 0.30 -0.14 0.890\robsday2m | -0.10 0.51 -0.19 0.849\r|\rgroup#trtphase |\rMedication#treatment | 34.28 4.00 8.58 0.000\rEducation#treatment | 1.65 3.97 0.41 0.678\r|\rgroup#c.obsday1m |\rMedication | 0.40 0.42 0.95 0.343\rEducation | -0.14 0.42 -0.34 0.735\r|\rgroup#c.obsday2m |\rMedication | -0.29 0.72 -0.40 0.686\rEducation | 2.48 0.72 3.45 0.001\r|\r_cons | 351.36 2.41 145.54 0.000\r---------------------------------------------------------------\rWe can now use the margins command to compute the predicted mean of sleep for each group at four key points—for the beginning of the study, the first day of the treatment phase (with and without treatment), and the 49th day of the study. (The noatlegend option is specified to save space.) margins group,at(obsday1m = 1 obsday2m = 0 trtphase=0) /// at(obsday1m = 31 obsday2m = 0 trtphase=0) /// at(obsday1m = 31 obsday2m = 0 trtphase=1) /// at(obsday1m = 49 obsday2m = 18 trtphase=1)nopvalues noatlegend Adjusted predictions Number of obs = 600\rExpression: Linear pre","date":"2024-01-13","objectID":"/16.chapter16time-as-a-continuous-predictor/:4:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter16 ：Time as a continuous predictor","uri":"/16.chapter16time-as-a-continuous-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4.1 Baseline slope [means the prior to the treatment phase] We can test the equality of all the baseline slopes using the contrast command below. This test is not significant. contrast group#c.obsday1m Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------\r| df chi2 P\u003echi2\r-----------------+----------------------------------\rsleep |\rgroup#c.obsday1m | 2 1.78 0.4108\r----------------------------------------------------\rYou can estimate the baseline slope for each group using the margins command below. margins, dydx(obsday1m) over(group) Average marginal effects Number of obs = 600\rExpression: Linear prediction, fixed portion, predict()\rdy/dx wrt: obsday1m\rOver: group\r------------------------------------------------------------------------------\r| Delta-method\r| dy/dx std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\robsday1m |\rgroup |\rControl | -0.04 0.30 -0.14 0.890 -0.63 0.55\rMedication | 0.36 0.30 1.20 0.229 -0.23 0.95\rEducation | -0.19 0.30 -0.62 0.537 -0.77 0.40\r------------------------------------------------------------------------------\r","date":"2024-01-13","objectID":"/16.chapter16time-as-a-continuous-predictor/:4:1","tags":["Multilevel","Interaction","stata"],"title":"Chapter16 ：Time as a continuous predictor","uri":"/16.chapter16time-as-a-continuous-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4.2 Change in slopes: Treatment versus baseline You can test the equality of the changes in slope coefficients for all three groups using the contrast command below. This test is significant. contrast group#c.obsday2m Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------\r| df chi2 P\u003echi2\r-----------------+----------------------------------\rsleep |\rgroup#c.obsday2m | 2 17.95 0.0001\r----------------------------------------------------\rYou can estimate the change in slope for each group using the margins command below. The change in slope for the control group and medication groups is not significant. For the education group (group 3), the slope during the treatment phase is significantly greater than the slope during the baseline phase. margins,dydx(obsday2m) over(group) Average marginal effects Number of obs = 600\rExpression: Linear prediction, fixed portion, predict()\rdy/dx wrt: obsday2m\rOver: group\r------------------------------------------------------------------------------\r| Delta-method\r| dy/dx std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\robsday2m |\rgroup |\rControl | -0.10 0.51 -0.19 0.849 -1.09 0.90\rMedication | -0.39 0.51 -0.76 0.447 -1.39 0.61\rEducation | 2.38 0.51 4.70 0.000 1.39 3.38\r------------------------------------------------------------------------------\r","date":"2024-01-13","objectID":"/16.chapter16time-as-a-continuous-predictor/:4:2","tags":["Multilevel","Interaction","stata"],"title":"Chapter16 ：Time as a continuous predictor","uri":"/16.chapter16time-as-a-continuous-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4.3 Jump at treatment We can test the equality of the jump or drop for all three groups using the contrast command below. This test is significant. contrast group#trtphase Contrasts of marginal linear predictions\rMargins: asbalanced\r--------------------------------------------------\r| df chi2 P\u003echi2\r---------------+----------------------------------\rsleep |\rgroup#trtphase | 2 92.58 0.0000\r--------------------------------------------------\rWe can estimate the size of the jump or drop in sleep durations at the start of the treatment phase for each group using the contrast command below. contrast trtphase@group,pveffects nowald Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------------------------------\r| Contrast Std. err. z P\u003e|z|\r--------------------------------+---------------------------------------\rsleep |\rtrtphase@group |\r(treatment vs base) Control | -4.20 2.79 -1.51 0.132\r(treatment vs base) Medication | 30.08 2.86 10.52 0.000\r(treatment vs base) Education | -2.56 2.83 -0.90 0.366\r------------------------------------------------------------------------\r","date":"2024-01-13","objectID":"/16.chapter16time-as-a-continuous-predictor/:4:3","tags":["Multilevel","Interaction","stata"],"title":"Chapter16 ：Time as a continuous predictor","uri":"/16.chapter16time-as-a-continuous-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4.4 Comparisons among groups Let’s now use the margins command to compare each group with the control group at each of these days. (The noatlegend option is specified to save space.) margins r.group,at(obsday1m=30 obsday2m=0 trtphase=0) /// at(obsday1m=35 obsday2m=4 trtphase=1) /// at(obsday1m=40 obsday2m=9 trtphase=1) /// at(obsday1m=45 obsday2m=14 trtphase=1) /// contrast(nowald pveffects)noatlegend Contrasts of adjusted predictions Number of obs = 600\rExpression: Linear prediction, fixed portion, predict()\r-------------------------------------------------------------------\r| Delta-method\r| Contrast std. err. z P\u003e|z|\r---------------------------+---------------------------------------\rgroup@_at |\r(Medication vs Control) 1 | 9.98 12.37 0.81 0.420\r(Medication vs Control) 2 | 45.10 12.87 3.50 0.000\r(Medication vs Control) 3 | 45.65 13.50 3.38 0.001\r(Medication vs Control) 4 | 46.21 14.68 3.15 0.002\r(Education vs Control) 1 | 12.00 12.38 0.97 0.332\r(Education vs Control) 2 | 22.84 12.87 1.77 0.076\r(Education vs Control) 3 | 34.51 13.50 2.56 0.011\r(Education vs Control) 4 | 46.18 14.67 3.15 0.002\r-------------------------------------------------------------------\rFocusing on the comparison of the medication group with the control group, the difference is not significant when observation day is 30 ($p=0.420$); prior to the treatment phase. However, the difference is significant at each of the time points tested during the treatment phase, when observation day is 35, 40, and 45 ( = 0.000, 0.001, and 0.002, respectively). Shifting our attention to the comparison of the education group with the control group, the comparison is not significant when observation day is 30 ($p=0.332$); prior to the start of the treatment phase. The difference remains nonsignificant early in the treatment phase when observation day is 35 ($p=0.076$) but is significant later inthe treatment phase when observation day is 40 ($p=0.011$) and 45 ($p=0.002$). ","date":"2024-01-13","objectID":"/16.chapter16time-as-a-continuous-predictor/:4:4","tags":["Multilevel","Interaction","stata"],"title":"Chapter16 ：Time as a continuous predictor","uri":"/16.chapter16time-as-a-continuous-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter illustrates how to fit models in which a continuous variable, fit in a piecewise manner, is interacted with a categorical variable.  ","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter illustrates how to fit models in which a continuous variable, fit in a piecewise manner, is interacted with a categorical variable. ","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:0:0","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 One knot and one jump For example, education was modeled in a piecewise manner including one knot at 12 years of education. This knot signifies both a change of slope and change in intercept (jump). The model also includes gender as a categorical variable and estimates separate slopes and intercepts for each level of gender Piecewise regression with one knot and one jump, labeled with estimated slopes\rFigure shows the adjusted means of income as a function of education with labels for each of the slopes. For men, the slope for non–high school graduates is labeled $\\beta\\tiny M1$ and the slope for high school graduates is labeled $\\beta\\tiny M2$ . For women, the slope for non–high school graduates is labeled $\\beta\\tiny F1$ and the slope for high school graduates is labeled $\\beta\\tiny F2$. The income jump at 12 years of education for men is labeled as $\\alpha\\tiny M1$. The corresponding jump at 12 years of education for women is labeled as $\\alpha\\tiny F1$. Note how an arrow head points to a sudden jump in income at 12 years of education To fit this model, we will use separate slope and separate intercept coding with respect to the gender groups. This means that we will estimate separate intercept terms for men and women (this includes separate jumps,$\\alpha\\tiny M1$ and $\\alpha\\tiny F1$ ). It will also estimate separate slope terms for men (that is, $\\beta\\tiny M1$ and $\\beta\\tiny M2$ ) and separate slope terms for women (that is,$\\beta\\tiny F1$ and $\\beta\\tiny F2$). We first use the mkspline command to create the variables ed1 and ed2 with a knot at 12 years of education. the variables ed1 and ed2 will reflect the slopes of the individual line segments before and after the knot. mkspline ed1 12 ed2 = educ We are now ready to run the piecewise regression model, shown in the regress command below. The regress command includes ibn.gender used in combination with the noconstant option to yield separate estimates of the intercept for each gender. reg realrinc ibn.gender ibn.gender#i.hsgrad /// ibn.gender#c.ed1 ibn.gender#c.ed2 i.race, vce(robust) noconstant noci Linear regression Number of obs = 3,126\rF(6, 3120) = 614.01\rProb \u003e F = 0.0000\rR-squared = 0.5132\rRoot MSE = 20860\r--------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t|\r----------------+---------------------------------------\rgender |\rFemale | 10502.44 1991.84 5.27 0.000\r|\rgender#hsgrad |\rFemale#HS Grad | 2891.28 893.57 3.24 0.001\r|\rgender#c.ed1 |\rFemale | 95.54 211.64 0.45 0.652\r|\rgender#c.ed2 |\rFemale | 3260.42 247.08 13.20 0.000\r|\rrace |\rblack | -1174.41 902.16 -1.30 0.193\rother | -4732.20 1169.14 -4.05 0.000\r--------------------------------------------------------\rNote! Model shortcut Stata expands the expression ibn.gender#(i.hsgrad c.ed1 c.ed2) to become ibn.gender#i.hsgrad ibn.gender#c.ed1 ibn.gender#c.ed2, yielding the same model we saw before. The first two columns of the table repeat the name and estimate of the coefficient from the regress output. The third column shows the symbol used to represent the coefficient in figure above, providing a cross reference between the output of the regression model and figure. The last column shows the symbolic name of the regression coefficient that we can use with the lincom command for making comparisons among the coefficients. Summary of piecewise regression results with one knot\rLet’s begin by interpreting the change in intercept (jump) terms. The jump in the adjusted mean of income for men at 12 years of education is 3,382.01, and the corresponding jump for women is 1,748.10. Each of these jumps is statistically significant. Let’s now interpret the slopes. For men, the educ slope is 86.71 for non–high school graduates and is 4,057.64 for high school graduates. For women, the slope is 241.61 for non–high school graduates and is 2,529.55 for high school graduates. Aside from the educ slope for male non–high school graduates (86.71), all of these slopes are significantl","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:1:0","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.1 Comparing slopes across gender Let’s begin by testing the equality of the educ slopes of women and men who have not graduated high school. In other words, is $\\beta\\tiny F1$= $\\beta\\tiny M1$ ? contrast gender#c.ed1,pveffects nowald Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r------------------+---------------------------------------\rgender#c.ed1 |\r(Female vs base) | 154.90 179.32 0.86 0.388\r----------------------------------------------------------\rLet’s compare the educ slopes of men and women for those who graduated high school by testing whether $\\beta\\tiny F2$ = $\\beta\\tiny M2$ . contrast gender#c.ed2,pveffects nowald Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r------------------+---------------------------------------\rgender#c.ed2 |\r(Female vs base) | -1528.09 187.09 -8.17 0.000\r----------------------------------------------------------\rThe difference in the slopes (women versus men) is $-1,528.09$. For every year of education beyond the 12th year, the income for men increases by $1,528.09$ more than for women. ","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:1:1","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.2 Comparing slopes across education let’s examine the change in slope between high school graduates and non–high school graduates for men. is $\\beta\\tiny M2$ = $\\beta\\tiny M1$ ? As shown in table , the symbolic names for these coefficients are 1.gender#ed2 and 1.gender#ed1. We can compare these coefficients using the lincom command, as shown below. lincom 1.gender#c.ed2 - 1.gender#c.ed1 ( 1) - 1bn.gender#c.ed1 + 1bn.gender#c.ed2 = 0\r------------------------------------------------------------------------------\rrealrinc | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\r(1) | 3970.93 212.70 18.67 0.000 3554.04 4387.83\r------------------------------------------------------------------------------\rMen show a significantly higher slope after graduating high school than men who have not graduated high school. The difference (comparing high school graduates with non–high school graduates) is 3,970.93. We can formulate the same kind of test for women, comparing the educ slope for high school graduates with that for non–high school graduates. lincom 2.gender#c.ed2 - 2.gender#c.ed1 Is $\\beta\\tiny F2$ = $\\beta\\tiny F1$ ? ( 1) - 2.gender#c.ed1 + 2.gender#c.ed2 = 0\r------------------------------------------------------------------------------\rrealrinc | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\r(1) | 2287.94 147.26 15.54 0.000 1999.30 2576.58\r------------------------------------------------------------------------------\r","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:1:2","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.3 Difference in differences of slopes（gain in slope） The previous section showed that, for men, the educ slope after graduating high school minus the slope before graduating high school equals 3,970.93. Let’s call this the gain in slope due to graduating high school. For women, the gain in slope due to graduating high school is 2,287.94. We might ask if the gain in slope due to graduating high school differs by gender. The lincom command below tests the gain in slope for men compared with the gain in slope for women. lincom (1.gender#c.ed2 - 1.gender#c.ed1) - (2.gender#c.ed2 - 2.gender#c.ed1) ( 1) - 1bn.gender#c.ed1 + 2.gender#c.ed1 + 1bn.gender#c.ed2 - 2.gender#c.ed2 = 0\r------------------------------------------------------------------------------\rrealrinc | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\r(1) | 1682.99 259.29 6.49 0.000 1174.78 2191.20\r------------------------------------------------------------------------------\r","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:1:3","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.4 Comparing changes in intercepts Let’s now ask whether the jump in income at 12 years of education is equal for men and women. the income for men jumps by $3,382.01$ at 12 years of education, whereas the corresponding jump for women is $1,748.10$. Are these jumps equal? Is $\\alpha\\tiny F1$ = $\\alpha\\tiny M1$ ? contrast gender#hsgrad,pveffects nowald Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r------------------------------------+---------------------------------------\rgender#hsgrad |\r(Female vs base) (HS Grad vs base) | -1633.91 766.98 -2.13 0.033\r----------------------------------------------------------------------------\rSome people might find that this test is more intuitive if performed using the following lincom command below. lincom 2.gender#1.hsgrad - 1.gender#1.hsgrad ( 1) - 1bn.gender#1.hsgrad + 2.gender#1.hsgrad = 0\r------------------------------------------------------------------------------\rrealrinc | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\r(1) | -1633.91 766.98 -2.13 0.033 -3137.23 -130.59\r------------------------------------------------------------------------------\r","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:1:4","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.5 Computing and comparing adjusted means Let’s now turn our attention to how we can compute adjusted means for this regression model. Before we can compute adjusted means with respect to education (that is, educ), we need to know how to express the level of education in terms of hsgrad, ed1, and ed2. showcoding educ hsgrad ed1 ed2 //the command is not fit for stata18 Let’s now estimate the adjusted mean for a female (that is, gender=2) with 15 years of education, as shown below. To indicate 15 years of education, we specify that hsgrad equals 1, ed1 equals 12, and ed2 equals 3. margins,nopvalues at(gender=2 hsgrad=1 ed1=12 ed2=3) Predictive margins Number of obs = 32,183\rModel VCE: Robust\rExpression: Linear prediction, predict()\rAt: gender = 2\rhsgrad = 1\red1 = 12\red2 = 3\r--------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r-------------+------------------------------------------------\r_cons | 18993.22 234.17 18534.24 19452.20\r--------------------------------------------------------------\rWe can estimate the adjusted mean of income for men and women with 15 years of education using the margins command below. margins gender,nopvalues at(hsgrad=1 ed1=12 ed2=3) Predictive margins Number of obs = 32,183\rModel VCE: Robust\rExpression: Linear prediction, predict()\rAt: hsgrad = 1\red1 = 12\red2 = 3\r--------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r-------------+------------------------------------------------\rgender |\rMale | 33171.77 351.09 32483.62 33859.93\rFemale | 18993.22 234.17 18534.24 19452.20\r--------------------------------------------------------------\rSpecifying the r. contrast operator indicates we want to use reference group comparisons, comparing the adjusted mean of income for women versus men. margins r.gender,at(hsgrad=1 ed1=12 ed2=3) contrast(nowald pveffects) Contrasts of predictive margins Number of obs = 32,183\rModel VCE: Robust\rExpression: Linear prediction, predict()\rAt: hsgrad = 1\red1 = 12\red2 = 3\r----------------------------------------------------------\r| Delta-method\r| Contrast std. err. t P\u003e|t|\r------------------+---------------------------------------\rgender |\r(Female vs Male) | -14178.55 422.06 -33.59 0.000\r----------------------------------------------------------\r","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:1:5","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.6 Graphing adjusted means Let’s graph the adjusted means as a function of education and gender. To make this graph, we need to compute the adjusted means separately for men and women with 0 years of education, 12 years of education (assuming the absence and presence of a high school diploma), and 20 years of education. margins gender,at(hsgrad=0 ed1=0 ed2=0) /// at(hsgrad=0 ed1=12 ed2=0) /// at(hsgrad=1 ed1=12 ed2=0) /// at(hsgrad=1 ed1=12 ed2=8) nopvalues Predictive margins Number of obs = 32,183\rModel VCE: Robust\rExpression: Linear prediction, predict()\r1._at: hsgrad = 0\red1 = 0\red2 = 0\r2._at: hsgrad = 0\red1 = 12\red2 = 0\r3._at: hsgrad = 1\red1 = 12\red2 = 0\r4._at: hsgrad = 1\red1 = 12\red2 = 8\r--------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r-------------+------------------------------------------------\r_at#gender |\r1#Male | 16576.36 1384.62 13862.46 19290.26\r1#Female | 6757.19 910.63 4972.32 8542.05\r2#Male | 17616.85 590.78 16458.91 18774.80\r2#Female | 9656.48 345.83 8978.65 10334.32\r3#Male | 20998.86 297.50 20415.75 21581.96\r3#Female | 11404.58 179.77 11052.22 11756.94\r4#Male | 53459.97 1042.02 51417.56 55502.37\r4#Female | 31640.96 761.69 30148.02 33133.90\r--------------------------------------------------------------\rpreserve clear input educ yhatm yhatf 0 16576.36 6757.187 12 17616.85 9656.485 12 20998.86 11404.58 20 53459.97 31640.96 end graph twoway line yhatm yhatf educ,xline(12)legend(label(1 \"Men\")label(2 \"Women\")) /// xtitle(Education)ytitle(Adjusted mean) Fitted values from piecewise model with one knot and one jump at educ = 12\rWe can automate the creation of this graph by extending the strategy First, we use the matrix command to create a matrix named yhat that contains the adjusted means computed by the margins command. **First,rerun the -margins- command from above quietly margins gender, at(hsgrad=0 ed1=0 ed2=0) /// at(hsgrad=0 ed1=12 ed2=0) /// at(hsgrad=1 ed1=12 ed2=0) /// at(hsgrad=1 ed1=12 ed2=8) ** store the adjusted means in a matrix named -yhat- matrix yhat = r(b)' //NOTE: We must use transpose here Now, we encounter a twist because the adjusted means are computed as a function two variables (education and gender). Looking at the output of the margins command, let’s focus on the order in which the adjusted means are displayed with respect to education and gender. Eight adjusted means are shown, corresponding to the following levels of educ, hsgrad, and gender: educ=0, hsgrad=0, gender=1 educ=0, hsgrad=0, gender=2 educ=12, hsgrad=0, gender=1 educ=12, hsgrad=0, gender=2 educ=12, hsgrad=1, gender=1 educ=12, hsgrad=1, gender=2 educ=20, hsgrad=1, gender=1 educ=20, hsgrad=1, gender=2 The matrix command is used to create a matrix named educ that reflects the levels of education shown in the bulleted list above. The matrix command is used again, this time to create a matrix called gender that contains the levels of gender shown in the bulleted list above. * store levels of education in a matrix named -educ- matrix educ = (0\\0\\12\\12\\12\\12\\20\\20) * store levels of gender in a matrix named -gender- matrix gender = (1\\2\\1\\2\\1\\2\\1\\2) The svmat command is then used three times, to save the matrices named yhat, educ, and gender to the current dataset. The list command is then used to show the variables yhat1, educ1, and gender1 for the first 10 observations of the dataset. svmat yhat //save the matrix -yhat- to the current dataset svmat educ //save the matrix -educ- to the current dataset svmat gender //save the matrix -gender- to the current dataset list yhat1 educ1 gender1 in 1/10,sep(2) +----------------------------+\r| yhat1 educ1 gender1 |\r|----------------------------|\r1. | 16576.36 0 1 |\r2. | 6757.188 0 2 |\r|----------------------------|\r3. | 17616.85 12 1 |\r4. | 9656.484 12 2 |\r|----------------------------|\r5. | 20998.86 12 1 |\r6. | 11404.58 12 2 |\r|----------------------------|\r7. | 53459.96 20 1 |\r8. | 31640.96 20 2 |\r|----------------------","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:1:6","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2 Two knots and two jumps This section covers piecewise regression models with two knots and two jumps interacted with a categorical variable. For example,income was predicted from education modeled in a piecewise fashion with two knots and two jumps at 12 and 16 years of education.Furthermore, these piecewise terms were interacted with gender. Piecewise regression with two knots and two jumps, labeled with estimated slopes\rThe jump at 12 years of education for men is labeled as $\\alpha\\tiny M1$. The jump at 16 years of education for men is labeled as $\\alpha\\tiny M2$. These corresponding jumps for women are labeled $\\alpha\\tiny F1$and $\\alpha\\tiny F2$. Piecewise regression with two knots and two jumps, labeled with estimated intercepts\rLet’s now illustrate how to perform this analysis. First, the mkspline command is used to create the variables ed1, ed2, and ed3 based on the knots that are specified at 12 and 16 years of education use gss_ivrm.dta mkspline ed1 12 ed2 16 ed3 = educ The variables are now ready to run the piecewise regression model. The regress command, shown below, includes ibn.gender used in combination with the noconstant option. This models separate intercepts for men and women. The model also includes the interaction of ibn.gender with i.hsgrad and i.cograd. This models the jump in income due to graduating high school and graduating college separately for men and women. Finally, the model includes the interaction of ibn.gender with c.ed1, c.ed2, and c.ed3. This models the educ slope for non–high school graduates, high school graduates, and college graduates, estimating these slopes separately for men and women. The variable i.race is included as a covariate. reg realrinc ibn.gender ibn.gender#(hsgrad cograd c.ed1 c.ed2 c.ed3) i.race,vce(robust) noconstant noci Linear regression Number of obs = 32,183\rF(14, 32169) = 2602.08\rProb \u003e F = 0.0000\rR-squared = 0.4885\rRoot MSE = 24897\r--------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t|\r----------------+---------------------------------------\rgender |\rMale | 17084.37 1389.70 12.29 0.000\rFemale | 7262.50 921.90 7.88 0.000\r|\rgender#hsgrad |\rMale#HS Grad | 4927.53 655.33 7.52 0.000\rFemale#HS Grad | 2400.89 372.21 6.45 0.000\r|\rgender#cograd |\rMale#CO Grad | 9230.72 1224.50 7.54 0.000\rFemale#CO Grad | 1615.19 789.62 2.05 0.041\r|\rgender#c.ed1 |\rMale | 89.40 151.40 0.59 0.555\rFemale | 243.64 98.84 2.46 0.014\r|\rgender#c.ed2 |\rMale | 1592.64 255.81 6.23 0.000\rFemale | 1724.74 182.70 9.44 0.000\r|\rgender#c.ed3 |\rMale | 4279.36 521.83 8.20 0.000\rFemale | 3608.45 496.99 7.26 0.000\r|\rrace |\rblack | -3361.24 246.84 -13.62 0.000\rother | -1813.49 840.47 -2.16 0.031\r--------------------------------------------------------\rLet’s now see how to form comparisons among these coefficients. Specifically, let’s learn how to compare slopes between men and women compare slopes across the levels of education, comparing college graduates, high school graduates, and non–high school graduates compare changes in slope between men and women due to graduating high school and due to graduating college compare changes in intercept (the jumps in income due to graduating high school and college) by gender compare changes in intercept (the jumps in income due to graduating high school and college) across levels of education ","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:2:0","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.1 Comparing slopes across gender (Cross Section) Let’s begin by testing the equality of the slopes between women and men who have not graduated high school—testing whether $\\beta\\tiny F1$= $\\beta\\tiny M1$. contrast gender#c.ed1,pveffects nowald //slope between Men and Women (not-grad) Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r------------------+---------------------------------------\rgender#c.ed1 |\r(Female vs base) | 154.25 179.30 0.86 0.390\r----------------------------------------------------------\rthe difference in the educ slope between women and men before graduating high school is 154.25. However, this difference is not statistically significant. In other words, prior to graduating high school, the educ slope is not significantly different for men and women. contrast gender#c.ed2,pveffects nowald //slope between Men and Women (grad) Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r------------------+---------------------------------------\rgender#c.ed2 |\r(Female vs base) | 132.11 314.04 0.42 0.674\r----------------------------------------------------------\rcontrast gender#c.ed3,pveffects nowald //slope between Men and Women (grad-col) Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r------------------+---------------------------------------\rgender#c.ed3 |\r(Female vs base) | -670.91 721.07 -0.93 0.352\r----------------------------------------------------------\r","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:2:1","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.2 Comparing slopes across education Let’s begin by comparing the slopes for high school graduates with non–high school graduates, starting with men. testing whether $\\beta\\tiny M2$ = $\\beta\\tiny M1$ . comparing the slopes for high school graduates with non-high school graduates (Men). lincom 1.gender#c.ed2 - 1.gender#c.ed1 ( 1) - 1bn.gender#c.ed1 + 1bn.gender#c.ed2 = 0\r------------------------------------------------------------------------------\rrealrinc | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\r(1) | 1503.24 297.13 5.06 0.000 920.85 2085.63\r------------------------------------------------------------------------------\rcomparing the slopes for high school graduates with non-high school graduates (Women). lincom 2.gender#c.ed2 - 2.gender#c.ed1 ( 1) - 2.gender#c.ed1 + 2.gender#c.ed2 = 0\r------------------------------------------------------------------------------\rrealrinc | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\r(1) | 1481.10 208.81 7.09 0.000 1071.82 1890.38\r------------------------------------------------------------------------------\rMen : cograd vs. hsgrad lincom 1.gender#c.ed3 - 1.gender#c.ed2 ( 1) - 1bn.gender#c.ed2 + 1bn.gender#c.ed3 = 0\r------------------------------------------------------------------------------\rrealrinc | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\r(1) | 2686.72 581.27 4.62 0.000 1547.41 3826.03\r------------------------------------------------------------------------------\rWomen : cograd vs. hsgrad lincom 2.gender#c.ed3 - 2.gender#c.ed2 ( 1) - 2.gender#c.ed2 + 2.gender#c.ed3 = 0\r------------------------------------------------------------------------------\rrealrinc | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\r(1) | 1883.71 530.20 3.55 0.000 844.49 2922.93\r------------------------------------------------------------------------------\r","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:2:2","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.3 Difference in differences of slopes Let’s now test the gain in slope due to graduating high school for men compared with the gain in slope due to graduating high school for women, which is ($\\beta\\tiny M2$ - $\\beta\\tiny M1$) -($\\beta\\tiny F2$ - $\\beta\\tiny F1$) lincom (1.gender#c.ed2 - 1.gender#c.ed1) - (2.gender#c.ed2 - 2.gender#c.ed1) ( 1) - 1bn.gender#c.ed1 + 2.gender#c.ed1 + 1bn.gender#c.ed2 - 2.gender#c.ed2 = 0\r------------------------------------------------------------------------------\rrealrinc | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\r(1) | 22.14 361.97 0.06 0.951 -687.33 731.61\r------------------------------------------------------------------------------\rlincom (1.gender#c.ed3 - 1.gender#c.ed2) - (2.gender#c.ed3 - 2.gender#c.ed2) ( 1) - 1bn.gender#c.ed2 + 2.gender#c.ed2 + 1bn.gender#c.ed3 - 2.gender#c.ed3 = 0\r------------------------------------------------------------------------------\rrealrinc | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\r(1) | 803.01 786.64 1.02 0.307 -738.84 2344.87\r------------------------------------------------------------------------------\r","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:2:3","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.4 Comparing changes in intercepts by gender Let’s now ask whether the jump in income at 12 years of education is the same for men and women. Let’s test whether these jumps are equal (whether $\\alpha\\tiny F1$= $\\alpha\\tiny M1$ ). contrast gender#hsgrad,pveffects nowald Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r------------------------------------+---------------------------------------\rgender#hsgrad |\r(Female vs base) (HS Grad vs base) | -2526.64 752.53 -3.36 0.001\r----------------------------------------------------------------------------\rtest whether these jump are equal(αF2=αM2？) contrast gender#cograd,pveffects nowald Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r------------------------------------+---------------------------------------\rgender#cograd |\r(Female vs base) (CO Grad vs base) | -7615.52 1455.05 -5.23 0.000\r----------------------------------------------------------------------------\r","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:2:4","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.5 Comparing changes in intercepts by education We might ask whether the jump due to graduating college ($9,230.72$) is equal to the jump due to graduating high school ($4,927.53$). whether the jump due to graduating college is equal to the jump due to graduating high school(Men) lincom 1.gender#1.cograd - 1.gender#1.hsgrad ( 1) - 1bn.gender#1.hsgrad + 1bn.gender#1.cograd = 0\r------------------------------------------------------------------------------\rrealrinc | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\r(1) | 4303.19 1327.65 3.24 0.001 1700.93 6905.44\r------------------------------------------------------------------------------\rlincom 2.gender#1.cograd - 2.gender#1.hsgrad ( 1) - 2.gender#1.hsgrad + 2.gender#1.cograd = 0\r------------------------------------------------------------------------------\rrealrinc | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\r(1) | -785.70 831.22 -0.95 0.345 -2414.92 843.52\r------------------------------------------------------------------------------\r","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:2:5","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.6 Computing and comparing adjusted means Let’s now see how to compute adjusted means in the context of this model. To compute adjusted means with respect to education, we need to express education in terms of hsgrad, cograd, ed1, ed2, and ed3. showcoding educ hsgrad cograd ed1 ed2 ed3 We can estimate the adjusted mean for men and women (separately) in one margins command. hoding education constant at 15-years of education margins gender,nopvalues at(hsgrad=1 cograd=0 ed1=12 ed2=3 ed3=0) Predictive margins Number of obs = 32,183\rModel VCE: Robust\rExpression: Linear prediction, predict()\rAt: hsgrad = 1\rcograd = 0\red1 = 12\red2 = 3\red3 = 0\r--------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r-------------+------------------------------------------------\rgender |\rMale | 27322.43 652.72 26043.07 28601.78\rFemale | 17221.21 488.53 16263.68 18178.75\r--------------------------------------------------------------\rLet’s now use the margins command to compare the adjusted mean of income for women versus men among those with 15 years of education. Specifying the r. contrast operator indicates we want to use reference group comparisons. margins r.gender,at(hsgrad=1 cograd=0 ed1=12 ed2=3 ed3=0) Contrasts of predictive margins Number of obs = 32,183\rModel VCE: Robust\rExpression: Linear prediction, predict()\rAt: hsgrad = 1\rcograd = 0\red1 = 12\red2 = 3\red3 = 0\r------------------------------------------------\r| df F P\u003eF\r-------------+----------------------------------\rgender | 1 154.10 0.0000\r|\rDenominator | 32169\r------------------------------------------------\r-------------------------------------------------------------------\r| Delta-method\r| Contrast std. err. [95% conf. interval]\r------------------+------------------------------------------------\rgender |\r(Female vs Male) | -10101.21 813.72 -11696.14 -8506.28\r-------------------------------------------------------------------\rat 15 years of education, the adjusted mean of income for men is $10,101.21$ higher than for women. ","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:2:6","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.7 Graphing adjusted means margins gender,at(hsgrad=0 cograd=0 ed1=0 ed2=0 ed3=0) /// at(hsgrad=0 cograd=0 ed1=12 ed2=0 ed3=0) /// at(hsgrad=1 cograd=0 ed1=12 ed2=0 ed3=0) /// at(hsgrad=1 cograd=0 ed1=12 ed2=4 ed3=0) /// at(hsgrad=1 cograd=1 ed1=12 ed2=4 ed3=0) /// at(hsgrad=1 cograd=1 ed1=12 ed2=4 ed3=4) Predictive margins Number of obs = 32,183\rModel VCE: Robust\rExpression: Linear prediction, predict()\r1._at: hsgrad = 0\rcograd = 0\red1 = 0\red2 = 0\red3 = 0\r2._at: hsgrad = 0\rcograd = 0\red1 = 12\red2 = 0\red3 = 0\r3._at: hsgrad = 1\rcograd = 0\red1 = 12\red2 = 0\red3 = 0\r4._at: hsgrad = 1\rcograd = 0\red1 = 12\red2 = 4\red3 = 0\r5._at: hsgrad = 1\rcograd = 1\red1 = 12\red2 = 4\red3 = 0\r6._at: hsgrad = 1\rcograd = 1\red1 = 12\red2 = 4\red3 = 4\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_at#gender |\r1#Male | 16544.24 1384.77 11.95 0.000 13830.04 19258.45\r1#Female | 6722.38 909.92 7.39 0.000 4938.90 8505.85\r2#Male | 17616.99 590.89 29.81 0.000 16458.82 18775.15\r2#Female | 9646.09 345.63 27.91 0.000 8968.64 10323.54\r3#Male | 22544.52 282.66 79.76 0.000 21990.49 23098.54\r3#Female | 12046.99 142.20 84.72 0.000 11768.27 12325.71\r4#Male | 28915.06 896.20 32.26 0.000 27158.47 30671.66\r4#Female | 18945.96 667.08 28.40 0.000 17638.46 20253.45\r5#Male | 38145.78 830.91 45.91 0.000 36517.16 39774.40\r5#Female | 20561.15 422.35 48.68 0.000 19733.33 21388.97\r6#Male | 55263.21 1736.33 31.83 0.000 51859.94 58666.48\r6#Female | 34994.95 1760.66 19.88 0.000 31543.99 38445.92\r------------------------------------------------------------------------------\rpreserve clear input educ yhatm yhatf 0 16544.24 6722.377 12 17616.99 9646.094 12 22544.52 12046.99 16 28915.06 18945.96 16 38145.78 20561.15 20 55263.21 34994.95 end graph twoway line yhatm yhatf educ,xlabel(0(4)20) xline(12 16) restore Fitted values from piecewise model with two knots and two jumps\rAutomate the creation of the graph *First,rerun the -margins- command from above quietly margins gender,at(hsgrad=0 cograd=0 ed1=0 ed2=0 ed3=0) /// at(hsgrad=0 cograd=0 ed1=12 ed2=0 ed3=0) /// at(hsgrad=1 cograd=0 ed1=12 ed2=0 ed3=0) /// at(hsgrad=1 cograd=0 ed1=12 ed2=4 ed3=0) /// at(hsgrad=1 cograd=1 ed1=12 ed2=4 ed3=0) /// at(hsgrad=1 cograd=1 ed1=12 ed2=4 ed3=4) /// *Store the adjusted means(from the -margins- command) in a matrix named -yhat- matrix yhat = r(b)' //note: we must transpose r(b) here *Store levels of education in a matrix named -educ- matrix educ = (0\\0\\12\\12\\12\\12\\16\\16\\16\\16\\20\\20) *Store levels of gender in a matrix named -gender- matrix gender = (1\\2\\1\\2\\1\\2\\1\\2\\1\\2\\1\\2) svmat yhat //save the matrix -yhat- to the current dataset svmat educ //save the matrix -educ- to the current dataset svmat gender //save the matrix -gender- to the current dataset graph twoway (line yhat1 educ1 if gender1==1) /// (line yhat1 educ1 if gender1==2), /// xline(12 16) legend(label(1 \"Men\") label(2 \"Women\")) /// xtitle(Education) ytitle(Adjusted mean) ","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:2:7","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3 Comparing coding schemes This chapter has focused on one coding scheme for fitting models that interact a categorical variable and a continuous variable fit in a piecewise manner.Depending on your research question, another coding scheme might be more useful. ","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:3:0","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.1 Coding scheme #1 This coding scheme will be called coding scheme #1. The noheader option is used in this and subsequent examples to save space. use gss_ivrm.dta mkspline ed1 12 ed2 = educ reg realrinc ibn.gender ibn.gender#(i.hsgrad c.ed1 c.ed2) i.race,vce(robust) noconstant noheader noci --------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t|\r----------------+---------------------------------------\rgender |\rMale | 17144.66 1389.51 12.34 0.000\rFemale | 7325.48 922.52 7.94 0.000\r|\rgender#hsgrad |\rMale#HS Grad | 3382.00 661.55 5.11 0.000\rFemale#HS Grad | 1748.10 389.36 4.49 0.000\r|\rgender#c.ed1 |\rMale | 86.71 151.38 0.57 0.567\rFemale | 241.61 98.92 2.44 0.015\r|\rgender#c.ed2 |\rMale | 4057.64 150.38 26.98 0.000\rFemale | 2529.55 110.43 22.91 0.000\r|\rrace |\rblack | -3521.34 246.66 -14.28 0.000\rother | -1946.16 839.73 -2.32 0.020\r--------------------------------------------------------\rIntercept and slope coefficients from piecewise regression fit using coding scheme #1\r","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:3:1","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.2 Coding scheme #2 Let’s now fit a model using what I call coding scheme #2. This coding scheme is the same as coding scheme #1, except that the marginal option is used on the mkspline command. use gss_ivrm.dta mkspline ed1m 12 ed2m = educ,marginal reg realrinc ibn.gender ibn.gender#(i.hsgrad c.ed1m c.ed2m) i.race,vce(robust) noconstant noheader noci --------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t|\r----------------+---------------------------------------\rgender |\rMale | 17144.66 1389.51 12.34 0.000\rFemale | 7325.48 922.52 7.94 0.000\r|\rgender#hsgrad |\rMale#HS Grad | 3382.00 661.55 5.11 0.000\rFemale#HS Grad | 1748.10 389.36 4.49 0.000\r|\rgender#c.ed1m |\rMale | 86.71 151.38 0.57 0.567\rFemale | 241.61 98.92 2.44 0.015\r|\rgender#c.ed2m |\rMale | 3970.93 212.70 18.67 0.000\rFemale | 2287.94 147.26 15.54 0.000\r|\rrace |\rblack | -3521.34 246.66 -14.28 0.000\rother | -1946.16 839.73 -2.32 0.020\r--------------------------------------------------------\rSummary of regression results and meaning of coefficients for coding schemes #1 and #2\rYou can see that the only difference is in the final row of the table. Coding scheme #1 estimates $\\beta\\tiny M2$ and $\\beta\\tiny F2$, whereas coding scheme #2 estimates ($\\beta\\tiny M2$-$\\beta\\tiny M1$)and ($\\beta\\tiny F2$-$\\beta\\tiny F1$). ","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:3:2","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.3 Coding scheme #3 This coding scheme is like coding scheme #1, in that the marginal option is omitted from the mkspline command. Unlike coding scheme #1, coding scheme #3 specifies i.gender (instead of ibn.gender) and omits the noconstant option use gss_ivrm.dta mkspline ed1 12 ed2 = educ reg realrinc i.gender##(i.hsgrad c.ed1 c.ed2) i.race,vce(robust) noheader noci --------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t|\r----------------+---------------------------------------\rgender |\rFemale | -9819.17 1638.56 -5.99 0.000\r|\rhsgrad |\rHS Grad | 3382.00 661.55 5.11 0.000\red1 | 86.71 151.38 0.57 0.567\red2 | 4057.64 150.38 26.98 0.000\r|\rgender#hsgrad |\rFemale#HS Grad | -1633.91 766.98 -2.13 0.033\r|\rgender#c.ed1 |\rFemale | 154.90 179.32 0.86 0.388\r|\rgender#c.ed2 |\rFemale | -1528.09 187.09 -8.17 0.000\r|\rrace |\rblack | -3521.34 246.66 -14.28 0.000\rother | -1946.16 839.73 -2.32 0.020\r|\r_cons | 17144.66 1389.51 12.34 0.000\r--------------------------------------------------------\r","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:3:3","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.4 Coding scheme #4 Finally, let’s consider a fourth coding scheme. This coding scheme is like coding scheme #3, except that the marginal option is included on the mkspline command. use gss_ivrm.dta mkspline ed1m 12 ed2m = educ,marginal reg realrinc i.gender##(i.hsgrad c.ed1m c.ed2m) i.race,vce(robust) noheader noci --------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t|\r----------------+---------------------------------------\rgender |\rFemale | -9819.17 1638.56 -5.99 0.000\r|\rhsgrad |\rHS Grad | 3382.00 661.55 5.11 0.000\red1m | 86.71 151.38 0.57 0.567\red2m | 3970.93 212.70 18.67 0.000\r|\rgender#hsgrad |\rFemale#HS Grad | -1633.91 766.98 -2.13 0.033\r|\rgender#c.ed1m |\rFemale | 154.90 179.32 0.86 0.388\r|\rgender#c.ed2m |\rFemale | -1682.99 259.29 -6.49 0.000\r|\rrace |\rblack | -3521.34 246.66 -14.28 0.000\rother | -1946.16 839.73 -2.32 0.020\r|\r_cons | 17144.66 1389.51 12.34 0.000\r--------------------------------------------------------\rSummary of regression results and meaning of coefficients for coding schemes #3 and #4\rIn comparing coding schemes #3 and #4, the only difference is in the final row of the table. Coding scheme #3 estimates $\\beta\\tiny MM2$ and ($\\beta\\tiny F2$-$\\beta\\tiny M2$). By comparison, coding scheme #4 estimates ($\\beta\\tiny M2$-$\\beta\\tiny M1$) and($\\beta\\tiny F2$-$\\beta\\tiny F1$)-($\\beta\\tiny M2$-$\\beta\\tiny M1$) . ","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:3:4","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.5 Choosing coding schemes We can deliberately choose the coding scheme that might make the most sense given our research question. Say that the emphasis of your research study was to test gender differences in the educ slope among high school graduates (that is,$\\beta\\tiny F2$-$\\beta\\tiny M2$). In that case, coding scheme #3 might be the most useful, because the coefficient associated with gender#ed2 directly estimates this difference ($\\beta\\tiny F2$-$\\beta\\tiny M2$). Had you chosen coding scheme #1, you could still estimate this difference, but would need to also use the contrast gender#c.ed1 command to compute this difference. Instead, imagine that your research question focused on gender differences in the educ slope for high school graduates versus non–high school graduates. In that case, coding system #4 might be the most useful because the coefficient associated with gender#ed2m directly estimates this difference ($\\beta\\tiny F2$-$\\beta\\tiny F1$)-($\\beta\\tiny M2$-$\\beta\\tiny M1$). ","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:3:5","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter explores models that include a categorical variable interacted with two continuous variables. ","date":"2024-01-11","objectID":"/13.chapter13continuous-by-continuous-by-categorical-interactions/","tags":["Interaction","stata"],"title":"Chapter13 ：Continuous by continuous by categorical interactions","uri":"/13.chapter13continuous-by-continuous-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter explores models that include a categorical variable interacted with two continuous variables. ","date":"2024-01-11","objectID":"/13.chapter13continuous-by-continuous-by-categorical-interactions/:0:0","tags":["Interaction","stata"],"title":"Chapter13 ：Continuous by continuous by categorical interactions","uri":"/13.chapter13continuous-by-continuous-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 Linear by linear by categorical interactions In this section, we will explore whether the size of the c.age##c.educ interaction depends on gender. ","date":"2024-01-11","objectID":"/13.chapter13continuous-by-continuous-by-categorical-interactions/:1:0","tags":["Interaction","stata"],"title":"Chapter13 ：Continuous by continuous by categorical interactions","uri":"/13.chapter13continuous-by-continuous-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.1 Fitting separate models for males and females To get a general sense of the size of the c.age#c.educ interaction separately for males and females, let’s fit separate models for males and females. Doing so separately for each level of gender by including the by gender, sort: prefix. (The vsquish and noheader options are used to save space.) The first set of results is restricted to analyzing only males, and the second set is restricted to analyzing only females. use gss_ivrm.dta keep if (age\u003e=22 \u0026 age\u003c=55) \u0026 (educ\u003e=12) by gender,sort:regress realrinc c.age##c.educ,vce(robust)vsquish noheader --------------------------------------------------------------------------------------------------------------\r-\u003e gender = Male\r------------------------------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\rage | -1092.29 195.71 -5.58 0.000 -1475.92 -708.65\reduc | -1831.07 526.01 -3.48 0.001 -2862.14 -799.99\rc.age#c.educ | 134.46 14.35 9.37 0.000 106.33 162.60\r_cons | 25275.91 7128.77 3.55 0.000 11302.24 39249.58\r------------------------------------------------------------------------------\r--------------------------------------------------------------------------------------------------------------\r-\u003e gender = Female\r------------------------------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\rage | -876.56 158.24 -5.54 0.000 -1186.74 -566.39\reduc | -966.11 426.78 -2.26 0.024 -1802.69 -129.54\rc.age#c.educ | 87.95 11.97 7.34 0.000 64.47 111.42\r_cons | 17020.70 5661.95 3.01 0.003 5922.30 28119.10\r------------------------------------------------------------------------------\rWe can visualize this in two different ways, by focusing on the slope in the direction of age or by focusing on the slope in the direction of educ. Let’s begin by making a graph that focuses on the slope in the direction of age by placing age on the $x$ axis, with separate lines for educ. Fitted values for age by education interaction for males (left) and females (right)\rAmong males, the age slope increases by 134.46 units for every oneyear increase in educ. Among females, the age slope increases by 87.95 units for every one-year increase in educ. We can visualize the c.age#c.educ interaction another way, focusing on the educ slope by placing educ on the $x$ axis, with separate lines for age. Fitted values for age by education interaction for males (left) and females (right) with education on the $x$ axis\rFor males, the educ slope increases by 134.46 units for every one-year increase in age. For females, the educ slope increases by 87.95 units for every oneyear increase in age. To summarize, the coefficient for c.age#c.educ is 134.46 for males and is 87.95 for females. This suggests that the size of the c.age#c.educ coefficient may be significantly greater for males than for females. Let’s test this by analyzing males and females together in a single model. ","date":"2024-01-11","objectID":"/13.chapter13continuous-by-continuous-by-categorical-interactions/:1:1","tags":["Interaction","stata"],"title":"Chapter13 ：Continuous by continuous by categorical interactions","uri":"/13.chapter13continuous-by-continuous-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.2 Fitting a combined model for males and females We now create a regression model that includes both males and females and predicts realrinc from age, educ, and the interaction of these two continuous variables. By specifying ibn.gender in conjunction with the noconstant option, the model fits separate intercepts by gender. By specifying the interaction of ibn.gender with c.age, with c.educ, and with c.age#c.educ, the model fits separate estimates of age, educ, and age#educ by gender. reg realrinc ibn.gender ibn.gender#(c.age c.educ c.age#c.educ)i.race,vce(robust) noconstant noci Linear regression Number of obs = 22,367\rF(10, 22357) = 3126.19\rProb \u003e F = 0.0000\rR-squared = 0.5339\rRoot MSE = 24543\r------------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t|\r--------------------+---------------------------------------\rgender |\rMale | 27044.09 7125.42 3.80 0.000\rFemale | 19020.31 5695.44 3.34 0.001\r|\rgender#c.age |\rMale | -1116.24 195.43 -5.71 0.000\rFemale | -903.50 158.76 -5.69 0.000\r|\rgender#c.educ |\rMale | -1929.04 526.08 -3.67 0.000\rFemale | -1063.55 428.25 -2.48 0.013\r|\rgender#c.age#c.educ |\rMale | 136.00 14.34 9.48 0.000\rFemale | 89.52 12.01 7.46 0.000\r|\rrace |\rblack | -3067.11 305.07 -10.05 0.000\rother | 493.01 1192.16 0.41 0.679\r------------------------------------------------------------\rWe can express these results as two though there are two regression equations, one for males and one for females. \\begin{align} Males:\\widehat{realrinc} = 27044.09 + - 1116.24age + - 1929.04educ \\notag\\ \\end{align} \\begin{align} \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\ +136.00age*educ + -3067.11black + 493.01other \\notag \\ \\end{align} \\begin{align} Females:\\widehat{realrinc} = 19020.31 + - 903.50age + - 1063.55educ \\notag\\ \\end{align} \\begin{align} \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\ +89.52age*educ + -3067.11black + 493.01other \\notag \\ \\end{align} Let’s now test the difference of the c.age#c.educ interaction for females versus males using the contrast command below. contrast gender#c.age#c.educ,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r--------------------+---------------------------------------\rgender#c.age#c.educ |\r(Female vs base) | -46.48 18.71 -2.48 0.013\r------------------------------------------------------------\rThis test is significant,This means that the c.age#c.educ interaction is significantly lower for females than for males. Let’s explore how to interpret this interaction. Note! What about the lower order effects? We have been focusing on the gender#c.age#c.educ interaction, but you might wonder about the lower order effects, such as gender#c.age or gender#c.educ. It is important to include these effects in the model to preserve the interpretation of the gender#c.age#c.educ interaction. However, there is little to gain by trying to interpret these effects. ","date":"2024-01-11","objectID":"/13.chapter13continuous-by-continuous-by-categorical-interactions/:1:2","tags":["Interaction","stata"],"title":"Chapter13 ：Continuous by continuous by categorical interactions","uri":"/13.chapter13continuous-by-continuous-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.3 Interpreting the interaction focusing in the age slope To help interpret this interaction, let’s visualize it by making a graph that focuses on the age slope. margins gender, at(age=(22 55) educ=(12(2)20)) marginsplot,bydimension(gender) plotdimension(educ,allsimple)legend(subtitle(Education) rows(2)) noci Fitted values by age ( axis), education (separate lines), and gender (separate panels)\rFor males, the age slope increases by 136.00 units for every one-unit increase in education. For females, the age slope increases by 89.52 units for every one-unit increase in education. The test of the gender#c.age#c.educ effect represents the difference in these interaction terms and this graph shows one way to visualize this. The margins command can be used to show the size of the age slope by specifying the dydx(age) option. Let’s use the margins command to estimate the age slope for each of the levels of education expressed as a separate line in figure margins gender,at(educ=(12(2)20)) dydx(age) Average marginal effects Number of obs = 22,367\rModel VCE: Robust\rExpression: Linear prediction, predict()\rdy/dx wrt: age\r1._at: educ = 12\r2._at: educ = 14\r3._at: educ = 16\r4._at: educ = 18\r5._at: educ = 20\r------------------------------------------------------------------------------\r| Delta-method\r| dy/dx std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\rage |\r_at#gender |\r1#Male | 515.76 34.61 14.90 0.000 447.93 583.60\r1#Female | 170.70 19.56 8.73 0.000 132.36 209.03\r2#Male | 787.76 28.15 27.98 0.000 732.58 842.94\r2#Female | 349.73 16.77 20.85 0.000 316.85 382.61\r3#Male | 1059.76 45.09 23.50 0.000 971.38 1148.14\r3#Female | 528.76 36.51 14.48 0.000 457.19 600.33\r4#Male | 1331.76 70.14 18.99 0.000 1194.28 1469.24\r4#Female | 707.80 59.48 11.90 0.000 591.20 824.39\r5#Male | 1603.76 97.22 16.50 0.000 1413.20 1794.32\r5#Female | 886.83 83.05 10.68 0.000 724.06 1049.60\r------------------------------------------------------------------------------\r","date":"2024-01-11","objectID":"/13.chapter13continuous-by-continuous-by-categorical-interactions/:1:3","tags":["Interaction","stata"],"title":"Chapter13 ：Continuous by continuous by categorical interactions","uri":"/13.chapter13continuous-by-continuous-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.4 Interpreting the interaction focusing on the educ slope Now, let’s visualize this interaction by making a graph that focuses on the educ slope. margins gender, at(age=(25(10)55) educ=(12 20)) marginsplot, bydimension(gender) xdimension(educ) noci legend(rows(2)subtitle(Age)) Fitted values by education ( axis), age (separate lines), and gender (separate panels)\rThis graph illustrates the gender#c.age#c.educ interaction by showing how the educ slope increases more rapidly as a function of age for males than for females. The dydx(educ) option can be used with the margins command to compute the educ slope for each of the lines displayed in figure margins gender,at(age=(25(10)55)) dydx(educ) Average marginal effects Number of obs = 22,367\rModel VCE: Robust\rExpression: Linear prediction, predict()\rdy/dx wrt: educ\r1._at: age = 25\r2._at: age = 35\r3._at: age = 45\r4._at: age = 55\r------------------------------------------------------------------------------\r| Delta-method\r| dy/dx std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\reduc |\r_at#gender |\r1#Male | 1470.96 206.19 7.13 0.000 1066.82 1875.10\r1#Female | 1174.36 155.32 7.56 0.000 869.91 1478.81\r2#Male | 2830.96 144.29 19.62 0.000 2548.14 3113.78\r2#Female | 2069.52 104.23 19.85 0.000 1865.22 2273.83\r3#Male | 4190.97 200.68 20.88 0.000 3797.61 4584.32\r3#Female | 2964.69 162.58 18.24 0.000 2646.02 3283.35\r4#Male | 5550.97 317.61 17.48 0.000 4928.43 6173.50\r4#Female | 3859.85 266.13 14.50 0.000 3338.21 4381.49\r------------------------------------------------------------------------------\r","date":"2024-01-11","objectID":"/13.chapter13continuous-by-continuous-by-categorical-interactions/:1:4","tags":["Interaction","stata"],"title":"Chapter13 ：Continuous by continuous by categorical interactions","uri":"/13.chapter13continuous-by-continuous-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.5 Estimating and comparing adjusted means by gender The difference in the adjusted means by gender varies as a function of age and educ due to the interaction of gender with c.age#c.educ. Thus, any comparisons by gender should be performed by specifying a particular value of age and educ. The size of the difference (as well as the significance of the difference) between males and females depends on both educ and age. You could repeat the margins commands above to obtain comparisons between males and females for a variety ofvalues of age and educ. Or you can specify multiple values at once within the margins command. or example, the margins command below estimates the adjusted mean for males and females for the combinations of 12, 15, and 20 years of education and 30, 40, and 50 years of age margins gender,at(educ=(12 16 20)age=(30 40 50)) Predictive margins Number of obs = 22,367\rModel VCE: Robust\rExpression: Linear prediction, predict()\r1._at: age = 30\reduc = 12\r2._at: age = 30\reduc = 16\r3._at: age = 30\reduc = 20\r4._at: age = 40\reduc = 12\r5._at: age = 40\reduc = 16\r6._at: age = 40\reduc = 20\r7._at: age = 50\reduc = 12\r8._at: age = 50\reduc = 16\r9._at: age = 50\reduc = 20\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_at#gender |\r1#Male | 18995.85 290.94 65.29 0.000 18425.60 19566.10\r1#Female | 11006.03 210.47 52.29 0.000 10593.50 11418.57\r2#Male | 27599.71 480.25 57.47 0.000 26658.37 28541.04\r2#Female | 17493.80 332.35 52.64 0.000 16842.37 18145.22\r3#Male | 36203.56 1106.97 32.71 0.000 34033.83 38373.29\r3#Female | 23981.56 788.13 30.43 0.000 22436.77 25526.36\r4#Male | 24153.48 377.11 64.05 0.000 23414.31 24892.64\r4#Female | 12713.02 213.38 59.58 0.000 12294.77 13131.26\r5#Male | 38197.33 507.34 75.29 0.000 37202.92 39191.75\r5#Female | 22781.44 373.68 60.96 0.000 22048.99 23513.88\r6#Male | 52241.19 1088.80 47.98 0.000 50107.08 54375.31\r6#Female | 32849.85 845.68 38.84 0.000 31192.26 34507.45\r7#Male | 29311.10 662.83 44.22 0.000 28011.91 30610.30\r7#Female | 14420.00 351.09 41.07 0.000 13731.84 15108.17\r8#Male | 48794.96 831.12 58.71 0.000 47165.91 50424.02\r8#Female | 28069.07 659.91 42.53 0.000 26775.61 29362.54\r9#Male | 68278.83 1742.40 39.19 0.000 64863.60 71694.06\r9#Female | 41718.15 1479.36 28.20 0.000 38818.50 44617.79\r------------------------------------------------------------------------------\rmargins r.gender,at(educ=(12 16 20) age=(30 40 50)) Contrasts of predictive margins Number of obs = 22,367\rModel VCE: Robust\rExpression: Linear prediction, predict()\r1._at: age = 30\reduc = 12\r2._at: age = 30\reduc = 16\r3._at: age = 30\reduc = 20\r4._at: age = 40\reduc = 12\r5._at: age = 40\reduc = 16\r6._at: age = 40\reduc = 20\r7._at: age = 50\reduc = 12\r8._at: age = 50\reduc = 16\r9._at: age = 50\reduc = 20\r-------------------------------------------------------\r| df F P\u003eF\r--------------------+----------------------------------\rgender@_at |\r(Female vs Male) 1 | 1 499.54 0.0000\r(Female vs Male) 2 | 1 296.57 0.0000\r(Female vs Male) 3 | 1 80.27 0.0000\r(Female vs Male) 4 | 1 701.62 0.0000\r(Female vs Male) 5 | 1 595.57 0.0000\r(Female vs Male) 6 | 1 196.14 0.0000\r(Female vs Male) 7 | 1 395.11 0.0000\r(Female vs Male) 8 | 1 381.32 0.0000\r(Female vs Male) 9 | 1 134.57 0.0000\rJoint | 4 483.36 0.0000\r|\rDenominator | 22357\r-------------------------------------------------------\r---------------------------------------------------------------------\r| Delta-method\r| Contrast std. err. [95% conf. interval]\r--------------------+------------------------------------------------\rgender@_at |\r(Female vs Male) 1 | -7989.82 357.48 -8690.50 -7289.14\r(Female vs Male) 2 | -10105.91 586.83 -11256.13 -8955.68\r(Female vs Male) 3 | -12222.00 1364.16 -14895.84 -9548.16\r(Female vs Male) 4 | -11440.46 431.91 -12287.03 -10593.89\r(Female vs Male) 5 | -15415.90 631.69 -16654.05 -14177.75\r(Female vs Male) 6 | ","date":"2024-01-11","objectID":"/13.chapter13continuous-by-continuous-by-categorical-interactions/:1:5","tags":["Interaction","stata"],"title":"Chapter13 ：Continuous by continuous by categorical interactions","uri":"/13.chapter13continuous-by-continuous-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2 Linear by quadratic by categorical interactions In this section, we will explore whether the size of the c.educ#c.age#c.age interaction depends on gender. ","date":"2024-01-11","objectID":"/13.chapter13continuous-by-continuous-by-categorical-interactions/:2:0","tags":["Interaction","stata"],"title":"Chapter13 ：Continuous by continuous by categorical interactions","uri":"/13.chapter13continuous-by-continuous-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.1 Fitting separate models for males and females Let’s begin by fitting a model that estimates the c.educ#c.age#c.age interaction in two separate models: one fit for males and another fit for females. This is performed by using the by gender, sort: prefix before the regress command that predicts realrinc from c.educ#c.age#c.age. use gss_ivrm.dta keep if (age\u003e=22 \u0026 age\u003c=80) \u0026 (educ\u003e=12) by gender,sort:reg realrinc c.educ##c.age##c.age,vce(robust)vsquish noheader ------------------------------------------------------------------------------------\r-\u003e gender = Male\r------------------------------------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\r-------------------+----------------------------------------------------------------\reduc | -12420.54 1305.49 -9.51 0.000 -14979.50 -9861.58\rage | -6160.36 876.56 -7.03 0.000 -7878.55 -4442.17\rc.educ#c.age | 661.99 65.00 10.18 0.000 534.57 789.41\rc.age#c.age | 56.96 9.96 5.72 0.000 37.43 76.48\rc.educ#c.age#c.age | -6.21 0.74 -8.43 0.000 -7.66 -4.77\r_cons | 131388.59 17570.69 7.48 0.000 96947.42 165829.75\r------------------------------------------------------------------------------------\r------------------------------------------------------------------------------------\r-\u003e gender = Female\r------------------------------------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\r-------------------+----------------------------------------------------------------\reduc | -6103.40 867.43 -7.04 0.000 -7803.68 -4403.12\rage | -3703.10 599.49 -6.18 0.000 -4878.18 -2528.02\rc.educ#c.age | 366.29 45.81 8.00 0.000 276.49 456.08\rc.age#c.age | 35.83 7.23 4.95 0.000 21.65 50.00\rc.educ#c.age#c.age | -3.57 0.56 -6.43 0.000 -4.66 -2.48\r_cons | 69959.52 11406.57 6.13 0.000 47600.98 92318.06\r------------------------------------------------------------------------------------\rThis suggests that the size of this interaction might be more negative for males than for females. Before pursuing whether this difference is significant, let’s first visualize the c.educ#c.age#c.age interaction by gender to gain a further understanding of what this interaction means. Fitted values for education by age-squared interaction for males (left) and females (right)\rthe male exhibit a greater increase in the curvature in the relationship between age and income as a function of education than do females. Let’s test whether the c.educ#c.age#c.age interaction is significantly different for males versus females. To do this, we fit a combined model that includes both males and females to permit a statistical test of c.educ#c.age#c.age by gender. ","date":"2024-01-11","objectID":"/13.chapter13continuous-by-continuous-by-categorical-interactions/:2:1","tags":["Interaction","stata"],"title":"Chapter13 ：Continuous by continuous by categorical interactions","uri":"/13.chapter13continuous-by-continuous-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.2 Fitting a common model for males and females Let’s fit one model for males and females together using a separate intercept and separate slopes coding system that provides separate intercept and slope estimates for males and females. Separate intercepts by gender are obtained by specifying ibn.gender in conjunction with the noconstant option. Then, the model uses the shortcut notation to interact ibn.gender with each term created by c.educ#c.age#c.age. reg realrinc ibn.gender ibn.gender#(c.educ##c.age##c.age) i.race,vce(robust)noconstant noci Linear regression Number of obs = 25,964\rF(14, 25950) = 2544.34\rProb \u003e F = 0.0000\rR-squared = 0.5263\rRoot MSE = 25748\r------------------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t|\r--------------------------+---------------------------------------\rgender |\rMale | 133604.98 17548.76 7.61 0.000\rFemale | 73711.44 11412.10 6.46 0.000\r|\rgender#c.educ |\rMale | -12550.64 1305.36 -9.61 0.000\rFemale | -6329.01 867.97 -7.29 0.000\r|\rgender#c.age |\rMale | -6202.10 875.06 -7.09 0.000\rFemale | -3809.82 598.99 -6.36 0.000\r|\rgender#c.educ#c.age |\rMale | 665.21 64.93 10.24 0.000\rFemale | 374.24 45.78 8.17 0.000\r|\rgender#c.age#c.age |\rMale | 57.18 9.95 5.75 0.000\rFemale | 36.76 7.22 5.09 0.000\r|\rgender#c.educ#c.age#c.age |\rMale | -6.23 0.74 -8.47 0.000\rFemale | -3.65 0.55 -6.58 0.000\r|\rrace |\rblack | -3474.03 291.35 -11.92 0.000\rother | -299.12 1110.20 -0.27 0.788\r------------------------------------------------------------------\rWe can express results as separate regression equations for males and females, as shown below \\begin{align} Males:\\widehat{realrinc} = 133605 + -12550.64educ + -6202.10age \\notag\\ \\end{align} \\begin{align} \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\ +665.21educ × age + 57.18age^2+-6.23educ × age^2 \\notag \\ \\end{align} \\begin{align} \\quad \\quad \\quad \\quad \\ +-3474.03black + -299.12other \\notag \\ \\end{align} \\begin{align} Females:\\widehat{realrinc} = 73711.44 + -6329.01educ + -3809.82age \\notag\\ \\end{align} \\begin{align} \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\ +665.21educ × age + 57.18age^2+-6.23educ × age^2 \\notag \\ \\end{align} \\begin{align} \\quad \\quad \\quad \\quad \\ +-3474.03black + -299.12other \\notag \\ \\end{align} The coefficient for the c.educ#c.age#c.age interaction is $-6.23$ for males and is $-3.65$ for females. We can compare these coefficients using the contrast command, as shown below. ","date":"2024-01-11","objectID":"/13.chapter13continuous-by-continuous-by-categorical-interactions/:2:2","tags":["Interaction","stata"],"title":"Chapter13 ：Continuous by continuous by categorical interactions","uri":"/13.chapter13continuous-by-continuous-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.3 Interpreting the interaction To help us understand this interaction, let’s visualize it by making a graph showing the c.educ#c.age#c.age interaction separately for males and females. compute adjusted means for age 22 to 90(in one-year increments) and for 12 to 20 years of education (in two-year increments),separately for males and females. margins gender,at(age=(22(1)80) educ=(12(2)20)) marginsplot,bydimension(gender) xdimension(age)plotdimension(educ,allsimp) noci recast(line) scheme(s1mono)legend(subtitle(Education)rows(1)) Fitted values by age ( axis), education (separate lines), and gender (separate panels)\rWe see a similar pattern for both males and females; as educ increases, the inverted U-shape for the relationship between income and age becomes more pronounced. However, this effect appears stronger for males than for females. This is confirmed by the significant gender#c.educ#c.age#c.age interaction. In other words, this interaction shows that the degree to which the quadratic effect of age changes as a function of educ is stronger for males than it is for females. ","date":"2024-01-11","objectID":"/13.chapter13continuous-by-continuous-by-categorical-interactions/:2:3","tags":["Interaction","stata"],"title":"Chapter13 ：Continuous by continuous by categorical interactions","uri":"/13.chapter13continuous-by-continuous-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.4 Estimating and comparing adjusted means by gender The difference in income for males and females depends on both age and gender. The margins command can be used to compute estimates of, and differences in, the adjusted means by gender. margins gender,at(educ=16 age=30) Predictive margins Number of obs = 25,964\rModel VCE: Robust\rExpression: Linear prediction, predict()\rAt: educ = 16\rage = 30\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\rgender |\rMale | 27288.24 430.84 63.34 0.000 26443.78 28132.71\rFemale | 17867.20 303.52 58.87 0.000 17272.28 18462.12\r------------------------------------------------------------------------------\rBy adding the r. contrast operator to gender, we estimate the difference in these adjusted means, comparing females with males. This difference is significant. margins r.gender,at(educ=16 age=30) contrast(nowald pveffects) Contrasts of predictive margins Number of obs = 25,964\rModel VCE: Robust\rExpression: Linear prediction, predict()\rAt: educ = 16\rage = 30\r----------------------------------------------------------\r| Delta-method\r| Contrast std. err. t P\u003e|t|\r------------------+---------------------------------------\rgender |\r(Female vs Male) | -9421.04 529.92 -17.78 0.000\r----------------------------------------------------------\rWe can specify multiple values in the at() option to estimate the mean for males and females for various combinations of age and educ. The margins command below estimates the mean for males and females for the combinations of 12, 16, and 20 years of education and 30, 40, and 50 years of age. margins gender,at(educ=(12 16 20) age=(30 40 50)) Predictive margins Number of obs = 25,964\rModel VCE: Robust\rExpression: Linear prediction, predict()\r1._at: educ = 12\rage = 30\r2._at: educ = 12\rage = 40\r3._at: educ = 12\rage = 50\r4._at: educ = 16\rage = 30\r5._at: educ = 16\rage = 40\r6._at: educ = 16\rage = 50\r7._at: educ = 20\rage = 30\r8._at: educ = 20\rage = 40\r9._at: educ = 20\rage = 50\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_at#gender |\r1#Male | 20106.88 284.18 70.75 0.000 19549.88 20663.89\r1#Female | 11414.41 198.42 57.53 0.000 11025.50 11803.33\r2#Male | 25574.42 439.87 58.14 0.000 24712.25 26436.58\r2#Female | 13294.97 249.62 53.26 0.000 12805.70 13784.24\r3#Male | 27517.24 502.91 54.72 0.000 26531.51 28502.98\r3#Female | 13767.08 273.95 50.25 0.000 13230.11 14304.04\r4#Male | 27288.24 430.84 63.34 0.000 26443.78 28132.71\r4#Female | 17867.20 303.52 58.87 0.000 17272.28 18462.12\r5#Male | 41909.91 621.06 67.48 0.000 40692.60 43127.22\r5#Female | 24497.60 435.14 56.30 0.000 23644.71 25350.50\r6#Male | 48019.96 712.27 67.42 0.000 46623.87 49416.06\r6#Female | 26799.67 508.67 52.69 0.000 25802.64 27796.70\r7#Male | 34469.61 988.60 34.87 0.000 32531.90 36407.31\r7#Female | 24319.99 710.54 34.23 0.000 22927.30 25712.68\r8#Male | 58245.41 1334.69 43.64 0.000 55629.34 60861.49\r8#Female | 35700.24 981.01 36.39 0.000 33777.40 37623.07\r9#Male | 68522.68 1525.11 44.93 0.000 65533.38 71511.98\r9#Female | 39832.27 1144.65 34.80 0.000 37588.69 42075.85\r------------------------------------------------------------------------------\rmargins r.gender,at(educ=(12 16 20)age=(30 40 50)) Contrasts of predictive margins Number of obs = 25,964\rModel VCE: Robust\rExpression: Linear prediction, predict()\r1._at: educ = 12\rage = 30\r2._at: educ = 12\rage = 40\r3._at: educ = 12\rage = 50\r4._at: educ = 16\rage = 30\r5._at: educ = 16\rage = 40\r6._at: educ = 16\rage = 50\r7._at: educ = 20\rage = 30\r8._at: educ = 20\rage = 40\r9._at: educ = 20\rage = 50\r-------------------------------------------------------\r| df F P\u003eF\r--------------------+----------------------------------\rgender@_at |\r(Female vs","date":"2024-01-11","objectID":"/13.chapter13continuous-by-continuous-by-categorical-interactions/:2:4","tags":["Interaction","stata"],"title":"Chapter13 ：Continuous by continuous by categorical interactions","uri":"/13.chapter13continuous-by-continuous-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter illustrates models that involve a continuous variable modeled using a polynomial term interacted with a categorical variable. ","date":"2024-01-10","objectID":"/11.chapter11polynomial-by-categorical-interactions/","tags":["Interaction","stata"],"title":"Chapter11 ：Polynomial by categorical interactions","uri":"/11.chapter11polynomial-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter illustrates models that involve a continuous variable modeled using a polynomial term interacted with a categorical variable. This chapter covers two types of polynomial terms: quadratic and cubic. ","date":"2024-01-10","objectID":"/11.chapter11polynomial-by-categorical-interactions/:0:0","tags":["Interaction","stata"],"title":"Chapter11 ：Polynomial by categorical interactions","uri":"/11.chapter11polynomial-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 Quadratic by categorical interactions ","date":"2024-01-10","objectID":"/11.chapter11polynomial-by-categorical-interactions/:1:0","tags":["Interaction","stata"],"title":"Chapter11 ：Polynomial by categorical interactions","uri":"/11.chapter11polynomial-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.1 Quadratic by two-level categorical Let’s now use the GSS dataset to fit a model predicting income from age (modeled using a quadratic term), whether one is a college graduate, as well as the interaction of these variables. use gss_ivrm.dta keep if age\u003e=22 \u0026 age\u003c=80 lowess realrinc age,generate(yhat_lowess) nograph graph twoway line yhat_lowess age,sort Let’s begin by looking at the relationship between age and income using a lowess smoother. The lowess command is used to create the variable yhat_lowess that contains the lowess smoothed values of realrinc. The graph command creates a graph showing the lowess smoothed values across age. Lowess smoothed values of income by age\rLet’s create this same kind of graph but separating people based on the variable cograd, which is coded: 0 = noncollege graduate and 1 = college graduate. The lowess command is issued twice, each with an if specification. lowess realrinc age if cograd == 0,generate(yhat_lowess0)nograph lowess realrinc age if cograd == 1,generate(yhat_lowess1)nograph graph twoway line yhat_lowess0 yhat_lowess1 age,sort Lowess smoothed values of income predicted from age by college graduation status\rBased on this visual inspection of the data, a regression model predicting realrinc from age and cograd would not only need to account for the quadratic trend in age, but also the difference in the quadratic trend in age for college graduates versus noncollege graduates. Let’s fit a model that includes an intercept, age, and age squared for noncollege graduates, and a separate intercept, age, and age squared for college graduates. Specifying ibn.cograd with the noconstant option yields separate intercept estimates by college graduation status. Specifying ibn.cograd#c.age yields separate age estimates by college graduation status, and ibn.cograd#c.age#c.age yields separate age#age estimates by college graduation status. reg realrinc ibn.cograd ibn.cograd#c.age ibn.cograd#c.age#c.age female,noconstant vce(robust) Linear regression Number of obs = 30,576\rF(7, 30569) = 5127.56\rProb \u003e F = 0.0000\rR-squared = 0.5088\rRoot MSE = 24865\r------------------------------------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\r-------------------+----------------------------------------------------------------\rcograd |\rNot CO Grad | -11096.99 1107.78 -10.02 0.000 -13268.29 -8925.69\rCO Grad | -52730.93 3688.05 -14.30 0.000 -59959.66 -45502.21\r|\rcograd#c.age |\rNot CO Grad | 1594.87 57.26 27.85 0.000 1482.64 1707.10\rCO Grad | 3941.46 194.37 20.28 0.000 3560.49 4322.43\r|\rcograd#c.age#c.age |\rNot CO Grad | -16.20 0.66 -24.48 0.000 -17.50 -14.90\rCO Grad | -37.81 2.30 -16.46 0.000 -42.31 -33.30\r|\rfemale | -12457.11 278.57 -44.72 0.000 -13003.12 -11911.11\r------------------------------------------------------------------------------------\rNote! Model shortcut Stata expands the expression ibn.cograd#(c.age c.age#c.age) to become ibn.cograd#c.age ibn.cograd#c.age#c.age, yielding the same model shown previously. $$Non-college-grad: \\widehat{realrinc}=-11096.99 + 1594.87age + - 16.20age^2 + - 12457.11female $$ $$College-grad: \\widehat{realrinc}=-52730.93 + 3941.46age + -37.81age^2 + -12457.11female $$ Let’s visualize the impact of the differences in these quadratic coefficients by graphing the adjusted means as a function of age and college graduation status (that is, cograd), adjusting for gender. margins cograd, nopvalues at(age=(22(1)80)) marginsplot,noci recast(line) scheme(simono) Fitted values from quadratic by two-level categorical model\rWe can ask whether the degree of curvature between college graduates and non-college graduates is significantly different. The contrast command below tests whether the quadratic term for non-college graduates is equal to the quadratic term for college graduates. contrast cograd#c.age#c.age Contrasts of marginal linear predictions\rMargins: asbalanced\r-----------------------------------------------","date":"2024-01-10","objectID":"/11.chapter11polynomial-by-categorical-interactions/:1:1","tags":["Interaction","stata"],"title":"Chapter11 ：Polynomial by categorical interactions","uri":"/11.chapter11polynomial-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.2 Quadratic by three-level categorical Let’s begin our exploration by using the lowess command to generate variables that contain the smoothed relationship between realrinc and age, separately for each of the three levels of educ3. use gss_ivrm.dta keep if age\u003e=22 \u0026 age\u003c=80 lowess realrinc age if educ3 == 1,generate(yhat_lowess1) nograph lowess realrinc age if educ3 == 2,generate(yhat_lowess2) nograph lowess realrinc age if educ3 == 3,generate(yhat_lowess3) nograph graph twoway line yhat_lowess1 yhat_lowess2 yhat_lowess3 age,sort legend(order(1 \"Non-HS grad\" 2 \"HS grad\" 3 \"CO grad\")) Lowess smoothed values of income by age, separated by three levels of education\rLet’s now fit a model that includes a quadratic term for age as well as an interaction of educ3 and the quadratic term for age. reg realrinc ibn.educ3 ibn.educ3#(c.age c.age#c.age) female,noconstant vce(robust) Linear regression Number of obs = 30,576\rF(10, 30566) = 3691.50\rProb \u003e F = 0.0000\rR-squared = 0.5129\rRoot MSE = 24761\r-----------------------------------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\r------------------+----------------------------------------------------------------\reduc3 |\rnot hs | -8609.97 1907.08 -4.51 0.000 -12347.92 -4872.01\rHS | -11115.20 1375.93 -8.08 0.000 -13812.07 -8418.32\rColl | -52556.01 3687.82 -14.25 0.000 -59784.28 -45327.74\r|\reduc3#c.age |\rnot hs | 1245.11 93.00 13.39 0.000 1062.82 1427.40\rHS | 1625.58 72.45 22.44 0.000 1483.59 1767.58\rColl | 3940.66 194.33 20.28 0.000 3559.76 4321.57\r|\reduc3#c.age#c.age |\rnot hs | -12.46 1.00 -12.40 0.000 -14.43 -10.49\rHS | -16.04 0.87 -18.54 0.000 -17.74 -14.34\rColl | -37.81 2.30 -16.46 0.000 -42.31 -33.31\r|\rfemale | -12750.67 278.86 -45.72 0.000 -13297.24 -12204.10\r-----------------------------------------------------------------------------------\rThe regression equation is written in this fashion as shown below. $$NonHis-grad:\\widehat{realrinc}=-8609.67 + 1245.11age + - 12.46age^2 + - 12750.67female$$ $$His-grad:\\widehat{realrinc}=-11115.2 + 1625.59age + -16.04age^2 + -12750.67female$$ $$College-grad:\\widehat{realrinc}=-52556.01 + 3940.66age + -37.81age^2 + -12750.67female$$ To help interpret the regression coefficients, let’s use the margins and marginsplot commands to visualize the adjusted means as a function of age and educ3. margins educ3,at(age=(22(1)80)) marginsplot,noci recast(line) scheme(simono) Fitted values from age (quadratic) by education level\rThe contrast command can be used to compare the quadratic coefficients among the educational groups. The contrast command below tests the equality of the quadratic coefficient for all three levels of educ3. contrast educ3#c.age#c.age Contrasts of marginal linear predictions\rMargins: asbalanced\r-----------------------------------------------------\r| df F P\u003eF\r------------------+----------------------------------\reduc3#c.age#c.age | 2 51.24 0.0000\r|\rDenominator | 30566\r-----------------------------------------------------\rThis test is significant, indicating that the quadratic term for age is not equal across all three groups. the a. contrast operator is used below to compare the quadratic terms for adjacent education groups.(Specific compare) contrast a.educ3#c.age#c.age,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r------------------+---------------------------------------\reduc3#c.age#c.age |\r(not hs vs HS) | 3.58 1.33 2.70 0.007\r(HS vs Coll) | 21.77 2.45 8.87 0.000\r----------------------------------------------------------\rLet’s now see how to use the margins command to compute adjusted means. The margins command below computes the adjusted mean holding age constant at 30, separately for each education group. margins educ3,nopvalues at(age=30) Predictive margins Number of obs = 30,576\rModel VCE: Robust\rExpression: Linear prediction, predict()\rAt: ","date":"2024-01-10","objectID":"/11.chapter11polynomial-by-categorical-interactions/:1:2","tags":["Interaction","stata"],"title":"Chapter11 ：Polynomial by categorical interactions","uri":"/11.chapter11polynomial-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2 Cubic by categorical interactions This section describes models that involve interactions of a categorical variable with a continuous variable where the continuous variable is fit using a cubic polynomial term. A cubic by categorical interaction allows the groups formed by the categorical variable to differ in cubic trend. In that example, we saw a cubic relationship between year of birth and number of children. Suppose that we divided women into two groups: those who graduated college and those who did not graduate college. Those who graduated college might show a different kind of cubic trend across years of birth compared with non-college graduates. use gss_ivrm.dta keep if (age\u003e=45 \u0026 age \u003c=55) \u0026 (yrborn\u003e=1920 \u0026 yrborn\u003c=1960) \u0026 female==1 Let’s begin by visualizing the nature of the relationship between year of birth and number of children separately for college graduates and non-college graduates. We can do this using a lowess smoothed regression relating year of birth to number of children. lowess children yrborn if cograd==0,generate(yhatlowess0) nograph lowess children yrborn if cograd==1,generate(yhatlowess1) nograph graph twoway line yhatlowess0 yhatlowess1 yrborn,sort Lowess smoothed values of number of children by year of birth, separated by college graduation status\rThe graph in figure suggests that the relationship between year of birth and number of children may show a cubic trend. Furthermore, the cubic trend may differ based on whether the woman graduated college, suggesting an interaction of the cubic trend for year of birth and whether the woman graduated college. Let’s fit a model using a separate slope and separate intercept strategy that allows us to compare the cubic trend in year of birth for college graduates with non-college graduates. Such a model is fit using the regress command below. The first term in the model is ibn.cograd. When specified in combination with noconstant option, the model fits separate intercepts by college graduation status. The model also includes ibn.cograd interacted with the linear term for year of birth, the quadratic term for year of birth, and the cubic term for year of birth. This yields separate estimates of these terms by college graduation status. reg children ibn.cograd ibn.cograd#(c.yrborn40 c.yrborn40#c.yrborn40 c.yrborn40#c.yrborn40#c.yrborn40) i.race,noconstant noci Note：the centered variable, yrborn40, is used to represent year of birth to reduce collinearity. Source | SS df MS Number of obs = 5,037\r-------------+---------------------------------- F(10, 5027) = 1223.60\rModel | 34370.3789 10 3437.03789 Prob \u003e F = 0.0000\rResidual | 14120.6211 5,027 2.80895585 R-squared = 0.7088\r-------------+---------------------------------- Adj R-squared = 0.7082\rTotal | 48491 5,037 9.62696049 Root MSE = 1.676\r--------------------------------------------------------------------------------\rchildren | Coefficient Std. err. t P\u003e|t|\r----------------------------------------+---------------------------------------\rcograd |\rNot CO Grad | 2.80 0.04 65.64 0.000\rCO Grad | 1.96 0.09 22.43 0.000\r|\rcograd#c.yrborn40 |\rNot CO Grad | -0.09 0.01 -15.21 0.000\rCO Grad | -0.06 0.01 -5.35 0.000\r|\rcograd#c.yrborn40#c.yrborn40 |\rNot CO Grad | -0.00 0.00 -4.07 0.000\rCO Grad | 0.00 0.00 0.41 0.685\r|\rcograd#c.yrborn40#c.yrborn40#c.yrborn40 |\rNot CO Grad | 0.00 0.00 9.42 0.000\rCO Grad | 0.00 0.00 1.88 0.060\r|\rrace |\rblack | 0.57 0.07 8.49 0.000\rother | 0.67 0.13 5.26 0.000\r--------------------------------------------------------------------------------\rThe regression equation are devided to two part as shown below: \\begin{align} Non-college-grad:\\widehat{children} = -2.8 - 0.088yrborn40 + -0.00093yrborn40^2 \\notag\\ \\end{align} \\begin{align} \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\ +0.00021yrborn40^3 + 0.57black + 0.67other \\notag \\ \\end{align} \\begin{align} College-grad:\\widehat{children} = 1.96 + -0.063yrborn + 0.00024yrborn40^2 \\notag\\ \\end{align} \\beg","date":"2024-01-10","objectID":"/11.chapter11polynomial-by-categorical-interactions/:2:0","tags":["Interaction","stata"],"title":"Chapter11 ：Polynomial by categorical interactions","uri":"/11.chapter11polynomial-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter illustrates models that involve interactions of categorical and continuous variables.","date":"2024-01-09","objectID":"/10.chapter10linear-by-categorical-interactions/","tags":["Interaction","stata"],"title":"Chapter10 ：Linear by categorical interactions","uri":"/10.chapter10linear-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter illustrates models that involve interactions of categorical and continuous variables. ","date":"2024-01-09","objectID":"/10.chapter10linear-by-categorical-interactions/:0:0","tags":["Interaction","stata"],"title":"Chapter10 ：Linear by categorical interactions","uri":"/10.chapter10linear-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 Linear and two-level categorical: No interaction ","date":"2024-01-09","objectID":"/10.chapter10linear-by-categorical-interactions/:1:0","tags":["Interaction","stata"],"title":"Chapter10 ：Linear by categorical interactions","uri":"/10.chapter10linear-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.1 Overview This section introduces the concepts involving models that combine continuous and categorical predictors with no interaction. Let’s begin by considering a hypothetical simple regression model predicting income from age, focusing on people who are aged 22 to 55. $$ \\widehat{realrinc}=9000 + 400age $$ Simple linear regression predicting income from age\rLet’s now expand upon this model and introduce a categorical variable with two levels reflecting whether the respondent graduated college (named cograd in the dataset gss_ivrm.dta). It is coded 0 if the respondent did not graduate college and 1 if the respondent did graduate college. One continuous and one categorical predictor with labels for slopes and intercepts\rthe intercept for those who did not graduate college is 4,000 and for those who did graduate college is 21,000. The difference in theseintercepts is 17,000. One equation is given for noncollege graduates and another for college graduates, as shown below. $$ Noncollege - graduate:\\widehat{realrinc}=4000 + 400age $$ $$ College - graduate:\\widehat{realrinc}=21000 + 400age $$ This regression model can also be expressed as one equation, as shown below. $$ \\widehat{realrinc}=4000 + 17000cograd + 400age $$ The predicted values at 30, 40, and 50 years of age have been computed both for those who did and for those who did not graduate college and are graphed in figure below： One continuous and one categorical predictor with labels for predicted values\r","date":"2024-01-09","objectID":"/10.chapter10linear-by-categorical-interactions/:1:1","tags":["Interaction","stata"],"title":"Chapter10 ：Linear by categorical interactions","uri":"/10.chapter10linear-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.2 Examples using the GSS This section applies the model predicting realrinc from age and cograd using the GSS dataset. use gss_ivrm.dta keep if age\u003e=22 \u0026 age\u003c=55 reg realrinc age i.cograd female,vce(robust) Linear regression Number of obs = 25,718\rF(3, 25714) = 906.12\rProb \u003e F = 0.0000\rR-squared = 0.1569\rRoot MSE = 23938\r------------------------------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\rage | 539.90 15.27 35.36 0.000 509.98 569.83\r|\rcograd |\rCO Grad | 14176.56 425.16 33.34 0.000 13343.22 15009.90\rfemale | -12119.58 295.27 -41.05 0.000 -12698.33 -11540.83\r_cons | 4171.57 518.76 8.04 0.000 3154.76 5188.37\r------------------------------------------------------------------------------\rBefore interpreting the coefficients for this model, let’s create a graph showing the adjusted means as a function of age and cograd.2 First, we use the margins command to compute the adjusted means at ages 22 and 55 for each level of cograd. margins cograd,nopvalues at(age=(22 55)) vsquish marginsplot,noci Predictive margins Number of obs = 25,718\rModel VCE: Robust\rExpression: Linear prediction, predict()\r1._at: age = 22\r2._at: age = 55\r----------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r---------------+------------------------------------------------\r_at#cograd |\r1#Not CO Grad | 10048.09 216.20 9624.33 10471.85\r1#CO Grad | 24224.65 398.82 23442.95 25006.35\r2#Not CO Grad | 27864.91 344.23 27190.20 28539.62\r2#CO Grad | 42041.46 553.37 40956.82 43126.11\r----------------------------------------------------------------\rFitted values of continuous and categorical model without interaction\rThese parallel lines have the same slope, as represented by the coefficient for age. Regardless of whether you graduated college, the age slope is 539.90. We can use the margins command to compute adjusted means based on this model. For example, the adjusted mean for someone who graduated college and was 40 years old, adjusting for gender, is 33,942.91. margins,nopvalues at(cograd=1 age=40) Predictive margins Number of obs = 25,718\rModel VCE: Robust\rExpression: Linear prediction, predict()\rAt: age = 40\rcograd = 1\r--------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r-------------+------------------------------------------------\r_cons | 33942.91 419.99 33119.71 34766.11\r--------------------------------------------------------------\rLet’s repeat this command, but this time obtain the adjusted means separately for those who did and for those who did not graduate college. margins cograd,nopvalues at(age=40) Predictive margins Number of obs = 25,718\rModel VCE: Robust\rExpression: Linear prediction, predict()\rAt: age = 40\r--------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r-------------+------------------------------------------------\rcograd |\rNot CO Grad | 19766.35 151.46 19469.48 20063.23\rCO Grad | 33942.91 419.99 33119.71 34766.11\r--------------------------------------------------------------\rNote the difference between these two values corresponds to the main effect of cograd. Here $33942.91 - 19766.35 = 14176.56$ . You could repeat the above command for any given level of age and the difference in the adjusted means would remain the same. The adjusted means are computed by setting all the covariates (that is, age and female) to the average value of the entire sample. In this example, adding the at((mean) age female) option specifies that age and female should be held constant at their mean. margins cograd,nopvalues at((mean) age female) Adjusted predictions Number of obs = 25,718\rModel VCE: Robust\rExpression: Linear prediction, predict()\rAt: age = 37.28214 (mean)\rfemale = .4951785 (mean)\r-----------------------------","date":"2024-01-09","objectID":"/10.chapter10linear-by-categorical-interactions/:1:2","tags":["Interaction","stata"],"title":"Chapter10 ：Linear by categorical interactions","uri":"/10.chapter10linear-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2 Linear by two-level categorical interactions ","date":"2024-01-09","objectID":"/10.chapter10linear-by-categorical-interactions/:2:0","tags":["Interaction","stata"],"title":"Chapter10 ：Linear by categorical interactions","uri":"/10.chapter10linear-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.1 Overview Let’s build upon the model that was illustrated by including age and cograd as predictors, as well as the interaction between age and cograd. Linear by two-level categorical predictor with labels for intercepts and slopes\rThis figure includes labels showing the intercept and slope for those who did not graduate college (labeled “Int0” and “Slope0”) and the intercept and slope for those who did graduate college (labeled “Int1” and “Slope1”). The regression equation for this hypothetical example can be written as shown below. $$ \\widehat{realrinc}=9300 + - 1300cograd + 250age + 450cograd*age $$ The intercept in the regression equation corresponds to the intercept for those who did not graduate college (that is, 9,300). The coefficient for cograd is the difference in the intercepts (that is, ). Note how this represents the difference between a college graduate versus a noncollege graduate when age is held constant at zero (which is implausible and completely absurd). This is often called the main effect of cograd, but that is misleading because in the presence of the interaction, this term represents the effect of cograd when age is held constant at zero. The interaction term (cograd*age) is the difference in the slopes comparing those who graduated college with those who did not graduate college (that is,$700 - 250 = 450$ ). This interaction term compares the slope of college graduates with the slope of noncollege graduates. It can be easier to understand this model when it is written as two equations $$ Noncollege - graduate:\\widehat{realrinc}=9300 + 250age $$ $$ College - graduate:\\widehat{realrinc}=8000 + 700age $$ ","date":"2024-01-09","objectID":"/10.chapter10linear-by-categorical-interactions/:2:1","tags":["Interaction","stata"],"title":"Chapter10 ：Linear by categorical interactions","uri":"/10.chapter10linear-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.2 Examples using the GSS use gss_ivrm.dta keep if age\u003e=22 \u0026 age\u003c=55 reg realrinc i.cograd age i.cograd#c.age female,vce(robust) Linear regression Number of obs = 25,718\rF(4, 25713) = 710.76\rProb \u003e F = 0.0000\rR-squared = 0.1661\rRoot MSE = 23807\r------------------------------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\rcograd |\rCO Grad | -8648.27 1426.59 -6.06 0.000 -11444.48 -5852.07\rage | 375.15 14.68 25.56 0.000 346.39 403.92\r|\rcograd#c.age |\rCO Grad | 607.52 42.24 14.38 0.000 524.73 690.31\r|\rfemale | -12004.56 293.46 -40.91 0.000 -12579.76 -11429.37\r_cons | 10224.97 480.32 21.29 0.000 9283.52 11166.41\r------------------------------------------------------------------------------\rAs a shorthand, we can specify i.cograd##c.age. The ## operator includes both the main effects and interactions of the variables specified. reg realrinc i.cograd##c.age female,vce(robust) Linear regression Number of obs = 25,718\rF(4, 25713) = 710.76\rProb \u003e F = 0.0000\rR-squared = 0.1661\rRoot MSE = 23807\r------------------------------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\rcograd |\rCO Grad | -8648.27 1426.59 -6.06 0.000 -11444.48 -5852.07\rage | 375.15 14.68 25.56 0.000 346.39 403.92\r|\rcograd#c.age |\rCO Grad | 607.52 42.24 14.38 0.000 524.73 690.31\r|\rfemale | -12004.56 293.46 -40.91 0.000 -12579.76 -11429.37\r_cons | 10224.97 480.32 21.29 0.000 9283.52 11166.41\r------------------------------------------------------------------------------\r$$ \\widehat{realrinc}=10224.97 + - 8648.27cograd + 375.15age + 607.5224cograd*age + - 12004.56female $$ Let’s create a graph to aid in the process of interpreting the results. compute the adjusted means for ages22 and 55 separately for each level of cograd margins cograd,nopvalues at(age=(22 55)) marginsplot,noci Predictive margins Number of obs = 25,718\rModel VCE: Robust\rExpression: Linear prediction, predict()\r1._at: age = 22\r2._at: age = 55\r----------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r---------------+------------------------------------------------\r_at#cograd |\r1#Not CO Grad | 12533.97 185.57 12170.25 12897.69\r1#CO Grad | 17251.19 549.53 16174.07 18328.31\r2#Not CO Grad | 24914.08 349.75 24228.55 25599.61\r2#CO Grad | 49679.54 950.38 47816.74 51542.34\r----------------------------------------------------------------\rFitted values for linear by two-level categorical predictor model\rThe figure indicate the slope between cograd \u0026 notcograd are significant different. 2.2.1 Estimates of slopes Let’s use the margins command to compute the age slope for college graduates and noncollege graduates. The dydx(age) option is used with the over(cograd) option to compute the age slope at each level of cograd. margins,dydx(age) over(cograd) Average marginal effects Number of obs = 25,718\rModel VCE: Robust\rExpression: Linear prediction, predict()\rdy/dx wrt: age\rOver: cograd\r------------------------------------------------------------------------------\r| Delta-method\r| dy/dx std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\rage |\rcograd |\rNot CO Grad | 375.15 14.68 25.56 0.000 346.39 403.92\rCO Grad | 982.68 39.55 24.84 0.000 905.15 1060.20\r------------------------------------------------------------------------------\r2.2.2 Estimates and contrasts on means Let’s begin the investigation of the effect of cograd by computing the adjusted mean of income for each level of cograd, holding age constant at 30. margins cograd, nopvalues at(age=30) Predictive margins Number of obs = 25,718\rModel VCE: Robust\rExpression: Linear prediction, predict()\rAt: age = 30\r--------------------------------------","date":"2024-01-09","objectID":"/10.chapter10linear-by-categorical-interactions/:2:2","tags":["Interaction","stata"],"title":"Chapter10 ：Linear by categorical interactions","uri":"/10.chapter10linear-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3 Linear by three-level categorical interactions Let’s now explore a model in which a three-level categorical variable is interacted with a continuous variable. Let’s extend the previous example by considering three educational groups: 1) non–high school graduates, 2) high school graduates, and 3) college graduates. Although not shown in the graph, the intercept is 3,000 for group 1, 3,700 for group 2, and for group 3. Linear by three-level categorical predictor with labels for slopes\r$$ \\widehat{realrinc}=3000 + 700hsgrad + -800cograd + 300age + 100hsgradage + 700cogradage $$ The value of 3,000 is the intercept for those who did not graduate college. The coefficient for hsgrad is the difference in the intercepts between high school graduates and non–high school graduates (that is,$3700 - 3000 = 700$ ). The coefficient for cograd is the difference in the intercepts between college graduates and non–high school graduates (that is,$- 5000 - 3000 = -8000$). $$ Non-hsgrad:\\widehat{realrinc}=3000 + 300age $$ $$ Hsgrad:\\widehat{realrinc}=3700 + 400age $$ $$ College-grad:\\widehat{realrinc}=-5000 + 1000age $$ If we do not reject this null hypothesis, then the education by interaction terms may no longer be needed and could be omitted from the model. If we do reject this null hypothesis, we might be further interested in forming specific contrasts among the slopes. We might be further interested in forming specific contrasts among the slopes, for example, We might be interested in comparing the different educational groups given different levels of age. As an illustration, the predicted mean of income for those aged 30, 40, and 50 for eacheducational group have been computed and are plotted in figure below. ","date":"2024-01-09","objectID":"/10.chapter10linear-by-categorical-interactions/:3:0","tags":["Interaction","stata"],"title":"Chapter10 ：Linear by categorical interactions","uri":"/10.chapter10linear-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.1 Examples using the GSS Let’s continue to use age as the continuous predictor and realrinc as the outcome, but now we will use a three-category education variable, educ3. use gss_ivrm.dta keep if age\u003e=22 \u0026 age\u003c=55 reg realrinc i.educ3##c.age female,vce(robust) Linear regression Number of obs = 25,718\rF(6, 25711) = 563.58\rProb \u003e F = 0.0000\rR-squared = 0.1728\rRoot MSE = 23712\r------------------------------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\reduc3 |\rHS | 1906.98 1098.01 1.74 0.082 -245.18 4059.14\rColl | -6287.54 1643.87 -3.82 0.000 -9509.63 -3065.46\r|\rage | 299.15 28.35 10.55 0.000 243.58 354.72\r|\reduc3#c.age |\rHS | 120.00 32.96 3.64 0.000 55.40 184.60\rColl | 682.88 48.71 14.02 0.000 587.42 778.35\r|\rfemale | -12258.80 293.44 -41.78 0.000 -12833.96 -11683.63\r_cons | 8012.50 944.86 8.48 0.000 6160.51 9864.49\r------------------------------------------------------------------------------\rBefore interpreting these results, let’s make a graph showing the adjusted means by age and educ3. compute the adjusted means at ages22 and 55 for each level of educ3 margins educ3,nopvalues at(age=(22 55)) marginsplot,noci Predictive margins Number of obs = 25,718\rModel VCE: Robust\rExpression: Linear prediction, predict()\r1._at: age = 22\r2._at: age = 55\r--------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r-------------+------------------------------------------------\r_at#educ3 |\r1#not hs | 8523.52 361.18 7815.59 9231.45\r1#HS | 13070.41 208.67 12661.40 13479.43\r1#Coll | 17259.39 549.76 16181.83 18336.94\r2#not hs | 18395.49 655.31 17111.04 19679.94\r2#HS | 26902.25 405.94 26106.58 27697.93\r2#Coll | 49666.47 950.09 47804.25 51528.69\r--------------------------------------------------------------\rFitted values for linear by three-level categorical predictor model\rThe dydx(age) is combined with the over(educ3) option to compute the age slope separately for each level of educ3. 3.1.1 Estimates and contrasts on slopes margins,dydx(age) over(educ3) Average marginal effects Number of obs = 25,718\rModel VCE: Robust\rExpression: Linear prediction, predict()\rdy/dx wrt: age\rOver: educ3\r------------------------------------------------------------------------------\r| Delta-method\r| dy/dx std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\rage |\reduc3 |\rnot hs | 299.15 28.35 10.55 0.000 243.58 354.72\rHS | 419.15 16.85 24.87 0.000 386.12 452.18\rColl | 982.03 39.55 24.83 0.000 904.51 1059.55\r------------------------------------------------------------------------------\rThe output not only shows the age slope at each level of educ3 but also includes the standard error, confidence interval, and a test of whether the slope is significantly different from 0. Let’s test whether these slopes are equal to each other. In other words,let’s test the null hypothesis： $$ H_{0} = \\beta{1} = \\beta{2} = \\beta{3} $$ contrast educ3#c.age Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------\r| df F P\u003eF\r-------------+----------------------------------\reduc3#c.age | 2 105.89 0.0000\r|\rDenominator | 25711\r------------------------------------------------\rThis interaction is significant, so we can reject the null hypothesis that these slopes are all equal. $$ H_{0} = \\beta{1} = \\beta{3} $$ $$ H_{0} = \\beta{2} = \\beta{3} $$ The following contrast command tests these two null hypotheses.Specifying rb3.educ3 uses reference group comparisons with group 3 as the baseline (comparison) group. We can reject both null hypotheses. contrast rb3.educ3#c.age,nowald pveffect Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r------------------+---------------------","date":"2024-01-09","objectID":"/10.chapter10linear-by-categorical-interactions/:3:1","tags":["Interaction","stata"],"title":"Chapter10 ：Linear by categorical interactions","uri":"/10.chapter10linear-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter illustrates models involving interactions of three categorical variables, with an emphasis on how to interpret the interaction of the three categorical variables.","date":"2024-01-07","objectID":"/9.chapter9categorical-by-categorical-by-categorical-interactions/","tags":["Categorical","Interaction","stata"],"title":"Chapter9 ：Categorical by categorical by categorical interactions","uri":"/9.chapter9categorical-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter illustrates models involving interactions of three categorical variables, with an emphasis on how to interpret the interaction of the three categorical variables. This chapter focuses on three types of interactions: two by two by two model two by two by three model three by three by four model ","date":"2024-01-07","objectID":"/9.chapter9categorical-by-categorical-by-categorical-interactions/:0:0","tags":["Categorical","Interaction","stata"],"title":"Chapter9 ：Categorical by categorical by categorical interactions","uri":"/9.chapter9categorical-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 Two by two by two models a new study using a two by two by two model in which treatment has two levels (control group versus happiness therapy), depression status has two levels (nondepressed versus mildly depressed), and season has two levels (winter and summer). opt-2by2by2.dta anova opt depstat##treat##season Number of obs = 240 R-squared = 0.5677\rRoot MSE = 8.01794 Adj R-squared = 0.5546\rSource | Partial SS df MS F Prob\u003eF\r---------------------+----------------------------------------------------\rModel | 19584.517 7 2797.7881 43.52 0.0000\r|\rdepstat | 1601.6667 1 1601.6667 24.91 0.0000\rtreat | 13470.017 1 13470.017 209.53 0.0000\rdepstat#treat | 35.266667 1 35.266667 0.55 0.4596\rseason | 3713.0667 1 3713.0667 57.76 0.0000\rdepstat#season | 220.41667 1 220.41667 3.43 0.0653\rtreat#season | 112.06667 1 112.06667 1.74 0.1880\rdepstat#treat#season | 432.01667 1 432.01667 6.72 0.0101\r|\rResidual | 14914.667 232 64.287356 ---------------------+----------------------------------------------------\rTotal | 34499.183 239 144.34805 Note! Three-way interaction shortcut Specifying depstat##treat##season is a shortcut for specifying all main effects, two-way interactions, and the three-way interaction of depstat, treat, and season. This both saves time and helps ensure that you include all lower order effects. Even if not significant, these lower order effects should be included in the model. The depstat#treat#season interaction is significant ($F = 6.72$,$p = 0.0101$). Let’s use the margins command to show the mean of optimism broken down by these three factors. margins depstat#treat#season,nopvalues Adjusted predictions Number of obs = 240\rExpression: Linear prediction, predict()\r----------------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r---------------------+------------------------------------------------\rdepstat#treat#season |\rNon#Con#Winter | 44.40 1.46 41.52 47.28\rNon#Con#Summer | 51.67 1.46 48.78 54.55\rNon#HT#Winter | 59.93 1.46 57.05 62.82\rNon#HT#Summer | 64.57 1.46 61.68 67.45\rMild#Con#Winter | 39.23 1.46 36.35 42.12\rMild#Con#Summer | 44.97 1.46 42.08 47.85\rMild#HT#Winter | 50.93 1.46 48.05 53.82\rMild#HT#Summer | 64.77 1.46 61.88 67.65\r----------------------------------------------------------------------\rLet’s then use the marginsplot command to make a graph of the means, showing treat on the $x$ axis and the different seasons in separate panels. marginsplot,xdimension(treat) bydimension(season)noci Optimism by treatment, depression status, and season\r","date":"2024-01-07","objectID":"/9.chapter9categorical-by-categorical-by-categorical-interactions/:1:0","tags":["Categorical","Interaction","stata"],"title":"Chapter9 ：Categorical by categorical by categorical interactions","uri":"/9.chapter9categorical-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.1 Simple interactions by season One way that we can dissect the three-way interaction is by looking at the simple interactions of treatment by depression status at each season. Looking at figure，it appears that the interaction is not significant during the winter and is significant during the summer. We test this using the contrast command, which tests the treat#depstat interaction at each level of season. contrast treat#depstat@season Contrasts of marginal linear predictions\rMargins: asbalanced\r--------------------------------------------------------\r| df F P\u003eF\r---------------------+----------------------------------\rtreat#depstat@season |\rWinter | 1 1.71 0.1917\rSummer | 1 5.55 0.0193\rJoint | 2 3.63 0.0279\r|\rDenominator | 232\r--------------------------------------------------------\r","date":"2024-01-07","objectID":"/9.chapter9categorical-by-categorical-by-categorical-interactions/:1:1","tags":["Categorical","Interaction","stata"],"title":"Chapter9 ：Categorical by categorical by categorical interactions","uri":"/9.chapter9categorical-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.2 Simple interactions by depression status Another way to dissect this three-way interaction is by looking at the simple interaction of treatment by season at each level of depression status. To visualize this, let’s rerun the margins command and then use the marginsplot command to graph the means showing treat on the $x$ axis and separate panels for those who are nondepressed and mildly depressed. margins depstat#treat#season marginsplot,xdimension(treat)bydimension(depstat) noci Adjusted predictions Number of obs = 240\rExpression: Linear prediction, predict()\r--------------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r---------------------+----------------------------------------------------------------\rdepstat#treat#season |\rNon#Con#Winter | 44.40 1.46 30.33 0.000 41.52 47.28\rNon#Con#Summer | 51.67 1.46 35.29 0.000 48.78 54.55\rNon#HT#Winter | 59.93 1.46 40.94 0.000 57.05 62.82\rNon#HT#Summer | 64.57 1.46 44.11 0.000 61.68 67.45\rMild#Con#Winter | 39.23 1.46 26.80 0.000 36.35 42.12\rMild#Con#Summer | 44.97 1.46 30.72 0.000 42.08 47.85\rMild#HT#Winter | 50.93 1.46 34.79 0.000 48.05 53.82\rMild#HT#Summer | 64.77 1.46 44.24 0.000 61.88 67.65\r--------------------------------------------------------------------------------------\rOptimism by treatment, season, and depression status\rThere is an interaction of treatment by season for those who are mildly depressed, but no such interaction for those who are not depressed. We can test the interaction of treatment by season at each level of depression status using the following contrast command: contrast treat#season@depstat Contrasts of marginal linear predictions\rMargins: asbalanced\r--------------------------------------------------------\r| df F P\u003eF\r---------------------+----------------------------------\rtreat#season@depstat |\rNon | 1 0.81 0.3693\rMild | 1 7.65 0.0061\rJoint | 2 4.23 0.0157\r|\rDenominator | 232\r--------------------------------------------------------\r","date":"2024-01-07","objectID":"/9.chapter9categorical-by-categorical-by-categorical-interactions/:1:2","tags":["Categorical","Interaction","stata"],"title":"Chapter9 ：Categorical by categorical by categorical interactions","uri":"/9.chapter9categorical-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.3 Simple effects We might want to know whether the effect of happiness therapy is significant for each combination of season and depression status. contrast treat@season#depstat,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r--------------------------+---------------------------------------\rtreat@season#depstat |\r(HT vs base) Winter#Non | 15.53 2.07 7.50 0.000\r(HT vs base) Winter#Mild | 11.70 2.07 5.65 0.000\r(HT vs base) Summer#Non | 12.90 2.07 6.23 0.000\r(HT vs base) Summer#Mild | 19.80 2.07 9.56 0.000\r------------------------------------------------------------------\rThe mean optimism for those in the happiness therapy group is always greater than the control group at each level of season and at each level of depression status. ","date":"2024-01-07","objectID":"/9.chapter9categorical-by-categorical-by-categorical-interactions/:1:3","tags":["Categorical","Interaction","stata"],"title":"Chapter9 ：Categorical by categorical by categorical interactions","uri":"/9.chapter9categorical-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2 Two by two by three models Let’s now consider an example with three factors, two of which have two levels and one of which has three levels. In this example, the treatment variable now has three levels: 1.control group, 2. traditional therapy, and 3. happiness therapy. use opt-3by2by2.dta Let’s now use the anova command to predict opt from depstat, treat, season, all two-way interactions of these variables, and the three-way interaction. anova opt depstat##treat##season Number of obs = 360 R-squared = 0.4912\rRoot MSE = 7.9879 Adj R-squared = 0.4751\rSource | Partial SS df MS F Prob\u003eF\r---------------------+----------------------------------------------------\rModel | 21435.233 11 1948.6576 30.54 0.0000\r|\rdepstat | 2423.2111 1 2423.2111 37.98 0.0000\rtreat | 13811.217 2 6905.6083 108.23 0.0000\rdepstat#treat | 3.9055556 2 1.9527778 0.03 0.9699\rseason | 3960.1 1 3960.1 62.06 0.0000\rdepstat#season | 253.34444 1 253.34444 3.97 0.0471\rtreat#season | 469.11667 2 234.55833 3.68 0.0263\rdepstat#treat#season | 514.33889 2 257.16944 4.03 0.0186\r|\rResidual | 22204.667 348 63.806513 ---------------------+----------------------------------------------------\rTotal | 43639.9 359 121.55961 The three-way interaction is significant ( $F = 4.03$ , $p = 0.0186$ ). Let’s use the margins and marginsplot commands to display and graph the means by each of these categorical variables. margins depstat#treat#season,nopvalues marginsplot,bydimension(depstat) noci Adjusted predictions Number of obs = 360\rExpression: Linear prediction, predict()\r-----------------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r----------------------+------------------------------------------------\rdepstat#treat#season |\rNon#Con#Winter (S1) | 44.70 1.46 41.83 47.57\rNon#Con#Summer (S2) | 49.70 1.46 46.83 52.57\rNon#TT#Winter (S1) | 54.33 1.46 51.46 57.20\rNon#TT#Summer (S2) | 59.40 1.46 56.53 62.27\rNon#HT#Winter (S1) | 59.77 1.46 56.90 62.64\rNon#HT#Summer (S2) | 64.57 1.46 61.70 67.44\rMild#Con#Winter (S1) | 39.63 1.46 36.76 42.50\rMild#Con#Summer (S2) | 44.20 1.46 41.33 47.07\rMild#TT#Winter (S1) | 49.23 1.46 46.36 52.10\rMild#TT#Summer (S2) | 54.70 1.46 51.83 57.57\rMild#HT#Winter (S1) | 49.33 1.46 46.46 52.20\rMild#HT#Summer (S2) | 64.23 1.46 61.36 67.10\r-----------------------------------------------------------------------\rOptimism by treatment, season, and depression status\r","date":"2024-01-07","objectID":"/9.chapter9categorical-by-categorical-by-categorical-interactions/:2:0","tags":["Categorical","Interaction","stata"],"title":"Chapter9 ：Categorical by categorical by categorical interactions","uri":"/9.chapter9categorical-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.1 Simple interactions by depression status It appears that the treat#season interaction might not be significant for those who are not depressed (see the left panel of figure) but might be significant for those who are mildly depressed (see the right panel of figure ). We can explore this by assessing the treat#season interaction at each level of depstat using the contrastcommand below. contrast treat#season@depstat Contrasts of marginal linear predictions\rMargins: asbalanced\r--------------------------------------------------------\r| df F P\u003eF\r---------------------+----------------------------------\rtreat#season@depstat |\rNon | 2 0.00 0.9955\rMild | 2 7.70 0.0005\rJoint | 4 3.85 0.0044\r|\rDenominator | 348\r--------------------------------------------------------\rLet’s further dissect this simple interaction by applying contrasts to the treatment factor through the use of simple partial interactions. ","date":"2024-01-07","objectID":"/9.chapter9categorical-by-categorical-by-categorical-interactions/:2:1","tags":["Categorical","Interaction","stata"],"title":"Chapter9 ：Categorical by categorical by categorical interactions","uri":"/9.chapter9categorical-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.2 Simple partial interaction by depression status Say that we want to form two comparisons with respect to treat, comparing group 2 versus 1 (traditional therapy versus control group) and comparing group 3 versus 2 (happiness therapy versus traditional therapy). The interaction of treatment (traditional therapy versus control group) by season for those who are mildly depressed is visualized in the left panel of figure below, and the interaction of treatment (happiness therapy versus traditional therapy) by season for those who are mildly depressed is visualized in the right panel of figure below. Simple partial interactions\rTo understand this, let’s break it into two parts. The first part, ar.treat#season, creates the interactions of treatment (traditional therapy versus control group) by season, and treatment (happiness therapy versus traditional therapy) by season. The second part, @2.depstat indicates the contrasts will be performed only for level 2 of depstat (the mildly depressed group) contrast ar.treat#season@2.depstat Contrasts of marginal linear predictions\rMargins: asbalanced\r-------------------------------------------------------------\r| df F P\u003eF\r--------------------------+----------------------------------\rtreat#season@depstat |\r(TT vs Con) (joint) Mild | 1 0.10 0.7578\r(HT vs TT) (joint) Mild | 1 10.46 0.0013\rJoint | 2 7.70 0.0005\r|\rDenominator | 348\r-------------------------------------------------------------\rLet’s now further understand this simple partial interaction through the use of simple contrasts. ","date":"2024-01-07","objectID":"/9.chapter9categorical-by-categorical-by-categorical-interactions/:2:2","tags":["Categorical","Interaction","stata"],"title":"Chapter9 ：Categorical by categorical by categorical interactions","uri":"/9.chapter9categorical-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.3 Simple contrasts In particular, let’s ask whether there is a difference between happiness therapy and traditional therapy separately for each season focusing only on those who are mildly depressed. We can perform this test by specifying ar3.treat@season#2.depstat on the contrast command. Let’s break this into two parts. The first part, ar3.treat, requests the comparison of treatment group 3 versus 2 (happiness therapy versus traditional therapy). The second part, @season#2.depstat, requests that the contrasts be performed at each level of season and at level 2 of depression status (mildly depressed) contrast ar3.treat@season#2.depstat,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r---------------------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r-----------------------------+---------------------------------------\rtreat@season#depstat |\r(HT vs TT) Winter (S1)#Mild | 0.10 2.06 0.05 0.961\r(HT vs TT) Summer (S2)#Mild | 9.53 2.06 4.62 0.000\r---------------------------------------------------------------------\r","date":"2024-01-07","objectID":"/9.chapter9categorical-by-categorical-by-categorical-interactions/:2:3","tags":["Categorical","Interaction","stata"],"title":"Chapter9 ：Categorical by categorical by categorical interactions","uri":"/9.chapter9categorical-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.4 Partial interactions Say that we wanted to further understand this interaction by applying adjacent group contrasts to the treatment factor. These contrasts compare group 2 versus 1 (traditional therapy versus control group) and group 3 versus 2 (happiness therapy versus traditional therapy). We could interact these contrasts with season and depression status. Simple partial interaction: T2 vs. T1 by season by depstat\rSimple partial interaction: T3 vs. T2 by season by depstat\rWe can test each of these partial interactions using the contrast command below. contrast ar.treat#season#depstat Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------------\r| df F P\u003eF\r-----------------------------+----------------------------------\rtreat#season#depstat |\r(TT vs Con) (joint) (joint) | 1 0.04 0.8400\r(HT vs TT) (joint) (joint) | 1 5.53 0.0193\rJoint | 2 4.03 0.0186\r|\rDenominator | 348\r----------------------------------------------------------------\r","date":"2024-01-07","objectID":"/9.chapter9categorical-by-categorical-by-categorical-interactions/:2:4","tags":["Categorical","Interaction","stata"],"title":"Chapter9 ：Categorical by categorical by categorical interactions","uri":"/9.chapter9categorical-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3 Three by three by three models and beyond Let’s consider an extension of the example involving the factors treat, depstat, and season, except that these factors have three, three and four levels, respectively. The three levels of treat are 1) control group, 2) traditional therapy, and 3) happiness therapy. The three levels of depstat are 1) nondepressed, 2) mildly depressed, and 3) severely depressed. The four levels of season are 1) winter, 2) spring, 3) summer, and 4)fall. use opt-3by3by4.dta anova opt depstat##treat##season Number of obs = 1,080 R-squared = 0.6172\rRoot MSE = 6.00663 Adj R-squared = 0.6043\rSource | Partial SS df MS F Prob\u003eF\r---------------------+----------------------------------------------------\rModel | 60718.9 35 1734.8257 48.08 0.0000\r|\rdepstat | 18782.039 2 9391.0194 260.29 0.0000\rtreat | 34636.039 2 17318.019 480.00 0.0000\rdepstat#treat | 1702.0056 4 425.50139 11.79 0.0000\rseason | 3522.8333 3 1174.2778 32.55 0.0000\rdepstat#season | 1004.2722 6 167.3787 4.64 0.0001\rtreat#season | 185.11667 6 30.852778 0.86 0.5275\rdepstat#treat#season | 886.59444 12 73.88287 2.05 0.0179\r|\rResidual | 37667.067 1,044 36.079566 ---------------------+----------------------------------------------------\rTotal | 98385.967 1,079 91.182546 The three-way interaction of depstat#treat#season is significant ( $F = 2.05$ , $p=0.0179$ ). To begin to understand the nature of this interaction, let’s first graph the interaction using the margins and marginsplot commands. margins depstat#treat#season marginsplot,xdimension(treat)bydimension(season) noci legend(rows(1)) Adjusted predictions Number of obs = 1,080\rExpression: Linear prediction, predict()\r--------------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r---------------------+----------------------------------------------------------------\rdepstat#treat#season |\rNon#Con#Winter | 44.63 1.10 40.70 0.000 42.48 46.79\rNon#Con#Spring | 47.53 1.10 43.34 0.000 45.38 49.69\rNon#Con#Summer | 49.70 1.10 45.32 0.000 47.55 51.85\rNon#Con#Fall | 47.83 1.10 43.62 0.000 45.68 49.99\rNon#TT#Winter | 54.30 1.10 49.51 0.000 52.15 56.45\rNon#TT#Spring | 57.33 1.10 52.28 0.000 55.18 59.49\rNon#TT#Summer | 60.37 1.10 55.05 0.000 58.21 62.52\rNon#TT#Fall | 57.03 1.10 52.01 0.000 54.88 59.19\rNon#HT#Winter | 61.30 1.10 55.90 0.000 59.15 63.45\rNon#HT#Spring | 62.70 1.10 57.17 0.000 60.55 64.85\rNon#HT#Summer | 64.70 1.10 59.00 0.000 62.55 66.85\rNon#HT#Fall | 62.87 1.10 57.33 0.000 60.71 65.02\rMild#Con#Winter | 39.60 1.10 36.11 0.000 37.45 41.75\rMild#Con#Spring | 42.73 1.10 38.97 0.000 40.58 44.89\rMild#Con#Summer | 44.50 1.10 40.58 0.000 42.35 46.65\rMild#Con#Fall | 42.73 1.10 38.97 0.000 40.58 44.89\rMild#TT#Winter | 49.17 1.10 44.83 0.000 47.01 51.32\rMild#TT#Spring | 52.20 1.10 47.60 0.000 50.05 54.35\rMild#TT#Summer | 54.10 1.10 49.33 0.000 51.95 56.25\rMild#TT#Fall | 52.03 1.10 47.45 0.000 49.88 54.19\rMild#HT#Winter | 51.20 1.10 46.69 0.000 49.05 53.35\rMild#HT#Spring | 56.37 1.10 51.40 0.000 54.21 58.52\rMild#HT#Summer | 65.33 1.10 59.58 0.000 63.18 67.49\rMild#HT#Fall | 56.17 1.10 51.22 0.000 54.01 58.32\rSevere#Con#Winter | 36.67 1.10 33.44 0.000 34.51 38.82\rSevere#Con#Spring | 39.67 1.10 36.17 0.000 37.51 41.82\rSevere#Con#Summer | 39.60 1.10 36.11 0.000 37.45 41.75\rSevere#Con#Fall | 39.97 1.10 36.44 0.000 37.81 42.12\rSevere#TT#Winter | 47.20 1.10 43.04 0.000 45.05 49.35\rSevere#TT#Spring | 50.23 1.10 45.81 0.000 48.08 52.39\rSevere#TT#Summer | 49.23 1.10 44.89 0.000 47.08 51.39\rSevere#TT#Fall | 50.00 1.10 45.59 0.000 47.85 52.15\rSevere#HT#Winter | 46.67 1.10 42.55 0.000 44.51 48.82\rSevere#HT#Spring | 49.77 1.10 45.38 0.000 47.61 51.92\rSevere#HT#Summer | 48.57 1.10 44.29 0.000 46.41 50.72\rSevere#HT#Fall | 50.20 1.10 45.78 0.000 48.05 52.35\r--------------------------------------------------------------------------------------\rOptimism by treatment, season, and depression status\r","date":"2024-01-07","objectID":"/9.chapter9categorical-by-categorical-by-categorical-interactions/:3:0","tags":["Categorical","Interaction","stata"],"title":"Chapter9 ：Categorical by categorical by categorical interactions","uri":"/9.chapter9categorical-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.1 Partial interactions and interaction contrasts Let’s explore ways to dissect the three-way interaction of depstat#treat#season by applying contrasts to one or more of the factors. Let’s begin by testing whether the interaction of treat#depstat is the same for each season compared with winter (season 1). This is performed by applying the r. contrast operator to season and interacting that with treat and depstat. contrast r.season#treat#depstat Contrasts of marginal linear predictions\rMargins: asbalanced\r-----------------------------------------------------------------------\r| df F P\u003eF\r------------------------------------+----------------------------------\rseason#treat#depstat |\r(Spring vs Winter) (joint) (joint) | 4 0.46 0.7619\r(Summer vs Winter) (joint) (joint) | 4 5.36 0.0003\r(Fall vs Winter) (joint) (joint) | 4 0.40 0.8115\rJoint | 12 2.05 0.0179\r|\rDenominator | 1044\r-----------------------------------------------------------------------\rNote! Fall versus spring A custom contrast is applied to season to obtain the comparison of fall versus spring (season 4 versus 2).[ contrast {season 0 -1 0 1}#treat#depstat ] The contrast command below performs this comparison by applying the r3. contrast to season to compare summer with winter (season 3 versus 1) and the r2. contrast on depstat to compare those who are mildly depressed with those who are nondepressed (levels 2 versus 1). These terms are all interacted (that is, r3.season#r2.depstat#treat) yielding a test of summer versus winter (season 3 versus 1) by mildly depressed versus nondepressed (levels 2 versus 1) by treatment. *Season(winter vs. summer) by depstat(nondepressed vs. mildly depressed)by treat contrast r3.season#r2.depstat#treat Contrasts of marginal linear predictions\rMargins: asbalanced\r--------------------------------------------------------\r| df F P\u003eF\r---------------------+----------------------------------\rseason#depstat#treat | 2 9.03 0.0001\r|\rDenominator | 1044\r--------------------------------------------------------\rInteraction contrast of season (winter versus summer) by depression status (mildly depressed versus nondepressed) by treatment\rThis significant test indicates that the two-way interaction formed by interacting depression status (nondepressed versus mildly depressed) by treatment differs by season (winter versus summer) Say that we wanted to take this test and focus on the contrast of happiness therapy versus traditional therapy (group 3 versus 2).1 This yields an interaction of season (winter versus summer) by depression status (nondepressed versus mildly depressed) by treatment (happiness therapy versus traditional therapy). *season(winter vs. summer) by depstat (nondepressed vs mildly depressed) by treat(HT vs. TT) contrast r3.season#r2.depstat#r3b2.treat,noeffects Contrasts of marginal linear predictions\rMargins: asbalanced\r--------------------------------------------------------\r| df F P\u003eF\r---------------------+----------------------------------\rseason#depstat#treat | 1 14.64 0.0001\r|\rDenominator | 1044\r--------------------------------------------------------\rInteraction contrast of season (winter versus summer) by depression status (mildly depressed versus nondepressed) by treatment (HT versus TT)\r","date":"2024-01-07","objectID":"/9.chapter9categorical-by-categorical-by-categorical-interactions/:3:1","tags":["Categorical","Interaction","stata"],"title":"Chapter9 ：Categorical by categorical by categorical interactions","uri":"/9.chapter9categorical-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.2 Simple interactions Let’s now explore a different way to dissect the three-way interaction through the use of simple interaction tests. contrast depstat#treat@season Contrasts of marginal linear predictions\rMargins: asbalanced\r--------------------------------------------------------\r| df F P\u003eF\r---------------------+----------------------------------\rdepstat#treat@season |\rWinter | 4 3.75 0.0049\rSpring | 4 2.29 0.0575\rSummer | 4 9.88 0.0000\rFall | 4 2.01 0.0904\rJoint | 16 4.48 0.0000\r|\rDenominator | 1044\r--------------------------------------------------------\rThe treat#depstat interaction is significant for winter (season 1) and summer (season 3). The treat#depstat interaction is not significant in the spring (season 2) or fall (season 4) Let’s perform this same contrast command, but apply the r2. contrast to depstat that compares those who are nondepressed versus mildly depressed. This yields four partial interactions of depression status (nondepressed versus mildly depressed) by treatment at each level of season. contrast r2.depstat#treat@season Contrasts of marginal linear predictions\rMargins: asbalanced\r-----------------------------------------------------------------\r| df F P\u003eF\r------------------------------+----------------------------------\rdepstat#treat@season |\r(Mild vs Non) (joint) Winter | 2 3.49 0.0309\r(Mild vs Non) (joint) Spring | 2 0.27 0.7631\r(Mild vs Non) (joint) Summer | 2 5.74 0.0033\r(Mild vs Non) (joint) Fall | 2 0.38 0.6851\rJoint | 8 2.47 0.0119\r|\rDenominator | 1044\r-----------------------------------------------------------------\rOptimism by treatment and season focusing on mildly depressed versus nondepressed\rLet’s also apply the r3b2. contrast to treat, comparing group 3 with group 2 (happiness therapy to traditional therapy). This yields an interaction contrast of depression status (mildly depressed versus nondepressed) by treatment (happiness therapy versus traditional therapy) performed at each of the four seasons. contrast r2.depstat#r3b2.treat@season,noeffects //r2 means non-dep vs mild-dep Contrasts of marginal linear predictions\rMargins: asbalanced\r--------------------------------------------------------------------\r| df F P\u003eF\r---------------------------------+----------------------------------\rdepstat#treat@season |\r(Mild vs Non) (HT vs TT) Winter | 1 5.13 0.0238\r(Mild vs Non) (HT vs TT) Spring | 1 0.30 0.5844\r(Mild vs Non) (HT vs TT) Summer | 1 9.90 0.0017\r(Mild vs Non) (HT vs TT) Fall | 1 0.60 0.4385\rJoint | 4 3.98 0.0033\r|\rDenominator | 1044\r--------------------------------------------------------------------\rSimple interaction contrast of depression status by treatment at each season\r","date":"2024-01-07","objectID":"/9.chapter9categorical-by-categorical-by-categorical-interactions/:3:2","tags":["Categorical","Interaction","stata"],"title":"Chapter9 ：Categorical by categorical by categorical interactions","uri":"/9.chapter9categorical-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.3 Simple effects and simple comparisons We might be interested in focusing on the simple effects of treatment across the levels of season and depression status. contrast treat@season#depstat We could further refine this test by focusing on the comparison of happiness therapy versus traditional therapy (group 3 versus 2) by applying the r3b2. contrast operator to treat, as shown below. contrast r3b2.treat@season#depstat,nowald pveffects this contrast is specified as r3b2.treat,which indicates to compare group 3 with group2. This could have also been specified as a custom contrast,{treat 0 -1 1} Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r--------------------------+---------------------------------------\rtreat@season#depstat |\r(HT vs TT) Winter#Non | 7.00 1.55 4.51 0.000\r(HT vs TT) Winter#Mild | 2.03 1.55 1.31 0.190\r(HT vs TT) Winter#Severe | -0.53 1.55 -0.34 0.731\r(HT vs TT) Spring#Non | 5.37 1.55 3.46 0.001\r(HT vs TT) Spring#Mild | 4.17 1.55 2.69 0.007\r(HT vs TT) Spring#Severe | -0.47 1.55 -0.30 0.764\r(HT vs TT) Summer#Non | 4.33 1.55 2.79 0.005\r(HT vs TT) Summer#Mild | 11.23 1.55 7.24 0.000\r(HT vs TT) Summer#Severe | -0.67 1.55 -0.43 0.667\r(HT vs TT) Fall#Non | 5.83 1.55 3.76 0.000\r(HT vs TT) Fall#Mild | 4.13 1.55 2.67 0.008\r(HT vs TT) Fall#Severe | 0.20 1.55 0.13 0.897\r------------------------------------------------------------------\rThis shows that for some combinations of season and depstat, the difference between happiness and traditional therapy is significant. For example, in season 1 (winter) and depression status 1 (nondepressed), the difference between happiness and traditional therapy is significant, with happiness therapy yielding optimism scores that are 7 points greater than traditional therapy ","date":"2024-01-07","objectID":"/9.chapter9categorical-by-categorical-by-categorical-interactions/:3:3","tags":["Categorical","Interaction","stata"],"title":"Chapter9 ：Categorical by categorical by categorical interactions","uri":"/9.chapter9categorical-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter illustrates models that involve the interaction of two categorical variables. ","date":"2024-01-06","objectID":"/8.chapter8categorical-by-categorical-interactions/","tags":["Categorical","Interaction","stata"],"title":"Chapter8 ：Categorical by categorical interactions","uri":"/8.chapter8categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter illustrates models that involve the interaction of two categorical variables.The emphasis of this chapter is not only how to test for interactions between factor variables but also how to understand and dissect those interactions. This chapter focuses on three types of interactions: two by two interactions. two by three interactions. three by three interactions. ","date":"2024-01-06","objectID":"/8.chapter8categorical-by-categorical-interactions/:0:0","tags":["Categorical","Interaction","stata"],"title":"Chapter8 ：Categorical by categorical interactions","uri":"/8.chapter8categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 Two by two models: Example 1 In this first hypothetical study, she seeks to determine the effectiveness of happiness therapy by comparing the optimism of people who have completed happiness therapy treatment with the optimism of people in a control group who received no treatment. The researcher is interested in not only assessing the effectiveness of happiness therapy but also assessing whether its effectiveness depends on whether the person has been diagnosed as clinically depressed. This yields a two by two research design, crossing treatment group assignment (control group versus happiness therapy) with depression status (nondepressed versus depressed). The variable treat indicates the treatment assignment, coded: 1 = control group (Con) and 2 = happiness therapy (HT). The variable depstat reflects the person’s depression status at the beginning of the study and is coded: 1 = nondepressed and 2 = depressed. The variable opt is the optimism score at the end of the study. In this dataset, opt has a mean of 44.5, a minimum of 16, and a maximum of 80. Let’s now run an analysis that predicts opt based on treat, depstat, and the interaction of these two variables. This analysis uses the anova command (instead of the regress command) because the anova command directly shows the significance tests for each of the main effects as well as the interaction. use opt-2by2.dta anova opt depstat##treat Number of obs = 120 R-squared = 0.4889\rRoot MSE = 10.0136 Adj R-squared = 0.4757\rSource | Partial SS df MS F Prob\u003eF\r--------------+----------------------------------------------------\rModel | 11126 3 3708.6667 36.99 0.0000\r|\rdepstat | 7426.1333 1 7426.1333 74.06 0.0000\rtreat | 2803.3333 1 2803.3333 27.96 0.0000\rdepstat#treat | 896.53333 1 896.53333 8.94 0.0034\r|\rResidual | 11631.467 116 100.27126 --------------+----------------------------------------------------\rTotal | 22757.467 119 191.23922 Note! The anova and regress commands if you want use regress command in stead of the anova command to fit the previous model, there are two caveats: the regress command will require an extra step using the contrast command to test the overall interaction (for example, contrast depstat#treat). the tests of the main effects differ when using the regress command compared with the anova command. The significant interaction indicates that the effect of happiness therapy for those who are nondepressed is significantly different from the effect of happiness therapy for those who are depressed. margins treat#depstat,nopvalues marginsplot Adjusted predictions Number of obs = 120\rExpression: Linear prediction, predict()\r---------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r--------------+------------------------------------------------\rtreat#depstat |\rCon#Non | 44.87 1.83 41.25 48.49\rCon#Dep | 34.60 1.83 30.98 38.22\rHT#Non | 60.00 1.83 56.38 63.62\rHT#Dep | 38.80 1.83 35.18 42.42\r---------------------------------------------------------------\rGraph of means\r","date":"2024-01-06","objectID":"/8.chapter8categorical-by-categorical-interactions/:1:0","tags":["Categorical","Interaction","stata"],"title":"Chapter8 ：Categorical by categorical interactions","uri":"/8.chapter8categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.1 Simple effects The significant interaction indicates that the effect of happiness therapy is different for those who are depressed versus nondepressed. Each of these effects is called a simple effect, because they reflect the effect of one variable while holding another variable constant. We can estimate and test these simple effects using the contrast command, as shown below. Note the use of the @ symbol. This requests the simple effect of treat at each level of depstat. contrast treat@depstat contrast treat@depstat,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r------------------+---------------------------------------\rtreat@depstat |\r(HT vs base) Non | 15.13 2.59 5.85 0.000\r(HT vs base) Dep | 4.20 2.59 1.62 0.107\r----------------------------------------------------------\rBy adding the nowald and pveffects options, the contrast command displays a table with the estimate of the simple effect, the standard error, and a significance test of the simple effect. This shows the effect of treatment among those who are nondepressed equals 15.1, and this effect is significant ( $t = 5.85,p=0.000$ ). Among those who are depressed, the treatment effect is 4.2, and this difference is not significant ( $t = 1.62,p=0.107$ ). Note! Contrast options Combining the nowald and pveffects options provides a concise output that includes an estimate of the size of the contrast and a test of its significance. ","date":"2024-01-06","objectID":"/8.chapter8categorical-by-categorical-interactions/:1:1","tags":["Categorical","Interaction","stata"],"title":"Chapter8 ：Categorical by categorical interactions","uri":"/8.chapter8categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.2 Estimating the size of the interaction As we saw in the analysis of the simple effects, the simple effect of treatment is 4.2 for those who are depressed and is 15.1 for those who are nondepressed. Taking the difference in these simple effects ($4.2 - 15.1$) gives us an estimate of the size of the interaction, which is $-10.9$. This is the same value that we obtain if we estimate the interaction using the contrast command below. Contrasts of marginal linear predictions\rMargins: asbalanced\r--------------------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r----------------------------+---------------------------------------\rtreat#depstat |\r(HT vs base) (Dep vs base) | -10.93 3.66 -2.99 0.003\r--------------------------------------------------------------------\rthe $p$-value for this test matches the $p$-value of the treat#depstat interaction from the original anova command. ","date":"2024-01-06","objectID":"/8.chapter8categorical-by-categorical-interactions/:1:2","tags":["Categorical","Interaction","stata"],"title":"Chapter8 ：Categorical by categorical interactions","uri":"/8.chapter8categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.3 More about interaction let’s further explore what we mean by an interaction by considering the hypothetical pattern of results shown in below. Two by two with no interaction\rThis is an example pattern in which there is no interaction between treatment and depression status. One way we can see the absence of an interaction is by seeing that the line for D1 is parallel to the line for D2. ","date":"2024-01-06","objectID":"/8.chapter8categorical-by-categorical-interactions/:1:3","tags":["Categorical","Interaction","stata"],"title":"Chapter8 ：Categorical by categorical interactions","uri":"/8.chapter8categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2 Two by three models his section considers models where one of the categorical variables has two levels and the other categorical variable has three levels. ","date":"2024-01-06","objectID":"/8.chapter8categorical-by-categorical-interactions/:2:0","tags":["Categorical","Interaction","stata"],"title":"Chapter8 ：Categorical by categorical interactions","uri":"/8.chapter8categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.1 Example 2 Referring to the example from the previous section, depression status had two levels, nondepressed and depressed. Suppose that we instead use three categories for depression status: nondepressed, mildly depressed, and severely depressed. Let’s now perform an analysis predicting optimism from treatment group, depression status, and the interaction of these two variables. use opt-2by3-ex1.dta anova opt depstat##treat Number of obs = 180 R-squared = 0.5402\rRoot MSE = 10.0154 Adj R-squared = 0.5270\rSource | Partial SS df MS F Prob\u003eF\r--------------+----------------------------------------------------\rModel | 20505.828 5 4101.1656 40.89 0.0000\r|\rdepstat | 15432.844 2 7716.4222 76.93 0.0000\rtreat | 3183.6056 1 3183.6056 31.74 0.0000\rdepstat#treat | 1889.3778 2 944.68889 9.42 0.0001\r|\rResidual | 17453.567 174 100.30785 --------------+----------------------------------------------------\rTotal | 37959.394 179 212.06366 As expected, the depstat#treat interaction is significant. We can compute the mean optimism as a function of depression status and treatment group by using the margins command below. Had there been additional predictors in the model, the margins command would have produced adjusted means, adjusting for the other predictors in the model. margins treat#depstat,nopvalues marginsplot Adjusted predictions Number of obs = 180\rExpression: Linear prediction, predict()\r---------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r--------------+------------------------------------------------\rtreat#depstat |\rCon#Non | 44.23 1.83 40.62 47.84\rCon#Mild | 39.63 1.83 36.02 43.24\rCon#Sev | 29.80 1.83 26.19 33.41\rHT#Non | 59.60 1.83 55.99 63.21\rHT#Mild | 49.73 1.83 46.12 53.34\rHT#Sev | 29.57 1.83 25.96 33.18\r---------------------------------------------------------------\rGraph of means\r2.1.1 Simple effects We can ask whether the effect of happiness therapy is significant at each level of depression status. contrast treat@depstat,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r-----------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r-------------------+---------------------------------------\rtreat@depstat |\r(HT vs base) Non | 15.37 2.59 5.94 0.000\r(HT vs base) Mild | 10.10 2.59 3.91 0.000\r(HT vs base) Sev | -0.23 2.59 -0.09 0.928\r-----------------------------------------------------------\rThis test of simple effects tells us that happiness therapy is significantly better than the control group for those who are nondepressed and for those who are mildly depressed. For those who are severely depressed, happiness therapy is not significantly different from being in the control group 2.1.2 Partial interactions Another way to dissect a two by three interaction is through the use of partial interactions. In this example, a partial interaction is constructed by applying a contrast operator to depstat and interacting that with treat. For example, applying the a.contrast operator to depstat yields two contrasts: group 1 versus 2 (Non vs Mild depression); and group 2 versus 3 (Mild vs Sev depression). Interacting a.depstat with treat forms two partial interactions. contrast a.depstat#treat Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------\r| df F P\u003eF\r-----------------------+----------------------------------\rdepstat#treat |\r(Non vs Mild) (joint) | 1 2.07 0.1516\r(Mild vs Sev) (joint) | 1 7.98 0.0053\rJoint | 2 9.42 0.0001\r|\rDenominator | 174\r----------------------------------------------------------\rThe first partial interaction is not significant ( $F = 2.07 , p = 0.1516$ ).The effect of happiness therapy (compared with the control group) is not significantly different for those who are nondepressed versus mildly depressed. The second partial interaction is significant ( $F = 7.98 , p = 0.0053$ ). The effect of happiness therapy (compared with the con","date":"2024-01-06","objectID":"/8.chapter8categorical-by-categorical-interactions/:2:1","tags":["Categorical","Interaction","stata"],"title":"Chapter8 ：Categorical by categorical interactions","uri":"/8.chapter8categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3 Three by three models: Example 4 Let’s now consider an example that illustrates a three by three design. there are three levels of treatment (control group, traditional therapy, and happiness therapy), and three depression groups (nondepressed, mildly depressed, and severely depressed) use opt-3by3.dta.dta anova opt depstat##treat Number of obs = 270 R-squared = 0.4898\rRoot MSE = 10.023 Adj R-squared = 0.4742\rSource | Partial SS df MS F Prob\u003eF\r--------------+----------------------------------------------------\rModel | 25175.519 8 3146.9398 31.32 0.0000\r|\rdepstat | 17664.096 2 8832.0481 87.92 0.0000\rtreat | 5250.363 2 2625.1815 26.13 0.0000\rdepstat#treat | 2261.0593 4 565.26481 5.63 0.0002\r|\rResidual | 26220.367 261 100.46117 --------------+----------------------------------------------------\rTotal | 51395.885 269 191.06277 margins treat#depstat,nopvalues marginsplot Adjusted predictions Number of obs = 270\rExpression: Linear prediction, predict()\r---------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r--------------+------------------------------------------------\rtreat#depstat |\rCon#Non | 44.20 1.83 40.60 47.80\rCon#Mild | 39.70 1.83 36.10 43.30\rCon#Sev | 29.90 1.83 26.30 33.50\rTT#Non | 54.53 1.83 50.93 58.14\rTT#Mild | 49.53 1.83 45.93 53.14\rTT#Sev | 39.80 1.83 36.20 43.40\rHT#Non | 59.33 1.83 55.73 62.94\rHT#Mild | 49.87 1.83 46.26 53.47\rHT#Sev | 30.10 1.83 26.50 33.70\r---------------------------------------------------------------\rMean optimism by treatment group and depression status\rWe can statistically dissect this interaction in four different ways: using simple effects , simple contrasts , partial interactions , or interaction contrasts . Each of these techniques is illustrated below ","date":"2024-01-06","objectID":"/8.chapter8categorical-by-categorical-interactions/:3:0","tags":["Categorical","Interaction","stata"],"title":"Chapter8 ：Categorical by categorical interactions","uri":"/8.chapter8categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.1 Simple effects One way to dissect the interaction is by looking at the effect of treatment at each level of depression status. contrast treat@depstat Contrasts of marginal linear predictions\rMargins: asbalanced\r-------------------------------------------------\r| df F P\u003eF\r--------------+----------------------------------\rtreat@depstat |\rNon | 2 17.86 0.0000\rMild | 2 9.96 0.0001\rSev | 2 9.56 0.0001\rJoint | 6 12.46 0.0000\r|\rDenominator | 261\r-------------------------------------------------\rThese results show that the effect of treat is significant at each level of depstat. ","date":"2024-01-06","objectID":"/8.chapter8categorical-by-categorical-interactions/:3:1","tags":["Categorical","Interaction","stata"],"title":"Chapter8 ：Categorical by categorical interactions","uri":"/8.chapter8categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.2 Simple contrast contrast r.treat@1.depstat,nowald pveffects This yields a comparison of each treatment group with the reference group (that is, group 1, the control group) at each level of depression status. Contrasts of marginal linear predictions\rMargins: asbalanced\r---------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r-----------------+---------------------------------------\rtreat@depstat |\r(TT vs Con) Non | 10.33 2.59 3.99 0.000\r(HT vs Con) Non | 15.13 2.59 5.85 0.000\r---------------------------------------------------------\rLet’s now perform these simple contrasts for those who are mildly depressed. contrast r.treat@2.depstat,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r------------------+---------------------------------------\rtreat@depstat |\r(TT vs Con) Mild | 9.83 2.59 3.80 0.000\r(HT vs Con) Mild | 10.17 2.59 3.93 0.000\r----------------------------------------------------------\rFinally, let’s perform these simple contrasts for those who are severely depressed. contrast r.treat@3.depstat,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r---------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r-----------------+---------------------------------------\rtreat@depstat |\r(TT vs Con) Sev | 9.90 2.59 3.83 0.000\r(HT vs Con) Sev | 0.20 2.59 0.08 0.938\r---------------------------------------------------------\rIf you prefer, you can obtain all six of these simple contrasts at once using the contrast command below contrast r.treat@depstat,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r------------------+---------------------------------------\rtreat@depstat |\r(TT vs Con) Non | 10.33 2.59 3.99 0.000\r(TT vs Con) Mild | 9.83 2.59 3.80 0.000\r(TT vs Con) Sev | 9.90 2.59 3.83 0.000\r(HT vs Con) Non | 15.13 2.59 5.85 0.000\r(HT vs Con) Mild | 10.17 2.59 3.93 0.000\r(HT vs Con) Sev | 0.20 2.59 0.08 0.938\r----------------------------------------------------------\r","date":"2024-01-06","objectID":"/8.chapter8categorical-by-categorical-interactions/:3:2","tags":["Categorical","Interaction","stata"],"title":"Chapter8 ：Categorical by categorical interactions","uri":"/8.chapter8categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.3 Partial interaction I used the means from the margins command to create a visual depiction of these two partial interactions, shown in figure below. The left panel of figure illustrates the comparison of treatment group 2 versus 1 (traditional therapy versus control) interacted with depression status. The right panel illustrates the comparison of treatment group 3 versus 1 (happiness therapy versus control) interacted with depression status Partial interactions\rcontrast r.treat#depstat Contrasts of marginal linear predictions\rMargins: asbalanced\r--------------------------------------------------------\r| df F P\u003eF\r---------------------+----------------------------------\rtreat#depstat |\r(TT vs Con) (joint) | 2 0.01 0.9891\r(HT vs Con) (joint) | 2 8.64 0.0002\rJoint | 4 5.63 0.0002\r|\rDenominator | 261\r--------------------------------------------------------\rThe first partial interaction is not significant.The difference in optimism between traditional therapy and the control group does not differ among the levels of depression status. This result represent the left panel:The effect of traditional therapy (versus the control group) is similar for all three lines (representing the three levels of depression). The first partial interaction is not significant.The difference in optimism between happiness therapy and the control group depends on the level of depression. This result represent the right panel:the effect of happiness therapy (compared with the control group) may be similar for those who are nondepressed and mildly depressed but different for those who are severely depressed. ","date":"2024-01-06","objectID":"/8.chapter8categorical-by-categorical-interactions/:3:3","tags":["Categorical","Interaction","stata"],"title":"Chapter8 ：Categorical by categorical interactions","uri":"/8.chapter8categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.4 Interaction contrasts Suppose we applied the r. contrast operator to treatment group and the a. contrast operator to depression status. This would create contrasts of each treatment group against the control group (that is, group 2 versus 1 and group 3 versus 1) interacted with contrasts of adjacent levels of depression groups (that is, group 1 versus 2 and group 2 versus 3). contrast a.depstat#r.treat,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r-------------------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r---------------------------+---------------------------------------\rdepstat#treat |\r(Non vs Mild) (TT vs Con) | 0.50 3.66 0.14 0.891\r(Non vs Mild) (HT vs Con) | 4.97 3.66 1.36 0.176\r(Mild vs Sev) (TT vs Con) | -0.07 3.66 -0.02 0.985\r(Mild vs Sev) (HT vs Con) | 9.97 3.66 2.72 0.007\r-------------------------------------------------------------------\rInteraction contrasts\rLet’s begin by interpreting the fourth interaction contrast, the only one that was significant. The significance of this interaction contrast indicates that the effect of happiness therapy (compared with the control group) is different for those who are mildly depressed compared with those who are severely depressed. ","date":"2024-01-06","objectID":"/8.chapter8categorical-by-categorical-interactions/:3:4","tags":["Categorical","Interaction","stata"],"title":"Chapter8 ：Categorical by categorical interactions","uri":"/8.chapter8categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4 Unbalanced designs Even in the context of a randomized experiment, it is unusual to have the same number of observations in each cell. This section presents an example of an unbalanced design. This will allow us to consider two different strategies that can be used for estimating adjusted means, the asobserved strategy and the as-balanced strategy. As we will see, the asbalanced option can be used with the margins command to estimate margins as though the design were balanced, even if the actual design is not balanced. use gss_ivrm.dta tab married cograd if !missing(happy7),row +----------------+\r| Key |\r|----------------|\r| frequency |\r| row percentage |\r+----------------+\rmarital: |\rmarried=1, |\runmarried= | College graduate\r0 | (1=yes, 0=no)\r(recoded) | Not CO Gr CO Grad | Total\r-----------+----------------------+----------\rUnmarried | 454 147 | 601 | 75.54 24.46 | 100.00 -----------+----------------------+----------\rMarried | 403 153 | 556 | 72.48 27.52 | 100.00 -----------+----------------------+----------\rTotal | 857 300 | 1,157 | 74.07 25.93 | 100.00 Before performing the analysis, let’s compute the mean of happy7 by married and cograd using the tabulate command below tab married cograd,sum(happy7) marital: |\rmarried=1, |\runmarried= | College graduate\r0 | (1=yes, 0=no)\r(recoded) | Not CO Gr CO Grad | Total\r-----------+----------------------+----------\rUnmarried | 5.3039648 5.5170068 | 5.3560732\r| 1.0590229 .99556373 | 1.0470596\r| 454 147 | 601\r-----------+----------------------+----------\rMarried | 5.7121588 5.6862745 | 5.705036\r| .8986037 .79032462 | .86953016\r| 403 153 | 556\r-----------+----------------------+----------\rTotal | 5.495916 5.6033333 | 5.5237684\r| 1.0071217 .89926887 | .9810472\r| 857 300 | 1157\rLet’s now perform an analysis that predicts happy7 from married, cograd, and the interaction of these two variables. anova happy7 married##cograd Number of obs = 1,157 R-squared = 0.0362\rRoot MSE = .964375 Adj R-squared = 0.0337\rSource | Partial SS df MS F Prob\u003eF\r---------------+----------------------------------------------------\rModel | 40.284425 3 13.428142 14.44 0.0000\r|\rmarried | 18.502336 1 18.502336 19.89 0.0000\rcograd | 1.94355 1 1.94355 2.09 0.1486\rmarried#cograd | 3.1674385 1 3.1674385 3.41 0.0652\r|\rResidual | 1072.3119 1,153 .93001903 ---------------+----------------------------------------------------\rTotal | 1112.5964 1,156 .96245361 We can see that the married#cograd interaction is not significant ($p = 0.0652$). Let’s assume that we want to retain this interaction. (We may want to retain it based on theoretical considerations or because its -value is close to 0.05.) Let’s now turn our attention to married, which is significant ($p\u003c0.001$). To understand this significant result, let’s use the margins command to compute the adjusted means by the levels of married. margins married,nopvalues Predictive margins Number of obs = 1,157\rExpression: Linear prediction, predict()\r--------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r-------------+------------------------------------------------\rmarried |\rUnmarried | 5.36 0.04 5.28 5.44\rMarried | 5.71 0.04 5.63 5.79\r--------------------------------------------------------------\rWe can think of this adjusted mean as being computed by taking each cell mean of happy7 among those who are not married and weighing it by the corresponding proportion of those are college graduates or noncollege graduates, as illustrated below. $ 454 num ÷ 601 = 0.74$ $ 147 num ÷ 601 = 0.26$ $ 5.3039648 * 0.7407 + 5.5170068 * 0.2593 = 5.3592066 $ We can likewise compute the adjusted mean for those who are married using the same strategy, as shown below. $ 5.7121588 * 0.7407 + 5.6862745 * 0.2593 = 5.05447$ The key point is that the adjusted means are computed by creating a weighted average of cell means that is weighted by the observed proportions of observations in the data (in this case, the observed proportions of cograd)","date":"2024-01-06","objectID":"/8.chapter8categorical-by-categorical-interactions/:4:0","tags":["Categorical","Interaction","stata"],"title":"Chapter8 ：Categorical by categorical interactions","uri":"/8.chapter8categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"5 Main effects with interactions: anova versus regress This section considers the meaning of main effects in the presence of an interaction when using the regress command compared with the anova command. In the presence of interactions, this can lead to conflicting estimates of so-called main effects for the regress command versus the anova command. Let’s use the dataset for this example and show the mean optimism by treatment group and depression status. use opt-2by2.dta anova opt treat##depstat Number of obs = 120 R-squared = 0.4889\rRoot MSE = 10.0136 Adj R-squared = 0.4757\rSource | Partial SS df MS F Prob\u003eF\r--------------+----------------------------------------------------\rModel | 11126 3 3708.6667 36.99 0.0000\r|\rtreat | 2803.3333 1 2803.3333 27.96 0.0000\rdepstat | 7426.1333 1 7426.1333 74.06 0.0000\rtreat#depstat | 896.53333 1 896.53333 8.94 0.0034\r|\rResidual | 11631.467 116 100.27126 --------------+----------------------------------------------------\rTotal | 22757.467 119 191.23922 Let’s now perform this analysis but instead use the regress command. reg opt treat##depstat,vsquish Source | SS df MS Number of obs = 120\r-------------+---------------------------------- F(3, 116) = 36.99\rModel | 11126 3 3708.66667 Prob \u003e F = 0.0000\rResidual | 11631.4667 116 100.271264 R-squared = 0.4889\r-------------+---------------------------------- Adj R-squared = 0.4757\rTotal | 22757.4667 119 191.239216 Root MSE = 10.014\r-------------------------------------------------------------------------------\ropt | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r--------------+----------------------------------------------------------------\rtreat |\rHT | 15.13 2.59 5.85 0.000 10.01 20.25\rdepstat |\rDep | -10.27 2.59 -3.97 0.000 -15.39 -5.15\rtreat#depstat |\rHT#Dep | -10.93 3.66 -2.99 0.003 -18.18 -3.69\r_cons | 44.87 1.83 24.54 0.000 41.25 48.49\r-------------------------------------------------------------------------------\rLet’s now compare the results of the anova command with the results of the regress command, focusing on the significance tests. These comparisons are a bit tricky, because the anova command reports $F$ statistics, whereas the regress command reports $t$ statistics. But we can square the value from the regress command to convert it into an equivalent of an statistic. If we square $t$ the value of $-2.99$ from the regress command, we obtain the 8.94, the same value as $F$ the statistic from the anova command. Using the contrast treat#depstat command following the regress command also yields the same results as the anova command. The value from the contrast command is the same as the $F$ value from the anova command, 8.94. contrast treat#depstat Contrasts of marginal linear predictions\rMargins: asbalanced\r-------------------------------------------------\r| df F P\u003eF\r--------------+----------------------------------\rtreat#depstat | 1 8.94 0.0034\r|\rDenominator | 116\r-------------------------------------------------\rLet’s now compare the test of treat from the anova command with the regress command. We square of the $t$ value for the treat effect from the regress command(5.85) and obtain 34.22. This is different from the $F$ value for the treat effect from the anova command, 27.96. contrast treat Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------\r| df F P\u003eF\r-------------+----------------------------------\rtreat | 1 27.96 0.0000\r|\rDenominator | 116\r------------------------------------------------\rThis might seem perplexing, but there is a perfectly logical explanation for this. The reason for these discrepancies is because of differences in the coding used by the anova and regress commands. The regress command uses dummy (0/1) coding, whereas the anova command and the contrast command use effect ($-1/1$) coding.The interpretation of the interactions is the same whether you use effect coding or dummy coding, but the meaning of the main effects differ. The interpretation of the inte","date":"2024-01-06","objectID":"/8.chapter8categorical-by-categorical-interactions/:5:0","tags":["Categorical","Interaction","stata"],"title":"Chapter8 ：Categorical by categorical interactions","uri":"/8.chapter8categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"6 Interpreting confidence intervals use gss_ivrm.dta anova happy7 i.marital3##i.gender c.health Number of obs = 783 R-squared = 0.1246\rRoot MSE = .941504 Adj R-squared = 0.1178\rSource | Partial SS df MS F Prob\u003eF\r----------------+----------------------------------------------------\rModel | 97.864391 6 16.310732 18.40 0.0000\r|\rmarital3 | 26.17214 2 13.08607 14.76 0.0000\rgender | 3.8126465 1 3.8126465 4.30 0.0384\rmarital3#gender | 2.4708203 2 1.2354101 1.39 0.2488\rhealth | 51.985684 1 51.985684 58.65 0.0000\r|\rResidual | 687.86996 776 .88643037 ----------------+----------------------------------------------------\rTotal | 785.73436 782 1.0047754 Let’s compute the adjusted means of happiness by marital3 by gender using the margins command and graph them using the marginsplot command margins marital3#gender marginsplot Predictive margins Number of obs = 783\rExpression: Linear prediction, predict()\r---------------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r----------------------+----------------------------------------------------------------\rmarital3#gender |\rMarried#Male | 5.68 0.09 59.93 0.000 5.50 5.87\rMarried#Female | 5.69 0.06 98.87 0.000 5.57 5.80\rPrevmarried#Male | 5.13 0.11 44.95 0.000 4.90 5.35\rPrevmarried#Female | 5.29 0.08 64.16 0.000 5.13 5.46\rNever married#Male | 5.27 0.09 57.86 0.000 5.09 5.45\rNever married#Female | 5.55 0.09 60.82 0.000 5.37 5.73\r---------------------------------------------------------------------------------------\rAdjusted means of happiness by marital status and gender\rwe would need to test the effect of gender at each level of marital status using the margins command, as shown below. margins gender@marital3,contrast(nowald pveffects) Contrasts of predictive margins Number of obs = 783\rExpression: Linear prediction, predict()\r------------------------------------------------------------------------\r| Delta-method\r| Contrast std. err. t P\u003e|t|\r--------------------------------+---------------------------------------\rgender@marital3 |\r(Female vs base) Married | 0.01 0.11 0.05 0.961\r(Female vs base) Prevmarried | 0.17 0.14 1.20 0.231\r(Female vs base) Never married | 0.29 0.13 2.21 0.027\r------------------------------------------------------------------------\rIn summary, the marginsplot command provides a graphical display of the results calculated by the margins command. Sometimes, the appearance of the confidence intervals of individual groups might tempt you to inappropriately make statistical inferences about the comparisons between the groups. To avoid this trap, you can directly form comparisons among the groups of interest to ascertain the significance of the group differences. ","date":"2024-01-06","objectID":"/8.chapter8categorical-by-categorical-interactions/:6:0","tags":["Categorical","Interaction","stata"],"title":"Chapter8 ：Categorical by categorical interactions","uri":"/8.chapter8categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter covers models that involve categorical predictors.","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter covers models that involve categorical predictors. The emphasis is on how to make contrasts among levels of the categorical predictor to answer interesting questions regarding the differences among the categories. ","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:0:0","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 Comparing two groups using a t test The simplest kind of categorical predictor has two levels. Examples of such twolevel predictors include gender (male versus female), treatment assignment (treatment group versus control group), or whether one is married (married versus not married). The variable happy7 indicates the happiness rating of the respondent on a 1 to 7 scale, where 7 is completely happy and 1 is completely unhappy. To compare the average happiness between those who are married and unmarried, we can perform an independent groups $t$test, as shown below. use gss_ivrm.dta ttest happy7, by(married) The variable happy7 indicates the happiness rating of the respondent on a 1 to 7 scale, where 7 is completely happy and 1 is completely unhappy. To compare the average happiness between those who are married and unmarried, we can perform an independent groups $t$ test, Two-sample t test with equal variances\r------------------------------------------------------------------------------\rGroup | Obs Mean Std. err. Std. dev. [95% conf. interval]\r---------+--------------------------------------------------------------------\rUnmarrie | 604 5.35596 .0425197 1.044982 5.272456 5.439465\rMarried | 556 5.705036 .0368763 .8695302 5.632602 5.77747\r---------+--------------------------------------------------------------------\rCombined | 1,160 5.523276 .0287773 .9801179 5.466815 5.579737\r---------+--------------------------------------------------------------------\rdiff | -.3490757 .0567084 -.4603384 -.237813\r------------------------------------------------------------------------------\rdiff = mean(Unmarrie) - mean(Married) t = -6.1556\rH0: diff = 0 Degrees of freedom = 1158\rHa: diff \u003c 0 Ha: diff != 0 Ha: diff \u003e 0\rPr(T \u003c t) = 0.0000 Pr(|T| \u003e |t|) = 0.0000 Pr(T \u003e t) = 1.0000\rThe t-test command shows that the average happiness is 5.356 for those who are unmarried and 5.705 for those who are married. The difference between these means is $-0.349$, and that difference is significantly different from 0 (with a two-tailed value of 0.0000). The difference between these means is negative. We can interpret this result to say that those who are unmarried are significantly less happy than those who are married. We could also say that those who are married are significantly happier than those who are unmarried ","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:1:0","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2 More groups and more predictors We are seldom interested in simply comparing two groups in the absence of any additional predictors (covariates). Let’s extend the previous example in two ways. First, let’s use a five-level measure of marital status, which is coded: 1 = married, 2 = widowed, 3 = divorced, 4 = separated, and 5 = never married. Second, let’s include additional predictors (covariates): gender,2 race, and age. We begin by testing the overall null hypothesis that the average happiness is equal among the five marital status groups: $$ H_{0} = \\mu_{2} = \\mu_{3} = \\mu_{4} = \\mu_{5}$$ I included the i. prefix before each of the categorical variables and the c. prefix in front of age to specify that it is a continuous variable. use gss_ivrm.dta anova happy7 i.marital i.gender i.race c.age Number of obs = 1,156 R-squared = 0.0488\rRoot MSE = .957826 Adj R-squared = 0.0422\rSource | Partial SS df MS F Prob\u003eF\r-----------+----------------------------------------------------\rModel | 53.979104 8 6.747388 7.35 0.0000\r|\rmarital | 47.156057 4 11.789014 12.85 0.0000\rgender | 3.8954776 1 3.8954776 4.25 0.0396\rrace | .92449374 2 .46224687 0.50 0.6043\rage | 5.3200497 1 5.3200497 5.80 0.0162\r|\rResidual | 1052.2934 1,147 .91743103 -----------+----------------------------------------------------\rTotal | 1106.2725 1,155 .95781168 Note! The anova and regress commands In this chapter (as well as chapters 8 and 9), my focus will be on starting the analysis with an assessment of the main effects (and interactions, if any) using the omnibus $F$-tests provided by the anova command. I would emphasize, however, that these same results could be obtained via the regress command. Note! Omnibus $F$-tests Omnibus F-tests are statistical tests used to assess the overall significance or goodness-of-fit of a regression model. These tests evaluate the joint significance of multiple coefficients in the model or the overall explanatory power of the regression equation. In regression analysis, the Omnibus F-test is typically used to determine whether there is a significant relationship between the predictor variables and the dependent variable. It examines the overall fit of a model by comparing the variance explained by the model to the unexplained variance (residuals) around the regression line. The steps involved in performing an Omnibus F-test include: Fit the regression model using the given predictor variables. Calculate the overall F-statistic using the explained and unexplained variance from the model. Obtain the degrees of freedom for the F-distribution based on the number of predictors and the sample size. Compare the calculated F-statistic with the critical value from the F-distribution at a specified significance level (commonly 0.05 or 0.01). If the calculated F-statistic exceeds the critical value, the null hypothesis is rejected, indicating that the overall model is statistically significant. The overall test of marital is significant ( , ). After adjusting for gender, race, and age, we can reject the null hypothesis that the average happiness is equal among the five marital status groups. Let’s probe this finding in more detail. Suppose that our research hypothesis (prior to even seeing the data) was that those who are married will be happier than each of the four other marital status groups. We can frame this as four separate null hypotheses, shown below $$H_{0}: \\mu_{2} = \\mu_{1} $$ $$H_{0}: \\mu_{3} = \\mu_{1} $$ $$H_{0}: \\mu_{4} = \\mu_{1} $$ $$H_{0}: \\mu_{5} = \\mu_{1} $$ Let’s begin the exploration of these tests by using the margins command to compute the adjusted mean of happiness by marital status. margins marital marginsplot Predictive margins Number of obs = 1,156\rExpression: Linear prediction, predict()\r--------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r---------------+----------------------------------------------------------------\rmarital |","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:2:0","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3 Overview of contrast operators the contrast operators table provides a brief description of each contrast operator and shows the section of this chapter in which each contrast operator is covered. Summary of contrast operators\r","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:3:0","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4 Compare each group against a reference group This section provides further examples illustrating the r. contrast operator for making reference group contrasts. Let’s continue with the example that predicting happiness from marital status, adjusting for gender, race, and age. use gss_ivrm.dta anova happy7 i.marital i.gender i.race c.age Number of obs = 1,156 R-squared = 0.0488\rRoot MSE = .957826 Adj R-squared = 0.0422\rSource | Partial SS df MS F Prob\u003eF\r-----------+----------------------------------------------------\rModel | 53.979104 8 6.747388 7.35 0.0000\r|\rmarital | 47.156057 4 11.789014 12.85 0.0000\rgender | 3.8954776 1 3.8954776 4.25 0.0396\rrace | .92449374 2 .46224687 0.50 0.6043\rage | 5.3200497 1 5.3200497 5.80 0.0162\r|\rResidual | 1052.2934 1,147 .91743103 -----------+----------------------------------------------------\rTotal | 1106.2725 1,155 .95781168 ","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:4:0","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4.1 Selecting a specific contrast Suppose you wanted to focus on the contrast of group 3 to group 1 (divorced versus married) and group 5 to group 1 (never married versus married). You can perform those two contrasts by specifying the r(3 5). contrast operator. This compares each of the groups within the parentheses with the reference group (group 1). contrast r(3 5).marital,pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r---------------------------------------------------------------\r| df F P\u003eF\r----------------------------+----------------------------------\rmarital |\r(divorced vs married) | 1 39.38 0.0000\r(never married vs married) | 1 4.18 0.0411\rJoint | 2 19.78 0.0000\r|\rDenominator | 1147\r---------------------------------------------------------------\r--------------------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r----------------------------+---------------------------------------\rmarital |\r(divorced vs married) | -0.50 0.08 -6.28 0.000\r(never married vs married) | -0.16 0.08 -2.04 0.041\r--------------------------------------------------------------------\rNote! Options on the contrast command The previous contrast command included two options: nowald and pveffects. This yields concise output that fits well on the pages of this book. When you run such analyses yourself, I recommend specifying the options nowald and effects, which will display both significance tests and confidence intervals associated with each comparison. ","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:4:1","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4.2 Selecting a different reference group Suppose that instead we wanted to compare each group with a different reference group. We can specify the rb5. contrast operator, which requests reference group contrasts using group 5 (never married) as the baseline (reference) group. contrast rb5.marital,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r------------------------------+---------------------------------------\rmarital |\r(married vs never married) | 0.16 0.08 2.04 0.041\r(widowed vs never married) | -0.30 0.14 -2.06 0.040\r(divorced vs never married) | -0.34 0.09 -3.62 0.000\r(separated vs never married) | -0.39 0.18 -2.24 0.026\r----------------------------------------------------------------------\r","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:4:2","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4.3 Selecting a contrast and reference group You can both specify the reference group and specify the contrasts to be made at one time. contrast r(1 3)b5.marital,nowald pveffects //(1 versus 5)(3 versus 5) Contrasts of marginal linear predictions\rMargins: asbalanced\r---------------------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r-----------------------------+---------------------------------------\rmarital |\r(married vs never married) | 0.16 0.08 2.04 0.041\r(divorced vs never married) | -0.34 0.09 -3.62 0.000\r---------------------------------------------------------------------\r","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:4:3","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"5 Compare each group against the grand mean This section illustrates the g. contrast operator that compares each group with the grand mean of all groups. a researcher might be interested in comparing the mean happiness of each marital status group versus the grand mean of all groups. use gss_ivrm.dta anova happy7 i.marital i.gender i.race c.age margins g.marital,contrast(nowald pveffects) the prefix “g” means the mean happiness of each marital status group with the grand mean Contrasts of predictive margins Number of obs = 1,156\rExpression: Linear prediction, predict()\r-----------------------------------------------------------------\r| Delta-method\r| Contrast std. err. t P\u003e|t|\r-------------------------+---------------------------------------\rmarital |\r(married vs mean) | 0.33 0.05 6.12 0.000\r(widowed vs mean) | -0.12 0.10 -1.18 0.240\r(divorced vs mean) | -0.17 0.07 -2.44 0.015\r(separated vs mean) | -0.22 0.14 -1.61 0.107\r(never married vs mean) | 0.18 0.07 2.48 0.013\r-----------------------------------------------------------------\rmarginsplot,yline(0) When the confidence interval for a contrast excludes zero, the difference is significant at the 5% level. Contrasts comparing each group with the grand mean\r","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:5:0","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"6 Compare adjacent means This section illustrates contrasts that compare the means of adjacent groups, for example, group 1 versus 2, group 2 versus 3, group 3 versus 4. These kinds of contrasts are especially useful for studies where you expect a nonlinear relationship between an ordinal or interval predictor and outcome. For example, consider a hypothetical study about the dosage of a new pain medication where the researchers expect that at a certain dosage level the effects of the medication will kick in and lead to a statistically significant reduction in pain. The medication dosages range from 0 mg to 250 mg incrementing by 50 mg, yielding six dosage groups. use pain codebook dosegrp Type: Numeric (float)\rLabel: dosegrp\rRange: [1,6] Units: 1\rUnique values: 6 Missing .: 0/180\rTabulation: Freq. Numeric Label\r30 1 0mg\r30 2 50mg\r30 3 100mg\r30 4 150mg\r30 5 200mg\r30 6 250mg\rLet’s begin the analysis relating pain to medication dosage by testing the most general null hypothesis that could be tested—that the average pain is equal across all six dosage groups: $$ H_{0} = \\mu_{2} = \\mu_{3} = \\mu_{4} = \\mu_{5}$$ This null hypothesis is tested using the anova command shown below. anova pain i.dosegrp Number of obs = 180 R-squared = 0.4602\rRoot MSE = 10.4724 Adj R-squared = 0.4447\rSource | Partial SS df MS F Prob\u003eF\r-----------+----------------------------------------------------\rModel | 16271.694 5 3254.3389 29.67 0.0000\r|\rdosegrp | 16271.694 5 3254.3389 29.67 0.0000\r|\rResidual | 19082.633 174 109.67031 -----------+----------------------------------------------------\rTotal | 35354.328 179 197.51021 he $F$ value is 29.67 and is significant. We can reject the overall null hypothesis. Let’s use the margins command and the marginsplot command to display and graph the predicted mean of pain by dosegrp. margins dosegrp //the predicted mean of pain by dosegrp marginsplot Adjusted predictions Number of obs = 180\rExpression: Linear prediction, predict()\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\rdosegrp |\r0mg | 71.83 1.91 37.57 0.000 68.06 75.61\r50mg | 70.60 1.91 36.93 0.000 66.83 74.37\r100mg | 72.13 1.91 37.73 0.000 68.36 75.91\r150mg | 70.40 1.91 36.82 0.000 66.63 74.17\r200mg | 54.70 1.91 28.61 0.000 50.93 58.47\r250mg | 48.30 1.91 25.26 0.000 44.53 52.07\r------------------------------------------------------------------------------\rMean pain rating by dosage group\rFor this study, the research question of interest focuses on the test of each dosage against the previous dosage to determine the dosage that leads to a statistically significant decrease in pain. This leads us to five specific null hypotheses. $$H_{0}: \\mu_{1} = \\mu_{2} $$ $$H_{0}: \\mu_{2} = \\mu_{3} $$ $$H_{0}: \\mu_{3} = \\mu_{4} $$ $$H_{0}: \\mu_{4} = \\mu_{5} $$ $$H_{0}: \\mu_{5} = \\mu_{6} $$ Let’s now test each of the hypotheses using the contrast command with the a. contrast operator to compare each dosage with the adjacent (subsequent) dosage. margins a.dosegrp,contrast(nowald pveffects) margins a.dosegrp,contrast(nowald cieffects) //specify cieffects in lieu of pveffects marginsplot,yline(0) Contrasts of adjusted predictions Number of obs = 180\rExpression: Linear prediction, predict()\r----------------------------------------------------------\r| Delta-method\r| Contrast std. err. t P\u003e|t|\r------------------+---------------------------------------\rdosegrp |\r(0mg vs 50mg) | 1.23 2.70 0.46 0.649\r(50mg vs 100mg) | -1.53 2.70 -0.57 0.571\r(100mg vs 150mg) | 1.73 2.70 0.64 0.522\r(150mg vs 200mg) | 15.70 2.70 5.81 0.000\r(200mg vs 250mg) | 6.40 2.70 2.37 0.019\r----------------------------------------------------------\rContrasts of each dosage to the previous dosage\r","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:6:0","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"6.1 Reverse adjacent contrasts The ar. contrast operator, shown below, provides adjacent group contrasts in reverse order. contrast ar.dosegrp,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r------------------+---------------------------------------\rdosegrp |\r(50mg vs 0mg) | -1.23 2.70 -0.46 0.649\r(100mg vs 50mg) | 1.53 2.70 0.57 0.571\r(150mg vs 100mg) | -1.73 2.70 -0.64 0.522\r(200mg vs 150mg) | -15.70 2.70 -5.81 0.000\r(250mg vs 200mg) | -6.40 2.70 -2.37 0.019\r----------------------------------------------------------\r","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:6:1","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"6.2 Selecting a specific contrast When making adjacent group contrasts, you can select a specific contrast. contrast a1.dosegrp,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r-------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r---------------+---------------------------------------\rdosegrp |\r(0mg vs 50mg) | 1.23 2.70 0.46 0.649\r-------------------------------------------------------\rWe can also select contrasts using the ar. contrast operator. The ar. contrast operator forms contrasts with the previous group, so specifying ar3. (as shown below) contrasts group 3 (100 mg) with the previous group (group 2, 50 mg). contrast ar3.dosegrp,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r---------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r-----------------+---------------------------------------\rdosegrp |\r(100mg vs 50mg) | 1.53 2.70 0.57 0.571\r---------------------------------------------------------\rWe can combine selected contrasts as well. Suppose we wanted to test the equality of the mean pain ratings for the first four groups. contrast a(1 2 3).dosegrp Contrasts of marginal linear predictions\rMargins: asbalanced\r-----------------------------------------------------\r| df F P\u003eF\r------------------+----------------------------------\rdosegrp |\r(0mg vs 50mg) | 1 0.21 0.6489\r(50mg vs 100mg) | 1 0.32 0.5714\r(100mg vs 150mg) | 1 0.41 0.5223\rJoint | 3 0.21 0.8918\r|\rDenominator | 174\r-----------------------------------------------------\r-------------------------------------------------------------------\r| Contrast Std. err. [95% conf. interval]\r------------------+------------------------------------------------\rdosegrp |\r(0mg vs 50mg) | 1.23 2.70 -4.10 6.57\r(50mg vs 100mg) | -1.53 2.70 -6.87 3.80\r(100mg vs 150mg) | 1.73 2.70 -3.60 7.07\r-------------------------------------------------------------------\r","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:6:2","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"7 Comparing the mean of subsequent or previous levels This section describes contrasts that compare each group mean with the mean of the subsequent groups (also known as Helmert contrasts). use pain2 tab dosage Medication |\rdosage in |\rmg | Freq. Percent Cum.\r------------+-----------------------------------\r300 | 30 16.67 16.67\r400 | 30 16.67 33.33\r500 | 30 16.67 50.00\r600 | 30 16.67 66.67\r800 | 30 16.67 83.33\r1000 | 30 16.67 100.00\r------------+-----------------------------------\rTotal | 180 100.00\rLet’s begin by testing this overall null hypothesis using the anova command below. $$ H_{0} : \\mu_{300} = \\mu_{400} = \\mu_{500} = \\mu_{600}= \\mu_{800}= \\mu_{1000}$$ anova pain i.dosage Number of obs = 180 R-squared = 0.2052\rRoot MSE = 10.5056 Adj R-squared = 0.1824\rSource | Partial SS df MS F Prob\u003eF\r-----------+----------------------------------------------------\rModel | 4958.8667 5 991.77333 8.99 0.0000\r|\rdosage | 4958.8667 5 991.77333 8.99 0.0000\r|\rResidual | 19204.133 174 110.36858 -----------+----------------------------------------------------\rTotal | 24163 179 134.98883 Let’s use the margins and marginsplot commands to show and graph the means by the six levels of dosage. margins dosage marginsplot Adjusted predictions Number of obs = 180\rExpression: Linear prediction, predict()\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\rdosage |\r300 | 43.83 1.92 22.85 0.000 40.05 47.62\r400 | 37.60 1.92 19.60 0.000 33.81 41.39\r500 | 31.87 1.92 16.61 0.000 28.08 35.65\r600 | 29.63 1.92 15.45 0.000 25.85 33.42\r800 | 30.63 1.92 15.97 0.000 26.85 34.42\r1000 | 29.43 1.92 15.35 0.000 25.65 33.22\r------------------------------------------------------------------------------\rMean pain rating by dosage\r$$H_{0}: \\mu_{300} = \\mu \u003e _{\\text{300}}$$ $$H_{0}: \\mu_{400} = \\mu \u003e _{\\text{400}}$$ $$H_{0}: \\mu_{500} = \\mu \u003e _{\\text{500}}$$ $$H_{0}: \\mu_{600} = \\mu \u003e _{\\text{600}}$$ $$H_{0}: \\mu_{800} = \\mu \u003e _{\\text{1000}}$$ Let’s now test each of the null hypotheses below using the margins command combined with the h. contrast operator. margins h.dosage,contrast(nowald pveffects) Contrasts of adjusted predictions Number of obs = 180\rExpression: Linear prediction, predict()\r--------------------------------------------------------\r| Delta-method\r| Contrast std. err. t P\u003e|t|\r----------------+---------------------------------------\rdosage |\r(300 vs \u003e 300) | 12.00 2.10 5.71 0.000\r(400 vs \u003e 400) | 7.21 2.14 3.36 0.001\r(500 vs \u003e 500) | 1.97 2.21 0.89 0.376\r(600 vs \u003e 600) | -0.40 2.35 -0.17 0.865\r(800 vs 1000) | 1.20 2.71 0.44 0.659\r--------------------------------------------------------\rmarginsplot,yline(0) xlabel(, angle(45)) Mean pain rating by dosage\rWhen the confidence interval for the contrast excludes zero,the difference is significant at the 5% level. ","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:7:0","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"7.1 Comparing the mean of previous levels contrast j.dosage,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r---------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r-----------------+---------------------------------------\rdosage |\r(400 vs 300) | -6.23 2.71 -2.30 0.023\r(500 vs \u003c 500) | -8.85 2.35 -3.77 0.000\r(600 vs \u003c 600) | -8.13 2.21 -3.67 0.000\r(800 vs \u003c 800) | -5.10 2.14 -2.38 0.018\r(1000 vs \u003c1000) | -5.28 2.10 -2.51 0.013\r---------------------------------------------------------\r","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:7:1","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"7.2 Selecting a specific contrast contrast h400.dosage,nowald pveffects we wanted to focus only on the contrast of those whose value of dosage was 400 to those who have higher values of dosage. Contrasts of marginal linear predictions\rMargins: asbalanced\r-------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r---------------+---------------------------------------\rdosage |\r(400 vs \u003e400) | 7.21 2.14 3.36 0.001\r-------------------------------------------------------\rAlternatively, we might want to focus only on the contrasts of 400 mg versus the subsequent groups and 500 mg versus the subsequent groups. contrast h(400 500).dosage,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r-------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r---------------+---------------------------------------\rdosage |\r(400 vs \u003e400) | 7.21 2.14 3.36 0.001\r(500 vs \u003e500) | 1.97 2.21 0.89 0.376\r-------------------------------------------------------\rselect the contrast of 500mg versus the mean of the previous groups contrast j500.dosage,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r-------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r---------------+---------------------------------------\rdosage |\r(500 vs \u003c500) | -8.85 2.35 -3.77 0.000\r-------------------------------------------------------\r","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:7:2","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"Polynomial contrasts Let’s consider the use of polynomial contrasts for assessing nonlinear trends (for example, quadratic, cubic, or quartic). We can specify q.dosegrp on the contrast command to compute tests of polynomial trend with respect to dosegrp. (The noeffects option is used to save space and focus on the results of the Wald tests.) use pain contrast q.dosegrp,noeffects compute tests of polynomial trend with respect to dosegrp. Margins: asbalanced\r------------------------------------------------\r| df F P\u003eF\r-------------+----------------------------------\rdosegrp |\r(linear) | 1 109.12 0.0000\r(quadratic) | 1 29.25 0.0000\r(cubic) | 1 0.00 0.9824\r(quartic) | 1 8.39 0.0043\r(quintic) | 1 1.62 0.2048\rJoint | 5 29.67 0.0000\r|\rDenominator | 174\r------------------------------------------------\rSuppose you wanted to fit the relationship between a predictor and outcome using a linear term and wanted to assess whether there are significant nonlinear trends in the relationship between the predictor and outcome. You can use the contrast command to test only the nonlinear terms, as shown below. contrast q(2/6).dosegrp,noeffects Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------\r| df F P\u003eF\r-------------+----------------------------------\rdosegrp |\r(quadratic) | 1 29.25 0.0000\r(cubic) | 1 0.00 0.9824\r(quartic) | 1 8.39 0.0043\r(quintic) | 1 1.62 0.2048\rJoint | 4 9.81 0.0000\r|\rDenominator | 174\r------------------------------------------------\rThe joint test of all the nonlinear terms (powers 2 through 6) is significant.In such a case, it would be inadvisable to fit the relationship between the predictor and outcome using only a linear fit. The q.contrast operator assumes that the levels of dosegrp are equidistant from each other. In next example the level of dosage are not equidistant and we would have obtained different results by specifying q.dosage compared with specifying p.dosage. contrast p.dosage,noeffects the p.contrast operator to dosage.this tests the polynomial trends based on the actual dosage,accounting for the differing gaps among the levels of dosage. Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------\r| df F P\u003eF\r-------------+----------------------------------\rdosage |\r(linear) | 1 28.37 0.0000\r(quadratic) | 1 13.44 0.0003\r(cubic) | 1 2.60 0.1084\r(quartic) | 1 0.47 0.4956\r(quintic) | 1 0.05 0.8202\rJoint | 5 8.99 0.0000\r|\rDenominator | 174\r------------------------------------------------\rspecific on cubic,quartic,and quintic contrast p(3/6).dosage,noeffects ","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:8:0","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"9 Custom contrasts For those times when you want to make another kind of contrast, you can specify a custom contrast. Let’s begin by illustrating how to perform custom contrasts using simple examples that compare one group with another group. For the first example, let’s compare the mean of group 1 (married) with group 5 (not married). The custom contrast is enclosed within curly braces by specifying the variable name followed by the contrast coefficients. The contrast coefficients map to the levels (groups) of the variable. In this example, the contrast coefficient of 1 is applied to group 1, and is applied to group 5. (A contrast coefficient of 0 is applied to groups 2, 3, and 4.) The result is a contrast of group 1 minus group 5. contrast {marital 1 0 0 0 -1} the contrast coef of 1 is applied to group 1,and -1 is applied to group 5(a contrast coefficient of 0 is applied to group 2 3 and 4) Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------\r| df F P\u003eF\r-------------+----------------------------------\rmarital | 1 4.18 0.0411\r|\rDenominator | 1147\r------------------------------------------------\r--------------------------------------------------------------\r| Contrast Std. err. [95% conf. interval]\r-------------+------------------------------------------------\rmarital |\r(1) | 0.16 0.08 0.01 0.31\r--------------------------------------------------------------\rLet’s switch the above contrast. Let’s compare group 5 (not married) with group 1 (married), as shown below. contrast {marital -1 0 0 0 1},nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r-----------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r-------------+---------------------------------------\rmarital |\r(1) | -0.16 0.08 -2.04 0.041\r-----------------------------------------------------\rSay that we want to compare those who are married (group 1) with the average of those who are separated and divorced (groups 3 and 4). We can form that contrast as shown below. contrast{marital 1 0 -.5 -.5 0},nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r-----------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r-------------+---------------------------------------\rmarital |\r(1) | 0.52 0.10 5.34 0.000\r-----------------------------------------------------\rSuppose we want to compare those who are married (group 1) with the average of those who are widowed, divorced, and separated (groups 2, 3, and 4). contrast{marital 1 -.33333333 -.33333333 -.33333333 0},nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r-----------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r-------------+---------------------------------------\rmarital |\r(1) | 0.50 0.08 6.10 0.000\r-----------------------------------------------------\rNote! Contrasts must sum to zero The contrast coefficients that we specify in a custom contrast must sum to zero. In the previous example, the contrast coefficients for groups 2, 3, and 4 are expressed as -.33333333, using eight digits after the decimal point. Although the sum of the coefficients for that custom contrast is not exactly zero, it is close enough to zero to satisfy the margins command. You can use inline expansions to directly specify the fraction , as shown in the contrast command below. contrast {marital 1 ‘=-1/3’ ‘=-1/3’ ‘=-1/3’ 0}, nowald pveffects Let’s form a contrast of the average of those who are married (group 1) and separated (group 4) to the average of those who are widowed (group 2) and divorced (group 3). Note how the coefficients for groups 1 and 4 are specified as 0.5 and the coefficients for groups 2 and 3 are specified as . contrast{marital .5 -.5 -.5 .5 0},nowald pveffects Margins: asbalanced\r-----------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r-------------+---------------------------------------\rmarital |\r(1) | ","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:9:0","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"10 Weighted contrasts So far, the examples I have shown estimate the mean for groups 2 and 3 combined by obtaining the mean for group 2 and the mean for group 3, and then averaging those means. Stata calls this the as-balanced approach, because it gives equal weights to the groups even if their sample sizes are different. We could, instead, weight the means for groups 2 and 3 proportionate to their sample size. Stata calls this the as-observed approach, because the means are weighted in proportion to their observed sample size. anova happy7 i.marital3 margins h.marital3,contrast(nowald pveffects) Number of obs = 1,160 R-squared = 0.0369\rRoot MSE = .962698 Adj R-squared = 0.0352\rSource | Partial SS df MS F Prob\u003eF\r-----------+----------------------------------------------------\rModel | 41.07881 2 20.539405 22.16 0.0000\r|\rmarital3 | 41.07881 2 20.539405 22.16 0.0000\r|\rResidual | 1072.2927 1,157 .92678716 -----------+----------------------------------------------------\rTotal | 1113.3716 1,159 .96063119 Let’s use the h. contrast operator to compare each group with the mean of the subsequent groups, using the as-balanced approach. Let’s focus our attention on the first contrast, Married vs \u003eMarried, which compares those who are married (group 1) versus those who are previously married and never married (groups 2 and 3). Compare each group with the mean of the subsequent groups,using the as-balanced approach Contrasts of adjusted predictions Number of obs = 1,160\rExpression: Linear prediction, predict()\r-------------------------------------------------------------------------\r| Delta-method\r| Contrast std. err. t P\u003e|t|\r---------------------------------+---------------------------------------\rmarital3 |\r(Married vs \u003eMarried) | 0.34 0.06 6.07 0.000\r(Prevmarried vs Never married) | -0.20 0.08 -2.50 0.012\r-------------------------------------------------------------------------\rWe can manually compute this estimate: $5.705036 - (5.263323 + 5.459649)/2 = 0.34355$ Let’s compare this estimate with the as-observed approach, which weights the mean of groups 2 and 3 (previously married and never married) by their sample size. The hw. contrast operator is used to obtain the as-observed estimate. margins h.marital3,contrast(nowald pveffects) Contrasts of adjusted predictions Number of obs = 1,160\rExpression: Linear prediction, predict()\r-------------------------------------------------------------------------\r| Delta-method\r| Contrast std. err. t P\u003e|t|\r---------------------------------+---------------------------------------\rmarital3 |\r(Married vs \u003eMarried) | 0.34 0.06 6.07 0.000\r(Prevmarried vs Never married) | -0.20 0.08 -2.50 0.012\r-------------------------------------------------------------------------\rInstead of weighting groups 2 and 3 equally—by (1/2)—groups 2 and 3 are weighted by their individual sample size divided by the combined sample size. The $N$ for group 2 (previously married) is 319, and the $N$ for group 3 (never married) is 285, and the combined for the two groups is 604. We can manually compute this estimate: $5.705036 - (5.263323*(319/604)+ 5.459649*(285/604)) = 0.34907573$ Thus, if you want estimates that are weighted as a function of the proportion of observations in each group, then the as-observed estimates will provide you the kind of estimates that you desire. ","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:10:0","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"11 Pairwise comparisons Sometimes, you want to test all pairwise comparisons that can be formed for a factor variable. The pwcompare command can be used to form such comparisons. use gss_ivrm,clear anova happy7 i.marital i.gender i.race c.age We can use the pwcompare command to form all pairwise comparisons among the marital status groups. Because of the many comparisons, we might want to make adjustments to the values to account for the multiple comparisons. For example, let’s use Šidák’s method for adjusting for multiple comparisons by adding the mcompare(sidak) option. pwcompare marital,pveffects mcompare(sidak) Pairwise comparisons of marginal linear predictions\rMargins: asbalanced\r---------------------------\r| Number of\r| comparisons\r-------------+-------------\rmarital3 | 3\r---------------------------\r----------------------------------------------------------------------\r| Sidak\r| Contrast Std. err. t P\u003e|t|\r------------------------------+---------------------------------------\rmarital3 |\rPrevmarried vs Married | -0.44 0.07 -6.53 0.000\rNever married vs Married | -0.25 0.07 -3.50 0.001\rNever married vs Prevmarried | 0.20 0.08 2.50 0.037\r----------------------------------------------------------------------\rThe bonferroni or scheffe method could have alternatively been specified within the mcompare() option. When you have balanced data, the tukey, snk, duncan, or dunnett method can be specified within mcompare() (see [R] pwcompare for more details) ","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:11:0","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"12 Interpreting confidence intervals The marginsplot command displays margins and confidence intervals that were computed from the most recent margins command. Sometimes, these confidence intervals might tempt you into falsely believing that they tell us about differences among groups. use gss_ivrm,clear anova happy7 i.marital i.gender i.race c.age The margins command is used to estimate the adjusted means of happiness by marital. The output also includes the 95% confidence interval for each adjusted mean. margins marital marginsplot Predictive margins Number of obs = 1,156\rExpression: Linear prediction, predict()\r--------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r---------------+----------------------------------------------------------------\rmarital |\rmarried | 5.70 0.04 139.63 0.000 5.62 5.78\rwidowed | 5.25 0.12 44.51 0.000 5.01 5.48\rdivorced | 5.20 0.07 75.92 0.000 5.06 5.33\rseparated | 5.15 0.16 31.25 0.000 4.83 5.47\rnever married | 5.54 0.06 87.86 0.000 5.42 5.67\r--------------------------------------------------------------------------------\rAdjusted means of happiness by marital status\rOur eye might be tempted to use the overlap (or lack of overlap) of confidence intervals between groups to draw conclusions about the significance of the differences between groups. However, such conclusions would not be appropriate. For example, although the confidence intervals for those who are separated and never married overlap, the output from the contrast command below shows that the difference in these means is statistically significant ($P = 0.026$) contrast ar.marital,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r------------------------------+---------------------------------------\rmarital |\r(widowed vs married) | -0.45 0.12 -3.67 0.000\r(divorced vs widowed) | -0.05 0.13 -0.34 0.732\r(separated vs divorced) | -0.05 0.18 -0.28 0.777\r(never married vs separated) | 0.39 0.18 2.24 0.026\r----------------------------------------------------------------------\r","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:12:0","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"13 Testing categorical variables using regression The analyses in this chapter have been conducted using the anova command because it produces concise printed output. However, this is not to imply that we cannot perform such tests using the regress command. use gss_ivrm,clear anova happy7 i.marital i.gender i.race c.age reg happy7 i.marital i.gender i.race c.age Source | SS df MS Number of obs = 1,156\r-------------+---------------------------------- F(8, 1147) = 7.35\rModel | 53.9791041 8 6.74738801 Prob \u003e F = 0.0000\rResidual | 1052.29339 1,147 .917431026 R-squared = 0.0488\r-------------+---------------------------------- Adj R-squared = 0.0422\rTotal | 1106.27249 1,155 .957811681 Root MSE = .95783\r--------------------------------------------------------------------------------\rhappy7 | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r---------------+----------------------------------------------------------------\rmarital |\rwidowed | -0.45 0.12 -3.67 0.000 -0.70 -0.21\rdivorced | -0.50 0.08 -6.28 0.000 -0.66 -0.34\rseparated | -0.55 0.17 -3.23 0.001 -0.88 -0.22\rnever married | -0.16 0.08 -2.04 0.041 -0.31 -0.01\r|\rgender |\rFemale | 0.12 0.06 2.06 0.040 0.01 0.23\r|\rrace |\rblack | -0.04 0.09 -0.42 0.676 -0.20 0.13\rother | -0.11 0.12 -0.95 0.341 -0.34 0.12\r|\rage | 0.01 0.00 2.41 0.016 0.00 0.01\r_cons | 5.42 0.11 47.90 0.000 5.19 5.64\r--------------------------------------------------------------------------------\rThe output of the regress command is lengthier because it uses dummy coding for each of the categorical variables and shows the effect for each of the dummy variables. To obtain the test of the overall effect of marital and the test of the overall effect of race, we can use the contrast command, as shown below. contrast marital race Contrasts of marginal linear predictions Margins: asbalanced ------------------------------------------------\r| df F P\u003eF\r-------------+----------------------------------\rmarital | 4 12.85 0.0000\r|\rrace | 2 0.50 0.6043\r|\rDenominator | 1147\r------------------------------------------------\r**The contrast, margins, marginsplot, and pwcompare commands work the same way after the regress command as they do after the anova command.**For example, wecan use the margins command to obtain the adjusted means broken down by marital. contrast r.marital,nowald pveffects Predictive margins Number of obs = 1,156\rModel VCE: OLS\rExpression: Linear prediction, predict()\r--------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r---------------+----------------------------------------------------------------\rmarital |\rmarried | 5.70 0.04 139.63 0.000 5.62 5.78\rwidowed | 5.25 0.12 44.51 0.000 5.01 5.48\rdivorced | 5.20 0.07 75.92 0.000 5.06 5.33\rseparated | 5.15 0.16 31.25 0.000 4.83 5.47\rnever married | 5.54 0.06 87.86 0.000 5.42 5.67\r--------------------------------------------------------------------------------\r","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:13:0","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter explores models that involve the interaction of three continuous linear predictors.","date":"2024-01-04","objectID":"/6.continuous-by-continuous-by-continuous-interactions/","tags":["Continuous predictors","Interaction","stata"],"title":"Chapter6 ：Continuous by continuous by continuous interactions","uri":"/6.continuous-by-continuous-by-continuous-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter explores models that involve the interaction of three continuous linear predictors. ","date":"2024-01-04","objectID":"/6.continuous-by-continuous-by-continuous-interactions/:0:0","tags":["Continuous predictors","Interaction","stata"],"title":"Chapter6 ：Continuous by continuous by continuous interactions","uri":"/6.continuous-by-continuous-by-continuous-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 Overview Let’s first consider a hypothetical example looking at income (adjusted for inflation) as a function of age, education, and year born. To illustrate the meaning of an interaction of these three continuous predictor, let’s first consider a model that does not include a three-way interaction. $$ \\widehat{realrin}=3365394 + — 55747age + -66053educ + -1712yrborn + 109age * educ + 28age * yrborn + 33.2educ*yrborn$$ The variable age is plotted on the $x$(horizontal) axis, educ is on the $z$axis (representing depth), and the predicted value of realrinc is on the $y$(vertical) axis. Separate graphs are used to represent yrborn.The lines with respect to age are drawn thicker to help accentuate changes in the slope of the relationship between age and the outcome as a function of educ and yrborn. Three-dimensional graph of predicted values from model without the three-way interaction\rlet’s now consider a model that includes a three-way interaction.This regression model predicts realrinc from age, educ, yrborn, the two-way interactions of these predictors (ageeduc, ageyrborn, and educyrborn), and the three-way interaction (ageeduc*yrborn). \\begin{align} \\widehat{realrin} \u0026= -2004225 + 66407age + 313720educ + 1042yrborn +- 8524age\\ \\cdot educ \\notag\\ \\end{align} \\begin{align} +- 34.7age \\cdot yrborn +- 161.62educ \\cdot yrborn + 4.43age \\ \\cdot educ \\cdot yrborn \\notag \\end{align} Three-dimensional graph of predicted values from model with the three-way interaction\rage on the $x$axis, educ on the $z$axis, the predicted value of realrinc on the $y$axis. Separate graphs represent yrborn for the years of birth 1930, 1940, 1950, and 1960 shown from left to right and then top to bottom For those born in 1930 (the top left panel), the age slope is mildly negative for those with 12 years of education and remains mildly negative across the different levels of education. Skipping forward to those born in 1960 (the bottom right panel), the age slope is mildly positive for those with 12 years of education and is sharply positive for those with 20 years of education. Looking across the panels from those born in 1930 (top left) to those born in 1960 (bottom right), we can see how the increase in the age slope due to higher levels of education grows as the year of birthincreases from 1930 to 1960. For those born in 1930, the age slope remains largely the same for all education levels. By contrast, for those born in 1960, the age slope increases considerably with increasing levels of educ. In other words, the size of the ageeduc interaction changes as a function of yrborn. This is a result of, and a way to describe, the interaction of ageeduc*yrborn. Let’s see how these models can be visualized using two-dimensional graphs. Two-dimensional graph of adjusted means for model with the threeway interaction\rIn 1960, the age slope increases considerably with increasing education. We can compare how the age slope changes as a function of education for those born in 1930 with those born in 1960. For those born in 1930, the age slope remains mildly negative for all levels of education. By contrast, for those born in 1960, the age slope grows increasingly positive with increasing levels of education. As we saw in the three-dimensional graph, this two-dimensional graph illustrates how the age×educ interaction changes as a function of yrborn, which is another way of saying that there is an age × educ × yrborn interaction. ","date":"2024-01-04","objectID":"/6.continuous-by-continuous-by-continuous-interactions/:1:0","tags":["Continuous predictors","Interaction","stata"],"title":"Chapter6 ：Continuous by continuous by continuous interactions","uri":"/6.continuous-by-continuous-by-continuous-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.1 Example using GSS data Let’s now illustrate a model with an interaction of three continuous variables using the GSS dataset. This example predicts realrinc from age, educ, and yrborn. use gss_ivrm.dta keep if (age\u003e=30 \u0026 age\u003c=55) \u0026 (educ\u003e=12) \u0026 (yrborn\u003e=1930 \u0026 yrborn\u003c=1960) ","date":"2024-01-04","objectID":"/6.continuous-by-continuous-by-continuous-interactions/:2:0","tags":["Continuous predictors","Interaction","stata"],"title":"Chapter6 ：Continuous by continuous by continuous interactions","uri":"/6.continuous-by-continuous-by-continuous-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.1.1 A model without a three-way interaction Linear regression Number of obs = 11,765\rF(8, 11756) = 138.87\rProb \u003e F = 0.0000\rR-squared = 0.1142\rRoot MSE = 24595\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t|\rage | -48960.89 8316.82 -5.89 0.000\reduc | -61659.03 34481.49 -1.79 0.074\ryrborn | -1529.75 290.82 -5.26 0.000\r|\rc.age#c.educ | 109.44 18.74 5.84 0.000\r|\rc.age#c.yrborn | 24.53 4.28 5.73 0.000\r|\rc.educ#c.yrborn | 30.92 17.50 1.77 0.077\r|\rrace |\rblack | -5274.18 464.22 -11.36 0.000\rother | -4616.74 1145.59 -4.03 0.000\r|\r_cons | 3.01e+06 570438.47 5.28 0.000\rWe can visualize these results using the margins and the marginsplot commands. The margins command is used to obtain the fitted value for ages 30 and 55, for educations of 12 to 20 years (in two-year increments), and for the years of birth 1930, 1940, 1950, and 1960. The marginsplot command is then used to graph these values, placing age on the $x$axis, using separate panels for yrborn, and using separate lines for educ margins, at(age=(30 55) educ=(12(2)20) yrborn=(1930(10)1960)) marginsplot,xdimension(age) bydimension(yrborn) /// plotdimension(educ,allsimple) legend(row(2)subtitle(Education)) /// recast(line) scheme(s1mono) noci ylabel(,angle(0)) Adjusted means from model without the three-way interaction, showing age on the $x$ axis, separate panels for year of birth, and separate lines for education\rAlthough the people born in different years might show a different age slope at 12 years of education, the increase in the age slope as a function of education is the same across the years of birth. In fact, the coefficient for c.age#c.educ describes the exact degree of this increase. For every unit increase in educ, the age slope increases by 109.44. This is true for all years of birth. Let’s see this for ourselves using the margins command. The margins command below estimates the age slope for those with 14 and 15 years of education who were born in 1930. result compute: -88.59 + 109.44 = 20.85 margins,dydx(age) at(educ=(14 15) yrborn=1930) vsquish Average marginal effects Number of obs = 11,765\rModel VCE: Robust\rExpression: Linear prediction, predict()\rdy/dx wrt: age\r1._at: educ = 14\ryrborn = 1930\r2._at: educ = 15\ryrborn = 1930\r------------------------------------------------------------------------------\r| Delta-method\r| dy/dx std. err. t P\u003e|t| [95% conf. interval]\rage |\r_at |\r1 | -88.59 81.21 -1.09 0.275 -247.78 70.59\r2 | 20.85 86.72 0.24 0.810 -149.14 190.84\r------------------------------------------------------------------------------\rLet’s make the same comparison, except for those born in 1940. margins,dydx(age)at(educ=(14 15) yrborn=1940) vsquish result compute: 266.1371 - 156.6922 = 109.44 Average marginal effects Number of obs = 11,765\rModel VCE: Robust\rExpression: Linear prediction, predict()\rdy/dx wrt: age\r1._at: educ = 14\ryrborn = 1940\r2._at: educ = 15\ryrborn = 1940\r------------------------------------------------------------------------------\r| Delta-method\r| dy/dx std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\rage |\r_at |\r1 | 156.69 46.04 3.40 0.001 66.45 246.93\r2 | 266.14 53.97 4.93 0.000 160.35 371.92\r------------------------------------------------------------------------------\r","date":"2024-01-04","objectID":"/6.continuous-by-continuous-by-continuous-interactions/:2:1","tags":["Continuous predictors","Interaction","stata"],"title":"Chapter6 ：Continuous by continuous by continuous interactions","uri":"/6.continuous-by-continuous-by-continuous-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.1.2 A three-way interaction model Let’s now consider a model that includes a three-way interaction of age, educ, and yrborn when predicting realrinc. Such models can have a high degree of multicollinearity that can lead to numerical instability of the estimates and inflated standard errors. To avoid these problems, Aiken and West (1991) recommend centering the predictors prior to the analysis. Let’s adopt this advice and center age, educ, and yrborn before attempting an analysis that includes the interaction of these three variables. The following generate commands create centered versions of these variables, centering them around values that I chose to ease the interpretation of the centered values. generate yrborn30 = yrborn - 1930 generate age40 = age - 40 generate educ16 = educ - 16 The variable yrborn is centered by subtracting 1930, creating a new variable called yrborn30. This centered variable will range from 0 to 30, corresponding to the years of birth ranging from 1930 to 1960. The centered version of age is called age40 and contains the value of age minus 40. An age of 30 would be represented as using the centered version of age (age40). Finally, the centered version of educ is called educ16, which contains educ minus 16. We could refer to 20 years of education via the centered variable by specifying that educ16 equals 4. The rest of this section will use these centered variables for the analysis. Let’s now form a model that predicts realrinc from age40, educ16, and yrborn30, the two-way interactions among these variables, and the three-way interaction of these variables. reg realrinc c.age40##c.educ16##c.yrborn30 i.race,vce(robust) noci Linear regression Number of obs = 11,765\rF(9, 11755) = 126.24\rProb \u003e F = 0.0000\rR-squared = 0.1145\rRoot MSE = 24591\r--------------------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t|\r----------------------------+---------------------------------------\rage40 | -27.89 135.31 -0.21 0.837\reduc16 | 2800.94 366.46 7.64 0.000\r|\rc.age40#c.educ16 | 26.63 42.54 0.63 0.531\r|\ryrborn30 | -88.92 55.93 -1.59 0.112\r|\rc.age40#c.yrborn30 | 32.56 6.69 4.87 0.000\r|\rc.educ16#c.yrborn30 | 12.89 17.90 0.72 0.471\r|\rc.age40#c.educ16#c.yrborn30 | 4.32 2.19 1.97 0.049\r|\rrace |\rblack | -5279.07 463.47 -11.39 0.000\rother | -4648.00 1142.81 -4.07 0.000\r|\r_cons | 33441.15 1161.80 28.78 0.000\r--------------------------------------------------------------------\rWe can visualize the c.age40#c.educ16#c.yrborn30 interaction in a variety of ways. We can focus on the age40 slope, the educ16 slope, or the yrborn30 slope. As we have done throughout this chapter, let’s focus our attention on the age slope (via the centered variable age40). Note! Retaining main effects and two-way interactions Looking at the estimates from the regress command, you might notice that some of the main effects and two-way interaction terms are not significant. Even though they are not significant, it is important to retain these effects to preserve the interpretation of the three-way interaction term. 1.1.2.1 Visualizing the three-way interaction The margins command is used to compute the adjusted means at different values of the centered variable using the at() option. In terms of the uncentered variables, the at() option specifies ages 30 and 55, educations from 12 to 20 in 2-unit increments, and years of birth from 1930 to 1960 in 10-unit increments. Then, the marginsplot command is used to graph the fitted values, placing age40 on the $x$ axis, with separate lines for educ16 and with separate panels for yrborn30. margins, at(age40=(-10 15) educ16=(-4(2)4) yrborn30=(0(10)30)) marginsplot,xdimension(age40) bydimension(yrborn30) /// plotdimension(educ16,allsimple) legend(row(2) subtitle(Education)) /// recast(line) scheme(s1mono) noci ylabel(, angle(0)) Adjusted means from model with the three-way interaction as a function of age ( $x$ axis), year of birth (separate panels), and education (","date":"2024-01-04","objectID":"/6.continuous-by-continuous-by-continuous-interactions/:2:2","tags":["Continuous predictors","Interaction","stata"],"title":"Chapter6 ：Continuous by continuous by continuous interactions","uri":"/6.continuous-by-continuous-by-continuous-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter illustrates how to interpret interactions of two continuous predictors. ","date":"2024-01-01","objectID":"/5.continuous-by-continuous-interactions/","tags":["Continuous predictors","Interaction","stata"],"title":"Chapter5 ：Continuous by continuous interactions","uri":"/5.continuous-by-continuous-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter illustrates how to interpret interactions of two continuous predictors. ","date":"2024-01-01","objectID":"/5.continuous-by-continuous-interactions/:0:0","tags":["Continuous predictors","Interaction","stata"],"title":"Chapter5 ：Continuous by continuous interactions","uri":"/5.continuous-by-continuous-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 Linear by linear interactions A linear by linear interaction implies that the slope of the relationship between one of the predictors and the outcome changes as a linear function of the other predictor. Let’s begin with a hypothetical example in which we predict income (realrinc) from age (age) and education (educ) without an interaction. $$ \\widehat{realrin}=-41300 + 600age + 3000educ $$ Three-dimensional graph of fitted values from model without aninteraction\rLet’s now consider a second hypothetical regression model that contains an interaction of age (age) and education (educ). $$ \\widehat{realrin}=2400 + -1100age - 1600educ + 120age*educ $$ Three-dimensional graph of fitted values from model with an interaction\rLet’s transition to the use of two-dimensional graphs for illustrating the precise role of linear by linear interactions. Two-dimensional graph of fitted values from model without an interaction (left panel) and with an interaction (right panel)\rThe left panel shows a two-dimensional representation of the model without an interaction, and the right panel shows a two-dimensional representation of the model with an interaction. ","date":"2024-01-01","objectID":"/5.continuous-by-continuous-interactions/:1:0","tags":["Continuous predictors","Interaction","stata"],"title":"Chapter5 ：Continuous by continuous interactions","uri":"/5.continuous-by-continuous-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.1 Example using GSS data use gss_ivrm.dta keep if (age\u003e=22 \u0026 age \u003c=55) \u0026 (educ\u003e=12) Let’s fit a model where we predict realrinc from educ, age, and the interaction of these two variables. reg realrinc c.educ##c.age female, vce(robust) Linear regression Number of obs = 22,367\rF(4, 22362) = 664.04\rProb \u003e F = 0.0000\rR-squared = 0.1712\rRoot MSE = 24677\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\reduc | -1487.58 339.60 -4.38 0.000 -2153.21 -821.94\rage | -1036.79 126.23 -8.21 0.000 -1284.21 -789.38\r|\rc.educ#c.age | 114.91 9.38 12.25 0.000 96.52 133.30\r|\rfemale | -12553.79 328.28 -38.24 0.000 -13197.25 -11910.33\r_cons | 28738.43 4550.48 6.32 0.000 19819.18 37657.69\r","date":"2024-01-01","objectID":"/5.continuous-by-continuous-interactions/:1:1","tags":["Continuous predictors","Interaction","stata"],"title":"Chapter5 ：Continuous by continuous interactions","uri":"/5.continuous-by-continuous-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.2 Interpreting the interaction in terms of age We can use the margins and marginsplot commands to visualize this interaction. Let’s visualize this interaction by graphing the adjusted means with age on the X axis and with separate lines for each level of educ. The margins command is used with the at() option to compute the adjusted means for ages 22 and 55 and educations ranging from 12 to 20 in two-year increments. margins, at(age=(22 55) educ=(12(2)20)) Then, the marginsplot command is used to graph the adjusted means with age on the axis and with separate lines for each level of educ.The legend() option is included to customize the display of the graph legend. marginsplot,noci legend(rows(3) title(Education)) Adjusted means for a linear by linear interaction with age on the X axis\rwe can use the margins command combined with the dydx(age) option to estimate the slope for each of the lines displayed in figure margins, at(educ=(12(2)20)) dydx(age) vsquish Average marginal effects Number of obs = 22,367\rModel VCE: Robust\rExpression: Linear prediction, predict()\rdy/dx wrt: age\r1._at: educ = 12\r2._at: educ = 14\r3._at: educ = 16\r4._at: educ = 18\r5._at: educ = 20\r| Delta-method\r| dy/dx std. err. t P\u003e|t| [95% conf. interval]\rage |\r_at |\r1 | 342.16 19.78 17.30 0.000 303.39 380.93\r2 | 571.99 16.30 35.09 0.000 540.03 603.94\r3 | 801.81 29.06 27.59 0.000 744.85 858.77\r4 | 1031.64 46.12 22.37 0.000 941.23 1122.04\r5 | 1261.46 64.14 19.67 0.000 1135.73 1387.19\rThe estimates of the age slope increase as a function of educ. For example, at 12 years of education, the age slope is 342.16, and at 14 years of education, the age slope is 571.99. For a two-unit increase in education, the age slope increases by 229.83 ($571.99-342.16$). We can relate this to the coefficient for the c.educ#c.age interaction, which is the amount by which the age slope changes for every one-year increase in educ. For every one-year increase in educ, the age slope increases by 114.91($229.83÷2=114.91$). ","date":"2024-01-01","objectID":"/5.continuous-by-continuous-interactions/:1:2","tags":["Continuous predictors","Interaction","stata"],"title":"Chapter5 ：Continuous by continuous interactions","uri":"/5.continuous-by-continuous-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.3 Interpreting the interaction in terms of education Now, let’s explore the meaning of the interaction by focusing on the educ slope. Let’s visualize this by creating a graph of the adjusted means showing educ on the $x$ axis and separate lines for age. First, the margins command is used to create adjusted means for 12 and 20 years of education and ages ranging from 25 to 55 in 10-year increments. margins, at(educ=(12(2)20)) dydx(age) vsquish Predictive margins Number of obs = 22,367\rModel VCE: Robust\rExpression: Linear prediction, predict()\r1._at: educ = 12\rage = 25\r2._at: educ = 12\rage = 35\r3._at: educ = 12\rage = 45\r4._at: educ = 12\rage = 55\r5._at: educ = 20\rage = 25\r6._at: educ = 20\rage = 35\r7._at: educ = 20\rage = 45\r8._at: educ = 20\rage = 55\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r_at |\r1 | 13091.92 236.14 55.44 0.000 12629.07 13554.76\r2 | 16513.52 172.78 95.57 0.000 16174.85 16852.19\r3 | 19935.12 286.72 69.53 0.000 19373.12 20497.11\r4 | 23356.72 461.33 50.63 0.000 22452.47 24260.96\r5 | 24173.82 876.85 27.57 0.000 22455.13 25892.50\r6 | 36788.43 619.20 59.41 0.000 35574.75 38002.11\r7 | 49403.04 906.00 54.53 0.000 47627.21 51178.86\r8 | 62017.65 1442.62 42.99 0.000 59190.01 64845.28\rmarginsplot,noci legend(title(Age)) Adjusted means for a linear by linear interaction with education on the axis\rwe can estimate the slope of each line (that is, the educ slope) using the margins command, as shown below. margins,at(age=(25(10)55)) dydx(educ)vsquish Average marginal effects Number of obs = 22,367\rModel VCE: Robust\rExpression: Linear prediction, predict()\rdy/dx wrt: educ\r1._at: age = 25\r2._at: age = 35\r3._at: age = 45\r4._at: age = 55\r| Delta-method\r| dy/dx std. err. t P\u003e|t| [95% conf. interval]\reduc |\r_at |\r1 | 1385.24 129.78 10.67 0.000 1130.87 1639.61\r2 | 2534.36 90.87 27.89 0.000 2356.24 2712.48\r3 | 3683.49 131.45 28.02 0.000 3425.83 3941.15\r4 | 4832.62 209.54 23.06 0.000 4421.90 5243.33\rAs age increases, so does the educ slope. For every one-unitincrease in age, the educ slope increases by 114.91, the estimate of the age#educ interaction. For a 10-unit increase in age, the educ slope would be expected to increase by 1,149.12. ","date":"2024-01-01","objectID":"/5.continuous-by-continuous-interactions/:1:3","tags":["Continuous predictors","Interaction","stata"],"title":"Chapter5 ：Continuous by continuous interactions","uri":"/5.continuous-by-continuous-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.4 Interpreting the interaction in terms of age slope We can visualize the age by educ interaction by illustrating the way that the age slope changes as a function of education. The margins command below includes the dydx(age) option to estimate the age slope at each level of educ. margins,dydx(age) at(educ=(12(1)20))vsquish Average marginal effects Number of obs = 22,367\rModel VCE: Robust\rExpression: Linear prediction, predict()\rdy/dx wrt: age\r1._at: educ = 12\r2._at: educ = 13\r3._at: educ = 14\r4._at: educ = 15\r5._at: educ = 16\r6._at: educ = 17\r7._at: educ = 18\r8._at: educ = 19\r9._at: educ = 20\r| Delta-method\r| dy/dx std. err. t P\u003e|t| [95% conf. interval]\rage |\r_at |\r1 | 342.16 19.78 17.30 0.000 303.39 380.93\r2 | 457.07 15.51 29.47 0.000 426.68 487.47\r3 | 571.99 16.30 35.09 0.000 540.03 603.94\r4 | 686.90 21.61 31.78 0.000 644.54 729.26\r5 | 801.81 29.06 27.59 0.000 744.85 858.77\r6 | 916.72 37.39 24.52 0.000 843.44 990.01\r7 | 1031.64 46.12 22.37 0.000 941.23 1122.04\r8 | 1146.55 55.07 20.82 0.000 1038.61 1254.49\r9 | 1261.46 64.14 19.67 0.000 1135.73 1387.19\rThis shows that the age slope increases as a function of education. In fact, the age slope increases by 114.91 units for every one-unit increase in educ. We can visualize these age slopes as a function of education using the marginsplot command (shown below). marginsplot ","date":"2024-01-01","objectID":"/5.continuous-by-continuous-interactions/:1:4","tags":["Continuous predictors","Interaction","stata"],"title":"Chapter5 ：Continuous by continuous interactions","uri":"/5.continuous-by-continuous-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.5 Interpreting the interaction in terms of the educ slope We can visualize the age by educ interaction by focusing on the way that the educ slope changes as a function of age. The margins command below estimates the educ slope for ages ranging from 25 to 55 in five-year increments. margins,dydx(educ)at(age=(25(5)55))vsquish Average marginal effects Number of obs = 22,367\rModel VCE: Robust\rExpression: Linear prediction, predict()\rdy/dx wrt: educ\r1._at: age = 25\r2._at: age = 30\r3._at: age = 35\r4._at: age = 40\r5._at: age = 45\r6._at: age = 50\r7._at: age = 55\r| Delta-method\r| dy/dx std. err. t P\u003e|t| [95% conf. interval]\reduc |\r_at |\r1 | 1385.24 129.78 10.67 0.000 1130.87 1639.61\r2 | 1959.80 101.73 19.26 0.000 1760.40 2159.20\r3 | 2534.36 90.87 27.89 0.000 2356.24 2712.48\r4 | 3108.93 102.80 30.24 0.000 2907.43 3310.43\r5 | 3683.49 131.45 28.02 0.000 3425.83 3941.15\r6 | 4258.05 168.50 25.27 0.000 3927.78 4588.33\r7 | 4832.62 209.54 23.06 0.000 4421.90 5243.33\rThis shows that the educ slope increases as a function of age. For each five-year increase in age, the educ slope increases by 574.56 (that is, the age#educ coefficient multiplied by five, $114.91 × 5$). marginsplot,noci ","date":"2024-01-01","objectID":"/5.continuous-by-continuous-interactions/:1:5","tags":["Continuous predictors","Interaction","stata"],"title":"Chapter5 ：Continuous by continuous interactions","uri":"/5.continuous-by-continuous-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2 Linear by quadratic interactions This section considers models with an interaction of two continuous variables, where one of the variables is fit linearly and the other is fit using a quadratic term. A quadratic term introduces curvature in the fitted relationship between the predictor and outcome. Let’s first illustrate a model that contains a quadratic predictor and a linear predictor but no interaction between these predictors. This hypothetical example uses realrinc as the outcome variable, age as the quadratic predictor, and educ as the linear predictor. $$ \\widehat{realrin}=-7000 + 2100age + - age^2 + 3000educ $$ Three-dimensional graph of fitted values for linear and quadratic models without an interaction\rLet’s now consider a model that includes an interaction between age (as a quadratic term) and educ. This regression equation is shown below $$ \\widehat{realrin}=165000 + -8600age + 95age^2 +-14000educ + 780age*educ + - 8age^2 * educ $$ The key term is the age squared by education interaction. This governs the degree of the curvature in age as a function of education. Three-dimensional graph of fitted values for linear and quadratic models with an interaction\rAs education increases from 12 to 20years, the degree of the inverted U-shape grows as a function of education. This is due to the interaction of education with age squared. When quadratic terms become more negative, the inverted U-shape becomes more pronounced. Thus for every one-unit increase in education, the relationship between income and age shows more of an inverted U-shape. We can clearly see how the degree of the curvature in relationship between income and age is the same across the levels of education. The right panel of figure shows a two-dimensional representation of figure above, where there was an interaction of education with the quadratic term for age. Two-dimensional graph of fitted values for linear and quadratic models without an interaction (left panel) and with linear by quadratic interaction (right panel)\r","date":"2024-01-01","objectID":"/5.continuous-by-continuous-interactions/:2:0","tags":["Continuous predictors","Interaction","stata"],"title":"Chapter5 ：Continuous by continuous interactions","uri":"/5.continuous-by-continuous-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.1 Example using GSS data Let’s use the GSS dataset to fit a model that predicts realrinc from educ (treated as a continuous linear variable) interacted with age (treated as a continuous variable fit using a quadratic term). use gss_ivrm.dta keep if (age\u003e=22 \u0026 age\u003c=80) \u0026 (educ\u003e=12) reg realrinc c.educ c.age##c.age female,vce(robust) Let’s begin by fitting a model that predicts realrinc using educ as a linear term and age as a quadratic term but no interaction between education and age. Linear regression Number of obs = 25,964\rF(4, 25959) = 785.49\rProb \u003e F = 0.0000\rR-squared = 0.1658\rRoot MSE = 26078\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\reduc | 3071.27 93.46 32.86 0.000 2888.08 3254.45\rage | 2245.62 79.05 28.41 0.000 2090.67 2400.57\r|\rc.age#c.age | -21.77 0.94 -23.09 0.000 -23.62 -19.92\r|\rfemale | -13344.78 319.66 -41.75 0.000 -13971.33 -12718.23\r_cons | -64839.08 2060.09 -31.47 0.000 -68876.96 -60801.20\rThe quadratic term is significant and is negative. This negative coefficient indicates that the relationship between age and income has an inverted U-shape. Let’s use the margins and marginsplot commands to visualize the adjusted means of income as a function of age while holding education constant at 12 to 20 years of education (in two-year increments). We first compute the adjusted means as a function of age and education using the margins command. Then, the marginsplot command graphs the adjusted means on the $y$ axis and age on the $x$ axis, with separate lines for each level of education. margins,at(age=(22(1)80) educ=(12(2)20)) marginsplot,noci legend(rows(2))recast(line) scheme(s1mono) Adjusted means at 12, 14, 16, 18, and 20 years of education\rLet’s now fit a model that includes an interaction of educ with the quadratic term for age. reg realrinc c.educ##c.age##c.age female,vce(robust) noci Linear regression Number of obs = 25,964\rF(6, 25957) = 558.27\rProb \u003e F = 0.0000\rR-squared = 0.1757\rRoot MSE = 25924\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t|\reduc | -9317.78 817.82 -11.39 0.000\rage | -5009.26 551.97 -9.08 0.000\r|\rc.educ#c.age | 517.95 41.56 12.46 0.000\r|\rc.age#c.age | 46.88 6.39 7.34 0.000\r|\rc.educ#c.age#c.age | -4.91 0.48 -10.18 0.000\r|\rfemale | -13160.33 317.92 -41.40 0.000\r_cons | 108559.85 10897.04 9.96 0.000\rLet’s interpret this as a function of age by graphing the adjusted means on the axis and age on the axis, with separate lines to indicate the levels of education. margins,at(age=(22 25(5)80) educ=(12(2)20)) marginsplot,plotdimension(,allsimple)legend(subtitle(Education)rows(2))noci recast(line) scheme(s1mono) Adjusted means from linear by quadratic model\rthe c.educ#c.age#c.age coefficient describes the degree to which the inverted U-shape changes as a function of education. This coefficient, which is $-4.9$, represents the change in the quadratic term for age for a one-unit increase in educ. As education increases by one unit, the quadratic term for age decreases by 4.9, creating a more inverted U-shape. For those with lower educations, the relationship between age and income tends to be more linear, and for those with higher levels of education, the relationship between age and income is more curved, showing a greater rise and fall across ages. ","date":"2024-01-01","objectID":"/5.continuous-by-continuous-interactions/:2:1","tags":["Continuous predictors","Interaction","stata"],"title":"Chapter5 ：Continuous by continuous interactions","uri":"/5.continuous-by-continuous-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter illustrates the use of piecewise regression. This involves fitting separate line segments, demarcated by knots, that account for the nonlinearity between the predictor and outcome.  ","date":"2023-12-29","objectID":"/3.continuous-predictors_piecewise-models/","tags":["Continuous predictors","stata"],"title":"Chapter4 ：Continuous predictors: Piecewise models","uri":"/3.continuous-predictors_piecewise-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter illustrates the use of piecewise regression. This involves fitting separate line segments, demarcated by knots, that account for the nonlinearity between the predictor and outcome. ","date":"2023-12-29","objectID":"/3.continuous-predictors_piecewise-models/:0:0","tags":["Continuous predictors","stata"],"title":"Chapter4 ：Continuous predictors: Piecewise models","uri":"/3.continuous-predictors_piecewise-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 Introduction to piecewise regression models A piecewise regression goes by several names, including spline regression,broken line regression, broken stick regression, and even hockey stick models. Consider the example, predicting annual income from education, shown in the figure below A knot can signify a change of slope and a change of intercept, yielding an increase (or decrease) in the outcome upon attaining a particular milestone. Note! Instantaneous jumps? As a thought experiment, imagine someone being one day short of graduating high school and the wages they would obtain as they seek a job. Compare this person with an identical job seeker who has one more day of education (that is, they graduated high school). These two people are identical except that one crossed the threshold of getting a diploma. It is indeed plausible that the second person would be offered an annual income $2,200 more than the first person$. Suppose you have a predictor that shows a nonlinear relationship with the outcome, and you believe that a piecewise model with one knot signifying a change in slope would fit your data well. However, unlike the previous examples, you do not have a theoretical or practical basis for selecting the placement of the knot. You could haphazardly try a variety of placements for the knot, trying to find a placement that results in the best fitting model. Alternatively, you can let Stata do the work for you by using a least-squares procedure for selecting a location for the knot that produces the lowest residual sum of squares. ","date":"2023-12-29","objectID":"/3.continuous-predictors_piecewise-models/:1:0","tags":["Continuous predictors","stata"],"title":"Chapter4 ：Continuous predictors: Piecewise models","uri":"/3.continuous-predictors_piecewise-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2 Piecewise with one known knot This section illustrates piecewise regression with one knot. ","date":"2023-12-29","objectID":"/3.continuous-predictors_piecewise-models/:2:0","tags":["Continuous predictors","stata"],"title":"Chapter4 ：Continuous predictors: Piecewise models","uri":"/3.continuous-predictors_piecewise-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.1 Examples using the GSS Let’s use the GSS dataset to illustrate a piecewise model with one knot, focusing on the relationship between income and education. use gss_ivrm.dta reg realrinc i.educ,vce(robust) margins educ marginsplot,noci It looks like the relationship between education and income could be fit well with a piecewise model with a knot at 12 (corresponding to graduating high school). The following subsections illustrate two different ways to fit this kind of piecewise model: using individual slope coding and using change in slope coding. 2.1.1 Individual slope coding The individual slope coding scheme estimates the slope of each line segment of the piecewise model. The first step is to create two new variables that are coded to represent the educ slope before and after graduating high school. The mkspline command creates the variables ed1 and ed2 based on the original variable educ. The value of 12 is inserted between ed1 and ed2, indicating that this is the knot. mkspline ed1 12 ed2 = educ showcoding educ ed1 ed2 The next step is to use the regress command to predict realrinc from ed1 and ed2. This will yield a piecewise model with 12 years of education as the knot. reg realrinc ed1 ed2 female,vce(robust) Linear regression Number of obs = 32,183\rF(3, 32179) = 1030.24\rProb \u003e F = 0.0000\rR-squared = 0.1420\rRoot MSE = 25045\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\red1 | 832.27 72.38 11.50 0.000 690.40 974.14\red2 | 3441.33 93.42 36.84 0.000 3258.21 3624.44\rfemale | -12372.38 276.35 -44.77 0.000 -12914.05 -11830.72\r_cons | 12052.18 793.42 15.19 0.000 10497.04 13607.31\rAmong non–high school graduates(ed1), each additional year of education is predicted to increase income by $832.27$.The coefficient for ed2 is the slope for those with 12 or more years of education (high school graduates). Income increases by $3,441.33$ for each additional year of education beyond 12 years of education(ed2). Each of these slopes is significantly different from 0. Say that we want to compute the adjusted mean given that a person has eight years of education. We can compute this adjusted mean using the margins command; To graph the adjusted means as a function of education, we need to compute the adjusted means when education is 0, 12, and 20. The margins command below computes these three adjusted means by using the at() option three times. margins,at(ed1=0 ed2=0) at(ed1=12 ed2=0) at(ed1=12 ed2=8) vsquish Predictive margins Number of obs = 32,183\rModel VCE: Robust\rExpression: Linear prediction, predict()\r1._at: ed1 = 0\red2 = 0\r2._at: ed1 = 12\red2 = 0\r3._at: ed1 = 12\red2 = 8\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r_at |\r1 | 5964.98 794.24 7.51 0.000 4408.23 7521.72\r2 | 15952.22 161.46 98.80 0.000 15635.75 16268.69\r3 | 43482.83 657.66 66.12 0.000 42193.79 44771.86\rpreserve clear input educ yhat 0 5964.977 12 15952.22 20 43482.83 end graph twoway line yhat educ,xlabel(0(4)20) xline(12) restore 2.1.2 Change in slope coding This coding scheme estimates the slope for the line segment before the first knot (for example, for non–high school graduates) and then estimates the difference in the slopes of adjacent line segments (for example, for high school graduates versus non–high school graduates). This strategy emphasizes the change in slope that occurs at each knot. The key difference is that we add the marginal option to the mkspline command, as shown below. I name the variables ed1m and ed2m, adding the m to emphasize that these variables were created using the marginal option. mkspline ed1m 12 ed2m = educ,marginal The next step is to enter ed1m and ed2m as predictors of income, as shown below. reg realrinc ed1m ed2m female,vce(robust) Linear regression Number of obs = 32,183\rF(3, 32179) = 1030.24\rProb \u003e F = 0.0000\rR-squared = 0.1420\rRoot MSE = 25045\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\red1m | 832.27 72.38 11.50 0.000 690.40 974.14\red2m | 2609.06 134.40 19.41","date":"2023-12-29","objectID":"/3.continuous-predictors_piecewise-models/:2:1","tags":["Continuous predictors","stata"],"title":"Chapter4 ：Continuous predictors: Piecewise models","uri":"/3.continuous-predictors_piecewise-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3 Piecewise with two known knots ","date":"2023-12-29","objectID":"/3.continuous-predictors_piecewise-models/:3:0","tags":["Continuous predictors","stata"],"title":"Chapter4 ：Continuous predictors: Piecewise models","uri":"/3.continuous-predictors_piecewise-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.1 Examples using the GSS use gss_ivrm.dta mkspline ed1m 12 ed2m 16 ed3m = educ,marginal showcoding educ ed1m ed2m ed3m The coefficient for ed2m will represent the change in slope for high school graduates versus non–high school graduates. The coefficient for ed3m will represent the change in slope comparing college graduates with high school graduates. reg realrinc ed1m ed2m ed3m female,vce(robust) Linear regression Number of obs = 32,183\rF(4, 32178) = 784.42\rProb \u003e F = 0.0000\rR-squared = 0.1432\rRoot MSE = 25029\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\red1m | 939.67 70.55 13.32 0.000 801.40 1077.95\red2m | 2022.42 144.24 14.02 0.000 1739.71 2305.13\red3m | 1657.45 412.15 4.02 0.000 849.62 2465.28\rfemale | -12336.74 277.09 -44.52 0.000 -12879.84 -11793.64\r_cons | 11215.32 787.53 14.24 0.000 9671.73 12758.92\rWe can use the margins command to compute adjusted means for any given value of education by expressing education in terms of the variables ed1m, ed2m, and ed3m. To graph the entire range of education values, we need to compute the adjusted means for the minimum of education (0), for each of the knots (12 and 16), and for the maximum of education (20). The margins command below computes these adjusted means using the at() option once for each of these four levels of education. margins,at(ed1m=0 ed2m=0 ed3m=0) /// at(ed1m=12 ed2m=0 ed3m=0) /// at(ed1m=16 ed2m=4 ed3m=0) /// at(ed1m=20 ed2m=8 ed3m=4) vsquish Predictive margins Number of obs = 32,183\rModel VCE: Robust\rExpression: Linear prediction, predict()\r1._at: ed1m = 0\red2m = 0\red3m = 0\r2._at: ed1m = 12\red2m = 0\red3m = 0\r3._at: ed1m = 16\red2m = 4\red3m = 0\r4._at: ed1m = 20\red2m = 8\red3m = 4\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r_at |\r1 | 5145.66 785.16 6.55 0.000 3606.72 6684.60\r2 | 16421.73 145.60 112.79 0.000 16136.36 16707.11\r3 | 28270.11 383.13 73.79 0.000 27519.16 29021.06\r4 | 46748.30 1242.79 37.62 0.000 44312.38 49184.22\rWe can then manually input the adjusted means from the margins command into a dataset as shown below. The graph command is then used to graph the relationship between education and income. preserve clear input educ yhat 0 5154.66 12 16421.73 16 28270.11 20 46748.3 end graph twoway line yhat educ,xlabel(0(4)20) xline(12 16) xtitle(Education) ytitle(Adjusted mean) restore ","date":"2023-12-29","objectID":"/3.continuous-predictors_piecewise-models/:3:1","tags":["Continuous predictors","stata"],"title":"Chapter4 ：Continuous predictors: Piecewise models","uri":"/3.continuous-predictors_piecewise-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4 Piecewise with one knot and one jump ","date":"2023-12-29","objectID":"/3.continuous-predictors_piecewise-models/:4:0","tags":["Continuous predictors","stata"],"title":"Chapter4 ：Continuous predictors: Piecewise models","uri":"/3.continuous-predictors_piecewise-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4.1 Examples using the GSS use gss_ivrm.dta mkspline ed1m 12 ed2m = educ,marginal reg realrinc ed1m ed2m hsgrad female,vce(robust) The variable hsgrad is coded 1 if someone has 12 or more years of education, and 0 otherwise. Linear regression Number of obs = 32,183\rF(4, 32178) = 803.67\rProb \u003e F = 0.0000\rR-squared = 0.1425\rRoot MSE = 25039\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\red1m | 273.10 105.88 2.58 0.010 65.56 480.64\red2m | 3102.95 141.83 21.88 0.000 2824.97 3380.94\rhsgrad | 2721.54 412.82 6.59 0.000 1912.40 3530.67\rfemale | -12391.31 276.30 -44.85 0.000 -12932.87 -11849.74\r_cons | 16345.24 988.38 16.54 0.000 14407.98 18282.49\rAt 12 years of education, the adjusted mean is computed twice, once assuming the absence of a high school degree and once assuming a high school degree, illustrating the jump in income due to graduating high school. How tow interpret the regression coefficient? For each additional year of education (up to 12), income increases by $273.10.$figure shows the adjusted means given zero and one year of education. This difference in these adjusted means equals 273.10 $(10521.83-10248.73)$ The coefficient of ed2m is 3,102.95, which is the change (increase) in slope for high school graduates compared with non–high school graduates. Finally, the hsgrad coefficient (2,721.54) represents the predicted jump (increase in income) due to graduating high school. showcoding educ hsgrad ed1m ed2m margins,at(ed1m=0 ed2m=0 hsgrad=0) /// at(ed1m=12 ed2m=0 hsgrad=0) /// at(ed1m=12 ed2m=0 hsgrad=1) /// at(ed1m=20 ed2m=8 hsgrad=1) vsquish Predictive margins Number of obs = 32,183\rModel VCE: Robust\rExpression: Linear prediction, predict()\r1._at: ed1m = 0\red2m = 0\rhsgrad = 0\r2._at: ed1m = 12\red2m = 0\rhsgrad = 0\r3._at: ed1m = 12\red2m = 0\rhsgrad = 1\r4._at: ed1m = 20\red2m = 8\rhsgrad = 1\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r1 | 10248.73 987.42 10.38 0.000 8313.35 12184.10\r2 | 13525.93 374.97 36.07 0.000 12790.97 14260.89\r3 | 16247.46 174.79 92.95 0.000 15904.86 16590.07\r4 | 43255.90 664.00 65.14 0.000 41954.43 44557.37\rpreserve clear input educ yhat 0 10248.73 12 13525.93 12 16247.46 20 43255.9 end graph twoway line yhat educ,xlabel(0(4)20) xline(12) restore Note! Individual slope coding This model was fit using the change in slope coding method (that is, with the marginal option on the mkspline command). If you wished, you could fit this model using individual slope coding by omitting the marginal option on the mkspline command. ","date":"2023-12-29","objectID":"/3.continuous-predictors_piecewise-models/:4:1","tags":["Continuous predictors","stata"],"title":"Chapter4 ：Continuous predictors: Piecewise models","uri":"/3.continuous-predictors_piecewise-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"5 Piecewise with two knots and two jumps This section will illustrate a model with two knots signifying a change of slope and intercept. ","date":"2023-12-29","objectID":"/3.continuous-predictors_piecewise-models/:5:0","tags":["Continuous predictors","stata"],"title":"Chapter4 ：Continuous predictors: Piecewise models","uri":"/3.continuous-predictors_piecewise-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"5.1 Examples using the GSS use gss_ivrm.dta mkspline ed1m 12 ed2m 16 ed3m = educ,marginal reg realrinc ed1m ed2m ed3m hsgrad cograd female,vce(robust) Linear regression Number of obs = 32,183\rF(6, 32176) = 544.02\rProb \u003e F = 0.0000\rR-squared = 0.1458\rRoot MSE = 24991\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\red1m | 272.27 105.87 2.57 0.010 64.76 479.79\red2m | 1329.81 189.31 7.02 0.000 958.75 1700.88\red3m | 2540.17 398.71 6.37 0.000 1758.69 3321.65\rhsgrad | 3925.58 405.81 9.67 0.000 3130.18 4720.98\rcograd | 5740.76 733.17 7.83 0.000 4303.73 7177.80\rfemale | -12358.72 276.69 -44.67 0.000 -12901.04 -11816.40\r_cons | 16339.10 988.28 16.53 0.000 14402.04 18276.16\rshowcoding educ ed1m ed2m ed3m hsgrad cograd margins,at (ed1m=0 ed2m=0 ed3m=0 hsgrad=0 cograd=0) /// at(ed1m=12 ed2m=0 ed3m=0 hsgrad=0 cograd=0) /// at(ed1m=12 ed2m=0 ed3m=0 hsgrad=1 cograd=0) /// at(ed1m=16 ed2m=4 ed3m=0 hsgrad=1 cograd=0) /// at(ed1m=16 ed2m=4 ed3m=0 hsgrad=1 cograd=1) /// at(ed1m=20 ed2m=8 ed3m=4 hsgrad=1 cograd=1) vsquish noatlegend Predictive margins Number of obs = 32,183\rModel VCE: Robust\rExpression: Linear prediction, predict()\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r_at |\r1 | 10258.62 987.32 10.39 0.000 8323.44 12193.81\r2 | 13525.88 374.93 36.08 0.000 12791.01 14260.75\r3 | 17451.46 157.37 110.90 0.000 17143.02 17759.91\r4 | 23859.81 558.53 42.72 0.000 22765.07 24954.55\r5 | 29600.57 475.61 62.24 0.000 28668.35 30532.79\r6 | 46169.58 1255.73 36.77 0.000 43708.30 48630.87\rpreserve clear input educ yhat 0 10258.62 12 13525.88 12 17451.46 16 23858.81 16 29600.57 20 46169.58 end graph twoway line yhat educ,xlabel(0(4)20) xline(12 16) xtitle(Education) ytitle(Adjusted means) restore ","date":"2023-12-29","objectID":"/3.continuous-predictors_piecewise-models/:5:1","tags":["Continuous predictors","stata"],"title":"Chapter4 ：Continuous predictors: Piecewise models","uri":"/3.continuous-predictors_piecewise-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"6 Piecewise with an unknown knot This section illustrates a method for fitting a piecewise model with one knot where the location of the knot is uncertain. Consider the relationship between year of birth and level of education. In this dataset, the year of birth is recorded in the variable yrborn and education level is recorded in the variable educ. To visualize the relationship between year of birth and education, let’s make a graph showing the mean of educ at each level of yrborn. use gss_ivrm.dta keep if (yrborn\u003e=1905 \u0026 yrborn\u003c=1985) \u0026 !missing(educ) reg educ i.yrborn margins yrborn marginsplot,noci We could haphazardly select different years for the placement of the knot and select the knot that yields the best fitting model. Rather than manually doing this selection process, we can use the nl command to automate the process of selecting the optimal knot, by selecting a knot location that yields the lowest residual sum of squares. The code for fitting such a model is shown below. nl (educ = ({cons} + {b1}*yrborn)*(yrborn\u003c{knot}) + /// ({cons} + {b1}*{knot} + {b2}*(yrborn-{knot}))*(yrborn\u003e={knot})), /// initial(knot 1945 b1 .1 b2 -.0125 cons -181) Iteration 0: Residual SS = 464140\rIteration 1: Residual SS = 464092.3\rIteration 2: Residual SS = 464092.3\rSource | SS df MS\rNumber of obs = 52,873\rModel | 48574.186 3 16191.3952 R-squared = 0.0947\rResidual | 464092.28 52869 8.77815515 Adj R-squared = 0.0947\rRoot MSE = 2.962795\rTotal | 512666.47 52872 9.69636992 Res. dev. = 264897.3\reduc | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r/cons | -152.07 3.24 -46.88 0.000 -158.43 -145.71\r/b1 | 0.09 0.00 50.59 0.000 0.08 0.09\r/knot | 1946.97 0.51 3800.38 0.000 1945.96 1947.97\r/b2 | -0.01 0.00 -2.98 0.003 -0.01 -0.00\rNote: Parameter cons is used as a constant term during estimation. The coefficient labeled cons is the constant for the model (the predicted value of educ when yrborn is 0). This value is generally uninteresting. The coefficient labeled knot is the location of the knot, selected as 1946.97 (which we can round to 1947). The coefficient labeled b1 is the slope of the relationship between education and year of birth for those born before the knot (before 1947). The coefficient labeled b2 is the slope for those born in or after 1947. The key is that 1947 was selected as the optimal location for the knot. How to compute the coef respectively and interpret the graph ? In place of educ, you would insert your outcome variable, and in place of yrborn, you would place your predictor variable. Then for the initial() option, you would insert plausible values for knot, b1, b2, and cons. These correspond to the placement of the knot, the slope before the knot, the slope after the knot, and the constant, respectively. In this example, I used 1945 as the knot. To estimate b1, I looked at the graph and saw that the mean education rose about 4 units from 1905 to 1945. Taking a change of 4 units divided by 40 gave me an estimate of 0.1 for b1. Likewise, I estimated that education declined by about 0.5 units from 1945 to 1985 and divided $-0.5$ units by 40 to yield $-0.0125$ as an estimate of b2. Finally, to estimate cons I estimated the average education to be 9.5 units at 1905 and then used the estimated slope of 0.1 to estimate the education at year 0 would be $9.5-1905×0.1=-181.$ ","date":"2023-12-29","objectID":"/3.continuous-predictors_piecewise-models/:6:0","tags":["Continuous predictors","stata"],"title":"Chapter4 ：Continuous predictors: Piecewise models","uri":"/3.continuous-predictors_piecewise-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"7 Piecewise model with multiple unknown knots This section illustrates the use of piecewise regression models where there could be multiple knots in the relationship between the predictor and outcome, and you have no basis for the placement of the knots. Consider the relationship between age and income. I would suspect that income would initially rise rapidly with increasing age, hit a peak, and then decline with increasing age. In such a case, we can intentionally fit a model with too many knots and then progressively remove the superfluous knots. use gss_ivrm.dta keep if age\u003c=80 * Model 0 reg realrinc i.age,vce(robust) The test of these indicators is significant, and they have an $R^2$ value of 0.0581. This $R^2$ value is the highest amount of variance we could hope to explain using age as a predictor. This is because this set of indicators accounts for every tiny bump and drop in the relationship between age and income. Any simpler model (for example, linear, quadratic, or piecewise) will not account for all the bumps and drops and thus will have a lower $R^2$ . However, a good model will account for the major bumps and drops and have an $R^2$ that is not too much smaller than the $R^2$ from the indicator model. This is our goal in fitting the piecewise model with multiple knots. Let’s visualize the relationship between age and income. We can do this by using the predict command to create predicted values based on the indicator model, in this case named yhatind. predict yhatind graph twoway line yhatind age,sort xlabel(18 20(5)80) graph twoway line yhatind age,xlabel(25 30 35 45 55 65) xline(25 30 35 45 55 65)sort Let’s first fit a model specified by the knots at ages 25, 30, 35, 45, 55, and 65, which we will call model 1. we will use the mkspline command to create knots at each of the ages, and this creates the variables age18to24m to age65to80m. model1 : full model* mkspline age18to24m 25 age25to29m 30 /// age30to34m 35 age35to44m 45 /// age45to54m 55 age55to64m 65 /// age65to80m=age,marginal reg realrinc age18to24m-age65to80m,vce(robust) Linear regression Number of obs = 32,100\rF(7, 32092) = 712.84\rProb \u003e F = 0.0000\rR-squared = 0.0559\rRoot MSE = 26153\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\rage18to24m | 1559.14 66.34 23.50 0.000 1429.10 1689.18\rage25to29m | -582.00 131.65 -4.42 0.000 -840.04 -323.96\rage30to34m | -45.67 188.34 -0.24 0.808 -414.82 323.48\rage35to44m | -494.46 184.93 -2.67 0.008 -856.93 -131.99\rage45to54m | -310.18 154.30 -2.01 0.044 -612.61 -7.76\rage55to64m | -827.83 190.35 -4.35 0.000 -1200.93 -454.73\rage65to80m | -4.33 214.92 -0.02 0.984 -425.59 416.93\r_cons | -25134.60 1502.49 -16.73 0.000 -28079.54 -22189.66\rWhen a coefficient regarding the change in the slope in the line segments is not significant, it indicates a knot that can be removed because the slopes before and after the knot are not significantly different. We can use this as a guide for removing superfluous knots. we can see that the slope for those who are 55 to 64 years old is similar to the slope for those who are 65 years old and older. Thus, the knot at age 65 is not really needed and we can assume one slope from ages 55 to 80. *model 2 :drop knot at age 65 drop age18to24m-age65to80m mkspline age18to24m 25 age25to29m 30 /// age30to34m 35 age35to44m 45 /// age45to54m 55 age55to80m=age,marginal reg realrinc age18to24m-age55to80m,vce(robust) Linear regression Number of obs = 32,100\rF(6, 32093) = 830.57\rProb \u003e F = 0.0000\rR-squared = 0.0559\rRoot MSE = 26152\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\rage18to24m | 1559.15 66.34 23.50 0.000 1429.11 1689.18\rage25to29m | -582.03 131.64 -4.42 0.000 -840.05 -324.01\rage30to34m | -45.57 188.23 -0.24 0.809 -414.50 323.36\rage35to44m | -494.68 184.31 -2.68 0.007 -855.94 -133.42\rage45to54m | -309.49 146.24 -2.12 0.034 -596.12 -22.87\rage55to80m | -829.99 133.15 -6.23 0.000 -1090.96 -569.01\r_cons | -25134.71 1502.46 -16.73 0.000 -28079.59 -22189.83\rI","date":"2023-12-29","objectID":"/3.continuous-predictors_piecewise-models/:7:0","tags":["Continuous predictors","stata"],"title":"Chapter4 ：Continuous predictors: Piecewise models","uri":"/3.continuous-predictors_piecewise-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"8 Automating graphs of piecewise models If you change your data (for example, fix incorrect values), the adjusted means will not be automatically updated to reflect the new data.This section illustrates a more efficient, but trickier, way of creating such graphs that does not require retyping the data. use gss_ivrm.dta mkspline ed1m 12 ed2m 16 ed3m = educ,marginal reg realrinc ed1m ed2m ed3m hsgrad cograd female,vce(robust) margins,at(ed1m=0 ed2m=0 ed3m=0 hsgrad=0 cograd=0) /// at(ed1m=1 ed2m=0 ed3m=0 hsgrad=0 cograd=0) /// at(ed1m=12 ed2m=0 ed3m=0 hsgrad=0 cograd=0) /// at(ed1m=12 ed2m=0 ed3m=0 hsgrad=1 cograd=0) /// at(ed1m=13 ed2m=1 ed3m=0 hsgrad=1 cograd=0) /// at(ed1m=16 ed2m=4 ed3m=0 hsgrad=1 cograd=0) /// at(ed1m=16 ed2m=4 ed3m=0 hsgrad=1 cograd=1) /// at(ed1m=17 ed2m=5 ed3m=1 hsgrad=1 cograd=1) /// at(ed1m=20 ed2m=8 ed3m=4 hsgrad=1 cograd=1) The following steps save the adjusted means from the margins command, as well as the corresponding values of education into the active dataset. The graph command is then used to graph the adjusted means by education, creating the graph shown below matrix yhat = r(b)' svmat yhat matrix educ = ( 0 \\ 1 \\ 12 \\ 12 \\ 13 \\ 16 \\ 16 \\ 17 \\ 20 ) svmat educ graph twoway line yhat1 educ1, xline(12 16) xtitle(Education) ytitle(Adjusted mean) Let’s walk through this process again, but do so more slowly. First, repeat the use gss_ivrm command, as well as the mkspline, regress, and margins commands from above. Now, the adjusted means computed by the margins command are stored in a matrix named r(b) with one row and nine columns, corresponding to the nine values specified with the at() option. use gss_ivrm.dta mkspline ed1m 12 ed2m 16 ed3m = educ,marginal reg realrinc ed1m ed2m ed3m hsgrad cograd female,vce(robust) margins,at(ed1m=0 ed2m=0 ed3m=0 hsgrad=0 cograd=0) /// at(ed1m=1 ed2m=0 ed3m=0 hsgrad=0 cograd=0) /// at(ed1m=12 ed2m=0 ed3m=0 hsgrad=0 cograd=0) /// at(ed1m=12 ed2m=0 ed3m=0 hsgrad=1 cograd=0) /// at(ed1m=13 ed2m=1 ed3m=0 hsgrad=1 cograd=0) /// at(ed1m=16 ed2m=4 ed3m=0 hsgrad=1 cograd=0) /// at(ed1m=16 ed2m=4 ed3m=0 hsgrad=1 cograd=1) /// at(ed1m=17 ed2m=5 ed3m=1 hsgrad=1 cograd=1) /// at(ed1m=20 ed2m=8 ed3m=4 hsgrad=1 cograd=1) matrix list r(b) r(b)[1,9]\r1. 2. 3. 4. 5. 6. 7. 8. 9.\r_at _at _at _at _at _at _at _at _at\rr1 10258.623 10530.894 13525.881 17451.464 19053.55 23859.81 29600.571 33742.824 46169.582\rLet’s store these adjusted means as a matrix called yhat and in the process transpose the matrix (converting the columns to rows). matrix yhat = r(b)' matrix list yhat yhat[9,1]\rr1\r1._at 10258.623\r2._at 10530.894\r3._at 13525.881\r4._at 17451.464\r5._at 19053.55\r6._at 23859.81\r7._at 29600.571\r8._at 33742.824\r9._at 46169.582\rWe can then save yhat into the current dataset with the svmat command. svmat yhat list yhat1 in 1/10 +----------+\r| yhat1 |\r|----------|\r1. | 10258.62 |\r2. | 10530.89 |\r3. | 13525.88 |\r4. | 17451.46 |\r5. | 19053.55 |\r6. | 23859.81 |\r7. | 29600.57 |\r8. | 33742.82 |\r9. | 46169.58 |\r10. | . |\r+----------+\rNow, let’s make a matrix containing the values of education. This is stored in the matrix named educ. We then save this matrix into the dataset, as shown below. matrix educ = (0 \\ 1 \\ 12 \\ 12 \\ 13 \\ 16 \\ 16 \\ 17 \\ 20) svmat educ list yhat1 educ1 in 1/10 +------------------+\r| yhat1 educ1 |\r|------------------|\r1. | 10258.62 0 |\r2. | 10530.89 1 |\r3. | 13525.88 12 |\r4. | 17451.46 12 |\r5. | 19053.55 13 |\r6. | 23859.81 16 |\r7. | 29600.57 16 |\r8. | 33742.82 17 |\r9. | 46169.58 20 |\r10. | . . |\r+------------------+\rNow, we can graph the adjusted means, called yhat1, by the levels of education, called educ1, as shown below. graph twoway line yhat1 educ1, xline(12 16)xtitle(Education) ytitle(Adjusted mean) Although the process of creating this graph is more complicated, the benefit is that it will automatically be updated if the dataset changes. This can be a little more work in the short run but saves us time in the long run. ","date":"2023-12-29","objectID":"/3.continuous-predictors_piecewise-models/:8:0","tags":["Continuous predictors","stata"],"title":"Chapter4 ：Continuous predictors: Piecewise models","uri":"/3.continuous-predictors_piecewise-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter focuses on the use of polynomial terms to account for nonlinearity in the relationship between a continuous predictor and a continuous outcome.  ","date":"2023-12-28","objectID":"/2.continuous-predictors_polynomials/","tags":["Continuous predictors","stata"],"title":"Chapter3 ：Continuous predictors: Polynomials","uri":"/2.continuous-predictors_polynomials/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This Chapter focuses on how to interpret the coefficient of a continuous predictor in a linear regression model. ","date":"2023-12-28","objectID":"/2.continuous-predictors_polynomials/:0:0","tags":["Continuous predictors","stata"],"title":"Chapter3 ：Continuous predictors: Polynomials","uri":"/2.continuous-predictors_polynomials/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 Quadratic (squared) terms A quadratic (squared) term can be used to model curved relationships, accounting for one bend in the relationship between the predictor and outcome. Let’s relate the nature of the curve to the regression equation predicting income (realrinc) from age (age), shown below. $$ \\widehat{realrinc}=-30000 + 2500age + (-25age^{2}) $$ For an inverted U-shaped curve, we can compute the value of x that corresponds to the maximum value of y. If we call the linear coefficient of age and the quadratic coefficient of age b2 , then the value of age that yields the maximum income is given by -b1/(2×b2). Substituting 2,500 for b1 and -25 for b2 yields a value of 50;therefore,the maximum income occurs when someone is 50 years old. The degree of curvature (U-shape) would increase as the quadratic coefficient increases. The linear coefficient would still determine the slope when the predictor is at zero. Because the curve is U-shaped, the minimum of that curve would be represented by -b1/(2×b2), where is the linear coefficient and is the quadratic coefficient. ","date":"2023-12-28","objectID":"/2.continuous-predictors_polynomials/:1:0","tags":["Continuous predictors","stata"],"title":"Chapter3 ：Continuous predictors: Polynomials","uri":"/2.continuous-predictors_polynomials/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.1 Examples use gss_ivrm.dta keep if (age\u003c=80) Before fitting a quadratic model relating income to age, let’s assess the shape of the relationship between these variables using a locally weighted smoother. The lowess command is used to create the variable yhatlowess, which is the predicted value based on the locally weighted regression predicting income from age. Checking for nonlinearity lowess realrinc age,nograph gen(yhatlowess) line yhatlowess age,sort Let’s fit a model with a quadratic (squared) term to account for the bend in the relationship between age and income. We do this using the interaction operator ## (as shown below), which creates a term that multiplies age by age. Specifying c.age indicates to Stata that age should be treated as a continuous variable (instead of treating it as factor variable). 1.1.1 Interpreting the relationship between age and income reg realrinc c.age##c.age female,vce(robust) Note! Why not square age? You might be wondering why we do not instead use the generate command to create a new variable, say, age2, that contains the age squared. If we do this, the results of the regress command would be the same, but this would confuse the margins command. The margins command would think that age and age2 are two completely different variables. Linear regression Number of obs = 32,100\rF(3, 32096) = 1252.67\rProb \u003e F = 0.0000\rR-squared = 0.1089\rRoot MSE = 25407\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\rage | 2412.339 58.058 41.55 0.000 2298.544 2526.135\r|\rc.age#c.age | -24.202 0.696 -34.78 0.000 -25.566 -22.838\r|\rfemale | -1.24e+04 280.475 -44.28 0.000 -1.30e+04 -1.19e+04\r_cons | -2.57e+04 1038.412 -24.73 0.000 -2.77e+04 -2.36e+04\rWe can use the formula -b1/(2×b2) to compute the age at which income is at its maximum. This yields -2412.34/(2×-24.20) , which equals 49.84. The adjusted mean of income is highest for those who are 49.84 years old. Suppose we wanted to estimate the age slope for any given value of age. We can do so using the margins command combined with the dydx(age) option.Below,we obtain the age slope for ages ranging from 30 to 70 in 10-year increments. margins,at(age=(30(10)70))dydx(age)vsquish Average marginal effects Number of obs = 32,100\rModel VCE: Robust\rExpression: Linear prediction, predict()\rdy/dx wrt: age\r1._at: age = 30\r2._at: age = 40\r3._at: age = 50\r4._at: age = 60\r5._at: age = 70\r| Delta-method\r| dy/dx std. err. t P\u003e|t| [95% conf. interval]\rage |\r_at |\r1 | 960.222 18.274 52.55 0.000 924.404 996.039\r2 | 476.183 9.823 48.47 0.000 456.928 495.437\r3 | -7.856 15.700 -0.50 0.617 -38.628 22.915\r4 | -491.896 27.998 -17.57 0.000 -546.772 -437.019\r5 | -975.935 41.336 -23.61 0.000 -1056.955 -894.915\r1.1.2 Graphing adjusted means with confidence intervals The marginsplot command can be used to create a graph of the adjusted means with a shaded region showing the confidence interval for the adjusted means. We can then run the marginsplot command, adding the recast(line) and recastci(rarea) options to display the adjusted means as a line and the confidence interval as a shaded region. The resulting graph is shown in below. margins,at(age=(18(1)80)) marginsplot,recast(line) recastci(rarea) ","date":"2023-12-28","objectID":"/2.continuous-predictors_polynomials/:1:1","tags":["Continuous predictors","stata"],"title":"Chapter3 ：Continuous predictors: Polynomials","uri":"/2.continuous-predictors_polynomials/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2 Cubic (third power)terms Let’s examine the relationship between the year of birth and number of children a woman has using the gss_ivrm.dta dataset, focusing on women aged 45 to 55 born between 1920 and 1960. use gss_ivrm.dta keep if (age\u003e=45 \u0026 age\u003c=55) \u0026 (yrborn\u003e=1920 \u0026 yrborn\u003c=1960) \u0026 female==1 lowess children yrborn,gen(yhatlowess) nograph graph twoway line yhatlowess yrborn,sort Below,we fit a model predicting children from yrborn fit using a cubic term.The model specifies c.yrborn##c.yrborn##c.yrborn, which includes the cubic term for yrborn, the quadratic term for yrborn, and the linear term for yrborn. reg children c.yrborn##c.yrborn##c.yrborn,noci note: c.yrborn#c.yrborn#c.yrborn omitted because of collinearity. Source | SS df MS Number of obs = 5,049\rF(2, 5046) = 193.19\rModel | 1163.13728 2 581.56864 Prob \u003e F = 0.0000\rResidual | 15189.893 5,046 3.01028399 R-squared = 0.0711\rAdj R-squared = 0.0708\rTotal | 16353.0303 5,048 3.2395068 Root MSE = 1.735\rchildren | Coefficient Std. err. t P\u003e|t|\ryrborn | 1.431 0.816 1.75 0.079\r|\rc.yrborn#c.yrborn | -0.000 0.000 -1.81 0.071\r|\rc.yrborn#c.yrborn#c.yrborn | 0.000 (omitted)\r|\r_cons | -1344.881 791.474 -1.70 0.089\rThere was a problem running this model. There is a note saying that the cubic term was omitted because of collinearity. This is a common problem when entering cubic terms, which can be solved by centering yrborn. The dataset includes a variable called yrborn40, which is the variable yrborn centered around the year 1940 (that is, 1940 is subtracted from each value of yrborn). Let’s try fitting the above model again but instead using the variable yrborn40. reg children c.yrborn40##c.yrborn40##c.yrborn40,noci . reg children c.yrborn40##c.yrborn40##c.yrborn40,noci Source | SS df MS Number of obs = 5,049\rF(3, 5045) = 166.84\rModel | 1475.96126 3 491.987087 Prob \u003e F = 0.0000\rResidual | 14877.069 5,045 2.94887394 R-squared = 0.0903\rAdj R-squared = 0.0897\rTotal | 16353.0303 5,048 3.2395068 Root MSE = 1.7172\rchildren | Coefficient Std. err. t P\u003e|t|\ryrborn40 | -0.091 0.005 -17.34 0.000\r|\rc.yrborn40#c.yrborn40 | -0.001 0.000 -3.66 0.000\r|\rc.yrborn40#c.yrborn40#c.yrborn40 | 0.000 0.000 10.30 0.000\r|\r_cons | 2.741 0.037 73.67 0.000\rNote! Including linear, quadratic, and cubic terms When fitting this kind of a cubic model, you might find that the linear or quadratic coefficients are not significant. For the sake of parsimony, you might be tempted to omit those variables because they are not significant. However, it is essential that these terms be included in the model (even if not significant) to preserve the interpretation of the cubic term. Specifying at(yrborn40=(-20(1)20)) yields predicted means for years of birth ranging from 1920 to 1960 in one-year increments. reg children c.yrborn40##c.yrborn40##c.yrborn40,noci The marginsplot command is then used to create the graph shown in figure. The recast(line) and recastci(rarea) options display the predicted means as a solid line and the confidence interval as a shaded area. You can further explore how the yrborn40 slope varies as a function of year of birth by specifying multiple values within the at() option, as shown below. margins,at(yrborn40=(-20(5)20)) dydx(yrborn40) vsquish Conditional marginal effects Number of obs = 5,049\rModel VCE: OLS\rExpression: Linear prediction, predict()\rdy/dx wrt: yrborn40\r1._at: yrborn40 = -20\r2._at: yrborn40 = -15\r3._at: yrborn40 = -10\r4._at: yrborn40 = -5\r5._at: yrborn40 = 0\r6._at: yrborn40 = 5\r7._at: yrborn40 = 10\r8._at: yrborn40 = 15\r9._at: yrborn40 = 20\r| Delta-method\r| dy/dx std. err. t P\u003e|t| [95% conf. interval]\ryrborn40 |\r_at |\r1 | 0.191 0.023 8.34 0.000 0.146 0.236\r2 | 0.074 0.012 6.02 0.000 0.050 0.097\r3 | -0.013 0.005 -2.35 0.019 -0.023 -0.002\r4 | -0.067 0.004 -15.70 0.000 -0.076 -0.059\r5 | -0.091 0.005 -17.34 0.000 -0.101 -0.081\r6 | -0.083 0.005 -18.12 0.000 -0.092 -0.074\r7 | -0.044 0.004 -9.82 0.000 -0.052 -0.035\r8 | 0.027 0.010 2.70 0.007 0.007 0.047\r9 | 0.129 0.020 6.50 0.000 0.090 0.16","date":"2023-12-28","objectID":"/2.continuous-predictors_polynomials/:2:0","tags":["Continuous predictors","stata"],"title":"Chapter3 ：Continuous predictors: Polynomials","uri":"/2.continuous-predictors_polynomials/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.1 Fractional polynomial regression Fractional polynomial regression is more flexible by considering other kinds of power terms as well, including negative powers and fractional powers. The fp prefix automates the process of selecting the best fitting fractional polynomial model.It fits a variety of polynomial terms (and combinations of polynomial terms) and shows you the best fitting model. The default set of power terms the fp prefix will try includes -2,-1,-0.5, 0, 0.5, 1, 2, and 3 (where 0 indicates that the natural log of the predictor is used). Let’s look at this canvas of shapes, beginning with the negative powers (-2,-1,and -0.5). These models take the form of y=b*Xpower, where could be positive or negative, and could be -2,-1, or -0.5. Figure shows some possible shapes of these models, showing the powers -2,-1,-0.5 and in columns 1, 2, and 3. The graphs in the first row (with the positive coefficients) are all typified by a steep descent and then reaching a floor. The more strongly negative the power term, the stronger the descent. The second row is a vertical mirror image of the first. When the coefficient is negative, there is a sharp ascent and then a ceiling is reached. The more negative power terms are associated with a sharper ascent. The shapes of the relationship between and for the powers 1, 2, and 3 are shown in figure The first column shows a linear relationship between and . The second and third columns show the second and third power, with the top row showing a U-shaped bend and the bottom row showing an inverted U-shaped bend. The higher power terms are associated with a more rapid change in the outcome for a unit change in the predictor. the fp prefix (by default) will not only fit each of these eight powers alone, but also includes all two-way combinations of these powers. The right panel shows the formula combining these two curves, Note how this curve combines the rapid rise of the left panel with the gradual inverted U-shape of the middle panel. As you can imagine, being able to combine two different fractional polynomials can yield a flexible set of possible curve shapes for modeling your data. 2.1.1 Example using fractional polynomial regression First, we will run a regression predicting educ fromage, treating age as a categorical variable use gss_ivrm.dta keep if (age\u003c=80) reg educ i.age predict yhatmean graph twoway line yhatmean age,sort xlabel(20(5)80) Note how there is a curvilinear aspect to the relationship between age and education, suggesting that we might try including a quadratic term for educ. reg educ c.age##c.age Source | SS df MS Number of obs = 53,070\rF(2, 53067) = 1616.60\rModel | 29981.1031 2 14990.5515 Prob \u003e F = 0.0000\rResidual | 492083.501 53,067 9.27287205 R-squared = 0.0574\rAdj R-squared = 0.0574\rTotal | 522064.604 53,069 9.83746828 Root MSE = 3.0451\reduc | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\rage | 0.138 0.005 28.22 0.000 0.129 0.148\r|\rc.age#c.age | -0.002 0.000 -36.02 0.000 -0.002 -0.002\r|\r_cons | 10.763 0.107 100.14 0.000 10.553 10.974\rLet’s use the predict command to compute the fitted values from the quadratic model, naming the variable yhatq. Then, let’s graph the fitted values from the quadratic model and the mean of education by age, as shown below. predict yhatq graph twoway line yhatmean yhatq age,sort xlabel(20(5)80) Note how the quadratic fit line yields predicted values that are too high in the younger years and too low in the older years. Another way of putting this is that the quadratic line does not account for the rapid rise in education in the late teens and early 20s, nor does it account for the slow decline of education in later years. A fractional polynomial model, with its increased flexibility, could provide a more appropriate fit. We fit such a model by adding the fp prefix to the regress command, as shown below. fp \u003cage\u003e:reg educ \u003cage\u003e Fractional polynomial comparisons:\r| Test Residual Deviance\rage | df Deviance std. dev. diff. P Powers","date":"2023-12-28","objectID":"/2.continuous-predictors_polynomials/:2:1","tags":["Continuous predictors","stata"],"title":"Chapter3 ：Continuous predictors: Polynomials","uri":"/2.continuous-predictors_polynomials/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.2 Main effects with polynomial terms The meaning of main effects changes when polynomial terms are included in the model. In fact, the inclusion of polynomial terms can substantially change the coefficient for the main effect when compared with the main effect–only model. use gss_ivrm.dta keep if (age\u003c=80) reg realrinc age female,vce(robust) Linear regression Number of obs = 32,100\rF(2, 32097) = 1170.51\rProb \u003e F = 0.0000\rR-squared = 0.0787\rRoot MSE = 25833\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\rage | 320.07 10.70 29.93 0.000 299.10 341.03\rfemale | -12373.25 285.03 -43.41 0.000 -12931.91 -11814.59\r_cons | 15106.17 403.04 37.48 0.000 14316.19 15896.16\rHowever, there is a problem. As we saw in section 1.1, the relationship between income and age is not linear. As we did in that section, let’s add a quadratic term for age, as shown below. reg realrinc c.age##c.age female,vce(robust) Linear regression Number of obs = 32,100\rF(3, 32096) = 1252.67\rProb \u003e F = 0.0000\rR-squared = 0.1089\rRoot MSE = 25407\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\rage | 2412.34 58.06 41.55 0.000 2298.54 2526.13\r|\rc.age#c.age | -24.20 0.70 -34.78 0.000 -25.57 -22.84\r|\rfemale | -12419.24 280.47 -44.28 0.000 -12968.98 -11869.49\r_cons | -25679.77 1038.41 -24.73 0.000 -27715.10 -23644.44\rThe quadratic term is significant, but we might suddenly become concerned that the main effect of age has skyrocketed from 320.07 in the linear model to 2,412.34 in the quadratic model. Why did the main effect change so much? Did we do something wrong? The key is that the term “main effect” is really a misnomer, because we expect this term to describe the general trend of the relationship between income and age. This value is meaningless for two reasons. First, nobody has income when they are zero years old. Second, this term no longer reflects the general trend. As we saw in section 1.1, the age slope changes for every level of age, so there is no such thing as a measure of general trend in this kind of model. ","date":"2023-12-28","objectID":"/2.continuous-predictors_polynomials/:2:2","tags":["Continuous predictors","stata"],"title":"Chapter3 ：Continuous predictors: Polynomials","uri":"/2.continuous-predictors_polynomials/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This Chapter focuses on how interpret the coefficient of a continuous predictor in a linear regression models. ","date":"2023-12-09","objectID":"/1.continuous-predictors_linear/","tags":["Continuous predictors","stata"],"title":"Chapter2 ：Continuous predictors:Linear","uri":"/1.continuous-predictors_linear/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This Chapter focuses on how to interpret the coefficient of a continuous predictor in a linear regression model. ","date":"2023-12-09","objectID":"/1.continuous-predictors_linear/:0:0","tags":["Continuous predictors","stata"],"title":"Chapter2 ：Continuous predictors:Linear","uri":"/1.continuous-predictors_linear/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 Simple linear regression This section illustrates the use of a continuous predictor for predicting a continuous outcome using ordinary least-squares regression. Terminology: Continuous and categorical variables When I use the termcontinuous variable, I am referring to a variable that is measured on an interval or ratio scale. By contrast, when I speak of a categorical variable,I am referring to either a nominal variable or an ordinal/interval/ratio variable that we wish to treat as though it were a nominal variable. Let’s run a simple regression model in which we predict the education of the respondent from the education of the respondent’s father. regress educ paeduc The result as shown below Source | SS df MS Number of obs = 696\rF(1, 694) = 228.14\rModel | 1649.70181 1 1649.70181 Prob \u003e F = 0.0000\rResidual | 5018.43038 694 7.23116769 R-squared = 0.2474\rAdj R-squared = 0.2463\rTotal | 6668.13218 695 9.5944348 Root MSE = 2.6891\reduc | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\rpaeduc | 0.359 0.024 15.10 0.000 0.313 0.406\r_cons | 9.740 0.286 34.08 0.000 9.179 10.301\rThe regression equation can be written as shown below $$ \\widehat{educ}=9.74 + 0.36paeduc $$ The intercept is the predicted mean of the respondent’s education when the father’s education is 0. For every one-year increase in the education of the father, we would predict that the education of the respondent increases by 0.36 years. ","date":"2023-12-09","objectID":"/1.continuous-predictors_linear/:1:0","tags":["Continuous predictors","stata"],"title":"Chapter2 ：Continuous predictors:Linear","uri":"/1.continuous-predictors_linear/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.1 Computing predicted means using the margins command Suppose we wanted to compute the predicted mean of education for the respondent, assuming separately that the father had 8, 12, or 16 years of education. margins,at (paeduc=(8 12 16)) vsquish Note: The vsquish option The vsquish option vertically squishes the output by omitting extra blank lines. Adjusted predictions Number of obs = 696\rModel VCE: OLS\rExpression: Linear prediction, predict()\r1._at: paeduc = 8\r2._at: paeduc = 12\r3._at: paeduc = 16\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r_at |\r1 | 12.616 0.128 98.93 0.000 12.365 12.866\r2 | 14.053 0.104 135.64 0.000 13.850 14.257\r3 | 15.491 0.153 101.42 0.000 15.191 15.791\rSometimes, we might want to compute the predicted means given a range of values for a predictor. For example, we might want to compute the predicted means when father’s education is 0, 4, 8, 12, 16, and 20. margins,at(paeduc=(0(4)20)) vsquish Rather than typing all of these values, we can specify 0(4)20, which tells Stata that we mean 0 to 20 in 4-unit increments. Adjusted predictions Number of obs = 696\rModel VCE: OLS\rExpression: Linear prediction, predict()\r1._at: paeduc = 0\r2._at: paeduc = 4\r3._at: paeduc = 8\r4._at: paeduc = 12\r5._at: paeduc = 16\r6._at: paeduc = 20\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r_at |\r1 | 9.740 0.286 34.08 0.000 9.179 10.301\r2 | 11.178 0.200 55.95 0.000 10.786 11.570\r3 | 12.616 0.128 98.93 0.000 12.365 12.866\r4 | 14.053 0.104 135.64 0.000 13.850 14.257\r5 | 15.491 0.153 101.42 0.000 15.191 15.791\r6 | 16.929 0.232 72.82 0.000 16.472 17.385\r","date":"2023-12-09","objectID":"/1.continuous-predictors_linear/:1:1","tags":["Continuous predictors","stata"],"title":"Chapter2 ：Continuous predictors:Linear","uri":"/1.continuous-predictors_linear/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.2 Graphing predicted means using the marginsplot command We can use the marginsplot command to create a graph showing the predicted means and confidence intervals based on the most recent margins command. marginsplot Note：The margins and marginsplot commands work together as a team. Let’s now create a graph that shows the fitted line with a shaded confidence interval. margins,at(paeduc=(0(1)20)) marginsplot,recast(line) recastci(rarea) The recast() option specifies that the fitted line should be displayed as a line graph (suppressing the markers). The recastci() option specifies that the confidence interval should be displayed as an rarea graph, displaying a shaded area for the confidence region. ","date":"2023-12-09","objectID":"/1.continuous-predictors_linear/:1:2","tags":["Continuous predictors","stata"],"title":"Chapter2 ：Continuous predictors:Linear","uri":"/1.continuous-predictors_linear/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2 Multiple regression Let’s now turn to a multiple regression model that predicts the respondent’s education from the father’s education (paeduc), the mother’s education (maeduc), and the age of the respondent (age). reg educ paeduc maeduc age Source | SS df MS Number of obs = 650\rF(3, 646) = 92.93\rModel | 1822.26082 3 607.420272 Prob \u003e F = 0.0000\rResidual | 4222.37918 646 6.53619069 R-squared = 0.3015\rAdj R-squared = 0.2982\rTotal | 6044.64 649 9.31377504 Root MSE = 2.5566\reduc | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\rpaeduc | 0.258 0.033 7.82 0.000 0.193 0.323\rmaeduc | 0.208 0.038 5.48 0.000 0.133 0.282\rage | 0.034 0.007 5.28 0.000 0.022 0.047\r_cons | 6.962 0.511 13.63 0.000 5.959 7.965\rThe equation as shown below $$ \\widehat{educ}=6.86 + 0.26paeduc + 0.21maeduc + 0.03age $$ The coefficients from this multiple regression model reflect the association between each predictor and the outcome after adjusting for all the other predictors. ","date":"2023-12-09","objectID":"/1.continuous-predictors_linear/:2:0","tags":["Continuous predictors","stata"],"title":"Chapter2 ：Continuous predictors:Linear","uri":"/1.continuous-predictors_linear/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.1 Computing adjusted means using the margins command The margins command allows us to hold more than one variable constant at a time. In the example below, we compute the adjusted means when the father’s education equals 8, 12, and 16, while holding the mother’s education constant at 14. reg educ paeduc maeduc age Predictive margins Number of obs = 650\rModel VCE: OLS\rExpression: Linear prediction, predict()\r1._at: paeduc = 8\rmaeduc = 14\r2._at: paeduc = 12\rmaeduc = 14\r3._at: paeduc = 16\rmaeduc = 14\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r_at |\r1 | 13.544 0.211 64.18 0.000 13.130 13.958\r2 | 14.576 0.128 113.76 0.000 14.325 14.828\r3 | 15.609 0.152 102.52 0.000 15.310 15.908\rTerminology: Adjusted means For example, we can say that the predicted mean, given the father has 8 years of education, is 13.544 after adjusting for all other predictors. We could also call this an adjusted mean. The term adjusted mean implies after adjusting for all other predictors in the model. When using nonlinear models (such as a logistic regression model), we will use a more general term, such as predictive margin. ","date":"2023-12-09","objectID":"/1.continuous-predictors_linear/:2:1","tags":["Continuous predictors","stata"],"title":"Chapter2 ：Continuous predictors:Linear","uri":"/1.continuous-predictors_linear/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3 Checking for nonlinearity graphically This section illustrates graphical approaches for checking for nonlinearity in the relationship between a predictor and outcome variable. These approaches include examining scatterplots of the predictor and outcome. examining residual-versusfitted plots. creating plots based on locally weighted smoothers. plotting the mean of the outcome for each level of the predictor. ","date":"2023-12-09","objectID":"/1.continuous-predictors_linear/:3:0","tags":["Continuous predictors","stata"],"title":"Chapter2 ：Continuous predictors:Linear","uri":"/1.continuous-predictors_linear/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.1 Using scatterplots to check for nonlinearity Let’s look at a scatterplot of the size of the engine (displacement) by length of the car (length) with a line showing the linear fit, as shown in figure use autosubset graph twoway (scatter displacement length) (lfit displacement length),ytitle(\"Engine displacement(cu in.) \") legend (off) The relationship between these two variables looks fairly linear, but the addition of the linear fit line helps us to see the nonlinearity. Note how for short cars (whenlength is below 160) the fit line underpredicts and for longer cars (when length is above 210) the fit line also underpredicts. Using a scatterplot like this can be a simple means of looking at the linearity of the simple relationship between a predictor and outcome variable. However, this does not account for other predictors that you might want to include in a model. To this end, let’s next look at how we can use the residuals for checking linearity ","date":"2023-12-09","objectID":"/1.continuous-predictors_linear/:3:1","tags":["Continuous predictors","stata"],"title":"Chapter2 ：Continuous predictors:Linear","uri":"/1.continuous-predictors_linear/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.2 Checking for nonlinearity using residuals For example, let’s run a regression predicting displacement from length, trunk, and weight, as shown below reg displacement length trunk weight We can then look at the residuals versus the fitted values, as shown in figure rvfplot Note the U-shaped pattern of the residuals. This pattern suggests that the relationship between the predictors and outcome is not linear. There are many excellent resources that illustrate Stata’s regression diagnostic tools, including the manual entry for [R] regress postestimation. You can also see the help entries for avplot, rvfplot, and rvpplot. ","date":"2023-12-09","objectID":"/1.continuous-predictors_linear/:3:2","tags":["Continuous predictors","stata"],"title":"Chapter2 ：Continuous predictors:Linear","uri":"/1.continuous-predictors_linear/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.3 Checking for nonlinearity using locally weighted smoother With larger datasets, it can be harder to visualize nonlinearity using scatterplots or residual-versus-fitted plots. Suppose we want to determine the nature of the relationship between the year that the respondent was born (yrborn) and education level (educ). use gss_ivrm.dta scatter educ yrborn,msymbol(oh) It is hard to discern the nature of the relationship between year of birth and education using this scatterplot. With so many observations, the scatterplot is saturated with data points creating one big blotch that tells us little about the shape of the relationship between the predictor and outcome The lowess command below creates a graph showing the locally weighted regression of education on year of birth, as shown in figure lowess educ yrborn,msymbol(p) The lowess graph suggests that there is nonlinearity in the relationship between year of birth and education. Education increases with year of birth until the 1950s, at which point the smoothed education values level out and then start to decline. The graph produced by the lowess command is much more informative than the scatterplot alone. ","date":"2023-12-09","objectID":"/1.continuous-predictors_linear/:3:3","tags":["Continuous predictors","stata"],"title":"Chapter2 ：Continuous predictors:Linear","uri":"/1.continuous-predictors_linear/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.4 Graphing outcome mean at each level of predictor Another way to visualize the relationship between the predictor and outcome is to create a graph showing the mean of the outcome at each level of the predictor. Using the example predicting education from year of birth means creating a graph of the average of education at each level of year of birth. Although the variable yrborn can assume many values (from 1883 to 1990), the variable is composed of discrete integers with reasonably many observations (usually more than 100) for each value. In such a case, we can explore the nature of the relationship between the predictor and outcome by creating a graph of the mean of the outcome variable (education level) for each level of the predictor (year of birth). This kind of graph imposes no structure on the shape of the relationship between year of birth and education and allows us to observe the nature of the relationship between the predictor and the outcome. One simple way to create such a graph is to fit a regression model predicting the outcome treating the predictor variable as a factor variable. Following that, the margins command is used to obtain the predicted mean of the outcome for each level of the predictor. In the regress command below, specifying i.yrborn indicates that the variable yrborn should be treated as a factor variable. The following margins command computes the predicted mean of the outcome (education) at each year of birth reg educ i.yrborn margins yrborn marginsplot The predicted means vary erratically for the years before 1900 because of the few observations per year during those years. For these years, the confidence intervals are much wider compared with later years, reflecting greater uncertainty of the estimates because of fewer observations. If all the years had such few observations, then the entire graph might be dominated by wild swings in the means and show little about the nature of the relationship between the predictor and outcome. ","date":"2023-12-09","objectID":"/1.continuous-predictors_linear/:3:4","tags":["Continuous predictors","stata"],"title":"Chapter2 ：Continuous predictors:Linear","uri":"/1.continuous-predictors_linear/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4 Checking for nonlinearity analytically This section shows how to check for nonlinearity using analytic approaches, including adding power terms and using factor variables. ","date":"2023-12-09","objectID":"/1.continuous-predictors_linear/:4:0","tags":["Continuous predictors","stata"],"title":"Chapter2 ：Continuous predictors:Linear","uri":"/1.continuous-predictors_linear/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4.1 Adding power terms Another way to check for nonlinearity of a continuous variable is to add power terms (for example, quadratic or cubic). Using the example with education as a function of year of birth. Let’s introduce a quadratic term (in addition to the linear term) by adding c.yrborn#c.yrborn to the model. This introduces a quadratic effect that would account for one bend in the line relating year of birth to education. We would expect the quadratic term to be significant based on the graphs we saw in figures use gss_ivrm.dta Source | SS df MS Number of obs = 54,745\rF(2, 54742) = 3567.49\rModel | 63778.2779 2 31889.1389 Prob \u003e F = 0.0000\rResidual | 489328.985 54,742 8.93882184 R-squared = 0.1153\rAdj R-squared = 0.1153\rTotal | 553107.263 54,744 10.103523 Root MSE = 2.9898\reduc | Coefficient Std. err. t P\u003e|t|\ryrborn | 4.218 0.102 41.46 0.000\r|\rc.yrborn#c.yrborn | -0.001 0.000 -41.00 0.000\r|\r_cons | -4129.000 98.816 -41.78 0.000\rNote! Using the noci option for clearer output When models include interactions,those labels can get rather wide, and Stata is forced to display those labels across multiple lines, which can make the output confusing and hard to read. This omits the display of the confidence intervals, which makes enough room to display the label for every term in the model in a single line. but!,the confidence interval are also important for our research. The quadratic term is significant in this model.Furthermore, the R2 for this model increased to 0.1153 compared with 0.0881 for the linear model. This provides analytic support for including a quadratic term for year of birth when predicting education. A cubic term would imply that the line fitting year of birth and education has a tendency to have two bends in it.c.yrborn##c.yrborn##c.yrborn as a predictor is a shorthand for including the linear, quadratic, and cubic terms for year of birth (yrborn). reg educ c.yrborn##c.yrborn##c.yrborn,noci note: c.yrborn#c.yrborn#c.yrborn omitted because of collinearity. Source | SS df MS Number of obs = 926\rF(2, 923) = 11.47\rModel | 219.843988 2 109.921994 Prob \u003e F = 0.0000\rResidual | 8846.14737 923 9.584125 R-squared = 0.0242\rAdj R-squared = 0.0221\rTotal | 9065.99136 925 9.80107174 Root MSE = 3.0958\reduc | Coefficient Std. err. t P\u003e|t|\ryrborn | 6.159 1.287 4.79 0.000\r|\rc.yrborn#c.yrborn | -0.002 0.000 -4.79 0.000\r|\rc.yrborn#c.yrborn#c.yrborn | 0.000 (omitted)\r|\r_cons | -6015.618 1259.800 -4.78 0.000\rThere was a problem running this model. There is a note saying that the cubic term was omitted because of collinearity. This is a common problem when entering cubic terms, which can be solved by centering yrborn. The dataset includes a variable called yrborn40, which is the variable yrborn centered around the year 1940 (that is, 1940 is subtracted from each value of yrborn). reg educ c.yrborn40##c.yrborn40##c.yrborn40,noci Source | SS df MS Number of obs = 926\rF(3, 922) = 7.72\rModel | 222.175819 3 74.0586064 Prob \u003e F = 0.0000\rResidual | 8843.81554 922 9.59199083 R-squared = 0.0245\rAdj R-squared = 0.0213\rTotal | 9065.99136 925 9.80107174 Root MSE = 3.0971\reduc | Coefficient Std. err. t P\u003e|t|\ryrborn40 | 0.058 0.014 4.20 0.000\r|\rc.yrborn40#c.yrborn40 | -0.002 0.001 -2.14 0.033\r|\rc.yrborn40#c.yrborn40#c.yrborn40 | 0.000 0.000 0.49 0.622\r|\r_cons | 13.427 0.201 66.87 0.000\rThe results show, as expected, that the cubic term is significant. However, this dataset has many observations, so the model has the statistical power to detect very small effects. Note how the is 0.1162 for the cubic model compared with 0.1153 for the quadratic model. This is a trivial increase, suggesting that the cubic trend is not really an important term to include in this model. ","date":"2023-12-09","objectID":"/1.continuous-predictors_linear/:4:1","tags":["Continuous predictors","stata"],"title":"Chapter2 ：Continuous predictors:Linear","uri":"/1.continuous-predictors_linear/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4.2 Using factor variables The strategy of using factor variables to check for nonlinearity makes sense only when you have a relatively limited number of levels of the predictor that are coded as whole numbers. let’s perform an analysis to detect any kind of nonlinearity in the relationship between decade of age and health status. use gss_ivrm.dta reg health c.agedec i.agedec note: 8.agedec omitted because of collinearity. Source | SS df MS Number of obs = 40,984\rF(7, 40976) = 451.59\rModel | 2111.10121 7 301.585887 Prob \u003e F = 0.0000\rResidual | 27364.9308 40,976 .667828261 R-squared = 0.0716\rAdj R-squared = 0.0715\rTotal | 29476.032 40,983 .719225826 Root MSE = .81721\rhealth | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\ragedec | -0.098 0.005 -18.46 0.000 -0.108 -0.087\r|\ragedec |\r20s | 0.110 0.028 3.97 0.000 0.055 0.164\r30s | 0.165 0.024 6.85 0.000 0.118 0.212\r40s | 0.135 0.022 6.24 0.000 0.093 0.178\r50s | 0.065 0.021 3.15 0.002 0.025 0.106\r60s | -0.006 0.021 -0.28 0.783 -0.047 0.036\r70s | -0.038 0.024 -1.60 0.110 -0.084 0.009\r80s | 0.000 (omitted)\r|\r_cons | 3.320 0.035 95.81 0.000 3.253 3.388\rThis unconventional-looking model divides the relationship between the age decade and the outcome into two pieces: the linear relationship, which is accounted for by c.agedec any remaining nonlinear components, which are explained by the indicator variables specified by i.agedec. Let’s now use the testparm command to perform a test of the indicator variables, giving us an overall test of the nonlinearity in the relationship between age decade and health status. testparm i.agedec (1) 2.agedec = 0\r(2) 3.agedec = 0\r(3) 4.agedec = 0\r(4) 5.agedec = 0\r(5) 6.agedec = 0\r(6) 7.agedec = 0\rF( 6, 40976) = 20.61\rProb \u003e F = 0.0000\rThis general strategy tells us that there is nonlinearity between the age decade and health status but does not pinpoint the exact nature of the nonlinearity. Let’s try another strategy that will pinpoint the nature of the nonlinearity. And use the contrast command with the p. contrast operator to obtain a detailed breakdown of possible nonlinear trends in the relationship between the age decade and health status reg health i.agedec contrast p.agedec Contrasts of marginal linear predictions Margins: asbalanced | df F P\u003eF\ragedec |\r(linear) | 1 1187.58 0.0000\r(quadratic) | 1 26.65 0.0000\r(cubic) | 1 52.99 0.0000\r(quartic) | 1 1.18 0.2778\r(quintic) | 1 1.00 0.3179\r(sextic) | 1 0.15 0.6959\r(septic) | 1 0.02 0.8748\rJoint | 7 451.59 0.0000\r|\rDenominator | 40976\r| Contrast Std. err. [95% conf. interval]\ragedec |\r(linear) | -0.260 0.008 -0.275 -0.245\r(quadratic) | -0.038 0.007 -0.053 -0.024\r(cubic) | 0.047 0.006 0.034 0.059\r(quartic) | 0.006 0.005 -0.005 0.016\r(quintic) | -0.004 0.004 -0.013 0.004\r(sextic) | 0.001 0.004 -0.006 0.009\r(septic) | -0.001 0.004 -0.008 0.006\r","date":"2023-12-09","objectID":"/1.continuous-predictors_linear/:4:2","tags":["Continuous predictors","stata"],"title":"Chapter2 ：Continuous predictors:Linear","uri":"/1.continuous-predictors_linear/"},{"categories":["documentation"],"content":"探索 Hugo - LoveIt 主题的全部内容和背后的核心概念.","date":"2020-03-06","objectID":"/theme-documentation-basics/","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"探索 Hugo - LoveIt 主题的全部内容和背后的核心概念. ","date":"2020-03-06","objectID":"/theme-documentation-basics/:0:0","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"1 准备 由于 Hugo 提供的便利性, Hugo 本身是这个主题唯一的依赖. 直接安装满足你操作系统 (Windows, Linux, macOS) 的最新版本  Hugo (\u003e 0.62.0). 为什么不支持早期版本的 Hugo?\r由于 Markdown 渲染钩子函数 在 Hugo 圣诞节版本 中被引入, 本主题只支持高于 0.62.0 的 Hugo 版本.\r推荐使用 Hugo extended 版本\r由于这个主题的一些特性需要将  SCSS 转换为  CSS, 推荐使用 Hugo extended 版本来获得更好的使用体验.\r","date":"2020-03-06","objectID":"/theme-documentation-basics/:1:0","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"2 安装 以下步骤可帮助你初始化新网站. 如果你根本不了解 Hugo, 我们强烈建议你按照此 快速入门文档 进一步了解它. ","date":"2020-03-06","objectID":"/theme-documentation-basics/:2:0","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"2.1 创建你的项目 Hugo 提供了一个 new 命令来创建一个新的网站: hugo new site my_website cd my_website ","date":"2020-03-06","objectID":"/theme-documentation-basics/:2:1","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"2.2 安装主题 LoveIt 主题的仓库是: https://github.com/dillonzq/LoveIt. 你可以下载主题的 最新版本  .zip 文件 并且解压放到 themes 目录. 另外, 也可以直接把这个主题克隆到 themes 目录: git clone https://github.com/dillonzq/LoveIt.git themes/LoveIt 或者, 初始化你的项目目录为 git 仓库, 并且把主题仓库作为你的网站目录的子模块: git init git submodule add https://github.com/dillonzq/LoveIt.git themes/LoveIt ","date":"2020-03-06","objectID":"/theme-documentation-basics/:2:2","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"2.3 基础配置 以下是 LoveIt 主题的基本配置: baseURL = \"http://example.org/\" # 更改使用 Hugo 构建网站时使用的默认主题 theme = \"LoveIt\" # 网站标题 title = \"我的全新 Hugo 网站\" # 网站语言, 仅在这里 CN 大写 [\"en\", \"zh-CN\", \"fr\", \"pl\", ...] languageCode = \"zh-CN\" # 语言名称 [\"English\", \"简体中文\", \"Français\", \"Polski\", ...] languageName = \"简体中文\" # 是否包括中日韩文字 hasCJKLanguage = true # 作者配置 [author] name = \"xxxx\" email = \"\" link = \"\" # 菜单配置 [menu] [[menu.main]] weight = 1 identifier = \"posts\" # 你可以在名称 (允许 HTML 格式) 之前添加其他信息, 例如图标 pre = \"\" # 你可以在名称 (允许 HTML 格式) 之后添加其他信息, 例如图标 post = \"\" name = \"文章\" url = \"/posts/\" # 当你将鼠标悬停在此菜单链接上时, 将显示的标题 title = \"\" [[menu.main]] weight = 2 identifier = \"tags\" pre = \"\" post = \"\" name = \"标签\" url = \"/tags/\" title = \"\" [[menu.main]] weight = 3 identifier = \"categories\" pre = \"\" post = \"\" name = \"分类\" url = \"/categories/\" title = \"\" # Hugo 解析文档的配置 [markup] # 语法高亮设置 (https://gohugo.io/content-management/syntax-highlighting) [markup.highlight] # false 是必要的设置 (https://github.com/dillonzq/LoveIt/issues/158) noClasses = false 注意\r在构建网站时, 你可以使用 --theme 选项设置主题. 但是, 我建议你修改配置文件 (config.toml) 将本主题设置为默认主题.\r","date":"2020-03-06","objectID":"/theme-documentation-basics/:2:3","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"2.4 创建你的第一篇文章 以下是创建第一篇文章的方法: hugo new posts/first_post.md 通过添加一些示例内容并替换文件开头的标题, 你可以随意编辑文章. 注意\r默认情况下, 所有文章和页面均作为草稿创建. 如果想要渲染这些页面, 请从元数据中删除属性 draft: true, 设置属性 draft: false 或者为 hugo 命令添加 -D/--buildDrafts 参数.\r","date":"2020-03-06","objectID":"/theme-documentation-basics/:2:4","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"2.5 在本地启动网站 使用以下命令启动网站: hugo serve 去查看 http://localhost:1313. 基本配置下的预览\r技巧\r当你运行 hugo serve 时, 当文件内容更改时, 页面会随着更改自动刷新.\r注意\r由于本主题使用了 Hugo 中的 .Scratch 来实现一些特性, 非常建议你为 hugo server 命令添加 --disableFastRender 参数来实时预览你正在编辑的文章页面. hugo serve --disableFastRender ","date":"2020-03-06","objectID":"/theme-documentation-basics/:2:5","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"2.6 构建网站 当你准备好部署你的网站时, 运行以下命令: hugo 会生成一个 public 目录, 其中包含你网站的所有静态内容和资源. 现在可以将其部署在任何 Web 服务器上. 技巧\r网站内容可以通过 Netlify 自动发布和托管 (了解有关通过 Netlify 进行 HUGO 自动化部署 的更多信息). 或者, 您可以使用 AWS Amplify, Github pages, Render 以及更多…\r","date":"2020-03-06","objectID":"/theme-documentation-basics/:2:6","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"3 配置 ","date":"2020-03-06","objectID":"/theme-documentation-basics/:3:0","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"3.1 网站配置 除了 Hugo 全局配置 和 菜单配置 之外, LoveIt 主题还允许您在网站配置中定义以下参数 (这是一个示例 config.toml, 其内容为默认值). 请打开下面的代码块查看完整的示例配置 : baseURL = \"http://example.org/\" # 更改使用 Hugo 构建网站时使用的默认主题 theme = \"LoveIt\" # 网站标题 title = \"我的全新 Hugo 网站\" # 网站语言, 仅在这里 CN 大写 [\"en\", \"zh-CN\", \"fr\", \"pl\", ...] languageCode = \"zh-CN\" # 语言名称 [\"English\", \"简体中文\", \"Français\", \"Polski\", ...] languageName = \"简体中文\" # 是否包括中日韩文字 hasCJKLanguage = true # 默认每页列表显示的文章数目 paginate = 12 # 谷歌分析代号 [UA-XXXXXXXX-X] googleAnalytics = \"\" # 版权描述，仅仅用于 SEO copyright = \"\" # 是否使用 robots.txt enableRobotsTXT = true # 是否使用 git 信息 enableGitInfo = true # 是否使用 emoji 代码 enableEmoji = true # 忽略一些构建错误 ignoreErrors = [\"error-remote-getjson\", \"error-missing-instagram-accesstoken\"] # 作者配置 [author] name = \"xxxx\" email = \"\" link = \"\" # 菜单配置 [menu] [[menu.main]] weight = 1 identifier = \"posts\" # 你可以在名称 (允许 HTML 格式) 之前添加其他信息, 例如图标 pre = \"\" # 你可以在名称 (允许 HTML 格式) 之后添加其他信息, 例如图标 post = \"\" name = \"文章\" url = \"/posts/\" # 当你将鼠标悬停在此菜单链接上时, 将显示的标题 title = \"\" [[menu.main]] weight = 2 identifier = \"tags\" pre = \"\" post = \"\" name = \"标签\" url = \"/tags/\" title = \"\" [[menu.main]] weight = 3 identifier = \"categories\" pre = \"\" post = \"\" name = \"分类\" url = \"/categories/\" title = \"\" [params] # 网站默认主题样式 [\"auto\", \"light\", \"dark\"] defaultTheme = \"auto\" # 公共 git 仓库路径，仅在 enableGitInfo 设为 true 时有效 gitRepo = \"\" # 哪种哈希函数用来 SRI, 为空时表示不使用 SRI # [\"sha256\", \"sha384\", \"sha512\", \"md5\"] fingerprint = \"\" # 日期格式 dateFormat = \"2006-01-02\" # 网站标题, 用于 Open Graph 和 Twitter Cards title = \"我的网站\" # 网站描述, 用于 RSS, SEO, Open Graph 和 Twitter Cards description = \"这是我的全新 Hugo 网站\" # 网站图片, 用于 Open Graph 和 Twitter Cards images = [\"/logo.png\"] # 页面头部导航栏配置 [params.header] # 桌面端导航栏模式 [\"fixed\", \"normal\", \"auto\"] desktopMode = \"fixed\" # 移动端导航栏模式 [\"fixed\", \"normal\", \"auto\"] mobileMode = \"auto\" # 页面头部导航栏标题配置 [params.header.title] # LOGO 的 URL logo = \"\" # 标题名称 name = \"\" # 你可以在名称 (允许 HTML 格式) 之前添加其他信息, 例如图标 pre = \"\" # 你可以在名称 (允许 HTML 格式) 之后添加其他信息, 例如图标 post = \"\" # 是否为标题显示打字机动画 typeit = false # 页面底部信息配置 [params.footer] enable = true # 自定义内容 (支持 HTML 格式) custom = '' # 是否显示 Hugo 和主题信息 hugo = true # 是否显示版权信息 copyright = true # 是否显示作者 author = true # 网站创立年份 since = 2019 # ICP 备案信息，仅在中国使用 (支持 HTML 格式) icp = \"\" # 许可协议信息 (支持 HTML 格式) license = '\u003ca rel=\"license external nofollow noopener noreffer\" href=\"https://creativecommons.org/licenses/by-nc/4.0/\" target=\"_blank\"\u003eCC BY-NC 4.0\u003c/a\u003e' # Section (所有文章) 页面配置 [params.section] # section 页面每页显示文章数量 paginate = 20 # 日期格式 (月和日) dateFormat = \"01-02\" # RSS 文章数目 rss = 10 # List (目录或标签) 页面配置 [params.list] # list 页面每页显示文章数量 paginate = 20 # 日期格式 (月和日) dateFormat = \"01-02\" # RSS 文章数目 rss = 10 # 应用图标配置 [params.app] # 当添加到 iOS 主屏幕或者 Android 启动器时的标题, 覆盖默认标题 title = \"我的网站\" # 是否隐藏网站图标资源链接 noFavicon = false # 更现代的 SVG 网站图标, 可替代旧的 .png 和 .ico 文件 svgFavicon = \"\" # Android 浏览器主题色 themeColor = \"#ffffff\" # Safari 图标颜色 iconColor = \"#5bbad5\" # Windows v8-10磁贴颜色 tileColor = \"#da532c\" # 搜索配置 [params.search] enable = true # 搜索引擎的类型 [\"lunr\", \"algolia\"] type = \"lunr\" # 文章内容最长索引长度 contentLength = 4000 # 搜索框的占位提示语 placeholder = \"\" # 最大结果数目 maxResultLength = 10 # 结果内容片段长度 snippetLength = 50 # 搜索结果中高亮部分的 HTML 标签 highlightTag = \"em\" # 是否在搜索索引中使用基于 baseURL 的绝对路径 absoluteURL = false [params.search.algolia] index = \"\" appID = \"\" searchKey = \"\" # 主页配置 [params.home] # RSS 文章数目 rss = 10 # 主页个人信息 [params.home.profile] enable = true # Gravatar 邮箱，用于优先在主页显示的头像 gravatarEmail = \"\" # 主页显示头像的 URL avatarURL = \"/images/avatar.png\" # 主页显示的网站标题 (支持 HTML 格式) title = \"\" # 主页显示的网站副标题 (允许 HTML 格式) subtitle = \"这是我的全新 Hugo 网站\" # 是否为副标题显示打字机动画 typeit = true # 是否显示社交账号 social = true # 免责声明 (支持 HTML 格式) disclaimer = \"\" # 主页文章列表 [params.home.posts] enable = true # 主页每页显示文章数量 paginate = 6 # 被 params.page 中的 hiddenFromHomePage 替代 # 当你没有在文章前置参数中设置 \"hiddenFromHomePage\" 时的默认行为 defaultHiddenFromHomePage = false # 作者的社交信息设置 [params.social] GitHub = \"xxxx\" Linkedin = \"\" Twitter = \"xxxx\" Instagram = \"xxxx\" Facebook = \"xxxx\" Telegram = \"xxxx\" Medium = \"\" Gitlab = \"\" Youtubelegacy = \"\" Youtubecustom = \"\" Youtu","date":"2020-03-06","objectID":"/theme-documentation-basics/:3:1","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"3.2 网站图标, 浏览器配置, 网站清单 强烈建议你把: apple-touch-icon.png (180x180) favicon-32x32.png (32x32) favicon-16x16.png (16x16) mstile-150x150.png (150x150) android-chrome-192x192.png (192x192) android-chrome-512x512.png (512x512) 放在 /static 目录. 利用 https://realfavicongenerator.net/ 可以很容易地生成这些文件. 可以自定义 browserconfig.xml 和 site.webmanifest 文件来设置 theme-color 和 background-color. ","date":"2020-03-06","objectID":"/theme-documentation-basics/:3:2","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"3.3 自定义样式 注意\rHugo extended 版本对于自定义样式是必需的.\r通过定义自定义 .scss 样式文件, LoveIt 主题支持可配置的样式. 包含自定义 .scss 样式文件的目录相对于 你的项目根目录 的路径为 assets/css. 在 assets/css/_override.scss 中, 你可以覆盖 themes/LoveIt/assets/css/_variables.scss 中的变量以自定义样式. 这是一个例子: @import url('https://fonts.googleapis.com/css?family=Fira+Mono:400,700\u0026display=swap\u0026subset=latin-ext'); $code-font-family: Fira Mono, Source Code Pro, Menlo, Consolas, Monaco, monospace; 在 assets/css/_custom.scss 中, 你可以添加一些 CSS 样式代码以自定义样式. ","date":"2020-03-06","objectID":"/theme-documentation-basics/:3:3","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"4 多语言和 i18n LoveIt 主题完全兼容 Hugo 的多语言模式, 并且支持在网页上切换语言. 语言切换\r","date":"2020-03-06","objectID":"/theme-documentation-basics/:4:0","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"4.1 兼容性 语言 Hugo 代码 HTML lang 属性 主题文档 Lunr.js 支持 英语 en en 简体中文 zh-cn zh-CN 繁體中文 zh-tw zh-TW 法语 fr fr 波兰语 pl pl 巴西葡萄牙语 pt-br pt-BR 意大利语 it it 西班牙语 es es 德语 de de 塞尔维亚语 pl pl 俄语 ru ru 罗马尼亚语 ro ro 越南语 vi vi 阿拉伯语 ar ar 加泰罗尼亚语 ca ca 泰语 th th 泰卢固语 te te 印尼语 id id 土耳其语 tr tr 韩语 ko ko 印地语 hi hi ","date":"2020-03-06","objectID":"/theme-documentation-basics/:4:1","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"4.2 基本配置 学习了 Hugo如何处理多语言网站 之后, 请在 站点配置 中定义你的网站语言. 例如, 一个支持英语, 中文和法语的网站配置: # 设置默认的语言 [\"en\", \"zh-cn\", \"fr\", \"pl\", ...] defaultContentLanguage = \"zh-cn\" [languages] [languages.en] weight = 1 title = \"My New Hugo Site\" languageCode = \"en\" languageName = \"English\" [[languages.en.menu.main]] weight = 1 identifier = \"posts\" pre = \"\" post = \"\" name = \"Posts\" url = \"/posts/\" title = \"\" [[languages.en.menu.main]] weight = 2 identifier = \"tags\" pre = \"\" post = \"\" name = \"Tags\" url = \"/tags/\" title = \"\" [[languages.en.menu.main]] weight = 3 identifier = \"categories\" pre = \"\" post = \"\" name = \"Categories\" url = \"/categories/\" title = \"\" [languages.zh-cn] weight = 2 title = \"我的全新 Hugo 网站\" languageCode = \"zh-CN\" languageName = \"简体中文\" hasCJKLanguage = true [[languages.zh-cn.menu.main]] weight = 1 identifier = \"posts\" pre = \"\" post = \"\" name = \"文章\" url = \"/posts/\" title = \"\" [[languages.zh-cn.menu.main]] weight = 2 identifier = \"tags\" pre = \"\" post = \"\" name = \"标签\" url = \"/tags/\" title = \"\" [[languages.zh-cn.menu.main]] weight = 3 identifier = \"categories\" pre = \"\" post = \"\" name = \"分类\" url = \"/categories/\" title = \"\" [languages.fr] weight = 3 title = \"Mon nouveau site Hugo\" languageCode = \"fr\" languageName = \"Français\" [[languages.fr.menu.main]] weight = 1 identifier = \"posts\" pre = \"\" post = \"\" name = \"Postes\" url = \"/posts/\" title = \"\" [[languages.fr.menu.main]] weight = 2 identifier = \"tags\" pre = \"\" post = \"\" name = \"Balises\" url = \"/tags/\" title = \"\" [[languages.fr.menu.main]] weight = 3 identifier = \"categories\" pre = \"\" post = \"\" name = \"Catégories\" url = \"/categories/\" title = \"\" 然后, 对于每个新页面, 将语言代码附加到文件名中. 单个文件 my-page.md 需要分为三个文件: 英语: my-page.en.md 中文: my-page.zh-cn.md 法语: my-page.fr.md 注意\r请注意, 菜单中仅显示翻译的页面. 它不会替换为默认语言内容.\r技巧\r也可以使用 文章前置参数 来翻译网址.\r","date":"2020-03-06","objectID":"/theme-documentation-basics/:4:2","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"4.3 修改默认的翻译字符串 翻译字符串用于在主题中使用的常见默认值. 目前提供一些语言的翻译, 但你可能自定义其他语言或覆盖默认值. 要覆盖默认值, 请在你项目的 i18n 目录 i18n/\u003clanguageCode\u003e.toml 中创建一个新文件，并从 themes/LoveIt/i18n/en.toml 中获得提示. 另外, 由于你的翻译可能会帮助到其他人, 请花点时间通过  创建一个 PR 来贡献主题翻译, 谢谢! ","date":"2020-03-06","objectID":"/theme-documentation-basics/:4:3","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"5 搜索 基于 Lunr.js 或 algolia, LoveIt 主题支持搜索功能. ","date":"2020-03-06","objectID":"/theme-documentation-basics/:5:0","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"5.1 输出配置 为了生成搜索功能所需要的 index.json, 请在你的 网站配置 中添加 JSON 输出文件类型到 outputs 部分的 home 字段中. [outputs] home = [\"HTML\", \"RSS\", \"JSON\"] ","date":"2020-03-06","objectID":"/theme-documentation-basics/:5:1","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"5.2 搜索配置 基于 Hugo 生成的 index.json 文件, 你可以激活搜索功能. 这是你的 网站配置 中的搜索部分: [params.search] enable = true # 搜索引擎的类型 [\"lunr\", \"algolia\"] type = \"lunr\" # 文章内容最长索引长度 contentLength = 4000 # 搜索框的占位提示语 placeholder = \"\" # 最大结果数目 maxResultLength = 10 # 结果内容片段长度 snippetLength = 50 # 搜索结果中高亮部分的 HTML 标签 highlightTag = \"em\" # 是否在搜索索引中使用基于 baseURL 的绝对路径 absoluteURL = false [params.search.algolia] index = \"\" appID = \"\" searchKey = \"\" 怎样选择搜索引擎?\r以下是两种搜索引擎的对比: lunr: 简单, 无需同步 index.json, 没有 contentLength 的限制, 但占用带宽大且性能低 (特别是中文需要一个较大的分词依赖库) algolia: 高性能并且占用带宽低, 但需要同步 index.json 且有 contentLength 的限制 文章内容被 h2 和 h3 HTML 标签切分来提高查询效果并且基本实现全文搜索. contentLength 用来限制 h2 和 h3 HTML 标签开头的内容部分的最大长度. 关于 algolia 的使用技巧\r你需要上传 index.json 到 algolia 来激活搜索功能. 你可以使用浏览器来上传 index.json 文件但是一个自动化的脚本可能效果更好. 官方提供的 Algolia CLI 是一个不错的选择. 为了兼容 Hugo 的多语言模式, 你需要上传不同语言的 index.json 文件到对应的 algolia index, 例如 zh-cn/index.json 或 fr/index.json…\r","date":"2020-03-06","objectID":"/theme-documentation-basics/:5:2","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"了解如何在 LoveIt 主题中快速, 直观地创建和组织内容.","date":"2020-03-05","objectID":"/theme-documentation-content/","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"了解如何在 LoveIt 主题中快速, 直观地创建和组织内容. ","date":"2020-03-05","objectID":"/theme-documentation-content/:0:0","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"1 内容组织 以下是一些方便你清晰管理和生成文章的目录结构建议: 保持博客文章存放在 content/posts 目录, 例如: content/posts/我的第一篇文章.md 保持简单的静态页面存放在 content 目录, 例如: content/about.md 本地资源组织 本地资源引用\r有三种方法来引用图片和音乐等本地资源: 使用页面包中的页面资源. 你可以使用适用于 Resources.GetMatch 的值或者直接使用相对于当前页面目录的文件路径来引用页面资源. 将本地资源放在 assets 目录中, 默认路径是 /assets. 引用资源的文件路径是相对于 assets 目录的. 将本地资源放在 static 目录中, 默认路径是 /static. 引用资源的文件路径是相对于 static 目录的. 引用的优先级符合以上的顺序. 在这个主题中的很多地方可以使用上面的本地资源引用, 例如 链接, 图片, image shortcode, music shortcode 和前置参数中的部分参数. 页面资源或者 assets 目录中的图片处理会在未来的版本中得到支持. 非常酷的功能! ","date":"2020-03-05","objectID":"/theme-documentation-content/:1:0","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"2 前置参数 Hugo 允许你在文章内容前面添加 yaml, toml 或者 json 格式的前置参数. 注意\r不是所有的以下前置参数都必须在你的每篇文章中设置. 只有在文章的参数和你的 网站设置 中的 page 部分不一致时才有必要这么做.\r这是一个前置参数例子: --- title: \"我的第一篇文章\" subtitle: \"\" date: 2020-03-04T15:58:26+08:00 lastmod: 2020-03-04T15:58:26+08:00 draft: true author: \"\" authorLink: \"\" description: \"\" license: \"\" images: [] tags: [] categories: [] featuredImage: \"\" featuredImagePreview: \"\" hiddenFromHomePage: false hiddenFromSearch: false twemoji: false lightgallery: true ruby: true fraction: true fontawesome: true linkToMarkdown: true rssFullText: false toc: enable: true auto: true code: copy: true maxShownLines: 50 math: enable: false # ... mapbox: # ... share: enable: true # ... comment: enable: true # ... library: css: # someCSS = \"some.css\" # 位于 \"assets/\" # 或者 # someCSS = \"https://cdn.example.com/some.css\" js: # someJS = \"some.js\" # 位于 \"assets/\" # 或者 # someJS = \"https://cdn.example.com/some.js\" seo: images: [] # ... --- title: 文章标题. subtitle: 文章副标题. date: 这篇文章创建的日期时间. 它通常是从文章的前置参数中的 date 字段获取的, 但是也可以在 网站配置 中设置. lastmod: 上次修改内容的日期时间. draft: 如果设为 true, 除非 hugo 命令使用了 --buildDrafts/-D 参数, 这篇文章不会被渲染. author: 文章作者. authorLink: 文章作者的链接. description: 文章内容的描述. license: 这篇文章特殊的许可. images: 页面图片, 用于 Open Graph 和 Twitter Cards. tags: 文章的标签. categories: 文章所属的类别. featuredImage: 文章的特色图片. featuredImagePreview: 用在主页预览的文章特色图片. hiddenFromHomePage: 如果设为 true, 这篇文章将不会显示在主页上. hiddenFromSearch: 如果设为 true, 这篇文章将不会显示在搜索结果中. twemoji: 如果设为 true, 这篇文章会使用 twemoji. lightgallery: 如果设为 true, 文章中的图片将可以按照画廊形式呈现. ruby: 如果设为 true, 这篇文章会使用 上标注释扩展语法. fraction: 如果设为 true, 这篇文章会使用 分数扩展语法. fontawesome: 如果设为 true, 这篇文章会使用 Font Awesome 扩展语法. linkToMarkdown: 如果设为 true, 内容的页脚将显示指向原始 Markdown 文件的链接. rssFullText: 如果设为 true, 在 RSS 中将会显示全文内容. toc: 和 网站配置 中的 params.page.toc 部分相同. code: 和 网站配置 中的 params.page.code 部分相同. math: 和 网站配置 中的 params.page.math 部分相同. mapbox: 和 网站配置 中的 params.page.mapbox 部分相同. share: 和 网站配置 中的 params.page.share 部分相同. comment: 和 网站配置 中的 params.page.comment 部分相同. library: 和 网站配置 中的 params.page.library 部分相同. seo: 和 网站配置 中的 params.page.seo 部分相同. 技巧\rfeaturedImage 和 featuredImagePreview 支持本地资源引用的完整用法. 如果带有在前置参数中设置了 name: featured-image 或 name: featured-image-preview 属性的页面资源, 没有必要在设置 featuredImage 或 featuredImagePreview: resources: - name: featured-image src: featured-image.jpg - name: featured-image-preview src: featured-image-preview.jpg ","date":"2020-03-05","objectID":"/theme-documentation-content/:2:0","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"3 内容摘要 LoveIt 主题使用内容摘要在主页中显示大致文章信息。Hugo 支持生成文章的摘要. 文章摘要预览\r","date":"2020-03-05","objectID":"/theme-documentation-content/:3:0","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"自动摘要拆分 默认情况下, Hugo 自动将内容的前 70 个单词作为摘要. 你可以通过在 网站配置 中设置 summaryLength 来自定义摘要长度. 如果您要使用 CJK中文/日语/韩语 语言创建内容, 并且想使用 Hugo 的自动摘要拆分功能，请在 网站配置 中将 hasCJKLanguage 设置为 true. ","date":"2020-03-05","objectID":"/theme-documentation-content/:3:1","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"手动摘要拆分 另外, 你也可以添加 \u003c!--more--\u003e 摘要分割符来拆分文章生成摘要. 摘要分隔符之前的内容将用作该文章的摘要. 注意\r请小心输入\u003c!--more--\u003e ; 即全部为小写且没有空格.\r","date":"2020-03-05","objectID":"/theme-documentation-content/:3:2","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"前置参数摘要 你可能希望摘要不是文章开头的文字. 在这种情况下, 你可以在文章前置参数的 summary 变量中设置单独的摘要. ","date":"2020-03-05","objectID":"/theme-documentation-content/:3:3","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"使用文章描述作为摘要 你可能希望将文章前置参数中的 description 变量的内容作为摘要. 你仍然需要在文章开头添加 \u003c!--more--\u003e 摘要分割符. 将摘要分隔符之前的内容保留为空. 然后 LoveIt 主题会将你的文章描述作为摘要. ","date":"2020-03-05","objectID":"/theme-documentation-content/:3:4","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"摘要选择的优先级顺序 由于可以通过多种方式指定摘要, 因此了解顺序很有用. 如下: 如果文章中有 \u003c!--more--\u003e 摘要分隔符, 但分隔符之前没有内容, 则使用描述作为摘要. 如果文章中有 \u003c!--more--\u003e 摘要分隔符, 则将按照手动摘要拆分的方法获得摘要. 如果文章前置参数中有摘要变量, 那么将以该值作为摘要. 按照自动摘要拆分方法. 注意\r不建议在摘要内容中包含富文本块元素, 这会导致渲染错误. 例如代码块, 图片, 表格等.\r","date":"2020-03-05","objectID":"/theme-documentation-content/:3:5","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"4 Markdown 基本语法 这部分内容在 Markdown 基本语法页面 中介绍. ","date":"2020-03-05","objectID":"/theme-documentation-content/:4:0","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"5 Markdown 扩展语法 LoveIt 主题提供了一些扩展的语法便于你撰写文章. ","date":"2020-03-05","objectID":"/theme-documentation-content/:5:0","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"Emoji 支持 这部分内容在 Emoji 支持页面 中介绍. ","date":"2020-03-05","objectID":"/theme-documentation-content/:5:1","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"数学公式 LoveIt 基于 $\\KaTeX$ 提供数学公式的支持. 在你的 网站配置 中的 [params.math] 下面设置属性 enable = true, 并在文章的前置参数中设置属性 math: true来启用数学公式的自动渲染. $\\KaTeX$ 根据 特定的分隔符 来自动渲染公式. 技巧\r有一份 $\\KaTeX$ 中支持的 $\\TeX$ 函数 清单.\r注意\r由于 Hugo 在渲染 Markdown 文档时会根据 _/*/\u003e\u003e 之类的语法生成 HTML 文档, 并且有些转义字符形式的文本内容 (如 \\(/\\)/\\[/\\]/\\\\) 会自动进行转义处理, 因此需要对这些地方进行额外的转义字符表达来实现自动渲染: _ -\u003e \\_ * -\u003e \\* \u003e\u003e -\u003e \\\u003e\u003e \\( -\u003e \\\\( \\) -\u003e \\\\) \\[ -\u003e \\\\[ \\] -\u003e \\\\] \\\\ -\u003e \\\\\\\\ LoveIt 主题支持 raw shortcode 以避免这些转义字符, 它可以帮助您编写原始数学公式内容. 一个 raw 示例: 行内公式: 公式块: 呈现的输出效果如下: 行内公式: 公式块: 行内公式 默认的行内公式分割符有: $ ... $ \\( ... \\) (转义的: \\\\( ... \\\\)) 例如: $c = \\pm\\sqrt{a^2 + b^2}$ 和 \\\\(f(x)=\\int_{-\\infty}^{\\infty} \\hat{f}(\\xi) e^{2 \\pi i \\xi x} d \\xi\\\\) 呈现的输出效果如下: $c = \\pm\\sqrt{a^2 + b^2}$ 和 \\(f(x)=\\int_{-\\infty}^{\\infty} \\hat{f}(\\xi) e^{2 \\pi i \\xi x} d \\xi\\) 公式块 默认的公式块分割符有: $$ ... $$ \\[ ... \\] (转义的: \\\\[ ... \\\\]) \\begin{equation} ... \\end{equation} (不编号的: \\begin{equation*} ... \\end{equation*}) \\begin{align} ... \\end{align} (不编号的: \\begin{align*} ... \\end{align*}) \\begin{alignat} ... \\end{alignat} (不编号的: \\begin{alignat*} ... \\end{alignat*}) \\begin{gather} ... \\end{gather} (不编号的: \\begin{gather*} ... \\end{gather*}) \\begin{CD} ... \\end{CD} 例如: $$ c = \\pm\\sqrt{a^2 + b^2} $$ \\\\[ f(x)=\\int_{-\\infty}^{\\infty} \\hat{f}(\\xi) e^{2 \\pi i \\xi x} d \\xi \\\\] \\begin{equation*} \\rho \\frac{\\mathrm{D} \\mathbf{v}}{\\mathrm{D} t}=\\nabla \\cdot \\mathbb{P}+\\rho \\mathbf{f} \\end{equation*} \\begin{equation} \\mathbf{E}=\\sum_{i} \\mathbf{E}\\_{i}=\\mathbf{E}\\_{1}+\\mathbf{E}\\_{2}+\\mathbf{E}_{3}+\\cdots \\end{equation} \\begin{align} a\u0026=b+c \\\\\\\\ d+e\u0026=f \\end{align} \\begin{alignat}{2} 10\u0026x+\u00263\u0026y = 2 \\\\\\\\ 3\u0026x+\u002613\u0026y = 4 \\end{alignat} \\begin{gather} a=b \\\\\\\\ e=b+c \\end{gather} \\begin{CD} A @\u003ea\\\u003e\u003e B \\\\\\\\ @VbVV @AAcA \\\\\\\\ C @= D \\end{CD} 呈现的输出效果如下: $$ c = \\pm\\sqrt{a^2 + b^2} $$ \\[ f(x)=\\int_{-\\infty}^{\\infty} \\hat{f}(\\xi) e^{2 \\pi i \\xi x} d \\xi \\] \\begin{equation*} \\rho \\frac{\\mathrm{D} \\mathbf{v}}{\\mathrm{D} t}=\\nabla \\cdot \\mathbb{P}+\\rho \\mathbf{f} \\end{equation*} \\begin{equation} \\mathbf{E}=\\sum_{i} \\mathbf{E}_{i}=\\mathbf{E}_{1}+\\mathbf{E}_{2}+\\mathbf{E}_{3}+\\cdots \\end{equation} \\begin{align} a\u0026=b+c \\\\ d+e\u0026=f \\end{align} \\begin{alignat}{2} 10\u0026x+\u00263\u0026y = 2 \\\\ 3\u0026x+\u002613\u0026y = 4 \\end{alignat} \\begin{gather} a=b \\\\ e=b+c \\end{gather} \\begin{CD} A @\u003ea\u003e\u003e B \\\\ @VbVV @AAcA \\\\ C @= D \\end{CD} 技巧\r你可以在 网站配置 中自定义行内公式和公式块的分割符.\rCopy-tex Copy-tex 是一个 $\\KaTeX$ 的插件. 通过这个扩展, 在选择并复制 $\\KaTeX$ 渲染的公式时, 会将其 $\\LaTeX$ 源代码复制到剪贴板. 在你的 网站配置 中的 [params.math] 下面设置属性 copyTex = true 来启用 Copy-tex. 选择并复制上一节中渲染的公式, 可以发现复制的内容为 $\\LaTeX$ 源代码. mhchem mhchem 是一个 $\\KaTeX$ 的插件. 通过这个扩展, 你可以在文章中轻松编写漂亮的化学方程式. 在你的 网站配置 中的 [params.math] 下面设置属性 mhchem = true 来启用 mhchem. $$ \\ce{CO2 + C -\u003e 2 CO} $$ $$ \\ce{Hg^2+ -\u003e[I-] HgI2 -\u003e[I-] [Hg^{II}I4]^2-} $$ 呈现的输出效果如下: $$ \\ce{CO2 + C -\u003e 2 CO} $$ $$ \\ce{Hg^2+ -\u003e[I-] HgI2 -\u003e[I-] [Hg^{II}I4]^2-} $$ ","date":"2020-03-05","objectID":"/theme-documentation-content/:5:2","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"字符注音或者注释 LoveIt 主题支持一种 字符注音或者注释 Markdown 扩展语法: [Hugo]^(一个开源的静态网站生成工具) 呈现的输出效果如下: Hugo一个开源的静态网站生成工具 ","date":"2020-03-05","objectID":"/theme-documentation-content/:5:3","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"分数 LoveIt 主题支持一种 分数 Markdown 扩展语法: [浅色]/[深色] [99]/[100] 呈现的输出效果如下: 浅色/深色 90/100 ","date":"2020-03-05","objectID":"/theme-documentation-content/:5:4","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"Font Awesome LoveIt 主题使用 Font Awesome 作为图标库. 你同样可以在文章中轻松使用这些图标. 从 Font Awesome 网站 上获取所需的图标 class. 去露营啦! :(fas fa-campground fa-fw): 很快就回来. 真开心! :(far fa-grin-tears): 呈现的输出效果如下: 去露营啦!  很快就回来. 真开心! ","date":"2020-03-05","objectID":"/theme-documentation-content/:5:5","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"转义字符 在某些特殊情况下 (编写这个主题文档时 ), 你的文章内容会与 Markdown 的基本或者扩展语法冲突, 并且无法避免. 转义字符语法可以帮助你渲染出想要的内容: 呈现的输出效果如下: :joy: 而不是 😂 技巧\r这个方法可以间接解决一个还未解决的 Hugo 的 issue.\r","date":"2020-03-05","objectID":"/theme-documentation-content/:5:6","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"Hugo 提供了多个内置的 Shortcodes, 以方便作者保持 Markdown 内容的整洁.","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"Hugo 提供了多个内置的 Shortcodes, 以方便作者保持 Markdown 内容的整洁. Hugo 使用 Markdown 为其简单的内容格式. 但是, Markdown 在很多方面都无法很好地支持. 你可以使用纯 HTML 来扩展可能性. 但这恰好是一个坏主意. 大家使用 Markdown, 正是因为它即使不经过渲染也可以轻松阅读. 应该尽可能避免使用 HTML 以保持内容简洁. 为了避免这种限制, Hugo 创建了 shortcodes. shortcode 是一个简单代码段, 可以生成合理的 HTML 代码, 并且符合 Markdown 的设计哲学. Hugo 附带了一组预定义的 shortcodes, 它们实现了一些非常常见的用法. 提供这些 shortcodes 是为了方便保持你的 Markdown 内容简洁. ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:0:0","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"1 figure figure 的文档 一个 figure 示例: {{\u003c figure src=\"/images/lighthouse.jpg\" title=\"Lighthouse (figure)\" \u003e}} 呈现的输出效果如下: Lighthouse (figure) 输出的 HTML 看起来像这样: \u003cfigure\u003e \u003cimg src=\"/images/lighthouse.jpg\"/\u003e \u003cfigcaption\u003e \u003ch4\u003eLighthouse (figure)\u003c/h4\u003e \u003c/figcaption\u003e \u003c/figure\u003e ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:1:0","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"2 gist gist 的文档 一个 gist 示例: {{\u003c gist spf13 7896402 \u003e}} 呈现的输出效果如下: 输出的 HTML 看起来像这样: \u003cscript type=\"application/javascript\" src=\"https://gist.github.com/spf13/7896402.js\"\u003e\u003c/script\u003e ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:2:0","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"3 highlight highlight 的文档 一个 highlight 示例: {{\u003c highlight html \u003e}} \u003csection id=\"main\"\u003e \u003cdiv\u003e \u003ch1 id=\"title\"\u003e{{ .Title }}\u003c/h1\u003e {{ range .Pages }} {{ .Render \"summary\"}} {{ end }} \u003c/div\u003e \u003c/section\u003e {{\u003c /highlight \u003e}} 呈现的输出效果如下: \u003csection id=\"main\"\u003e \u003cdiv\u003e \u003ch1 id=\"title\"\u003e{{ .Title }}\u003c/h1\u003e {{ range .Pages }} {{ .Render \"summary\"}} {{ end }} \u003c/div\u003e \u003c/section\u003e ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:3:0","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"4 instagram instagram 的文档 Instagram’s API was deprecated since October 24th, 2020\rThe instagram-shortcode refers an endpoint of Instagram’s API, that’s deprecated since October 24th, 2020. Thus, no images can be fetched from this API endpoint, resulting in an error when the instagram-shortcode is used. For more information please have a look at GitHub issue #7879.\r","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:4:0","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"5 param param 的文档 一个 param 示例: {{\u003c param description \u003e}} 呈现的输出效果如下: Hugo 提供了多个内置的 Shortcodes, 以方便作者保持 Markdown 内容的整洁. ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:5:0","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"6 ref 和 relref ref 和 relref 的文档 ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:6:0","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"7 tweet tweet 的文档 一个 tweet 示例: {{\u003c tweet 917359331535966209 \u003e}} 呈现的输出效果如下: ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:7:0","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"8 vimeo vimeo 的文档 一个 vimeo 示例: {{\u003c vimeo 146022717 \u003e}} 呈现的输出效果如下: ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:8:0","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"9 youtube youtube 的文档 一个 youtube 示例: {{\u003c youtube w7Ft2ymGmfc \u003e}} 呈现的输出效果如下: ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:9:0","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"LoveIt 主题在 Hugo 内置的 shortcode 的基础上提供多个扩展的 shortcode.","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"LoveIt 主题在 Hugo 内置的 shortcode 的基础上提供多个扩展的 shortcode. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:0:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"1 style 注意\rHugo extended 版本对于 style shortcode 是必需的.\rstyle shortcode 用来在你的文章中插入自定义样式. style shortcode 有两个位置参数. 第一个参数是自定义样式的内容. 它支持  SASS 中的嵌套语法, 并且 \u0026 指代这个父元素. 第二个参数是包裹你要更改样式的内容的 HTML 标签, 默认值是 div. 一个 style 示例: ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:1:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"2 link link shortcode 是 Markdown 链接语法 的替代. link shortcode 可以提供一些其它的功能并且可以在代码块中使用. 支持本地资源引用的完整用法. link shortcode 有以下命名参数: href [必需] (第一个位置参数) 链接的目标. content [可选] (第二个位置参数) 链接的内容, 默认值是 href 参数的值. 支持 Markdown 或者 HTML 格式. title [可选] (第三个位置参数) HTML a 标签 的 title 属性, 当悬停在链接上会显示的提示. rel [可选] HTML a 标签 的 rel 补充属性. class [可选] HTML a 标签 的 class 属性. 一个 link 示例: {{\u003c link \"https://assemble.io\" \u003e}} 或者 {{\u003c link href=\"https://assemble.io\" \u003e}} {{\u003c link \"mailto:contact@revolunet.com\" \u003e}} 或者 {{\u003c link href=\"mailto:contact@revolunet.com\" \u003e}} {{\u003c link \"https://assemble.io\" Assemble \u003e}} 或者 {{\u003c link href=\"https://assemble.io\" content=Assemble \u003e}} 呈现的输出效果如下: https://assemble.io mailto:contact@revolunet.com Assemble 一个带有标题的 link 示例: {{\u003c link \"https://github.com/upstage/\" Upstage \"Visit Upstage!\" \u003e}} 或者 {{\u003c link href=\"https://github.com/upstage/\" content=Upstage title=\"Visit Upstage!\" \u003e}} 呈现的输出效果如下 (将鼠标悬停在链接上，会有一行提示): Upstage ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:2:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"3 image image shortcode 是 figure shortcode 的替代. image shortcode 可以充分利用 lazysizes 和 lightGallery 两个依赖库. 支持本地资源引用的完整用法. image shortcode 有以下命名参数: src [必需] (第一个位置参数) 图片的 URL. alt [可选] (第二个位置参数) 图片无法显示时的替代文本, 默认值是 src 参数的值. 支持 Markdown 或者 HTML 格式. caption [可选] (第三个位置参数) 图片标题. 支持 Markdown 或者 HTML 格式. title [可选] 当悬停在图片上会显示的提示. class [可选] HTML figure 标签的 class 属性. src_s [可选] 图片缩略图的 URL, 用在画廊模式中, 默认值是 src 参数的值. src_l [可选] 高清图片的 URL, 用在画廊模式中, 默认值是 src 参数的值. height [可选] 图片的 height 属性. width [可选] 图片的 width 属性. linked [可选] 图片是否需要被链接, 默认值是 true. rel [可选] HTML a 标签 的 rel 补充属性, 仅在 linked 属性设置成 true 时有效. 一个 image 示例: {{\u003c image src=\"/images/lighthouse.jpg\" caption=\"Lighthouse (`image`)\" src_s=\"/images/lighthouse-small.jpg\" src_l=\"/images/lighthouse-large.jpg\" \u003e}} 呈现的输出效果如下: Lighthouse (image)\r","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:3:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"4 admonition admonition shortcode 支持 12 种 帮助你在页面中插入提示的横幅. 支持 Markdown 或者 HTML 格式. 注意\r一个 注意 横幅\r摘要\r一个 摘要 横幅\r信息\r一个 信息 横幅\r技巧\r一个 技巧 横幅\r成功\r一个 成功 横幅\r问题\r一个 问题 横幅\r警告\r一个 警告 横幅\r失败\r一个 失败 横幅\r危险\r一个 危险 横幅\rBug\r一个 Bug 横幅\r示例\r一个 示例 横幅\r引用\r一个 引用 横幅\radmonition shortcode 有以下命名参数: type [可选] (第一个位置参数) admonition 横幅的类型, 默认值是 note. title [可选] (第二个位置参数) admonition 横幅的标题, 默认值是 type 参数的值. open [可选] (第三个位置参数) 横幅内容是否默认展开, 默认值是 true. 一个 admonition 示例: {{\u003c admonition type=tip title=\"This is a tip\" open=false \u003e}} 一个 **技巧** 横幅 {{\u003c /admonition \u003e}} 或者 {{\u003c admonition tip \"This is a tip\" false \u003e}} 一个 **技巧** 横幅 {{\u003c /admonition \u003e}} 呈现的输出效果如下: This is a tip\r一个 技巧 横幅\r","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:4:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"5 mermaid mermaid shortcode 使用 Mermaid 库提供绘制图表和流程图的功能. 完整文档请查看页面 主题文档 - mermaid Shortcode. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:5:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"6 echarts echarts shortcode 使用 ECharts 库提供数据可视化的功能. 完整文档请查看页面 主题文档 - echarts Shortcode. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:6:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"7 mapbox mapbox shortcode 使用 Mapbox GL JS 库提供互动式地图的功能. 完整文档请查看页面 主题文档 - mapbox Shortcode. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:7:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"8 music music shortcode 基于 APlayer 和 MetingJS 库提供了一个内嵌的响应式音乐播放器. 完整文档请查看页面 主题文档 - music Shortcode. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:8:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"9 bilibili bilibili shortcode 提供了一个内嵌的用来播放 bilibili 视频的响应式播放器. 完整文档请查看页面 主题文档 - bilibili Shortcode. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:9:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"10 typeit typeit shortcode 基于 TypeIt 库提供了打字动画. 完整文档请查看页面 主题文档 - typeit Shortcode. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:10:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"11 script script shortcode 用来在你的文章中插入  Javascript 脚本. 注意\r脚本内容可以保证在所有的第三方库加载之后按顺序执行. 所以你可以自由地使用第三方库.\r一个 script 示例: {{\u003c script \u003e}} console.log('Hello LoveIt!'); {{\u003c /script \u003e}} 你可以在开发者工具的控制台中看到输出. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:11:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"12 raw raw shortcode 用来在你的文章中插入原始  HTML 内容. 一个 raw 示例: 行内公式: {{\u003c raw \u003e}}\\(\\mathbf{E}=\\sum_{i} \\mathbf{E}_{i}=\\mathbf{E}_{1}+\\mathbf{E}_{2}+\\mathbf{E}_{3}+\\cdots\\){{\u003c /raw \u003e}} 公式块: {{\u003c raw \u003e}} \\[ a=b+c \\\\ d+e=f \\] {{\u003c /raw \u003e}} 原始的带有 Markdown 语法的内容: {{\u003c raw \u003e}}**Hello**{{\u003c /raw \u003e}} 呈现的输出效果如下: 行内公式: 公式块: 原始的带有 Markdown 语法的内容: ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:12:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"13 person person shortcode 用来在你的文章中以 h-card 的格式插入个人网站链接. person shortcode 有以下命名参数: url [必需] (第一个位置参数) URL of the personal page. name [必需] (第二个位置参数) Name of the person. text [可选] (第三个位置参数) Text to display as hover tooltip of the link. picture [可选] (第四个位置参数) A picture to use as person’s avatar. nick [可选] Nickame of the person. 一个 person 示例: {{\u003c person url=\"https://evgenykuznetsov.org\" name=\"Evgeny Kuznetsov\" nick=\"nekr0z\" text=\"author of this shortcode\" picture=\"https://evgenykuznetsov.org/img/avatar.jpg\" \u003e}} 呈现的输出效果为  Evgeny Kuznetsov (nekr0z). 一个使用通用图标的 person 示例: {{\u003c person \"https://dillonzq.com/\" Dillon \"author of the LoveIt theme\" \u003e}} 呈现的输出效果为  Dillon. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:13:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["Markdown"],"content":"这篇文章展示了基本的 Markdown 语法和格式.","date":"2019-12-01","objectID":"/basic-markdown-syntax/","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"这篇文章提供了可以在 Hugo 的文章中使用的基本 Markdown 语法示例. 注意\r这篇文章借鉴了一篇很棒的来自 Grav 的文章. 如果你想了解 Loveit 主题的扩展 Markdown 语法, 请阅读扩展 Markdown 语法页面. 事实上, 编写 Web 内容很麻烦. WYSIWYG所见即所得 编辑器帮助减轻了这一任务. 但通常会导致代码太糟, 或更糟糕的是, 网页也会很丑. 没有通常伴随的所有复杂和丑陋的问题, Markdown 是一种更好的生成 HTML 内容的方式. 一些主要好处是: Markdown 简单易学, 几乎没有多余的字符, 因此编写内容也更快. 用 Markdown 书写时出错的机会更少. 可以产生有效的 XHTML 输出. 将内容和视觉显示保持分开, 这样就不会打乱网站的外观. 可以在你喜欢的任何文本编辑器或 Markdown 应用程序中编写内容. Markdown 使用起来很有趣! John Gruber, Markdown 的作者如是说: Markdown 格式的首要设计目标是更具可读性. 最初的想法是 Markdown 格式的文档应当以纯文本形式发布, 而不会看起来像被标签或格式说明所标记. 虽然 Markdown 的语法受到几种现有的文本到 HTML 转换工具的影响, 但 Markdown 语法的最大灵感来源是纯文本电子邮件的格式. – John Gruber 话不多说, 我们来回顾一下 Markdown 的主要语法以及生成的 HTML 样式! 技巧\r 将此页保存为书签，以备将来参考!\r","date":"2019-12-01","objectID":"/basic-markdown-syntax/:0:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"1 标题 从 h2 到 h6 的标题在每个级别上都加上一个 ＃: ## h2 标题 ### h3 标题 #### h4 标题 ##### h5 标题 ###### h6 标题 输出的 HTML 看起来像这样: \u003ch2\u003eh2 标题\u003c/h2\u003e \u003ch3\u003eh3 标题\u003c/h3\u003e \u003ch4\u003eh4 标题\u003c/h4\u003e \u003ch5\u003eh5 标题\u003c/h5\u003e \u003ch6\u003eh6 标题\u003c/h6\u003e 标题 ID\r要添加自定义标题 ID, 请在与标题相同的行中将自定义 ID 放在花括号中: ### 一个很棒的标题 {#custom-id} 输出的 HTML 看起来像这样: \u003ch3 id=\"custom-id\"\u003e一个很棒的标题\u003c/h3\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:1:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"2 注释 注释是和 HTML 兼容的： \u003c!-- 这是一段注释 --\u003e 不能看到以下的注释: ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:2:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"3 水平线 HTML 中的 \u003chr\u003e 标签是用来在段落元素之间创建一个 “专题间隔” 的. 使用 Markdown, 你可以用以下方式创建一个 \u003chr\u003e 标签: ___: 三个连续的下划线 ---: 三个连续的破折号 ***: 三个连续的星号 呈现的输出效果如下: ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:3:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"4 段落 按照纯文本的方式书写段落, 纯文本在呈现的 HTML 中将用 \u003cp\u003e/\u003c/p\u003e 标签包裹. 如下段落: Lorem ipsum dolor sit amet, graecis denique ei vel, at duo primis mandamus. Et legere ocurreret pri, animal tacimates complectitur ad cum. Cu eum inermis inimicus efficiendi. Labore officiis his ex, soluta officiis concludaturque ei qui, vide sensibus vim ad. 输出的 HTML 看起来像这样: \u003cp\u003eLorem ipsum dolor sit amet, graecis denique ei vel, at duo primis mandamus. Et legere ocurreret pri, animal tacimates complectitur ad cum. Cu eum inermis inimicus efficiendi. Labore officiis his ex, soluta officiis concludaturque ei qui, vide sensibus vim ad.\u003c/p\u003e 可以使用一个空白行进行换行. ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:4:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"5 内联 HTML 元素 如果你需要某个 HTML 标签 (带有一个类), 则可以简单地像这样使用: Markdown 格式的段落. \u003cdiv class=\"class\"\u003e 这是 \u003cb\u003eHTML\u003c/b\u003e \u003c/div\u003e Markdown 格式的段落. ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:5:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"6 强调 ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:6:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"加粗 用于强调带有较粗字体的文本片段. 以下文本片段会被 渲染为粗体. **渲染为粗体** __渲染为粗体__ 输出的 HTML 看起来像这样: \u003cstrong\u003e渲染为粗体\u003c/strong\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:6:1","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"斜体 用于强调带有斜体的文本片段. 以下文本片段被 渲染为斜体. *渲染为斜体* _渲染为斜体_ 输出的 HTML 看起来像这样: \u003cem\u003e渲染为斜体\u003c/em\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:6:2","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"删除线 按照 GFMGitHub flavored Markdown 你可以使用删除线. ~~这段文本带有删除线.~~ 呈现的输出效果如下: 这段文本带有删除线. 输出的 HTML 看起来像这样: \u003cdel\u003e这段文本带有删除线.\u003c/del\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:6:3","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"组合 加粗, 斜体, 和删除线可以 组合使用. ***加粗和斜体*** ~~**删除线和加粗**~~ ~~*删除线和斜体*~~ ~~***加粗, 斜体和删除线***~~ 呈现的输出效果如下: 加粗和斜体 删除线和加粗 删除线和斜体 加粗, 斜体和删除线 输出的 HTML 看起来像这样: \u003cem\u003e\u003cstrong\u003e加粗和斜体\u003c/strong\u003e\u003c/em\u003e \u003cdel\u003e\u003cstrong\u003e删除线和加粗\u003c/strong\u003e\u003c/del\u003e \u003cdel\u003e\u003cem\u003e删除线和斜体\u003c/em\u003e\u003c/del\u003e \u003cdel\u003e\u003cem\u003e\u003cstrong\u003e加粗, 斜体和删除线\u003c/strong\u003e\u003c/em\u003e\u003c/del\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:6:4","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"7 引用 用于在文档中引用其他来源的内容块. 在要引用的任何文本之前添加 \u003e: \u003e **Fusion Drive** combines a hard drive with a flash storage (solid-state drive) and presents it as a single logical volume with the space of both drives combined. 呈现的输出效果如下: Fusion Drive combines a hard drive with a flash storage (solid-state drive) and presents it as a single logical volume with the space of both drives combined. 输出的 HTML 看起来像这样: \u003cblockquote\u003e \u003cp\u003e \u003cstrong\u003eFusion Drive\u003c/strong\u003e combines a hard drive with a flash storage (solid-state drive) and presents it as a single logical volume with the space of both drives combined. \u003c/p\u003e \u003c/blockquote\u003e 引用也可以嵌套: \u003e Donec massa lacus, ultricies a ullamcorper in, fermentum sed augue. Nunc augue augue, aliquam non hendrerit ac, commodo vel nisi. \u003e\u003e Sed adipiscing elit vitae augue consectetur a gravida nunc vehicula. Donec auctor odio non est accumsan facilisis. Aliquam id turpis in dolor tincidunt mollis ac eu diam. 呈现的输出效果如下: Donec massa lacus, ultricies a ullamcorper in, fermentum sed augue. Nunc augue augue, aliquam non hendrerit ac, commodo vel nisi. Sed adipiscing elit vitae augue consectetur a gravida nunc vehicula. Donec auctor odio non est accumsan facilisis. Aliquam id turpis in dolor tincidunt mollis ac eu diam. ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:7:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"8 列表 ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:8:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"无序列表 一系列项的列表, 其中项的顺序没有明显关系. 你可以使用以下任何符号来表示无序列表中的项: * 一项内容 - 一项内容 + 一项内容 例如: * Lorem ipsum dolor sit amet * Consectetur adipiscing elit * Integer molestie lorem at massa * Facilisis in pretium nisl aliquet * Nulla volutpat aliquam velit * Phasellus iaculis neque * Purus sodales ultricies * Vestibulum laoreet porttitor sem * Ac tristique libero volutpat at * Faucibus porta lacus fringilla vel * Aenean sit amet erat nunc * Eget porttitor lorem 呈现的输出效果如下: Lorem ipsum dolor sit amet Consectetur adipiscing elit Integer molestie lorem at massa Facilisis in pretium nisl aliquet Nulla volutpat aliquam velit Phasellus iaculis neque Purus sodales ultricies Vestibulum laoreet porttitor sem Ac tristique libero volutpat at Faucibus porta lacus fringilla vel Aenean sit amet erat nunc Eget porttitor lorem 输出的 HTML 看起来像这样: \u003cul\u003e \u003cli\u003eLorem ipsum dolor sit amet\u003c/li\u003e \u003cli\u003eConsectetur adipiscing elit\u003c/li\u003e \u003cli\u003eInteger molestie lorem at massa\u003c/li\u003e \u003cli\u003eFacilisis in pretium nisl aliquet\u003c/li\u003e \u003cli\u003eNulla volutpat aliquam velit \u003cul\u003e \u003cli\u003ePhasellus iaculis neque\u003c/li\u003e \u003cli\u003ePurus sodales ultricies\u003c/li\u003e \u003cli\u003eVestibulum laoreet porttitor sem\u003c/li\u003e \u003cli\u003eAc tristique libero volutpat at\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003eFaucibus porta lacus fringilla vel\u003c/li\u003e \u003cli\u003eAenean sit amet erat nunc\u003c/li\u003e \u003cli\u003eEget porttitor lorem\u003c/li\u003e \u003c/ul\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:8:1","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"有序列表 一系列项的列表, 其中项的顺序确实很重要. 1. Lorem ipsum dolor sit amet 2. Consectetur adipiscing elit 3. Integer molestie lorem at massa 4. Facilisis in pretium nisl aliquet 5. Nulla volutpat aliquam velit 6. Faucibus porta lacus fringilla vel 7. Aenean sit amet erat nunc 8. Eget porttitor lorem 呈现的输出效果如下: Lorem ipsum dolor sit amet Consectetur adipiscing elit Integer molestie lorem at massa Facilisis in pretium nisl aliquet Nulla volutpat aliquam velit Faucibus porta lacus fringilla vel Aenean sit amet erat nunc Eget porttitor lorem 输出的 HTML 看起来像这样: \u003col\u003e \u003cli\u003eLorem ipsum dolor sit amet\u003c/li\u003e \u003cli\u003eConsectetur adipiscing elit\u003c/li\u003e \u003cli\u003eInteger molestie lorem at massa\u003c/li\u003e \u003cli\u003eFacilisis in pretium nisl aliquet\u003c/li\u003e \u003cli\u003eNulla volutpat aliquam velit\u003c/li\u003e \u003cli\u003eFaucibus porta lacus fringilla vel\u003c/li\u003e \u003cli\u003eAenean sit amet erat nunc\u003c/li\u003e \u003cli\u003eEget porttitor lorem\u003c/li\u003e \u003c/ol\u003e 技巧\r如果你对每一项使用 1., Markdown 将自动为每一项编号. 例如: 1. Lorem ipsum dolor sit amet 1. Consectetur adipiscing elit 1. Integer molestie lorem at massa 1. Facilisis in pretium nisl aliquet 1. Nulla volutpat aliquam velit 1. Faucibus porta lacus fringilla vel 1. Aenean sit amet erat nunc 1. Eget porttitor lorem 呈现的输出效果如下: Lorem ipsum dolor sit amet Consectetur adipiscing elit Integer molestie lorem at massa Facilisis in pretium nisl aliquet Nulla volutpat aliquam velit Faucibus porta lacus fringilla vel Aenean sit amet erat nunc Eget porttitor lorem ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:8:2","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"任务列表 任务列表使你可以创建带有复选框的列表. 要创建任务列表, 请在任务列表项之前添加破折号 (-) 和带有空格的方括号 ([ ]). 要选择一个复选框，请在方括号之间添加 x ([x]). - [x] Write the press release - [ ] Update the website - [ ] Contact the media 呈现的输出效果如下: Write the press release Update the website Contact the media ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:8:3","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"9 代码 ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:9:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"行内代码 用 ` 包装行内代码段. 在这个例子中, `\u003csection\u003e\u003c/section\u003e` 会被包裹成 **代码**. 呈现的输出效果如下: 在这个例子中, \u003csection\u003e\u003c/section\u003e 会被包裹成 代码. 输出的 HTML 看起来像这样: \u003cp\u003e 在这个例子中, \u003ccode\u003e\u0026lt;section\u0026gt;\u0026lt;/section\u0026gt;\u003c/code\u003e 会被包裹成 \u003cstrong\u003e代码\u003c/strong\u003e. \u003c/p\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:9:1","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"缩进代码 将几行代码缩进至少四个空格，例如: // Some comments line 1 of code line 2 of code line 3 of code 呈现的输出效果如下: // Some comments\rline 1 of code\rline 2 of code\rline 3 of code\r输出的 HTML 看起来像这样: \u003cpre\u003e \u003ccode\u003e // Some comments line 1 of code line 2 of code line 3 of code \u003c/code\u003e \u003c/pre\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:9:2","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"围栏代码块 使用 “围栏” ``` 来生成一段带有语言属性的代码块. ```markdown Sample text here... ``` 输出的 HTML 看起来像这样: \u003cpre language-html\u003e \u003ccode\u003eSample text here...\u003c/code\u003e \u003c/pre\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:9:3","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"语法高亮 GFMGitHub Flavored Markdown 也支持语法高亮. 要激活它，只需在第一个代码 “围栏” 之后直接添加你要使用的语言的文件扩展名, ```js, 语法高亮显示将自动应用于渲染的 HTML 中. 例如, 在以下 JavaScript 代码中应用语法高亮: ```js grunt.initConfig({ assemble: { options: { assets: 'docs/assets', data: 'src/data/*.{json,yml}', helpers: 'src/custom-helpers.js', partials: ['src/partials/**/*.{hbs,md}'] }, pages: { options: { layout: 'default.hbs' }, files: { './': ['src/templates/pages/index.hbs'] } } } }; ``` 呈现的输出效果如下: grunt.initConfig({ assemble: { options: { assets: 'docs/assets', data: 'src/data/*.{json,yml}', helpers: 'src/custom-helpers.js', partials: ['src/partials/**/*.{hbs,md}'] }, pages: { options: { layout: 'default.hbs' }, files: { './': ['src/templates/pages/index.hbs'] } } } }; 注意\rHugo 文档中的 语法高亮页面 介绍了有关语法高亮的更多信息, 包括语法高亮的 shortcode.\r","date":"2019-12-01","objectID":"/basic-markdown-syntax/:9:4","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"10 表格 通过在每个单元格之间添加竖线作为分隔线, 并在标题下添加一行破折号 (也由竖线分隔) 来创建表格. 注意, 竖线不需要垂直对齐. | Option | Description | | ------ | ----------- | | data | path to data files to supply the data that will be passed into templates. | | engine | engine to be used for processing templates. Handlebars is the default. | | ext | extension to be used for dest files. | 呈现的输出效果如下: Option Description data path to data files to supply the data that will be passed into templates. engine engine to be used for processing templates. Handlebars is the default. ext extension to be used for dest files. 输出的 HTML 看起来像这样: \u003ctable\u003e \u003cthead\u003e \u003ctr\u003e \u003cth\u003eOption\u003c/th\u003e \u003cth\u003eDescription\u003c/th\u003e \u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e \u003ctr\u003e \u003ctd\u003edata\u003c/td\u003e \u003ctd\u003epath to data files to supply the data that will be passed into templates.\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003eengine\u003c/td\u003e \u003ctd\u003eengine to be used for processing templates. Handlebars is the default.\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003eext\u003c/td\u003e \u003ctd\u003eextension to be used for dest files.\u003c/td\u003e \u003c/tr\u003e \u003c/tbody\u003e \u003c/table\u003e 文本右对齐或居中对齐\r在任何标题下方的破折号右侧添加冒号将使该列的文本右对齐. 在任何标题下方的破折号两边添加冒号将使该列的对齐文本居中. | Option | Description | |:------:| -----------:| | data | path to data files to supply the data that will be passed into templates. | | engine | engine to be used for processing templates. Handlebars is the default. | | ext | extension to be used for dest files. | 呈现的输出效果如下: Option Description data path to data files to supply the data that will be passed into templates. engine engine to be used for processing templates. Handlebars is the default. ext extension to be used for dest files. ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:10:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"11 链接 ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:11:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"基本链接 \u003chttps://assemble.io\u003e \u003ccontact@revolunet.com\u003e [Assemble](https://assemble.io) 呈现的输出效果如下 (将鼠标悬停在链接上，没有提示): https://assemble.io contact@revolunet.com Assemble 输出的 HTML 看起来像这样: \u003ca href=\"https://assemble.io\"\u003ehttps://assemble.io\u003c/a\u003e \u003ca href=\"mailto:contact@revolunet.com\"\u003econtact@revolunet.com\u003c/a\u003e \u003ca href=\"https://assemble.io\"\u003eAssemble\u003c/a\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:11:1","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"添加一个标题 [Upstage](https://github.com/upstage/ \"Visit Upstage!\") 呈现的输出效果如下 (将鼠标悬停在链接上，会有一行提示): Upstage 输出的 HTML 看起来像这样: \u003ca href=\"https://github.com/upstage/\" title=\"Visit Upstage!\"\u003eUpstage\u003c/a\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:11:2","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"定位标记 定位标记使你可以跳至同一页面上的指定锚点. 例如, 每个章节: ## Table of Contents * [Chapter 1](#chapter-1) * [Chapter 2](#chapter-2) * [Chapter 3](#chapter-3) 将跳转到这些部分: ## Chapter 1 \u003ca id=\"chapter-1\"\u003e\u003c/a\u003e Content for chapter one. ## Chapter 2 \u003ca id=\"chapter-2\"\u003e\u003c/a\u003e Content for chapter one. ## Chapter 3 \u003ca id=\"chapter-3\"\u003e\u003c/a\u003e Content for chapter one. 注意\r定位标记的位置几乎是任意的. 因为它们并不引人注目, 所以它们通常被放在同一行了.\r","date":"2019-12-01","objectID":"/basic-markdown-syntax/:11:3","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"12 脚注 脚注使你可以添加注释和参考, 而不会使文档正文混乱. 当你创建脚注时, 会在添加脚注引用的位置出现带有链接的上标编号. 读者可以单击链接以跳至页面底部的脚注内容. 要创建脚注引用, 请在方括号中添加插入符号和标识符 ([^1]). 标识符可以是数字或单词, 但不能包含空格或制表符. 标识符仅将脚注引用与脚注本身相关联 - 在脚注输出中, 脚注按顺序编号. 在中括号内使用插入符号和数字以及用冒号和文本来添加脚注内容 ([^1]：这是一段脚注). 你不一定要在文档末尾添加脚注. 可以将它们放在除列表, 引用和表格等元素之外的任何位置. 这是一个数字脚注[^1]. 这是一个带标签的脚注[^label] [^1]: 这是一个数字脚注 [^label]: 这是一个带标签的脚注 这是一个数字脚注1. 这是一个带标签的脚注2 ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:12:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"13 图片 图片的语法与链接相似, 但包含一个在前面的感叹号. ![Minion](https://octodex.github.com/images/minion.png) 或者: ![Alt text](https://octodex.github.com/images/stormtroopocat.jpg \"The Stormtroopocat\") The Stormtroopocat\r像链接一样, 图片也具有脚注样式的语法: ![Alt text][id] The Dojocat\r稍后在文档中提供参考内容, 用来定义 URL 的位置: [id]: https://octodex.github.com/images/dojocat.jpg \"The Dojocat\" 技巧\rLoveIt 主题提供了一个包含更多功能的 图片的 shortcode.\r这是一个数字脚注 ↩︎ 这是一个带标签的脚注 ↩︎ ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:13:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["documentation"],"content":"mermaid shortcode 使用 Mermaid 库提供绘制图表和流程图的功能.","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":" mermaid shortcode 使用 Mermaid 库提供绘制图表和流程图的功能. mermaid 是一个可以帮助你在文章中绘制图表和流程图的库, 类似 Markdown 的语法. 只需将你的 mermaid 代码插入 mermaid shortcode 中即可. ","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/:0:0","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":"流程图 一个 流程图 mermaid 示例: {{\u003c mermaid \u003e}} graph LR; A[Hard edge] --\u003e|Link text| B(Round edge) B --\u003e C{Decision} C --\u003e|One| D[Result one] C --\u003e|Two| E[Result two] {{\u003c /mermaid \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/:1:0","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":"时序图 一个 时序图 mermaid 示例: {{\u003c mermaid \u003e}} sequenceDiagram participant Alice participant Bob Alice-\u003e\u003eJohn: Hello John, how are you? loop Healthcheck John-\u003eJohn: Fight against hypochondria end Note right of John: Rational thoughts \u003cbr/\u003eprevail... John--\u003eAlice: Great! John-\u003eBob: How about you? Bob--\u003eJohn: Jolly good! {{\u003c /mermaid \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/:2:0","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":"甘特图 一个 甘特图 mermaid 示例: {{\u003c mermaid \u003e}} gantt dateFormat YYYY-MM-DD title Adding GANTT diagram to mermaid excludes weekdays 2014-01-10 section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d {{\u003c /mermaid \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/:3:0","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":"类图 一个 类图 mermaid 示例: {{\u003c mermaid \u003e}} classDiagram Animal \u003c|-- Duck Animal \u003c|-- Fish Animal \u003c|-- Zebra Animal : +int age Animal : +String gender Animal: +isMammal() Animal: +mate() class Duck{ +String beakColor +swim() +quack() } class Fish{ -int sizeInFeet -canEat() } class Zebra{ +bool is_wild +run() } {{\u003c /mermaid \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/:4:0","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":"状态图 一个 状态图 mermaid 示例: {{\u003c mermaid \u003e}} stateDiagram-v2 [*] --\u003e Still Still --\u003e [*] Still --\u003e Moving Moving --\u003e Still Moving --\u003e Crash Crash --\u003e [*] {{\u003c /mermaid \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/:5:0","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":"Git 图 一个 Git 图 mermaid 示例: {{\u003c mermaid \u003e}} gitGraph commit commit branch develop checkout develop commit commit checkout main merge develop commit commit {{\u003c /mermaid \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/:6:0","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":"实体关系图 一个 实体关系图 mermaid 示例: {{\u003c mermaid \u003e}} erDiagram CUSTOMER ||--o{ ORDER : places ORDER ||--|{ LINE-ITEM : contains CUSTOMER }|..|{ DELIVERY-ADDRESS : uses {{\u003c /mermaid \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/:7:0","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":"用户体验旅程图 一个 用户体验旅程图 mermaid 示例: {{\u003c mermaid \u003e}} journey title My working day section Go to work Make tea: 5: Me Go upstairs: 3: Me Do work: 1: Me, Cat section Go home Go downstairs: 5: Me Sit down: 5: Me {{\u003c /mermaid \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/:8:0","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":"饼图 一个 饼图 mermaid 示例: {{\u003c mermaid \u003e}} pie \"Dogs\" : 386 \"Cats\" : 85 \"Rats\" : 15 {{\u003c /mermaid \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/:9:0","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":"依赖图 一个 依赖图 mermaid 示例: {{\u003c mermaid \u003e}} requirementDiagram requirement test_req { id: 1 text: the test text. risk: high verifymethod: test } element test_entity { type: simulation } test_entity - satisfies -\u003e test_req {{\u003c /mermaid \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/:10:0","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["Markdown"],"content":"Hugo 和 LoveIt 中的 Emoji 的用法指南.","date":"2019-10-01","objectID":"/emoji-support/","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Emoji 可以通过多种方式在 Hugo 项目中启用. emojify 方法可以直接在模板中调用, 或者使用行内 Shortcodes. 要全局使用 emoji, 需要在你的网站配置中设置 enableEmoji 为 true, 然后你就可以直接在文章中输入 emoji 的代码. 它们以冒号开头和结尾，并且包含 emoji 的 代码: 去露营啦! :tent: 很快就回来. 真开心! :joy: 呈现的输出效果如下: 去露营啦! ⛺ 很快就回来. 真开心! 😂 以下符号清单是 emoji 代码的非常有用的参考. ","date":"2019-10-01","objectID":"/emoji-support/:0:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"表情与情感 ","date":"2019-10-01","objectID":"/emoji-support/:1:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"笑脸表情 图标 代码 图标 代码 😀 grinning 😃 smiley 😄 smile 😁 grin 😆 laughing satisfied 😅 sweat_smile 🤣 rofl 😂 joy 🙂 slightly_smiling_face 🙃 upside_down_face 😉 wink 😊 blush 😇 innocent ","date":"2019-10-01","objectID":"/emoji-support/:1:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"爱意表情 图标 代码 图标 代码 😍 heart_eyes 😘 kissing_heart 😗 kissing ☺️ relaxed 😚 kissing_closed_eyes 😙 kissing_smiling_eyes ","date":"2019-10-01","objectID":"/emoji-support/:1:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"吐舌头表情 图标 代码 图标 代码 😋 yum 😛 stuck_out_tongue 😜 stuck_out_tongue_winking_eye 😝 stuck_out_tongue_closed_eyes 🤑 money_mouth_face ","date":"2019-10-01","objectID":"/emoji-support/:1:3","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"带手的表情 图标 代码 图标 代码 🤗 hugs 🤔 thinking ","date":"2019-10-01","objectID":"/emoji-support/:1:4","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"中性表情 图标 代码 图标 代码 🤐 zipper_mouth_face 😐 neutral_face 😑 expressionless 😶 no_mouth 😏 smirk 😒 unamused 🙄 roll_eyes 😬 grimacing 🤥 lying_face ","date":"2019-10-01","objectID":"/emoji-support/:1:5","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"困倦的表情 图标 代码 图标 代码 😌 relieved 😔 pensive 😪 sleepy 🤤 drooling_face 😴 sleeping ","date":"2019-10-01","objectID":"/emoji-support/:1:6","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"不适的表情 图标 代码 图标 代码 😷 mask 🤒 face_with_thermometer 🤕 face_with_head_bandage 🤢 nauseated_face 🤧 sneezing_face 😵 dizzy_face ","date":"2019-10-01","objectID":"/emoji-support/:1:7","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"戴帽子的表情 图标 代码 图标 代码 🤠 cowboy_hat_face ","date":"2019-10-01","objectID":"/emoji-support/:1:8","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"戴眼镜的表情 图标 代码 图标 代码 😎 sunglasses 🤓 nerd_face ","date":"2019-10-01","objectID":"/emoji-support/:1:9","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"担心的表情 图标 代码 图标 代码 😕 confused 😟 worried 🙁 slightly_frowning_face ☹ frowning_face 😮 open_mouth 😯 hushed 😲 astonished 😳 flushed 😦 frowning 😧 anguished 😨 fearful 😰 cold_sweat 😥 disappointed_relieved 😢 cry 😭 sob 😱 scream 😖 confounded 😣 persevere 😞 disappointed 😓 sweat 😩 weary 😫 tired_face ","date":"2019-10-01","objectID":"/emoji-support/:1:10","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"否定的表情 图标 代码 图标 代码 😤 triumph 😡 pout rage 😠 angry 😈 smiling_imp 👿 imp 💀 skull ☠️ skull_and_crossbones ","date":"2019-10-01","objectID":"/emoji-support/:1:11","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"特殊打扮的表情 图标 代码 图标 代码 💩 hankey poop shit 🤡 clown_face 👹 japanese_ogre 👺 japanese_goblin 👻 ghost 👽 alien 👾 space_invader 🤖 robot ","date":"2019-10-01","objectID":"/emoji-support/:1:12","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"猫脸表情 图标 代码 图标 代码 😺 smiley_cat 😸 smile_cat 😹 joy_cat 😻 heart_eyes_cat 😼 smirk_cat 😽 kissing_cat 🙀 scream_cat 😿 crying_cat_face 😾 pouting_cat ","date":"2019-10-01","objectID":"/emoji-support/:1:13","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"猴脸表情 图标 代码 图标 代码 🙈 see_no_evil 🙉 hear_no_evil 🙊 speak_no_evil ","date":"2019-10-01","objectID":"/emoji-support/:1:14","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"情感 图标 代码 图标 代码 💋 kiss 💌 love_letter 💘 cupid 💝 gift_heart 💖 sparkling_heart 💗 heartpulse 💓 heartbeat 💞 revolving_hearts 💕 two_hearts 💟 heart_decoration ❣️ heavy_heart_exclamation 💔 broken_heart ❤️ heart 💛 yellow_heart 💚 green_heart 💙 blue_heart 💜 purple_heart 🖤 black_heart 💯 100 💢 anger 💥 boom collision 💫 dizzy 💦 sweat_drops 💨 dash 🕳️ hole 💣 bomb 💬 speech_balloon 👁️‍🗨️ eye_speech_bubble 🗯️ right_anger_bubble 💭 thought_balloon 💤 zzz ","date":"2019-10-01","objectID":"/emoji-support/:1:15","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"人与身体 ","date":"2019-10-01","objectID":"/emoji-support/:2:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"张开手掌的手势 图标 代码 图标 代码 👋 wave 🤚 raised_back_of_hand 🖐️ raised_hand_with_fingers_splayed ✋ hand raised_hand 🖖 vulcan_salute ","date":"2019-10-01","objectID":"/emoji-support/:2:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"部分手指的手势 图标 代码 图标 代码 👌 ok_hand ✌️ v 🤞 crossed_fingers 🤘 metal 🤙 call_me_hand ","date":"2019-10-01","objectID":"/emoji-support/:2:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"一根手指的手势 图标 代码 图标 代码 👈 point_left 👉 point_right 👆 point_up_2 🖕 fu middle_finger 👇 point_down ☝️ point_up ","date":"2019-10-01","objectID":"/emoji-support/:2:3","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"握紧的手势 图标 代码 图标 代码 👍 +1 thumbsup 👎 -1 thumbsdown ✊ fist fist_raised 👊 facepunch fist_oncoming punch 🤛 fist_left 🤜 fist_right ","date":"2019-10-01","objectID":"/emoji-support/:2:4","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"两只手 图标 代码 图标 代码 👏 clap 🙌 raised_hands 👐 open_hands 🤝 handshake 🙏 pray ","date":"2019-10-01","objectID":"/emoji-support/:2:5","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"握住东西的手势 图标 代码 图标 代码 ✍️ writing_hand 💅 nail_care 🤳 selfie ","date":"2019-10-01","objectID":"/emoji-support/:2:6","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"身体部位 图标 代码 图标 代码 💪 muscle 👂 ear 👃 nose 👀 eyes 👁️ eye 👅 tongue 👄 lips ","date":"2019-10-01","objectID":"/emoji-support/:2:7","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"人 图标 代码 图标 代码 👶 baby 👦 boy 👧 girl :blonde_man: blonde_man person_with_blond_hair 👨 man 👩 woman 👱‍♀️ blonde_woman 👴 older_man 👵 older_woman ","date":"2019-10-01","objectID":"/emoji-support/:2:8","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"身体动作 图标 代码 图标 代码 🙍‍♀️ frowning_woman person_frowning 🙍‍♂️ frowning_man 🙎‍♀️ person_with_pouting_face pouting_woman 🙎‍♂️ pouting_man 🙅‍♀️ ng_woman no_good no_good_woman 🙅‍♂️ ng_man no_good_man 🙆‍♀️ ok_woman 🙆‍♂️ ok_man 💁‍♀️ information_desk_person sassy_woman tipping_hand_woman 💁‍♂️ sassy_man tipping_hand_man 🙋‍♀️ raising_hand raising_hand_woman 🙋‍♂️ raising_hand_man 🙇 bow bowing_man 🙇‍♀️ bowing_woman 🤦‍♂️ man_facepalming 🤦‍♀️ woman_facepalming 🤷‍♂️ man_shrugging 🤷‍♀️ woman_shrugging ","date":"2019-10-01","objectID":"/emoji-support/:2:9","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"人物角色 图标 代码 图标 代码 👨‍⚕️ man_health_worker 👩‍⚕️ woman_health_worker 👨‍🎓 man_student 👩‍🎓 woman_student 👨‍🏫 man_teacher 👩‍🏫 woman_teacher 👨‍⚖️ man_judge 👩‍⚖️ woman_judge 👨‍🌾 man_farmer 👩‍🌾 woman_farmer 👨‍🍳 man_cook 👩‍🍳 woman_cook 👨‍🔧 man_mechanic 👩‍🔧 woman_mechanic 👨‍🏭 man_factory_worker 👩‍🏭 woman_factory_worker 👨‍💼 man_office_worker 👩‍💼 woman_office_worker 👨‍🔬 man_scientist 👩‍🔬 woman_scientist 👨‍💻 man_technologist 👩‍💻 woman_technologist 👨‍🎤 man_singer 👩‍🎤 woman_singer 👨‍🎨 man_artist 👩‍🎨 woman_artist 👨‍✈️ man_pilot 👩‍✈️ woman_pilot 👨‍🚀 man_astronaut 👩‍🚀 woman_astronaut 👨‍🚒 man_firefighter 👩‍🚒 woman_firefighter 👮‍♂️ cop policeman 👮‍♀️ policewoman 🕵 detective male_detective 🕵️‍♀️ female_detective 💂‍♂️ guardsman 💂‍♀️ guardswoman 👷‍♂️ construction_worker construction_worker_man 👷‍♀️ construction_worker_woman 🤴 prince 👸 princess 👳‍♂️ man_with_turban 👳‍♀️ woman_with_turban 👲 man_with_gua_pi_mao 🤵‍♂️ man_in_tuxedo 👰 bride_with_veil 🤰 pregnant_woman ","date":"2019-10-01","objectID":"/emoji-support/:2:10","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"幻想的人物 图标 代码 图标 代码 👼 angel 🎅 santa 🤶 mrs_claus ","date":"2019-10-01","objectID":"/emoji-support/:2:11","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"人物活动 图标 代码 图标 代码 💆‍♀️ massage massage_woman 💆‍♂️ massage_man 💇‍♀️ haircut haircut_woman 💇‍♂️ haircut_man 🚶‍♂️ walking walking_man 🚶‍♀️ walking_woman 🏃‍♂️ runner running running_man 🏃‍♀️ running_woman 💃 dancer 🕺 man_dancing 🕴️ business_suit_levitating 👯‍♀️ dancers dancing_women 👯‍♂️ dancing_men ","date":"2019-10-01","objectID":"/emoji-support/:2:12","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"体育 图标 代码 图标 代码 🤺 person_fencing 🏇 horse_racing ⛷️ skier 🏂 snowboarder 🏌️‍♂️ golfing_man 🏌️‍♀️ golfing_woman 🏄‍♂️ surfer surfing_man 🏄‍♀️ surfing_woman 🚣‍♂️ rowboat rowing_man 🚣‍♀️ rowing_woman 🏊‍♂️ swimmer swimming_man 🏊‍♀️ swimming_woman ⛹️‍♂️ basketball_man ⛹️‍♀️ basketball_woman 🏋️‍♂️ weight_lifting_man 🏋️‍♀️ weight_lifting_woman 🚴‍♂️ bicyclist biking_man 🚴‍♀️ biking_woman 🚵‍♂️ mountain_bicyclist mountain_biking_man 🚵‍♀️ mountain_biking_woman 🤸‍♂️ man_cartwheeling 🤸‍♀️ woman_cartwheeling 🤼‍♂️ men_wrestling 🤼‍♀️ women_wrestling 🤽‍♂️ man_playing_water_polo 🤽‍♀️ woman_playing_water_polo 🤾‍♂️ man_playing_handball 🤾‍♀️ woman_playing_handball 🤹‍♂️ man_juggling 🤹‍♀️ woman_juggling ","date":"2019-10-01","objectID":"/emoji-support/:2:13","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"休息 图标 代码 图标 代码 🛀 bath 🛌 sleeping_bed ","date":"2019-10-01","objectID":"/emoji-support/:2:14","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"家庭 图标 代码 图标 代码 👭 two_women_holding_hands 👫 couple 👬 two_men_holding_hands 👩‍❤️‍💋‍👨 couplekiss_man_woman 👨‍❤️‍💋‍👨 couplekiss_man_man 👩‍❤️‍💋‍👩 couplekiss_woman_woman 💑 couple_with_heart couple_with_heart_woman_man 👨‍❤️‍👨 couple_with_heart_man_man 👩‍❤️‍👩 couple_with_heart_woman_woman 👨‍👩‍👦 family family_man_woman_boy 👨‍👩‍👧 family_man_woman_girl 👨‍👩‍👧‍👦 family_man_woman_girl_boy 👨‍👩‍👦‍👦 family_man_woman_boy_boy 👨‍👩‍👧‍👧 family_man_woman_girl_girl 👨‍👨‍👦 family_man_man_boy 👨‍👨‍👧 family_man_man_girl 👨‍👨‍👧‍👦 family_man_man_girl_boy 👨‍👨‍👦‍👦 family_man_man_boy_boy 👨‍👨‍👧‍👧 family_man_man_girl_girl 👩‍👩‍👦 family_woman_woman_boy 👩‍👩‍👧 family_woman_woman_girl 👩‍👩‍👧‍👦 family_woman_woman_girl_boy 👩‍👩‍👦‍👦 family_woman_woman_boy_boy 👩‍👩‍👧‍👧 family_woman_woman_girl_girl 👨‍👦 family_man_boy 👨‍👦‍👦 family_man_boy_boy 👨‍👧 family_man_girl 👨‍👧‍👦 family_man_girl_boy 👨‍👧‍👧 family_man_girl_girl 👩‍👦 family_woman_boy 👩‍👦‍👦 family_woman_boy_boy 👩‍👧 family_woman_girl 👩‍👧‍👦 family_woman_girl_boy 👩‍👧‍👧 family_woman_girl_girl ","date":"2019-10-01","objectID":"/emoji-support/:2:15","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"人物符号 图标 代码 图标 代码 🗣 speaking_head 👤 bust_in_silhouette 👥 busts_in_silhouette 👣 footprints ","date":"2019-10-01","objectID":"/emoji-support/:2:16","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"动物与自然 ","date":"2019-10-01","objectID":"/emoji-support/:3:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"哺乳动物 图标 代码 图标 代码 🐵 monkey_face 🐒 monkey 🦍 gorilla 🐶 dog 🐕 dog2 🐩 poodle 🐺 wolf 🦊 fox_face 🐱 cat 🐈 cat2 🦁 lion 🐯 tiger 🐅 tiger2 🐆 leopard 🐴 horse 🐎 racehorse 🦄 unicorn 🦌 deer 🐮 cow 🐂 ox 🐃 water_buffalo 🐄 cow2 🐷 pig 🐖 pig2 🐗 boar 🐽 pig_nose 🐏 ram 🐑 sheep 🐐 goat 🐪 dromedary_camel 🐫 camel 🐘 elephant 🦏 rhinoceros 🐭 mouse 🐁 mouse2 🐀 rat 🐹 hamster 🐰 rabbit 🐇 rabbit2 🐿️ chipmunk 🦇 bat 🐻 bear 🐨 koala 🐼 panda_face 🐾 feet paw_prints ","date":"2019-10-01","objectID":"/emoji-support/:3:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"鸟类 图标 代码 图标 代码 🦃 turkey 🐔 chicken 🐓 rooster 🐣 hatching_chick 🐤 baby_chick 🐥 hatched_chick 🐦 bird 🐧 penguin 🕊 dove 🦅 eagle 🦆 duck 🦉 owl ","date":"2019-10-01","objectID":"/emoji-support/:3:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"两栖动物 icon code icon code 🐸 frog ","date":"2019-10-01","objectID":"/emoji-support/:3:3","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"爬虫类 图标 代码 图标 代码 🐊 crocodile 🐢 turtle 🦎 lizard 🐍 snake 🐲 dragon_face 🐉 dragon ","date":"2019-10-01","objectID":"/emoji-support/:3:4","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"海洋动物 图标 代码 图标 代码 🐳 whale 🐋 whale2 🐬 dolphin flipper 🐟 fish 🐠 tropical_fish 🐡 blowfish 🦈 shark 🐙 octopus 🐚 shell ","date":"2019-10-01","objectID":"/emoji-support/:3:5","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"虫类 图标 代码 图标 代码 🐌 snail 🦋 butterfly 🐛 bug 🐜 ant 🐝 bee honeybee 🪲 beetle 🕷️ spider 🕸️ spider_web 🦂 scorpion ","date":"2019-10-01","objectID":"/emoji-support/:3:6","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"花类植物 图标 代码 图标 代码 💐 bouquet 🌸 cherry_blossom 💮 white_flower 🏵️ rosette 🌹 rose 🥀 wilted_flower 🌺 hibiscus 🌻 sunflower 🌼 blossom 🌷 tulip ","date":"2019-10-01","objectID":"/emoji-support/:3:7","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"其它植物 图标 代码 图标 代码 🌱 seedling 🌲 evergreen_tree 🌳 deciduous_tree 🌴 palm_tree 🌵 cactus 🌾 ear_of_rice 🌿 herb ☘️ shamrock 🍀 four_leaf_clover 🍁 maple_leaf 🍂 fallen_leaf 🍃 leaves ","date":"2019-10-01","objectID":"/emoji-support/:3:8","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"食物与饮料 ","date":"2019-10-01","objectID":"/emoji-support/:4:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"水果 图标 代码 图标 代码 🍇 grapes 🍈 melon 🍉 watermelon 🍊 mandarin orange tangerine 🍋 lemon 🍌 banana 🍍 pineapple 🍎 apple 🍏 green_apple 🍐 pear 🍑 peach 🍒 cherries 🍓 strawberry 🥝 kiwi_fruit 🍅 tomato ","date":"2019-10-01","objectID":"/emoji-support/:4:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"蔬菜 图标 代码 图标 代码 🥑 avocado 🍆 eggplant 🥔 potato 🥕 carrot 🌽 corn 🌶️ hot_pepper 🥒 cucumber 🍄 mushroom 🥜 peanuts 🌰 chestnut ","date":"2019-10-01","objectID":"/emoji-support/:4:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"快餐 图标 代码 图标 代码 🍞 bread 🥐 croissant 🥖 baguette_bread 🥞 pancakes 🧀 cheese 🍖 meat_on_bone 🍗 poultry_leg 🥓 bacon 🍔 hamburger 🍟 fries 🍕 pizza 🌭 hotdog 🌮 taco 🌯 burrito 🥙 stuffed_flatbread 🥚 egg 🍳 fried_egg 🥘 shallow_pan_of_food 🍲 stew 🥗 green_salad 🍿 popcorn ","date":"2019-10-01","objectID":"/emoji-support/:4:3","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"亚洲食物 图标 代码 图标 代码 🍱 bento 🍘 rice_cracker 🍙 rice_ball 🍚 rice 🍛 curry 🍜 ramen 🍝 spaghetti 🍠 sweet_potato 🍢 oden 🍣 sushi 🍤 fried_shrimp 🍥 fish_cake 🍡 dango ","date":"2019-10-01","objectID":"/emoji-support/:4:4","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"海鲜 图标 代码 图标 代码 🦀 crab 🦐 shrimp 🦑 squid ","date":"2019-10-01","objectID":"/emoji-support/:4:5","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"甜点 图标 代码 图标 代码 🍦 icecream 🍧 shaved_ice 🍨 ice_cream 🍩 doughnut 🍪 cookie 🎂 birthday 🍰 cake 🍫 chocolate_bar 🍬 candy 🍭 lollipop 🍮 custard 🍯 honey_pot ","date":"2019-10-01","objectID":"/emoji-support/:4:6","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"饮料 图标 代码 图标 代码 🍼 baby_bottle 🥛 milk_glass ☕ coffee 🍵 tea 🍶 sake 🍾 champagne 🍷 wine_glass 🍸 cocktail 🍹 tropical_drink 🍺 beer 🍻 beers 🥂 clinking_glasses 🥃 tumbler_glass ","date":"2019-10-01","objectID":"/emoji-support/:4:7","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"餐具 图标 代码 图标 代码 🍽️ plate_with_cutlery 🍴 fork_and_knife 🥄 spoon 🔪 hocho knife 🏺 amphora ","date":"2019-10-01","objectID":"/emoji-support/:4:8","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"旅游与地理 ","date":"2019-10-01","objectID":"/emoji-support/:5:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"地图 图标 代码 图标 代码 🌍 earth_africa 🌎 earth_americas 🌏 earth_asia 🌐 globe_with_meridians 🗺️ world_map 🗾 japan ","date":"2019-10-01","objectID":"/emoji-support/:5:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"地理现象 图标 代码 图标 代码 🏔 mountain_snow ⛰️ mountain 🌋 volcano 🗻 mount_fuji 🏕️ camping ⛱ beach_umbrella 🏜️ desert 🏝️ desert_island 🏞️ national_park ","date":"2019-10-01","objectID":"/emoji-support/:5:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"建筑物 图标 代码 图标 代码 🏟️ stadium 🏛️ classical_building 🏗️ building_construction 🏘 houses 🏚 derelict_house 🏠 house 🏡 house_with_garden 🏢 office 🏣 post_office 🏤 european_post_office 🏥 hospital 🏦 bank 🏨 hotel 🏩 love_hotel 🏪 convenience_store 🏫 school 🏬 department_store 🏭 factory 🏯 japanese_castle 🏰 european_castle 💒 wedding 🗼 tokyo_tower 🗽 statue_of_liberty ","date":"2019-10-01","objectID":"/emoji-support/:5:3","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"宗教建筑 图标 代码 图标 代码 ⛪ church 🕌 mosque 🕍 synagogue ⛩️ shinto_shrine 🕋 kaaba ","date":"2019-10-01","objectID":"/emoji-support/:5:4","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"其它地点 图标 代码 图标 代码 ⛲ fountain ⛺ tent 🌁 foggy 🌃 night_with_stars 🏙️ cityscape 🌄 sunrise_over_mountains 🌅 sunrise 🌆 city_sunset 🌇 city_sunrise 🌉 bridge_at_night ♨️ hotsprings 🎠 carousel_horse 🎡 ferris_wheel 🎢 roller_coaster 💈 barber 🎪 circus_tent ","date":"2019-10-01","objectID":"/emoji-support/:5:5","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"陆路运输 图标 代码 图标 代码 🚂 steam_locomotive 🚃 railway_car 🚄 bullettrain_side 🚅 bullettrain_front 🚆 train2 🚇 metro 🚈 light_rail 🚉 station 🚊 tram 🚝 monorail 🚞 mountain_railway 🚋 train 🚌 bus 🚍 oncoming_bus 🚎 trolleybus 🚐 minibus 🚑 ambulance 🚒 fire_engine 🚓 police_car 🚔 oncoming_police_car 🚕 taxi 🚖 oncoming_taxi 🚗 car red_car 🚘 oncoming_automobile 🚙 blue_car 🚚 truck 🚛 articulated_lorry 🚜 tractor 🏎️ racing_car 🏍 motorcycle 🛵 motor_scooter 🚲 bike 🛴 kick_scooter 🚏 busstop 🛣️ motorway 🛤️ railway_track 🛢️ oil_drum ⛽ fuelpump 🚨 rotating_light 🚥 traffic_light 🚦 vertical_traffic_light 🛑 stop_sign 🚧 construction ","date":"2019-10-01","objectID":"/emoji-support/:5:6","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"水路运输 图标 代码 图标 代码 ⚓ anchor ⛵ boat sailboat 🛶 canoe 🚤 speedboat 🛳️ passenger_ship ⛴️ ferry 🛥️ motor_boat 🚢 ship ","date":"2019-10-01","objectID":"/emoji-support/:5:7","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"空中运输 图标 代码 图标 代码 ✈️ airplane 🛩️ small_airplane 🛫 flight_departure 🛬 flight_arrival 💺 seat 🚁 helicopter 🚟 suspension_railway 🚠 mountain_cableway 🚡 aerial_tramway 🛰️ artificial_satellite 🚀 rocket ","date":"2019-10-01","objectID":"/emoji-support/:5:8","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"旅馆 icon code icon code 🛎️ bellhop_bell ","date":"2019-10-01","objectID":"/emoji-support/:5:9","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"时间 图标 代码 图标 代码 ⌛ hourglass ⏳ hourglass_flowing_sand ⌚ watch ⏰ alarm_clock ⏱️ stopwatch ⏲️ timer_clock 🕰️ mantelpiece_clock 🕛 clock12 🕧 clock1230 🕐 clock1 🕜 clock130 🕑 clock2 🕝 clock230 🕒 clock3 🕞 clock330 🕓 clock4 🕟 clock430 🕔 clock5 🕠 clock530 🕕 clock6 🕡 clock630 🕖 clock7 🕢 clock730 🕗 clock8 🕣 clock830 🕘 clock9 🕤 clock930 🕙 clock10 🕥 clock1030 🕚 clock11 🕦 clock1130 ","date":"2019-10-01","objectID":"/emoji-support/:5:10","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"天空与天气 图标 代码 图标 代码 🌑 new_moon 🌒 waxing_crescent_moon 🌓 first_quarter_moon 🌔 moon waxing_gibbous_moon 🌕 full_moon 🌖 waning_gibbous_moon 🌗 last_quarter_moon 🌘 waning_crescent_moon 🌙 crescent_moon 🌚 new_moon_with_face 🌛 first_quarter_moon_with_face 🌜 last_quarter_moon_with_face 🌡️ thermometer ☀️ sunny 🌝 full_moon_with_face 🌞 sun_with_face ⭐ star 🌟 star2 🌠 stars 🌌 milky_way ☁️ cloud ⛅ partly_sunny ⛈ cloud_with_lightning_and_rain 🌤 sun_behind_small_cloud 🌥 sun_behind_large_cloud 🌦 sun_behind_rain_cloud 🌧 cloud_with_rain 🌨 cloud_with_snow 🌩 cloud_with_lightning 🌪️ tornado 🌫️ fog 🌬 wind_face 🌀 cyclone 🌈 rainbow 🌂 closed_umbrella ☂️ open_umbrella ☂️ umbrella ⛱️ parasol_on_ground ⚡ zap ❄️ snowflake ☃️ snowman_with_snow ☃️ snowman ☄️ comet 🔥 fire 💧 droplet 🌊 ocean ","date":"2019-10-01","objectID":"/emoji-support/:5:11","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"活动 ","date":"2019-10-01","objectID":"/emoji-support/:6:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"事件 图标 代码 图标 代码 🎃 jack_o_lantern 🎄 christmas_tree 🎆 fireworks 🎇 sparkler ✨ sparkles 🎈 balloon 🎉 tada 🎊 confetti_ball 🎋 tanabata_tree 🎍 bamboo 🎎 dolls 🎏 flags 🎐 wind_chime 🎑 rice_scene 🎀 ribbon 🎁 gift 🎗️ reminder_ribbon 🎟 tickets 🎫 ticket ","date":"2019-10-01","objectID":"/emoji-support/:6:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"奖杯与奖牌 图标 代码 图标 代码 🎖️ medal_military 🏆 trophy 🏅 medal_sports 🥇 1st_place_medal 🥈 2nd_place_medal 🥉 3rd_place_medal ","date":"2019-10-01","objectID":"/emoji-support/:6:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"体育运动 图标 代码 图标 代码 ⚽ soccer ⚾ baseball 🏀 basketball 🏐 volleyball 🏈 football 🏉 rugby_football 🎾 tennis 🎳 bowling 🦗 cricket 🏑 field_hockey 🏒 ice_hockey 🏓 ping_pong 🏸 badminton 🥊 boxing_glove 🥋 martial_arts_uniform 🥅 goal_net ⛳ golf ⛸️ ice_skate 🎣 fishing_pole_and_fish 🎽 running_shirt_with_sash 🎿 ski ","date":"2019-10-01","objectID":"/emoji-support/:6:3","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"游戏 图标 代码 图标 代码 🎯 dart 🎱 8ball 🔮 crystal_ball 🎮 video_game 🕹️ joystick 🎰 slot_machine 🎲 game_die ♠️ spades ♥️ hearts ♦️ diamonds ♣️ clubs 🃏 black_joker 🀄 mahjong 🎴 flower_playing_cards ","date":"2019-10-01","objectID":"/emoji-support/:6:4","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"艺术与工艺 图标 代码 图标 代码 🎭 performing_arts 🖼 framed_picture 🎨 art ","date":"2019-10-01","objectID":"/emoji-support/:6:5","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"物品 ","date":"2019-10-01","objectID":"/emoji-support/:7:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"服装 图标 代码 图标 代码 👓 eyeglasses 🕶️ dark_sunglasses 👔 necktie 👕 shirt tshirt 👖 jeans 👗 dress 👘 kimono 👙 bikini 👚 womans_clothes 👛 purse 👜 handbag 👝 pouch 🛍️ shopping 🎒 school_satchel 👞 mans_shoe shoe 👟 athletic_shoe 👠 high_heel 👡 sandal 👢 boot 👑 crown 👒 womans_hat 🎩 tophat 🎓 mortar_board ⛑️ rescue_worker_helmet 📿 prayer_beads 💄 lipstick 💍 ring 💎 gem ","date":"2019-10-01","objectID":"/emoji-support/:7:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"声音 图标 代码 图标 代码 🔇 mute 🔈 speaker 🔉 sound 🔊 loud_sound 📢 loudspeaker 📣 mega 📯 postal_horn 🔔 bell 🔕 no_bell ","date":"2019-10-01","objectID":"/emoji-support/:7:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"音乐 图标 代码 图标 代码 🎼 musical_score 🎵 musical_note 🎶 notes 🎙️ studio_microphone 🎚️ level_slider 🎛️ control_knobs 🎤 microphone 🎧 headphones 📻 radio ","date":"2019-10-01","objectID":"/emoji-support/:7:3","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"乐器 图标 代码 图标 代码 🎷 saxophone 🎸 guitar 🎹 musical_keyboard 🎺 trumpet 🎻 violin 🥁 drum ","date":"2019-10-01","objectID":"/emoji-support/:7:4","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"电话 图标 代码 图标 代码 📱 iphone 📲 calling ☎️ phone telephone 📞 telephone_receiver 📟 pager 📠 fax ","date":"2019-10-01","objectID":"/emoji-support/:7:5","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"电脑 图标 代码 图标 代码 🔋 battery 🔌 electric_plug 💻 computer 🖥️ desktop_computer 🖨️ printer ⌨️ keyboard 🖱 computer_mouse 🖲️ trackball 💽 minidisc 💾 floppy_disk 💿 cd 📀 dvd ","date":"2019-10-01","objectID":"/emoji-support/:7:6","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"灯光与影像 图标 代码 图标 代码 🎥 movie_camera 🎞️ film_strip 📽️ film_projector 🎬 clapper 📺 tv 📷 camera 📸 camera_flash 📹 video_camera 📼 vhs 🔍 mag 🔎 mag_right 🕯️ candle 💡 bulb 🔦 flashlight 🏮 izakaya_lantern lantern ","date":"2019-10-01","objectID":"/emoji-support/:7:7","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"书与纸张 图标 代码 图标 代码 📔 notebook_with_decorative_cover 📕 closed_book 📖 book open_book 📗 green_book 📘 blue_book 📙 orange_book 📚 books 📓 notebook 📒 ledger 📃 page_with_curl 📜 scroll 📄 page_facing_up 📰 newspaper 🗞️ newspaper_roll 📑 bookmark_tabs 🔖 bookmark 🏷️ label ","date":"2019-10-01","objectID":"/emoji-support/:7:8","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"钱 图标 代码 图标 代码 💰 moneybag 💴 yen 💵 dollar 💶 euro 💷 pound 💸 money_with_wings 💳 credit_card 💹 chart ","date":"2019-10-01","objectID":"/emoji-support/:7:9","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"邮件 图标 代码 图标 代码 ✉️ email envelope 📧 📧 📨 incoming_envelope 📩 envelope_with_arrow 📤 outbox_tray 📥 inbox_tray 📦 package 📫 mailbox 📪 mailbox_closed 📬 mailbox_with_mail 📭 mailbox_with_no_mail 📮 postbox 🗳 ballot_box ","date":"2019-10-01","objectID":"/emoji-support/:7:10","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"书写 图标 代码 图标 代码 ✏️ pencil2 ✒️ black_nib 🖋 fountain_pen 🖊 pen 🖌 paintbrush 🖍 crayon 📝 memo pencil ","date":"2019-10-01","objectID":"/emoji-support/:7:11","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"办公 图标 代码 图标 代码 💼 briefcase 📁 file_folder 📂 open_file_folder 🗂️ card_index_dividers 📅 date 📆 calendar 🗒 spiral_notepad 🗓 spiral_calendar 📇 card_index 📈 chart_with_upwards_trend 📉 chart_with_downwards_trend 📊 bar_chart 📋 clipboard 📌 pushpin 📍 round_pushpin 📎 paperclip 🖇 paperclips 📏 straight_ruler 📐 triangular_ruler ✂️ scissors 🗃️ card_file_box 🗄️ file_cabinet 🗑️ wastebasket ","date":"2019-10-01","objectID":"/emoji-support/:7:12","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"锁 图标 代码 图标 代码 🔒 lock 🔓 unlock 🔏 lock_with_ink_pen 🔐 closed_lock_with_key 🔑 key 🗝️ old_key ","date":"2019-10-01","objectID":"/emoji-support/:7:13","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"工具 图标 代码 图标 代码 🔨 hammer ⛏️ pick ⚒️ hammer_and_pick 🛠️ hammer_and_wrench 🗡 dagger ⚔️ crossed_swords 🔫 gun 🏹 bow_and_arrow 🛡️ shield 🔧 wrench 🔩 nut_and_bolt ⚙️ gear 🗜 clamp ⚖ balance_scale 🔗 link ⛓️ chains ","date":"2019-10-01","objectID":"/emoji-support/:7:14","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"科学 图标 代码 图标 代码 ⚗️ alembic 🔬 microscope 🔭 telescope 🛰️ satellite ","date":"2019-10-01","objectID":"/emoji-support/:7:15","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"医疗 图标 代码 图标 代码 💉 syringe 💊 pill ","date":"2019-10-01","objectID":"/emoji-support/:7:16","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"生活用品 图标 代码 图标 代码 🚪 door 🛏️ bed 🛋️ couch_and_lamp 🚽 toilet 🚿 shower 🛁 bathtub 🛒 shopping_cart ","date":"2019-10-01","objectID":"/emoji-support/:7:17","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"其它物品 图标 代码 图标 代码 🚬 smoking ⚰️ coffin ⚱️ funeral_urn 🗿 moyai ","date":"2019-10-01","objectID":"/emoji-support/:7:18","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"符号 ","date":"2019-10-01","objectID":"/emoji-support/:8:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"交通标识 图标 代码 图标 代码 🏧 atm 🚮 put_litter_in_its_place 🚰 potable_water ♿ wheelchair 🚹 mens 🚺 womens 🚻 restroom 🚼 baby_symbol 🚾 wc 🛂 passport_control 🛃 customs 🛄 baggage_claim 🛅 left_luggage ","date":"2019-10-01","objectID":"/emoji-support/:8:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"警告 图标 代码 图标 代码 ⚠️ warning 🚸 children_crossing ⛔ no_entry 🚫 no_entry_sign 🚳 no_bicycles 🚭 no_smoking 🚯 do_not_litter 🚱 🚱 🚷 no_pedestrians 📵 no_mobile_phones 🔞 underage ☢ radioactive ☣ biohazard ","date":"2019-10-01","objectID":"/emoji-support/:8:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"箭头 图标 代码 图标 代码 ⬆️ arrow_up ↗️ arrow_upper_right ➡️ arrow_right ↘️ arrow_lower_right ⬇️ arrow_down ↙️ arrow_lower_left ⬅️ arrow_left ↖️ arrow_upper_left ↕️ arrow_up_down ↔️ left_right_arrow ↩️ leftwards_arrow_with_hook ↪️ arrow_right_hook ⤴️ arrow_heading_up ⤵️ arrow_heading_down 🔃 arrows_clockwise 🔄 arrows_counterclockwise 🔙 back 🔚 end 🔛 on 🔜 soon 🔝 top ","date":"2019-10-01","objectID":"/emoji-support/:8:3","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"宗教 图标 代码 图标 代码 🛐 place_of_worship ⚛️ atom_symbol 🕉 om ✡️ star_of_david ☸️ wheel_of_dharma ☯️ yin_yang ✝️ latin_cross ☦️ orthodox_cross ☪️ star_and_crescent ☮️ peace_symbol 🕎 menorah 🔯 six_pointed_star ","date":"2019-10-01","objectID":"/emoji-support/:8:4","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"生肖 图标 代码 图标 代码 ♈ aries ♉ taurus ♊ gemini ♋ cancer ♌ leo ♍ virgo ♎ libra ♏ scorpius ♐ sagittarius ♑ capricorn ♒ aquarius ♓ pisces ⛎ ophiuchus ","date":"2019-10-01","objectID":"/emoji-support/:8:5","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"影像符号 图标 代码 图标 代码 🔀 twisted_rightwards_arrows 🔁 repeat 🔂 repeat_one ▶️ arrow_forward ⏩ fast_forward ⏭ next_track_button ⏯ play_or_pause_button ◀️ arrow_backward ⏪ rewind ⏮️ previous_track_button 🔼 arrow_up_small ⏫ arrow_double_up 🔽 arrow_down_small ⏬ arrow_double_down ⏸ pause_button ⏹ stop_button ⏺ record_button 🎦 cinema 🔅 low_brightness 🔆 high_brightness 📶 signal_strength 📳 vibration_mode 📴 mobile_phone_off ","date":"2019-10-01","objectID":"/emoji-support/:8:6","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"数学 图标 代码 图标 代码 ✖️ heavy_multiplication_x ➕ heavy_plus_sign ➖ heavy_minus_sign ➗ heavy_division_sign ","date":"2019-10-01","objectID":"/emoji-support/:8:7","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"标点符号 图标 代码 图标 代码 ‼️ bangbang ⁉️ interrobang ❓ question ❔ grey_question ❕ grey_exclamation ❗ exclamation heavy_exclamation_mark 〰️ wavy_dash ","date":"2019-10-01","objectID":"/emoji-support/:8:8","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"货币 图标 代码 图标 代码 💱 currency_exchange 💲 heavy_dollar_sign ","date":"2019-10-01","objectID":"/emoji-support/:8:9","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"按键符号 图标 代码 图标 代码 #️⃣ hash *️⃣ asterisk 0️⃣ zero 1️⃣ one 2️⃣ two 3️⃣ three 4️⃣ four 5️⃣ five 6️⃣ six 7️⃣ seven 8️⃣ eight 9️⃣ nine 🔟 keycap_ten ","date":"2019-10-01","objectID":"/emoji-support/:8:10","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"字母符号 图标 代码 图标 代码 🔠 capital_abcd 🔡 abcd 🔢 1234 🔣 symbols 🔤 abc 🅰️ a 🆎 ab 🅱️ b 🆑 cl 🆒 cool 🆓 free ℹ️ information_source 🆔 id ⓜ️ m 🆕 new 🆖 ng 🅾️ o2 🆗 ok 🅿️ parking 🆘 sos 🆙 up 🆚 vs 🈁 koko 🈂️ sa 🈷️ u6708 🈶 u6709 🈯 u6307 🉐 ideograph_advantage 🈹 u5272 🈚 u7121 🈲 u7981 🉑 accept 🈸 u7533 🈴 u5408 🈳 u7a7a ㊗️ congratulations ㊙️ secret 🈺 u55b6 🈵 u6e80 ","date":"2019-10-01","objectID":"/emoji-support/:8:11","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"几何符号 图标 代码 图标 代码 🔴 red_circle 🔵 large_blue_circle ⚫ black_circle ⚪ white_circle ⬛ black_large_square ⬜ white_large_square ◼️ black_medium_square ◻️ white_medium_square ◾ black_medium_small_square ◽ white_medium_small_square ▪️ black_small_square ▫️ white_small_square 🔶 large_orange_diamond 🔷 large_blue_diamond 🔸 small_orange_diamond 🔹 small_blue_diamond 🔺 small_red_triangle 🔻 small_red_triangle_down 💠 diamond_shape_with_a_dot_inside 🔘 radio_button 🔳 white_square_button 🔲 black_square_button ","date":"2019-10-01","objectID":"/emoji-support/:8:12","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"其它符合 图标 代码 图标 代码 ♻️ recycle ⚜️ fleur_de_lis 🔱 trident 📛 name_badge 🔰 beginner ⭕ o ✅ white_check_mark ☑️ ballot_box_with_check ✔️ heavy_check_mark ❌ x ❎ negative_squared_cross_mark ➰ curly_loop ➿ loop 〽️ part_alternation_mark ✳️ eight_spoked_asterisk ✴️ eight_pointed_black_star ❇️ sparkle ©️ copyright ®️ registered ™️ tm ","date":"2019-10-01","objectID":"/emoji-support/:8:13","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"旗帜 ","date":"2019-10-01","objectID":"/emoji-support/:9:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"常用旗帜 图标 代码 图标 代码 🏁 checkered_flag 🚩 triangular_flag_on_post 🎌 crossed_flags 🏴 black_flag 🏳 white_flag 🏳️‍🌈 rainbow_flag ","date":"2019-10-01","objectID":"/emoji-support/:9:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"国家和地区旗帜 图标 代码 图标 代码 🇦🇩 andorra 🇦🇪 united_arab_emirates 🇦🇫 afghanistan 🇦🇬 antigua_barbuda 🇦🇮 anguilla 🇦🇱 albania 🇦🇲 armenia 🇦🇴 angola 🇦🇶 antarctica 🇦🇷 argentina 🇦🇸 american_samoa 🇦🇹 austria 🇦🇺 australia 🇦🇼 aruba 🇦🇽 aland_islands 🇦🇿 azerbaijan 🇧🇦 bosnia_herzegovina 🇧🇧 barbados 🇧🇩 bangladesh 🇧🇪 belgium 🇧🇫 burkina_faso 🇧🇬 bulgaria 🇧🇭 bahrain 🇧🇮 burundi 🇧🇯 benin 🇧🇱 st_barthelemy 🇧🇲 bermuda 🇧🇳 brunei 🇧🇴 bolivia 🇧🇶 caribbean_netherlands 🇧🇷 brazil 🇧🇸 bahamas 🇧🇹 bhutan 🇧🇼 botswana 🇧🇾 belarus 🇧🇿 belize 🇨🇦 canada 🇨🇨 cocos_islands 🇨🇩 congo_kinshasa 🇨🇫 central_african_republic 🇨🇬 congo_brazzaville 🇨🇭 switzerland 🇨🇮 cote_divoire 🇨🇰 cook_islands 🇨🇱 chile 🇨🇲 cameroon 🇨🇳 cn 🇨🇴 colombia 🇨🇷 costa_rica 🇨🇺 cuba 🇨🇻 cape_verde 🇨🇼 curacao 🇨🇽 christmas_island 🇨🇾 cyprus 🇨🇿 czech_republic 🇩🇪 de 🇩🇯 djibouti 🇩🇰 denmark 🇩🇲 dominica 🇩🇴 dominican_republic 🇩🇿 algeria 🇪🇨 ecuador 🇪🇪 estonia 🇪🇬 egypt 🇪🇭 western_sahara 🇪🇷 eritrea 🇪🇸 es 🇪🇹 ethiopia 🇪🇺 eu european_union 🇫🇮 finland 🇫🇯 fiji 🇫🇰 falkland_islands 🇫🇲 micronesia 🇫🇴 faroe_islands 🇫🇷 fr 🇬🇦 gabon 🇬🇧 gb uk 🇬🇩 grenada 🇬🇪 georgia 🇬🇫 french_guiana 🇬🇬 guernsey 🇬🇭 ghana 🇬🇮 gibraltar 🇬🇱 greenland 🇬🇲 gambia 🇬🇳 guinea 🇬🇵 guadeloupe 🇬🇶 equatorial_guinea 🇬🇷 greece 🇬🇸 south_georgia_south_sandwich_islands 🇬🇹 guatemala 🇬🇺 guam 🇬🇼 guinea_bissau 🇬🇾 guyana 🇭🇰 hong_kong 🇭🇳 honduras 🇭🇷 croatia 🇭🇹 haiti 🇭🇺 hungary 🇮🇨 canary_islands 🇮🇩 indonesia 🇮🇪 ireland 🇮🇱 israel 🇮🇲 isle_of_man 🇮🇳 india 🇮🇴 british_indian_ocean_territory 🇮🇶 iraq 🇮🇷 iran 🇮🇸 iceland 🇮🇹 it 🇯🇪 jersey 🇯🇲 jamaica 🇯🇴 jordan 🇯🇵 jp 🇰🇪 kenya 🇰🇬 kyrgyzstan 🇰🇭 cambodia 🇰🇮 kiribati 🇰🇲 comoros 🇰🇳 st_kitts_nevis 🇰🇵 north_korea 🇰🇷 kr 🇰🇼 kuwait 🇰🇾 cayman_islands 🇰🇿 kazakhstan 🇱🇦 laos 🇱🇧 lebanon 🇱🇨 st_lucia 🇱🇮 liechtenstein 🇱🇰 sri_lanka 🇱🇷 liberia 🇱🇸 lesotho 🇱🇹 lithuania 🇱🇺 luxembourg 🇱🇻 latvia 🇱🇾 libya 🇲🇦 morocco 🇲🇨 monaco 🇲🇩 moldova 🇲🇪 montenegro 🇲🇬 madagascar 🇲🇭 marshall_islands 🇲🇰 macedonia 🇲🇱 mali 🇲🇲 myanmar 🇲🇳 mongolia 🇲🇴 macau 🇲🇵 northern_mariana_islands 🇲🇶 martinique 🇲🇷 mauritania 🇲🇸 montserrat 🇲🇹 malta 🇲🇺 mauritius 🇲🇻 maldives 🇲🇼 malawi 🇲🇽 mexico 🇲🇾 malaysia 🇲🇿 mozambique 🇳🇦 namibia 🇳🇨 new_caledonia 🇳🇪 niger 🇳🇫 norfolk_island 🇳🇬 nigeria 🇳🇮 nicaragua 🇳🇱 netherlands 🇳🇴 norway 🇳🇵 nepal 🇳🇷 nauru 🇳🇺 niue 🇳🇿 new_zealand 🇴🇲 oman 🇵🇦 panama 🇵🇪 peru 🇵🇫 french_polynesia 🇵🇬 papua_new_guinea 🇵🇭 philippines 🇵🇰 pakistan 🇵🇱 poland 🇵🇲 st_pierre_miquelon 🇵🇳 pitcairn_islands 🇵🇷 puerto_rico 🇵🇸 palestinian_territories 🇵🇹 portugal 🇵🇼 palau 🇵🇾 paraguay 🇶🇦 qatar 🇷🇪 reunion 🇷🇴 romania 🇷🇸 serbia 🇷🇺 ru 🇷🇼 rwanda 🇸🇦 saudi_arabia 🇸🇧 solomon_islands 🇸🇨 seychelles 🇸🇩 sudan 🇸🇪 sweden 🇸🇬 singapore 🇸🇭 st_helena 🇸🇮 slovenia 🇸🇰 slovakia 🇸🇱 sierra_leone 🇸🇲 san_marino 🇸🇳 senegal 🇸🇴 somalia 🇸🇷 suriname 🇸🇸 south_sudan 🇸🇹 sao_tome_principe 🇸🇻 el_salvador 🇸🇽 sint_maarten 🇸🇾 syria 🇸🇿 swaziland 🇹🇨 turks_caicos_islands 🇹🇩 chad 🇹🇫 french_southern_territories 🇹🇬 togo 🇹🇭 thailand 🇹🇯 tajikistan 🇹🇰 tokelau 🇹🇱 timor_leste 🇹🇲 turkmenistan 🇹🇳 tunisia 🇹🇴 tonga 🇹🇷 tr 🇹🇹 trinidad_tobago 🇹🇻 tuvalu 🇹🇼 taiwan 🇹🇿 tanzania 🇺🇦 ukraine 🇺🇬 uganda 🇺🇸 us 🇺🇾 uruguay 🇺🇿 uzbekistan 🇻🇦 vatican_city 🇻🇨 st_vincent_grenadines 🇻🇪 venezuela 🇻🇬 british_virgin_islands 🇻🇮 us_virgin_islands 🇻🇳 vietnam 🇻🇺 vanuatu 🇼🇫 wallis_futuna 🇼🇸 samoa 🇽🇰 kosovo 🇾🇪 yemen 🇾🇹 mayotte 🇿🇦 south_africa 🇿🇲 zambia 🇿🇼 zimbabwe ","date":"2019-10-01","objectID":"/emoji-support/:9:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["documentation"],"content":"echarts shortcode 使用 ECharts 库提供数据可视化的功能.","date":"2020-03-03","objectID":"/theme-documentation-echarts-shortcode/","tags":["shortcodes"],"title":"主题文档 - echarts Shortcode","uri":"/theme-documentation-echarts-shortcode/"},{"categories":["documentation"],"content":"echarts shortcode 使用 ECharts 库提供数据可视化的功能. ECharts 是一个帮助你生成交互式数据可视化的库. ECharts 提供了常规的 折线图, 柱状图, 散点图, 饼图, K线图, 用于统计的 盒形图, 用于地理数据可视化的 地图, 热力图, 线图, 用于关系数据可视化的 关系图, treemap, 旭日图, 多维数据可视化的 平行坐标, 还有用于 BI 的 漏斗图, 仪表盘, 并且支持图与图之间的混搭. 只需在 echarts shortcode 中以 JSON/YAML/TOML格式插入 ECharts 选项即可. 一个 JSON 格式的 echarts 示例: {{\u003c echarts \u003e}} { \"title\": { \"text\": \"折线统计图\", \"top\": \"2%\", \"left\": \"center\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"邮件营销\", \"联盟广告\", \"视频广告\", \"直接访问\", \"搜索引擎\"], \"top\": \"10%\" }, \"grid\": { \"left\": \"5%\", \"right\": \"5%\", \"bottom\": \"5%\", \"top\": \"20%\", \"containLabel\": true }, \"toolbox\": { \"feature\": { \"saveAsImage\": { \"title\": \"保存为图片\" } } }, \"xAxis\": { \"type\": \"category\", \"boundaryGap\": false, \"data\": [\"周一\", \"周二\", \"周三\", \"周四\", \"周五\", \"周六\", \"周日\"] }, \"yAxis\": { \"type\": \"value\" }, \"series\": [ { \"name\": \"邮件营销\", \"type\": \"line\", \"stack\": \"总量\", \"data\": [120, 132, 101, 134, 90, 230, 210] }, { \"name\": \"联盟广告\", \"type\": \"line\", \"stack\": \"总量\", \"data\": [220, 182, 191, 234, 290, 330, 310] }, { \"name\": \"视频广告\", \"type\": \"line\", \"stack\": \"总量\", \"data\": [150, 232, 201, 154, 190, 330, 410] }, { \"name\": \"直接访问\", \"type\": \"line\", \"stack\": \"总量\", \"data\": [320, 332, 301, 334, 390, 330, 320] }, { \"name\": \"搜索引擎\", \"type\": \"line\", \"stack\": \"总量\", \"data\": [820, 932, 901, 934, 1290, 1330, 1320] } ] } {{\u003c /echarts \u003e}} 一个 YAML 格式的 echarts 示例: {{\u003c echarts \u003e}} title: text: 折线统计图 top: 2% left: center tooltip: trigger: axis legend: data: - 邮件营销 - 联盟广告 - 视频广告 - 直接访问 - 搜索引擎 top: 10% grid: left: 5% right: 5% bottom: 5% top: 20% containLabel: true toolbox: feature: saveAsImage: title: 保存为图片 xAxis: type: category boundaryGap: false data: - 周一 - 周二 - 周三 - 周四 - 周五 - 周六 - 周日 yAxis: type: value series: - name: 邮件营销 type: line stack: 总量 data: - 120 - 132 - 101 - 134 - 90 - 230 - 210 - name: 联盟广告 type: line stack: 总量 data: - 220 - 182 - 191 - 234 - 290 - 330 - 310 - name: 视频广告 type: line stack: 总量 data: - 150 - 232 - 201 - 154 - 190 - 330 - 410 - name: 直接访问 type: line stack: 总量 data: - 320 - 332 - 301 - 334 - 390 - 330 - 320 - name: 搜索引擎 type: line stack: 总量 data: - 820 - 932 - 901 - 934 - 1290 - 1330 - 1320 {{\u003c /echarts \u003e}} 一个 TOML 格式的 echarts 示例: {{\u003c echarts \u003e}} [title] text = \"折线统计图\" top = \"2%\" left = \"center\" [tooltip] trigger = \"axis\" [legend] data = [ \"邮件营销\", \"联盟广告\", \"视频广告\", \"直接访问\", \"搜索引擎\" ] top = \"10%\" [grid] left = \"5%\" right = \"5%\" bottom = \"5%\" top = \"20%\" containLabel = true [toolbox] [toolbox.feature] [toolbox.feature.saveAsImage] title = \"保存为图片\" [xAxis] type = \"category\" boundaryGap = false data = [ \"周一\", \"周二\", \"周三\", \"周四\", \"周五\", \"周六\", \"周日\" ] [yAxis] type = \"value\" [[series]] name = \"邮件营销\" type = \"line\" stack = \"总量\" data = [ 120.0, 132.0, 101.0, 134.0, 90.0, 230.0, 210.0 ] [[series]] name = \"联盟广告\" type = \"line\" stack = \"总量\" data = [ 220.0, 182.0, 191.0, 234.0, 290.0, 330.0, 310.0 ] [[series]] name = \"视频广告\" type = \"line\" stack = \"总量\" data = [ 150.0, 232.0, 201.0, 154.0, 190.0, 330.0, 410.0 ] [[series]] name = \"直接访问\" type = \"line\" stack = \"总量\" data = [ 320.0, 332.0, 301.0, 334.0, 390.0, 330.0, 320.0 ] [[series]] name = \"搜索引擎\" type = \"line\" stack = \"总量\" data = [ 820.0, 932.0, 901.0, 934.0, 1290.0, 1330.0, 1320.0 ] {{\u003c /echarts \u003e}} 呈现的输出效果如下: echarts shortcode 还有以下命名参数: width [可选] (第一个位置参数) 数据可视化的宽度, 默认值是 100%. height [可选] (第二个位置参数) 数据可视化的高度, 默认值是 30rem. ","date":"2020-03-03","objectID":"/theme-documentation-echarts-shortcode/:0:0","tags":["shortcodes"],"title":"主题文档 - echarts Shortcode","uri":"/theme-documentation-echarts-shortcode/"},{"categories":["documentation"],"content":"mapbox shortcode 使用 Mapbox GL JS 库提供互动式地图的功能.","date":"2020-03-03","objectID":"/theme-documentation-mapbox-shortcode/","tags":["shortcodes"],"title":"主题文档 - mapbox Shortcode","uri":"/theme-documentation-mapbox-shortcode/"},{"categories":["documentation"],"content":" mapbox shortcode 使用 Mapbox GL JS 库提供互动式地图的功能. Mapbox GL JS 是一个 JavaScript 库，它使用 WebGL, 以 vector tiles 和 Mapbox styles 为来源, 将它们渲染成互动式地图. mapbox shortcode 有以下命名参数来使用 Mapbox GL JS: lng [必需] (第一个位置参数) 地图初始中心点的经度, 以度为单位. lat [必需] (第二个位置参数) 地图初始中心点的纬度, 以度为单位. zoom [可选] (第三个位置参数) 地图的初始缩放级别, 默认值是 10. marked [可选] (第四个位置参数) 是否在地图的初始中心点添加图钉, 默认值是 true. light-style [可选] (第五个位置参数) 浅色主题的地图样式, 默认值是前置参数或者网站配置中设置的值. dark-style [可选] (第六个位置参数) 深色主题的地图样式, 默认值是前置参数或者网站配置中设置的值. navigation [可选] 是否添加 NavigationControl, 默认值是前置参数或者网站配置中设置的值. geolocate [可选] 是否添加 GeolocateControl, 默认值是前置参数或者网站配置中设置的值. scale [可选] 是否添加 ScaleControl, 默认值是前置参数或者网站配置中设置的值. fullscreen [可选] 是否添加 FullscreenControl, 默认值是前置参数或者网站配置中设置的值. width [可选] 地图的宽度, 默认值是 100%. height [可选] 地图的高度, 默认值是 20rem. 一个简单的 mapbox 示例: {{\u003c mapbox 121.485 31.233 12 \u003e}} 或者 {{\u003c mapbox lng=121.485 lat=31.233 zoom=12 \u003e}} 呈现的输出效果如下: 一个带有自定义样式的 mapbox 示例: {{\u003c mapbox -122.252 37.453 10 false \"mapbox://styles/mapbox/streets-zh-v1?optimize=true\" \u003e}} 或者 {{\u003c mapbox lng=-122.252 lat=37.453 zoom=10 marked=false light-style=\"mapbox://styles/mapbox/streets-zh-v1?optimize=true\" \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-mapbox-shortcode/:0:0","tags":["shortcodes"],"title":"主题文档 - mapbox Shortcode","uri":"/theme-documentation-mapbox-shortcode/"},{"categories":["documentation"],"content":"music shortcode 基于 APlayer 和 MetingJS 库提供了一个内嵌的响应式音乐播放器.","date":"2020-03-03","objectID":"/theme-documentation-music-shortcode/","tags":["shortcodes"],"title":"主题文档 - music Shortcode","uri":"/theme-documentation-music-shortcode/"},{"categories":["documentation"],"content":"music shortcode 基于 APlayer 和 MetingJS 库提供了一个内嵌的响应式音乐播放器. 有三种方式使用 music shortcode. ","date":"2020-03-03","objectID":"/theme-documentation-music-shortcode/:0:0","tags":["shortcodes"],"title":"主题文档 - music Shortcode","uri":"/theme-documentation-music-shortcode/"},{"categories":["documentation"],"content":"1 自定义音乐 URL 支持本地资源引用的完整用法. music shortcode 有以下命名参数来使用自定义音乐 URL: server [必需] 音乐的链接. type [可选] 音乐的名称. artist [可选] 音乐的创作者. cover [可选] 音乐的封面链接. 一个使用自定义音乐 URL 的 music 示例: {{\u003c music url=\"/music/Wavelength.mp3\" name=Wavelength artist=oldmanyoung cover=\"/images/Wavelength.jpg\" \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-music-shortcode/:1:0","tags":["shortcodes"],"title":"主题文档 - music Shortcode","uri":"/theme-documentation-music-shortcode/"},{"categories":["documentation"],"content":"2 音乐平台 URL 的自动识别 music shortcode 有一个命名参数来使用音乐平台 URL 的自动识别: auto [必需]] (第一个位置参数) 用来自动识别的音乐平台 URL, 支持 netease, tencent 和 xiami 平台. 一个使用音乐平台 URL 的自动识别的 music 示例: {{\u003c music auto=\"https://music.163.com/#/playlist?id=60198\" \u003e}} 或者 {{\u003c music \"https://music.163.com/#/playlist?id=60198\" \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-music-shortcode/:2:0","tags":["shortcodes"],"title":"主题文档 - music Shortcode","uri":"/theme-documentation-music-shortcode/"},{"categories":["documentation"],"content":"3 自定义音乐平台, 类型和 ID music shortcode 有以下命名参数来使用自定义音乐平台: server [必需] (第一个位置参数) [netease, tencent, kugou, xiami, baidu] 音乐平台. type [必需] (第二个位置参数) [song, playlist, album, search, artist] 音乐类型. id [必需] (第三个位置参数) 歌曲 ID, 或者播放列表 ID, 或者专辑 ID, 或者搜索关键词, 或者创作者 ID. 一个使用自定义音乐平台的 music 示例: {{\u003c music server=\"netease\" type=\"song\" id=\"1868553\" \u003e}} 或者 {{\u003c music netease song 1868553 \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-music-shortcode/:3:0","tags":["shortcodes"],"title":"主题文档 - music Shortcode","uri":"/theme-documentation-music-shortcode/"},{"categories":["documentation"],"content":"4 其它参数 music shortcode 有一些可以应用于以上三种方式的其它命名参数: theme [可选] 音乐播放器的主题色, 默认值是 #448aff. fixed [可选] 是否开启固定模式, 默认值是 false. mini [可选] 是否开启迷你模式, 默认值是 false. autoplay [可选] 是否自动播放音乐, 默认值是 false. volume [可选] 第一次打开播放器时的默认音量, 会被保存在浏览器缓存中, 默认值是 0.7. mutex [可选] 是否自动暂停其它播放器, 默认值是 true. music shortcode 还有一些只适用于音乐列表方式的其它命名参数: loop [可选] [all, one, none] 音乐列表的循环模式, 默认值是 none. order [可选] [list, random] 音乐列表的播放顺序, 默认值是 list. list-folded [可选] 初次打开的时候音乐列表是否折叠, 默认值是 false. list-max-height [可选] 音乐列表的最大高度, 默认值是 340px. ","date":"2020-03-03","objectID":"/theme-documentation-music-shortcode/:4:0","tags":["shortcodes"],"title":"主题文档 - music Shortcode","uri":"/theme-documentation-music-shortcode/"},{"categories":["documentation"],"content":"bilibili shortcode 提供了一个内嵌的用来播放 bilibili 视频的响应式播放器.","date":"2020-03-03","objectID":"/theme-documentation-bilibili-shortcode/","tags":["shortcodes"],"title":"主题文档 - bilibili Shortcode","uri":"/theme-documentation-bilibili-shortcode/"},{"categories":["documentation"],"content":" bilibili shortcode 提供了一个内嵌的用来播放 bilibili 视频的响应式播放器. 如果视频只有一个部分, 则仅需要视频的 BV id, 例如: https://www.bilibili.com/video/BV1Sx411T7QQ 一个 bilibili 示例: {{\u003c bilibili BV1Sx411T7QQ \u003e}} 或者 {{\u003c bilibili id=BV1Sx411T7QQ \u003e}} 呈现的输出效果如下: 如果视频包含多个部分, 则除了视频的 BV id 之外, 还需要 p, 默认值为 1, 例如: https://www.bilibili.com/video/BV1TJ411C7An?p=3 一个带有 p 参数的 bilibili 示例: {{\u003c bilibili BV1TJ411C7An 3 \u003e}} 或者 {{\u003c bilibili id=BV1TJ411C7An p=3 \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-bilibili-shortcode/:0:0","tags":["shortcodes"],"title":"主题文档 - bilibili Shortcode","uri":"/theme-documentation-bilibili-shortcode/"},{"categories":["documentation"],"content":"typeit shortcode 基于 TypeIt 库提供了打字动画.","date":"2020-03-03","objectID":"/theme-documentation-typeit-shortcode/","tags":["shortcodes"],"title":"主题文档 - typeit Shortcode","uri":"/theme-documentation-typeit-shortcode/"},{"categories":["documentation"],"content":"typeit shortcode 基于 TypeIt 库提供了打字动画. 只需将你需要打字动画的内容插入 typeit shortcode 中即可. ","date":"2020-03-03","objectID":"/theme-documentation-typeit-shortcode/:0:0","tags":["shortcodes"],"title":"主题文档 - typeit Shortcode","uri":"/theme-documentation-typeit-shortcode/"},{"categories":["documentation"],"content":"1 简单内容 允许使用 Markdown 格式的简单内容, 并且 不包含 富文本的块内容, 例如图像等等… 一个 typeit 示例: {{\u003c typeit \u003e}} 这一个带有基于 [TypeIt](https://typeitjs.com/) 的 **打字动画** 的 *段落*... {{\u003c /typeit \u003e}} 呈现的输出效果如下: 另外, 你也可以自定义 HTML 标签. 一个带有 h4 标签的 typeit 示例: {{\u003c typeit tag=h4 \u003e}} 这一个带有基于 [TypeIt](https://typeitjs.com/) 的 **打字动画** 的 *段落*... {{\u003c /typeit \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-typeit-shortcode/:1:0","tags":["shortcodes"],"title":"主题文档 - typeit Shortcode","uri":"/theme-documentation-typeit-shortcode/"},{"categories":["documentation"],"content":"2 代码内容 代码内容也是允许的, 并且通过使用参数 code 指定语言类型可以实习语法高亮. 一个带有 code 参数的 typeit 示例: {{\u003c typeit code=java \u003e}} public class HelloWorld { public static void main(String []args) { System.out.println(\"Hello World\"); } } {{\u003c /typeit \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-typeit-shortcode/:2:0","tags":["shortcodes"],"title":"主题文档 - typeit Shortcode","uri":"/theme-documentation-typeit-shortcode/"},{"categories":["documentation"],"content":"3 分组内容 默认情况下, 所有打字动画都是同时开始的. 但是有时你可能需要按顺序开始一组 typeit 内容的打字动画. 一组具有相同 group 参数值的 typeit 内容将按顺序开始打字动画. 一个带有 group 参数的 typeit 示例: {{\u003c typeit group=paragraph \u003e}} **首先**, 这个段落开始 {{\u003c /typeit \u003e}} {{\u003c typeit group=paragraph \u003e}} **然后**, 这个段落开始 {{\u003c /typeit \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-typeit-shortcode/:3:0","tags":["shortcodes"],"title":"主题文档 - typeit Shortcode","uri":"/theme-documentation-typeit-shortcode/"},{"categories":null,"content":"关于 LoveIt","date":"2019-08-02","objectID":"/about/","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"  LoveIt 是一个由  Dillon 开发的简洁、优雅且高效的 Hugo 博客主题。 它的原型基于 LeaveIt 主题 和 KeepIt 主题。 Hugo 主题 LoveIt\r","date":"2019-08-02","objectID":"/about/:0:0","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"特性 ","date":"2019-08-02","objectID":"/about/:1:0","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"性能和 SEO  性能优化：在 Google PageSpeed Insights 中， 99/100 的移动设备得分和 100/100 的桌面设备得分  使用基于 JSON-LD 格式 的 SEO SCHEMA 文件进行 SEO 优化  支持 Google Analytics  支持 Fathom Analytics  支持 Plausible Analytics  支持 Yandex Metrica  支持搜索引擎的网站验证 (Google, Bind, Yandex and Baidu)  支持所有第三方库的 CDN  基于 lazysizes 自动转换图片为懒加载 ","date":"2019-08-02","objectID":"/about/:1:1","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"外观和布局  桌面端/移动端 响应式布局  浅色/深色 主题模式  全局一致的设计语言  支持分页  易用和自动展开的文章目录  支持多语言和国际化  美观的 CSS 动画 社交和评论系统  支持 Gravatar 头像  支持本地头像  支持多达 73 种社交链接  支持多达 24 种网站分享  支持 Disqus 评论系统  支持 Gitalk 评论系统  支持 Valine 评论系统  支持 Facebook comments 评论系统  支持 Telegram comments 评论系统  支持 Commento 评论系统  支持 utterances 评论系统  支持 giscus 评论系统 ","date":"2019-08-02","objectID":"/about/:1:2","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"扩展功能  支持基于 Lunr.js 或 algolia 的搜索  支持 Twemoji  支持代码高亮  一键复制代码到剪贴板  支持基于 lightGallery 的图片画廊  支持 Font Awesome 图标的扩展 Markdown 语法  支持上标注释的扩展 Markdown 语法  支持分数的扩展 Markdown 语法  支持基于 $\\KaTeX$ 的数学公式  支持基于 mermaid 的图表 shortcode  支持基于 ECharts 的交互式数据可视化 shortcode  支持基于 Mapbox GL JS 的 Mapbox shortcode  支持基于 APlayer 和 MetingJS 的音乐播放器 shortcode  支持 Bilibili 视频 shortcode  支持多种注释的 shortcode  支持自定义样式的 shortcode  支持自定义脚本的 shortcode  支持基于 TypeIt 的打字动画 shortcode  支持基于 cookieconsent 的 Cookie 许可横幅  支持人物标签的 shortcode … ","date":"2019-08-02","objectID":"/about/:1:3","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"许可协议 LoveIt 根据 MIT 许可协议授权。 更多信息请查看 LICENSE 文件。 ","date":"2019-08-02","objectID":"/about/:2:0","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"特别感谢 LoveIt 主题中用到了以下项目，感谢它们的作者： normalize.css Font Awesome Simple Icons Animate.css autocomplete Lunr.js algoliasearch lazysizes object-fit-images Twemoji emoji-data lightGallery clipboard.js Sharer.js TypeIt $\\KaTeX$ mermaid ECharts Mapbox GL JS APlayer MetingJS Gitalk Valine cookieconsent ","date":"2019-08-02","objectID":"/about/:3:0","tags":null,"title":"关于 LoveIt","uri":"/about/"}]