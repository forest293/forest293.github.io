[{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"This chapter we focus on the logit and probit versions of the ordinal regression model (ORM).","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"Ordinal variables are often coded as consecutive integers from 1 to the number of categories. Perhaps because of this coding, it is tempting to analyze ordinal outcomes with the linear regression model (LRM). However, an ordinal dependent variable violates the assumptions of the LRM, which can lead to incorrect conclusions, as demonstrated strikingly by McKelvey and Zavoina (1975, 117) and Winship and Mare (1984, 521-523). With ordinal outcomes, it is much better to use models that avoid the assumption that the distances between categories are equal. Although many models have been designed for ordinal outcomes, in this chapter we focus on the logit and probit versions of the ordinal regression model (ORM). The model was introduced by McKelvey and Zavoina (1975) in terms of an underlying latent variable, and in biostatistics by McCullagh (1980), who referred to the logit version as the proportional-odds model. In section 7.16, we review several less commonly used models for ordinal outcomes. As with the binary regression model (BRM), the ORM is nonlinear, and the magnitude of the change in the outcome probability for a given change in one of the independent variables depends on the levels of all the independent variables. As with the BRM, the challenge is to summarize the effects of the independent variables in a way that fully reflects key substantive processes without overwhelming and distracting detail. For ordinal outcomes, as well as for the models for nominal outcomes in chapter 8, the difficulty of this task is increased by having more than two outcomes to explain. Before proceeding, we caution that researchers should think carefully before concluding that their outcome is indeed ordinal. Do not assume that a variable should be analyzed as ordinal simply because the values of the variable can be ordered. A variable that can be ordered when considered for one purpose could be unordered or ordered differently when used for another purpose. Miller and Volker (1985) show how different assumptions about the ordering of occupations result in different conclusions. A variable might also reflect ordering on more than one dimension, such as attitude scales that reflect both the intensity and the direction of opinion. Moreover, surveys commonly include the category “don’t know”, which probably does not correspond to the middle category in a scale, even though analysts might be tempted to treat it this way. In general, ORMs restrict the nature of the relationship between the independent variables and the probabilities of outcome categories, as discussed in section 7.15. Even when an outcome seems clearly to be ordinal, such restrictions can be unrealistic, as illustrated in chapter 8. Indeed, we suggest that you always compare the results from ordinal models with those from a model that does not assume ordinality. We begin by reviewing the statistical model, followed by an examination of testing, fit, and methods of interpretation. These discussions are intended as a review for those who are familiar with the models. For a complete discussion, see Agresti (2010), Long (1997), or Hosmer, Lemeshow, and Sturdivant (2013). As explained in chapter 1, you can obtain sample do-files and data files by installing the spostl3_do package. ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:0:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.1 The statistical model The ORM can be developed in different ways, each of which leads to the same form of the model. These approaches to the model parallel those for the BRM. Indeed, the BRM can be viewed as a special case of the ordinal model in which the ordinal outcome has only two categories. ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:1:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.1.1 A latent-variable model The ORM is commonly presented as a latent-variable model. Defining y* as a latent variable ranging from −∞ to ∞, the structural model is $$y_{i}^{*}=\\mathbf{x_i}\\beta+\\varepsilon_{i}$$ where $i$ is the observation and $\\varepsilon$ is a random error, as discussed further below. For the case of one independent variable, The measurement model for binary outcomes from chapter 5 is expanded to divide $y^*$ into $J$ ordinal categories, $$y_i=m\\quad\\text{if }\\tau_{m-1}\\leq y_i^*\u003c\\tau_m\\quad\\text{for }m=1\\text{to }J$$ where the cutpoints $\\tau_1$ through $\\tau_j-1$ are estimated.(Some authors refer to these as thresholds.) We assume $t_0 = -\\infty$ and $t_j = \\infty$ for reasons that will be clear shortly. To illustrate the measurement model, consider the example used in this chapter. People are asked to respond to the following statement: If you were asked to use one of four names for your social class, which would you say you belong in: the lower class, the working class, the middle class, or the upper class? The underlying, continuous latent variable can be thought of as the propensity to identify oneself as having higher socioeconomic standing. The observed response categories are tied to the latent variable by the measurement model. Thus when the latent $y^*$ crosses a cutpoint, the observed category changes. Anderson (1984) referred to ordinal variables created in this fashion as grouped continuous variables and referred to the ORM as the grouped continuous model. 7.1 Relationship between observed y and latent y* in ORM with one independent variable\rFor a single independent variable, the structural model is $y^* = \\alpha + \\beta x + \\epsilon$, which is plotted in figure 7.1 along with the cutpoints for the measurement model. This figure is similar to that for the BRM, except that there are now three horizontal lines representing the cutpoints $\\tau_1$, $\\tau_2$, and $\\tau_3$. The three cutpoints lead to four levels of $y$ that are labeled on the right-hand side of the graph. The probability of an observed outcome $y$ for a given value of $x$, represented by the three vertical lines in the figure, is the area under the curve between a pair of cutpoints. For example, the probability of observing $y = m$ for given values of the $x$’s corresponds to the region of the distribution where $y^*$ falls between $\\tau_{m-1}$ and $\\tau_m$: $$\\Pr(y=m\\mid\\mathbf{x})=\\Pr\\left(\\tau_{m-1}\\leq y^*\u003c\\tau_m\\mid\\mathbf{x}\\right)$$ Substituting $x\\beta + \\epsilon$ for $y^*$ and using some algebra leads to the standard formula for the predicted probability in the ORM. $$\\mathrm{Pr}\\left(y=m\\mid\\mathbf{x}\\right)=F\\left(\\tau_{m}-\\mathbf{x}\\beta\\right)-F\\left(\\tau_{m-1}-\\mathbf{x}\\beta\\right)$$ Where $F$ is the cumulative distribution function for $\\epsilon$. In the ordinal probit model, $F$ is normal with $Var(e) = 1$; in the ordinal logit model, $F$ is logistic with $Var(e) = \\frac{\\pi^2}{3}$. For $y = 1$, the second term on the right drops out because $F(-\\infty - x\\beta) = 0$, and for $y = J$, the first term equals $F(\\infty - x\\beta) = 1$. 假设我们想了解学生的考试成绩 $ y $ 如何受到学习时间 $ x $ 的影响。我们可以使用一个称为“有序回归模型（ORM）”的统计工具来分析这个关系。在这个模型中，我们假设学习时间 $ x $ 对考试成绩 $ y $ 有影响，但不是直接的影响，而是通过一个隐变量 $ y^* $ 来间接影响。这个隐变量 $ y^* $ 可以被理解为一个学生的潜在考试能力，即在没有任何其他影响因素的情况下，学生能够取得的最佳成绩。 结构模型：结构模型描述了潜在变量 $y^※$ 和自变量 $x$ 之间的关系。在这个例子中，我们假设学生的考试成绩 $y^※$ 受到学习时间 $x$ 的影响。这种影响被表示为一个线性关系：$y^※= \\alpha + \\beta x + \\epsilon$。其中，$\\alpha$和 $\\beta$ 是模型的参数，表示了$x$对$y$的影响程度，$\\epsilon$ 是一个随机误差项，代表了其他未考虑到的因素对 $y^*$ 的影响。让我们通过一个例子来解释这个结构模型： 假设我们有一组学生，他们的学习时间 $x$ 分别为10小时、20小时和30小时。我们想知道他们的潜在考试能力 $y^*$ 是多少。根据结构模型，我们可以用下面的公式来计算： $$ \\begin{align*} \\text{学生1的 } y^* \u0026= \\alpha + \\beta \\times 10 + \\epsilon_1 \\end{align*} $$ $$ \\begin{align*} \\text{学生2的 } y^* \u0026= \\alpha + \\beta \\times 20 + \\epsilon_2 \\end{align*} $$ $$ \\begin{align*} \\text{学生3的 } y^* \u0026= \\alpha + \\beta \\times 30 + \\epsilon_3 \\end{align*} $$ 其中，$\\epsilon_1, \\epsilon_2, \\epsilon_3$ 分别代表了每个学生的随机误差。 测量模型：测量模型描述了潜在变量 $y^※$ 和观测到的变量 $y","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:1:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.1.2 A nonlinear probability model The ORM can also be developed as a nonlinear probability model without appealing to an underlying latent continuous variable. Here we show how this is done for the ordinal logit model. First, we define the odds that an outcome is less than or equal to $m$ versus greater than $m$ given $x$: $$\\Omega_{\\leq m|\u003em}\\left(\\mathbf{x}\\right)\\equiv\\frac{\\mathrm{Pr}\\left(y\\leq m\\mid\\mathbf{x}\\right)}{\\mathrm{Pr}\\left(y\u003em\\mid\\mathbf{x}\\right)}\\quad\\mathrm{for}m=1,J-1$$ For example, we could compute the odds of lower- or working-class identification (that is, $rn \u003c 2$) versus middle- or upper-class identification ($rn \u003e 2$). The log of the odds is assumed to equal $$\\ln\\Omega_{\\leq m|\u003em}\\left(\\mathbf{x}\\right)=\\tau_m-\\mathbf{x}\\boldsymbol{\\beta}$$ Critically, the $\\beta$’s are the same for all values of $m$. For a single independent variable and three categories, where we are fixing the intercept to equal 0 and estimating the $r$’s, the model is: $$\\ln\\frac{\\Pr\\left(y\\leq1\\mid\\mathbf{x}\\right)}{\\Pr\\left(y\u003e1\\mid\\mathbf{x}\\right)}=\\tau_{1}-\\beta x$$ $$\\ln\\frac{\\operatorname*{Pr}\\left(y\\leq2\\mid\\mathbf{x}\\right)}{\\operatorname*{Pr}\\left(y\u003e2\\mid\\mathbf{x}\\right)}=\\tau_{2}-\\beta x$$ Although it may seem confusing that we subtract $r \\cdot x$ rather than adding it, this is a consequence of computing the logit of $y \u003c r_m$ versus $y \u003e r_m$. We agree that it would be simpler to stick with $r_m + fix$, but this is not the way the model is normally presented. 在有序Logit模型中，我们想要研究一个有序分类变量 $y$，比如社会阶层。这个模型是如何描述和预测这个变量的呢？让我们一步步来看。 首先，我们有一个称为“几率比”的概念，它描述了一个结果小于或等于某个值 $m$ 的概率与大于 $m$ 的概率之间的关系。这个比率表示为 $\\Omega_{\\leq m|\u003em}(\\mathbf{x})$，其中 $\\mathbf{x}$ 是自变量的值。这个比率可以用以下公式表示： $$ \\Omega_{\\leq m|\u003em}(\\mathbf{x}) = \\frac{\\mathrm{Pr}(y \\leq m \\mid \\mathbf{x})}{\\mathrm{Pr}(y \u003e m \\mid \\mathbf{x})} \\quad \\text{for } m=1,J-1 $$ 这个比率告诉我们在给定自变量 $\\mathbf{x}$ 的情况下，结果 $y$ 小于或等于 $m$ 的概率与大于 $m$ 的概率之间的关系。这对于理解结果的相对概率非常有帮助。 接下来，我们使用对数几率来描述这个比率。对数几率是对上面比率的对数，可以用以下公式表示： $$ \\ln\\Omega_{\\leq m|\u003em}(\\mathbf{x}) = \\tau_m - \\mathbf{x}\\boldsymbol{\\beta} $$ 在这里，$\\tau_m$ 是与结果 $m$ 相关的常数参数，$\\boldsymbol{\\beta}$ 是自变量 $\\mathbf{x}$ 的参数。这个公式告诉我们，对数几率由两部分组成：一个与结果 $m$ 相关的常数部分 $\\tau_m$，以及一个与自变量 $\\mathbf{x}$ 相关的部分，通过参数 $\\boldsymbol{\\beta}$ 来表示。 最后，当我们只有一个自变量和三个结果时，模型可以简化为两个方程，它们描述了不同结果之间的对数几率。这些方程是： $$ \\ln\\frac{\\Pr(y \\leq 1 \\mid \\mathbf{x})}{\\Pr(y \u003e 1 \\mid \\mathbf{x})} = \\tau_{1} - \\beta x $$ $$ \\ln\\frac{\\Pr(y \\leq 2 \\mid \\mathbf{x})}{\\Pr(y \u003e 2 \\mid \\mathbf{x})} = \\tau_{2} - \\beta x $$ 在这里，参数 $r$ 是一个特殊的参数，表示不同结果之间的分界点。 困惑的一点是为什么在计算对数几率时要减去 $\\beta x$ 而不是加上它。这其实是因为我们在计算 $y \u003c r_m$ 与 $y \u003e r_m$ 的Logit几率时所采取的方法不同。在Logit模型中，我们通常对一个事件发生的概率取对数几率，以便于建模和分析。在这种情况下，对于有序Logit模型，我们计算的是结果小于某个分界点 $r_m$ 的对数几率和结果大于 $r_m$ 的对数几率。因此，为了计算这两个对数几率，我们需要考虑 $\\beta x$ 的影响。 作者提到了一种更简单的处理方法，即使用 $r_m$ + $\\betax$，但这不是该模型通常被呈现的方式。这可能是因为 $\\beta x$ 的形式更加直观，能够更清晰地表达出结果小于或大于分界点 $r_m$ 的对数几率之间的关系。尽管这种处理方法可能会导致一些困惑，但它在实践中是有效的，并且能够更好地解释模型的原理。 ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:1:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.2 Estimation using ologit and oprobit The ordered logit and probit models can be fit with the following commands and their basic options: ologit depvar [indepvars] [if] [m] [weight] [, vce(vcetype) or] oprobit depvar [indepvars] [if] [in] [weight] [, vce(vcetype)] In our experience, these models take more steps to converge than either models for binary outcomes fit using logit or probit or models for nominal outcomes fit using mlogit. Variable lists depvar is the dependent variable. The values assigned to the outcome categories are irrelevant, except that larger values are assumed to correspond to “higher” outcomes. For example, if you had three outcomes, you could use the values 1, 2, and 3, or —1.23, 2.3, and 999. To avoid confusion, however, we recommend coding your dependent variable as consecutive integers beginning with 1. indepvars is a list of independent variables. If indepvars is not included, Stata fits a model with only cutpoints. Specifying the estimation sample if and in qualifiers. These can be used to restrict the estimation sample. For example, if you want to fit an ordered logit model for only those surveyed in 1980 (year = 1), you could specify ologit class i.female i.white i.educ age inc if year == 1. Listwise deletion. Stata excludes cases in which there are missing values for any of the variables in the model. Accordingly, if two models are fit using the same dataset but have different sets of independent variables, it is possible to have different samples. We recommend that you use mark and markout (discussed in section 3.1.G) to explicitly remove cases with missing data. Weights and complex samples Both ologit and oprobit can be used with fweights, pweights, and iweights. Survey estimation can be done using the svy prefix. See section 3.1.7 for details. Options vce(vcetype) specifies the type of standard errors to be computed. See section 3.1.9 for details. or reports odds ratios for the ordered logit model. Additional options and information can be found in the Stata manual entries [R] ologit and [r] oprobit. ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:2:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.2.1 Example of ordinal logit model Our example is based on a question asked in the 1980, 1996, and 2012 General Social Surveys. These are repeated cross-sectional data, not panel data. That is, in each wave the survey was administered to new respondents from a new nationally representative sample. The following variables are in the model: use gssclass4, clear codebook class female white year ed age income, compact Variable Obs Unique Mean Min Max Label\r------------------------------------------------------------------------------\rclass 5620 4 2.437544 1 4 subjective class id\rfemale 5620 2 .5491103 0 1 respondent is female\rwhite 5620 2 .8140569 0 1 resondent is whte\ryear 5620 3 2.070996 1 3 year of GSS survey\reduc 5620 3 2.064769 1 3 educational attainment\rage 5620 72 45.15712 18 89 age of respondent\rincome 5620 62 68.07737 .51205 324.2425 household income\r------------------------------------------------------------------------------\rRespondents were asked to indicate the social class to which they think they most belong, using categories coded 1 = lower, 2 = working, 3 = middle, and 4 = upper. The resulting variable class has the distribution: tab class subjective |\rclass id | Freq. Percent Cum.\r------------+-----------------------------------\rlower | 394 7.01 7.01\rworking | 2,567 45.68 52.69\rmiddle | 2,465 43.86 96.55\rupper | 194 3.45 100.00\r------------+-----------------------------------\rTotal | 5,620 100.00\rThe variable educ is a categorical variable in which the categories are less than a high school diploma, high school diploma, and college diploma. The variable income is measured in 2012 dollars for all years of the survey. Using these data, we use ologit to fit the model $$\\mathrm{Pr}(\\mathbf{c}1\\mathbf{ass}=m\\mid\\mathbf{x_i})=F(\\tau_{m}-\\mathbf{x}\\beta)-F(\\tau_{m-1}-\\mathbf{x}\\beta)$$ where $$\\mathbf{x\\beta}=\\beta_\\text{female}{\\text{female}+\\beta_\\text{white}{ \\mathrm{white}}}$$ $$\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad+\\beta_{\\mathrm{year}\\left[1996\\right]}\\text{(year==1996)}+\\beta_{\\mathrm{year}\\left[2012\\right]}\\text{(year==2012)}$$ $$\\qquad \\qquad \\qquad \\qquad \\qquad \\quad +\\beta_{\\text{educ [ha only]}}\\text{(educ==2)}+\\beta_{\\text{educ}[\\text{college}]}\\text{(educ==3)}$$ + β c . a g e c.age + β c . a g e # c . a g e c.age#c.age + β income i n c o m e To specify the model, we use the factor-variable notation i.year to create indicators for the year of the survey, i.educ for education, and c.age##c.age to include age and age-squared, estimates store is used so that we can later make a table containing these results. ologit class i.female i.white i.year i.educ c.age##c.age income, nolog estimates store ologit Ordered logistic regression Number of obs = 5,620\rLR chi2(9) = 1453.95\rProb \u003e chi2 = 0.0000\rLog likelihood = -5016.2107 Pseudo R2 = 0.1266\r------------------------------------------------------------------------------\rclass | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rfemale |\rfemale | 0.02 0.05 0.30 0.765 -0.09 0.12\r|\rwhite |\rwhite | 0.24 0.07 3.28 0.001 0.09 0.38\r|\ryear |\r1996 | -0.08 0.07 -1.16 0.247 -0.22 0.06\r2012 | -0.50 0.08 -6.59 0.000 -0.65 -0.35\r|\reduc |\rhs only | 0.37 0.08 4.73 0.000 0.22 0.52\rcollege | 1.57 0.10 15.99 0.000 1.37 1.76\r|\rage | -0.05 0.01 -5.31 0.000 -0.07 -0.03\r|\rc.age#c.age | 0.00 0.00 7.65 0.000 0.00 0.00\r|\rincome | 0.01 0.00 22.20 0.000 0.01 0.01\r-------------+----------------------------------------------------------------\r/cut1 | -2.14 0.23 -2.59 -1.69\r/cut2 | 0.92 0.23 0.47 1.36\r/cut3 | 4.93 0.24 4.46 5.41\r------------------------------------------------------------------------------\rThe information in the header and the table of coefficients is in the same form as discussed in chapters 3 and 5, with the addition of estimates for the cutpoints at the end. Next, we fit the ordered probit model: oprobit class i.female i.white i.year i.educ c.age##c.age income, nolog estimates store oprobit","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:2:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.2.2 Predicting perfectly If either the highest or the lowest category of the dependent variable does not vary within one of the categories of an independent variable, there will be a problem with estimation. To see what happens, we created an artificial example with a dummy variable for whether respondents have a college degree. Tabulating college against class shows that in all cases where college is 1, respondents have values of class equal to 4, indicating upper-class identification. capture drop college gen college = (educ == 3 \u0026 class == 4) label var college \"Has college degree?\" label def college 0 no 1 yes label val college college tab class college subjective | Has college degree?\rclass id | no yes | Total\r-----------+----------------------+----------\rlower | 394 0 | 394 working | 2,567 0 | 2,567 middle | 2,465 0 | 2,465 upper | 81 113 | 194 -----------+----------------------+----------\rTotal | 5,507 113 | 5,620 Accordingly, if you know college is 1, you can predict perfectly that class is 4. Although we purposely constructed college so this would happen, perfect prediction occurs in real data, especially when samples are small or one of the outcome values is infrequent. When we fit the ordered logit model with college as a regressor, the perfectly predicted observations are retained in the estimation sample with a warning message appearing below the table of estimates. ologit class i.female i.white i.year i.college c.age##c.age income, nolog The note reflects that the standard error for college is enormous, indicating the problem that occurs when trying to estimate a coefficient that is effectively infinite. Another way of thinking about the large standard error is that the lack of variation in the outcome when college equals 1 means we do not have any information that would permit us to estimate the coefficient with precision. When this happens, our next step is to drop the 113 cases for which college equals 1 (you could use the command drop if college==1 to do this) and refit the model without college. This is done automatically for binary models fit by logit and probit (see section 5.2.3). There is no problem if an independent variable perfectly predicts one of the middle categories. For example, if all observations for which college is 1 reported being middle class, this would not cause problems for estimation. ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:2:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.3 Hypothesis testing Hypothesis tests of regression coefficients can be evaluated with the $z$ statistics in the estimation output, with test and testparm for Wald tests of simple and complex hypotheses, and with lrttest for likelihood-ratio tests. We briefly review each. See section 3.2 for additional information on these commands. ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:3:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.3.1 Testing individual coefficients If the assumptions of the model hold, the maximum likelihood estimators from ologit and oprobit are distributed asymptotically normally. The hypothesis $H_{0}:\\beta_{k}=\\beta^{*}$ can be tested with z = ( β ^ k − β ∗ ) / σ ^ β ^ k Under the assumptions justifying maximum likelihood, if $H_0$ is true, then $z$ is distributed approximately normally with a mean of 0 and a variance of 1 for large samples. For example, consider the results for the variable white from the ologit output above. We are using the sformat option to show more decimal places for the $z$ statistic 2: 当我们使用**ologit和oprobit**模型时，我们试图找到一组参数，这些参数可以最好地描述我们观察到的数据。其中一些参数代表我们感兴趣的变量对结果的影响。 假设我们对其中一个参数感兴趣，我们想知道这个参数的值是否等于某个特定值，比如说 $\\beta^*$。我们可以进行一种假设检验来验证这个问题。这种检验告诉我们，如果我们的假设是正确的，那么我们从数据中计算的统计量（称为 $z$ 统计量）应该会遵循一个特定的分布，即正态分布。 这个 $z$ 统计量实际上是一个数字，它告诉我们观察到的参数值与我们假设的值之间有多大的差异，以及这个差异有多大可能是由随机因素引起的。如果这个差异很大，而且很少可能是由随机因素引起的，那么我们就会得到一个很大的 $z$ 值。 当我们对很多样本进行分析时，我们可以看到这个 $z$ 值的分布。如果它们聚集在某个特定的范围内，而不是分散在整个范围内，那么我们就可以相信我们的假设是正确的。 在这种情况下，如果我们的假设是正确的，那么这个 $z$ 统计量的平均值应该接近于零，而且不同样本的 $z$ 值的变化应该是一样的。这就是所谓的“均值为0，方差为1”的正态分布。 We are displaying more decimal places to later demonstrate the equivalence of the z test and the corresponding chi-squared test. With any estimation command, the option format(fmt)(fmt) can be used to format the display of coefficients and standard errors. Likewise, option pformat formats the display of p-values and option sformat(fmt) formats the display of the test. Alternatively, the set command can also be used to change these formats either permanently for the rest of the current Stata session. See [R] set format in the Stata manual for details ologit class i.female i.white i.year i.educ c.age##c.age income, /// nolog sformat(%8.3f) Ordered logistic regression Number of obs = 5,620\rLR chi2(9) = 1453.95\rProb \u003e chi2 = 0.0000\rLog likelihood = -5016.2107 Pseudo R2 = 0.1266\r------------------------------------------------------------------------------\rclass | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rfemale |\rfemale | 0.02 0.05 0.298 0.765 -0.09 0.12\r|\rwhite |\rwhite | 0.24 0.07 3.277 0.001 0.09 0.38\r|\ryear |\r1996 | -0.08 0.07 -1.158 0.247 -0.22 0.06\r2012 | -0.50 0.08 -6.594 0.000 -0.65 -0.35\r|\reduc |\rhs only | 0.37 0.08 4.730 0.000 0.22 0.52\rcollege | 1.57 0.10 15.994 0.000 1.37 1.76\r|\rage | -0.05 0.01 -5.308 0.000 -0.07 -0.03\r|\rc.age#c.age | 0.00 0.00 7.646 0.000 0.00 0.00\r|\rincome | 0.01 0.00 22.203 0.000 0.01 0.01\r-------------+----------------------------------------------------------------\r/cut1 | -2.14 0.23 -2.59 -1.69\r/cut2 | 0.92 0.23 0.47 1.36\r/cut3 | 4.93 0.24 4.46 5.41\r------------------------------------------------------------------------------\rWe conclude the following: Whites and nonwhites significantly differ in their subjective social class identification (z = 3.28, p \u003c 0.01, two-tailed) Either a one-tailed or a two-tailed test can be used, as discussed in chapter 5 The z test in the output of estimation commands is a Wald test, which can also be computed using test. For example, to test $H_0{:}\\beta_{\\mathrm{white}}=0$,type test 1.white ( 1) [class]1.white = 0\rchi2( 1) = 10.74\rProb \u003e chi2 = 0.0011\rWe conclude the following: Whites and nonwhites significantly differ in their class identification $(x^{2}=10.74,df=1,p\u003c0.01).$ The value of a chi-squared test with 1 degree of freedom is identical to the square of the corresponding z test, which can be demonstrated with the display command: display \"z*z=\" 3.277*3.277 z*z=10.738729 A likelihood-ratio LR test is computed by comparing the log likelihood from a full model with that from a restricted model. To test a single coefficient, we begin by fitting the full model and storing the estimates: ologit class i.female i.white i.year i.educ c.age##c.age income, nolog estimates store fullmodel Then, we fit a model that excludes the variable white th at we want to test ologit class i.female","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:3:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.3.2 Testing multiple coefficients We can also test complex hypotheses that involve more than one coefficient. For example, our model has the demographic variables white, female, and age. To test that the effects of these variables are simultaneously equal to 0—that is, $H_0{:}\\beta_{\\mathrm{white}}=\\beta_{\\mathrm{female}}=\\beta_{\\mathrm{age}}=\\beta_{\\mathrm{age*age}}=0$ We can use either a Wald or an LR test. For the Wald test, we fit the full model and then use the test command: ologit class i.female i.white i.year i.educ c.age##c.age income, nolog test 1.white 1.female age age#age ( 1) [class]1.white = 0\r( 2) [class]1.female = 0\r( 3) [class]age = 0\r( 4) [class]c.age#c.age = 0\rchi2( 4) = 226.75\rProb \u003e chi2 = 0.0000\rBefore we interpret the results of the test, we want to clarify how coefficients are specified in the test command when factor-variable notation is used. The specification i.white added the variable 1.white to the model as shown in the output to ologit above. Accordingly, we are testing the coefficient associated with the variable 1.white, not i.white or white. The same rule applies for female. Age was entered into the model as c.age##c.age, which was expanded to estimate coefficients for c.age and c.age#c.age. When entering these coefficients into test, we do not need to include the c. prefix (although we could do so). Regardless of how we specify the test command, we conclude the following: The hypothesis that the demographic effects of age, race, and sex are simultaneously equal to 0 can be rejected at the 0.01 level (χ² = 226.8, df = 4, p \u003c 0.01). To compute an LR test of multiple coefficients, we first fit the full model and store the results with estimates store. Suppose we are interested in whether demographic characteristics matter at all for subjective class identification or whether identification is only a function of socioeconomic status and changes over time. To test $H_0{:}\\beta_{\\mathrm{white}}=\\beta_{\\mathrm{female}}=\\beta_{\\mathrm{age}}=\\beta_{\\mathrm{age*age}}=0$, we fit the model that excludes these four coefficients and run lrtest: ologit class i.female i.white i.year i.educ c.age##c.age income, nolog estimates store fullmodel ologit class i.year i.educ income, nolog estimates store dropdemog lrtest fullmodel dropdemog Likelihood-ratio test\rAssumption: dropdemog nested within fullmodel\rLR chi2(4) = 236.53\rProb \u003e chi2 = 0.0000\rWe conclude the following: The hypothesis that the demographic effects of age, race, and sex are simultaneously equal to 0 can be rejected at the 0.01 level (LR χ² = 236.5, df = 4, p \u003c 0.01). We find that the Wald and LR tests usually lead to the same decisions, and there is no reason why you would typically want to compute both tests. When there are differences, they generally occur when the tests are near the cutoff for statistical significance. Because the LR test is invariant to reparameterization, we prefer the LR test when both are available. However, only the Wald test can be used if robust standard errors, probability weights, or survey estimation are used. When a factor variable has more than two categories, such as year and educ in our model, you can specify each of the coefficients with test (for example, test 2.educ 3.educ) or you can use testparm: ologit class i.female i.white i.year i.educ c.age##c.age income, nolog testparm i.educ ( 1) [class]2.educ = 0\r( 2) [class]3.educ = 0\rchi2( 2) = 329.48\rProb \u003e chi2 = 0.0000\rtest can also be used to test the equality of coefficients, as shown in section 5.3.2. ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:3:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.4 Measures of fit using fitstat As we discussed in greater detail in chapter 3, scalar measures of fit can be used when comparing competing models (also see Long [1997, 85-113]). Several measures can be computed after either ologit or oprobit by using the SPost command fits. In this example, we compare a model for class identification that includes age but not age-squared with the model we have been using that includes age-squared: ologit class i.female i.white i.year i.educ age income, nolog quietly fitstat, save ologit class i.female i.white i.year i.educ c.age##c.age income, nolog fitstat, diff | Current Saved Difference -------------------------+--------------------------------------\rLog-likelihood | Model | -5016.211 -5045.903 29.692 Intercept-only | -5743.186 -5743.186 0.000 -------------------------+--------------------------------------\rChi-square | D(df=5608/5609/-1) | 10032.421 10091.806 -59.385 LR(df=9/8/1) | 1453.951 1394.566 59.385 p-value | 0.000 0.000 0.000 -------------------------+--------------------------------------\rR2 | McFadden | 0.127 0.121 0.005 McFadden(adjusted) | 0.124 0.119 0.005 McKelvey \u0026 Zavoina | 0.284 0.274 0.011 Cox-Snell/ML | 0.228 0.220 0.008 Cragg-Uhler/Nagelkerke | 0.262 0.252 0.009 Count | 0.605 0.600 0.005 Count(adjusted) | 0.273 0.264 0.009 -------------------------+--------------------------------------\rIC | AIC | 10056.421 10113.806 -57.385 AIC divided by N | 1.789 1.800 -0.010 BIC(df=12/11/1) | 10136.030 10186.781 -50.751 -------------------------+--------------------------------------\rVariance of | e | 3.290 3.290 0.000 y-star | 4.596 4.528 0.067 Note: Likelihood-ratio test assumes saved model nested in current model.\rDifference of 50.751 in BIC provides very strong support for current model.\rThe Bayesian information criterion (BIC), Akaike’s information criterion (AIC), and the LR test each provide evidence supporting the inclusion of age-squared in the model. Using simulations, Hagle and Mitchell (1992) and Windmeijer (1995) found that with ordinal outcomes, McKelvey and Zavoina’s pseudo-R² most closely approximates the R² obtained by fitting the LRM on the underlying latent variable. If you are using y*-standardized coefficients to interpret the ORM (see section 7.8.1), McKelvey and Zavoina’s R² could be used as a counterpart to the R² from the LRM. ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:4:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.5 (Advanced) Converting to a different parameterization We mark this section as advanced because the conversion we show is likely only pertinent to readers who also work with other statistics packages that fit the model by using the alternative parameterization. The section may still be useful to strengthen your understanding of how the intercept and thresholds of these models are related, as well as how the lincom command works. Earlier, we noted that the model can be identified by fixing either the intercept or one of the thresholds to equal 0. Stata sets $\\beta_{0}=0$ and estimates $\\tau_{1}$,whereas some programs fix $\\tau_{1}=0$ and estimate $\\beta_{0}$ Although all quantities of interest for interpretation (for example, predicted probabilities) are the same under both parameterizations, it is useful to see how Stata can fit the model with either parameterization. We can understand how this is done with the following equation, where we are simply adding $0=\\delta-\\delta $ and rearranging terms: Without further constraints, it is possible to estimate the differences $\\tau_{m}-\\delta $ and $\\beta_{0}-\\delta $, but not the parameters $\\tau_m$ and $\\beta_{0}$. To identify the model, Stata assumes $\\delta=\\beta_{0}$ which forces the estimate of $\\beta_{0}$ to be 0, Some programs assume $\\delta=\\tau_{1}$, which forces the estimate of $\\tau_{1}$ to be 0. The following table shows the differences in the parameterizations: Although you would only need to estimate the alternative parameterization if you wanted to compare your results with those produced by another statistics package,Seeing how this is done illustrates why the intercept and thresholds are arbitrary. To estimate the alternative parameterization, we use lincom to compute the difference between Stata’s estimates and the estimated value of the first cutpoint. We begin with the alternative parameterization of the intercept: ologit class i.female i.white i.year i.educ c.age##c.age income, nolog lincom 0 - _b[/cut1] // intercept ( 1) - [/]cut1 = 0\r------------------------------------------------------------------------------\rclass | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r(1) | 2.14 0.23 9.39 0.000 1.69 2.59\r------------------------------------------------------------------------------\rTo understand the lincom command, you need to know that _b[/cut1] is how Stata refers to the estimate of the first cutpoint. Accordingly, 0 - _b[/cut1] is the difference between 0 and the estimate of the first cutpoint, which simply reverses the sign of the first estimated cutpoint. For the other cutpoints, we are estimating $\\tau_2-\\tau_1$ and $\\tau_3-\\tau_1$, which correspond to _b[/cut2] - _b[/cut1] and _b[/cut3] - _b[/cut1]: lincom _b[/cut2] - _b[/cut1] // cutpoint 2 ( 1) - [/]cut1 + [/]cut2 = 0\r------------------------------------------------------------------------------\rclass | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r(1) | 3.06 0.06 53.29 0.000 2.94 3.17\r------------------------------------------------------------------------------\rlincom _b[/cut3] - _b[/cut1] // cutpoint 3 ( 1) - [/]cut1 + [/]cut3 = 0\r------------------------------------------------------------------------------\rclass | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r(1) | 7.08 0.11 64.44 0.000 6.86 7.29\r------------------------------------------------------------------------------\rThe estimate of $\\tau_3-\\tau_1$ is, of course, 0. These estimates would match those from a program using the alternative parameterization. ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:5:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.6 The parallel regression assumption 平行回归假设指的是在有序回归模型中的一个重要假设，即每个自变量对于每个结果类别的影响是平行的。简单来说，就是当自变量的取值变化时，不同结果类别之间的关系是保持一致的，只是存在一定的偏移或平移。 具体来说，在有序回归模型中，我们考虑多个有序的结果类别，比如低、中、高三个阶段。平行回归假设表明，自变量对于每个阶段的影响是一致的，只是在不同的阶段之间存在一定的平移或偏移。换句话说，自变量的影响随着结果类别的增加是保持不变的。 这个假设在统计建模中起着重要作用，因为它简化了模型的解释和推断过程。然而，在实际应用中，这个假设并不总是成立，因此需要进行相应的检验和处理。如果平行回归假设被拒绝，就需要考虑使用其他不需要这种假设的模型。 假设我们对一种药物治疗心脏病的效果进行研究，我们将患者的病情分为轻、中、重三个等级。我们想知道药物治疗时间对于病情的改善有何影响。 根据平行回归假设，药物治疗时间对于不同等级病情的改善效果应该是一致的，只是存在一定的平移。举个例子，假设我们发现增加药物治疗时间可以显著降低每个等级病情的严重程度，但是降低的幅度可能不同。比如，增加一周的药物治疗时间可能对于轻病情的改善效果是30%，对于中等病情是25%，对于重病情是20%。 这就是平行回归假设的含义，即自变量（药物治疗时间）对于不同等级的病情改善效果是平行的，只是存在一定的偏移。也就是说，药物治疗时间对于病情的改善效果随着病情等级的增加是保持一致的，只是改善的幅度有所不同。 然而，如果平行回归假设不成立，就意味着药物治疗时间对于不同等级的病情改善效果并不是平行的，可能存在交叉效应或者其他的差异。在这种情况下，我们需要考虑使用其他模型来更准确地描述药物治疗时间与病情之间的关系。 Before discussing interpretation, it is important to understand an assumption that is implicit in the ORM, known both as the parallel regression assumption and, for the ordinal logit model, the proportional-odds assumption. Using Equation (7.1), the ORM with $J$ outcome categories can be written as: $$\\Pr\\left(y=1\\mid\\mathbf{x}\\right)=F\\left(\\tau_1-\\mathbf{x}\\boldsymbol{\\beta}\\right)$$ $$\\Pr\\left(y=m\\mid\\mathbf{x}\\right)=F\\left(\\tau_{m}-\\mathbf{x}\\boldsymbol{\\beta}\\right)-F\\left(\\tau_{m-1}-\\mathbf{x}\\boldsymbol{\\beta}\\right)\\mathrm{~for~}m=2\\mathrm{~to~}J-1$$ $$\\Pr\\left(y=J\\mid\\mathbf{x}\\right)=1-F\\left(\\tau_{J-1}-\\mathbf{x}\\boldsymbol{\\beta}\\right)$$ Using these equations, the cumulative probabilities have the simple form $$\\Pr(y\\leq m\\mid\\mathbf{x})=F(\\tau_m-\\mathbf{x}\\boldsymbol{\\beta})\\quad\\mathrm{for~}m=1\\mathrm{~to~}J-1$$ Notice that $\\beta$ does not have a subscript $m$. Accordingly, this equation shows that the ORM is equivalent to $J - 1$ binary regressions with the critical assumption that the slope coefficients are identical in each binary regression. For example, with four outcomes and one independent variable, the cumulative probability equations are: $$\\begin{aligned}\\Pr\\left(y\\leq1\\mid\\mathbf{x}\\right)\u0026=F\\left(\\tau_1-\\beta x\\right)\\\\Pr\\left(y\\leq2\\mid\\mathbf{x}\\right)\u0026=F\\left(\\tau_2-\\beta x\\right)\\\\Pr\\left(y\\leq3\\mid\\mathbf{x}\\right)\u0026=F\\left(\\tau_3-\\beta x\\right)\\end{aligned}$$ Recall that the intercept $a$ is not in the equation because it was assumed to equal 0 to identify the model. These equations lead to the following figure: 第一部分：介绍 在这段文字中，作者讨论了有序回归模型（ORM）的一个重要假设，即“平行回归假设”或者在有序logit模型中叫做“比例几率假设”。 这个假设告诉我们，在这个模型中，无论有多少个结果类别，每一次增加一个类别，自变量对结果的影响都是一致的。换句话说，无论是第一个结果还是最后一个结果，自变量的影响都是一样的。 第二部分：方程表示 具体来说，我们用一系列的方程来表示每个结果类别的发生概率。比如，对于第一个结果类别，概率是根据一个函数 $ F $，结合一些参数 $ \\tau_1 $ 和 $ \\beta $ 计算得到的。这个方程是： $ \\Pr(y=1|\\mathbf{x}) = F(\\tau_1 - \\mathbf{x}\\boldsymbol{\\beta}) $ 第三部分：解释阈值参数 而对于其它类别，概率的计算与前面的类别有关，通过减去前一个类别的概率来得到。这些方程中包含了一些参数 $ \\tau_1, \\tau_2, …, \\tau_{J-1} $，它们是用来决定在哪些点概率发生变化的。 这些阈值参数告诉我们，在自变量的不同取值下，从一个结果类别跳到另一个结果类别的临界点在哪里。 第四部分：斜率参数的解释 还有一个参数 $ \\boldsymbol{\\beta} $，表示自变量对结果的影响。在这些方程中，参数 $ \\boldsymbol{\\beta} $ 是相同的，不管是哪个结果类别。这就是为什么我们说这个模型遵循“平行回归假设”，因为每个类别之间的关系都是平行的，斜率参数都相同。 这意味着，自变量对于每个结果类别的影响是一致的，只是在不同结果类别间有一定的偏移。这就是为什么我们称之为“平行回归假设”。 第五部分：方程的简化形式 在这个模型中，我们可以进一步简化方程，得到一种更简单的形式来表示累积概率。这种形式告诉我们，在给定自变量 $ \\mathbf{x} $ 的情况下，结果小于等于某个特定类别的累积概率是多少。这个形式如下： $ \\Pr(y \\leq m|\\mathbf{x}) = F(\\tau_m - \\mathbf{x}\\boldsymbol{\\beta}) \\quad \\text{for } m=1 \\text{ to } J-1 $ 第六部分：参数的一致性 最后，需要注意的是，尽管方程中的参数 $ \\boldsymbol{\\beta} $ 在不同的结果类别中是相同的，但这并不意味着这个参数在整个模型中是固定的。事实上，在模型估计过程中，我们会通过拟合数据来估计参数的值，以使模型尽可能地拟合数据。 因此，即使在这个假设下，参数 $ \\boldsymbol{\\beta} $ 是相同的，但在实际应用中，我们仍然需要对它进行估计，以获得最优的模型拟合效果。 Each probability curve differs only in being shifted to the left or right. The curves are parallel as a consequence of the assumption that the $\\beta’s$ are equal for each equation. This figure suggests that the parallel regression assumption can be tested by comparing the estimates from $J-1$ binary regressions. $$\\mathrm{Pr}(y\\leq m\\mid\\mathbf{x})=F(\\tau_m-\\mathbf{x}\\boldsymbol{\\beta}_m)\\quad\\mathrm{for}m=1\\mathrm{to}J-1$$ where the $\\beta’s$ are allowed to differ across the equations. The model","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:6:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.6.1 Testing the parallel regression assumption using oparallel oparallel can be used after ologit to compute the omnibus tests described above. The ic option provides the statistics AIC and BIC comparing the generalized ordered logit model with the ordered logit model. ssc install oparallel ologit class i.female i.white i.year i.educ c.age##c.age income, nolog oparallel, ic Tests of the parallel regression assumption\r| Chi2 df P\u003eChi2\r-----------------+----------------------\rWolfe Gould | 127.1 16 0.000\rBrant | 137 16 0.000\rscore | 144.9 16 0.000\rlikelihood ratio | 126.3 16 0.000\rWald | 141.2 16 0.000\rInformation criteria\r| ologit gologit difference ------+-----------------------------------\rAIC | 10609.85 10515.55 94.29 BIC | 10682.82 10694.67 -11.85 The results labeled “likelihood ratio” and “Wald” are the LR and Wald tests based on the generalized ordered logit model. The line “Wolfe Gould” contains the approximate LR test, “Brant” refers to the Brant test, and “score” is the score test. All tests reject the null hypothesis with $p \u003c 0.001$. The score and Wald tests have similar values, while the two LR tests are larger. We find that these tests are often, perhaps usually, significant. The AIC and BIC statistics can be used to evaluate the trade-off between the better fit of the generalized model and the loss of parsimony from having $J - 1$ coefficients for each independent variable instead of just one. In this example, the smaller values of both the AIC and BIC statistics for gologit compared with ologit provide evidence against the ologit model compared with the model in which the parallel regression assumption is relaxed. It is common for the BIC statistic to prefer the ologit model even when the significance tests reject the parallel regression assumption, and this sometimes happens with AIC as well. Although oparallel can be used with ologit but not with oprobit, the approximate LR test presented as “Wolfe Gould” can be performed with oprobit by using the user-written command omodel. omodel, however, does not support factor-variable notation. ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:6:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.6.2 Testing the parallel regression assumption using brant The SPost command brant also computes the Brant test for the ORM. The advantage of our command, which is used by oparallel to make its computations, is that it provides separate tests for each of the independent variables in the model. After running ologit, you run brant, which has the following syntax: brant [,detail] The detail option provides a table of coefficients from each of the binary models. For example, brant, detail Estimated coefficients from binary logits\r-----------------------------------------------\rVariable | y_gt_1 y_gt_2 y_gt_3 -------------+---------------------------------\rfemale |\rfemale | -0.382 -0.027 -0.135 | -3.43 -0.46 -0.90 |\rwhite |\rwhite | 0.323 0.396 0.088 | 2.60 5.11 0.42 |\ryear |\r1996 | -0.328 -0.141 -0.110 | -2.10 -1.91 -0.57 2012 | -1.075 -0.406 -0.309 | -6.94 -4.98 -1.45 |\reduc |\rhs only | 0.841 0.497 -0.151 | 6.89 5.96 -0.59 college | 2.668 2.092 1.456 | 10.26 20.70 5.96 |\rage | -0.006 -0.015 0.051 | -0.36 -1.54 1.88 |\rc.age#c.age | 0.000 0.000 -0.000 | 0.37 3.85 -1.28 |\r_cons | 2.377 -1.210 -5.286 | 5.60 -5.02 -7.34 -----------------------------------------------\rLegend: b/t\rBrant test of parallel regression assumption\r| chi2 p\u003echi2 df\r-------------+------------------------------\rAll | 137.04 0.000 16\r-------------+------------------------------\r1.female | 10.02 0.007 2\r1.white | 2.34 0.310 2\r2.year | 1.45 0.483 2\r3.year | 18.38 0.000 2\r2.educ | 14.41 0.001 2\r3.educ | 12.05 0.002 2\rage | 5.87 0.053 2\rc.age#c.age | 10.13 0.006 2\rA significant test statistic provides evidence that the parallel\rregression assumption has been violated.\rThe largest violation is for income, which may indicate particular problems with the parallel regression assumption for this variable. Looking at the coefficients from the binary logits, we see that for income the estimates from the binary logit of lower class versus working/middle/upper class differ from the other two binary logits. This suggests that income differences matter more for whether people report themselves as lower class than it does for either of the other thresholds. If the focus of our project was the relationship between income and subjective class identification, this would serve as a substantively interesting finding that we would have missed had we not used brant. 这段话介绍了使用 brant 命令进行 Brant 测试的方法和结果解释： Brant 测试：Brant 测试用于测试平行回归假设是否成立。这个假设是有序回归模型中的一个重要假设，即每个自变量对于每个结果类别的影响是平行的。 使用方法：首先，使用 ologit 命令拟合有序 logit 模型。然后，运行 brant 命令来进行 Brant 测试。该命令的语法是 brant [,detail]。使用 detail 选项可以得到每个二元模型的系数表格。 结果解释：在 Brant 测试的结果中，对每个自变量进行了单独的平行回归假设检验。如果检验统计量显著，则意味着平行回归假设被违反。 系数表格：系数表格展示了每个自变量在不同结果类别下的系数估计值，以及对应的 t 值。此外，还提供了对平行回归假设检验的结果。如果检验统计量显著，则表示对应的自变量违反了平行回归假设。 解释：如果某个自变量的平行回归假设检验显著，说明该自变量在不同结果类别下的影响不是平行的，即不同结果类别之间的效应存在差异。在本例中，发现收入（income）变量的违反程度最大，这可能意味着该变量对于结果类别的影响存在特定的问题。系数表格也显示了收入变量在不同二元模型中的估计值存在差异，这表明了收入对于被调查者报告自己为下层阶级的影响与其他两个阈值相比更加显著。如果研究项目的重点是收入与主观阶级认同之间的关系，那么这将作为一个具有实质性意义的发现。 ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:6:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.6.3 Caveat regarding the parallel regression assumption In the majority of the real-world applications of the ORM that we have seen, the hypothesis of parallel regressions is rejected. Keep in mind, however, that the tests of the parallel regression assumption are sensitive to other types of misspecification. Further, we have seen examples where the parallel regression assumption is violated but the predictions from ologit are very similar to those from the generalized ordered logit model or the multinomial logit model. When the hypothesis is rejected, consider alternative models that do not impose the constraint of parallel regressions. As illustrated in chapter 8, fitting the multinomial logit model on a seemingly ordinal outcome can lead to quite different conclusions. The generalized ordered logit model is another alternative to consider. Violation of the parallel regression assumption is not, however, a rationale for using the LRM. The assumptions implied by the application of the LRM to ordinal data are even stronger. 这段话讲述了在实际应用中，我们经常会看到平行回归假设被拒绝的情况。然而，需要记住的是，对平行回归假设的检验对于其他类型的错误规范也很敏感。此外，我们也看到过违反平行回归假设的例子，但是来自 ologit 的预测结果与广义有序 logit 模型或多项式 logit 模型的预测结果非常相似。当假设被拒绝时，应考虑不需要平行回归约束的备选模型。正如在第 8 章中所示，对看似有序的结果进行多项式 logit 模型拟合可能会得出完全不同的结论。广义有序 logit 模型是另一个要考虑的备选方案。然而，违反平行回归假设并不是使用多项式回归模型的理由。将多项式回归模型应用于有序数据所隐含的假设甚至更加严格。 ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:6:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.7 Overview of interpretation Most of the rest of the chapter focuses on interpreting results from the ORM. First, we consider methods of interpretation that are based on transforming the coefficients. If the idea of a latent variable makes substantive sense, or if you are tempted to run a linear regression on an ordinal outcome, interpretations based on rescaling $y^*$ to compute standardized coefficients can be used just like coefficients for the LRM. Coefficients can also be exponentiated and interpreted as odds ratios in the ordered logit model. We examine these strategies of interpretation first because they follow most straightforwardly from the methods of interpretation many readers are already familiar with from linear regression, but we regard them also as having important limitations that we discuss. We then consider approaches to interpretation that use predicted probabilities, extending each of the methods for the LRM to multiple outcomes. We typically find these approaches far more informative. Because the ORM is nonlinear in the outcome probabilities, no approach can fully describe the relationship between a variable and the outcome probabilities. Consequently, you should consider each of these methods before deciding which approach is most effective in your application. As with models for binary outcomes, the basic command for interpretations based on predictions is margins, although our mtable, mchange, and mgen commands make things much simpler. Not only do these commands have the advantages illustrated for binary models in chapter 6, but when there are multiple outcome categories, margins can only compute predictions for one outcome at a time. Our commands will compute predictions for all categories and combine the results. ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:7:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.8 Interpreting transformed coefficients As with the binary response model (BRM), coefficients for the ordered logit model are about 1.7 times larger than those for the ordered probit model because of the arbitrary assumption about the variance of the error term. For this reason, neither ordered logit nor ordered probit coefficients offer a direct interpretation that is readily meaningful. There are two ways we can transform the coefficients into more meaningful quantities: standardization and odds ratios. In both cases, these interpretations are permissible only when the independent variable is not specified using polynomial or interaction terms. ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:8:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.8.1 Marginal change in y* In the ORM, $y*=\\mathbf{x}\\boldsymbol{\\beta}+\\varepsilon $ and the marginal change in $y^*$ with respect to $x_k$ is $$\\frac{\\partial y^*}{\\partial x_k}=\\beta_k$$ Because $y*$ is latent, its true metric is unknown. The value of $y*$depends on the identification assumption we make about the variance of the errors. As a result, the marginal change in $y*$cannot be interpreted without standardizing by the estimated standard deviation of $y*$,which is computed as $$\\widehat{\\sigma}_{y^{*}}^{2}=\\widehat{\\beta}^{\\prime}\\widehat{\\mathrm{Var}}\\left(\\mathbf{x}\\right)\\widehat{\\beta}+\\mathrm{Var}\\left(\\varepsilon\\right)$$ where ${\\widehat{\\mathrm{Var}}}\\left(\\mathbf{x}\\right)$ is the covariance matrix for the observed $x’s,\\widehat{\\beta}$ contains maximum likelihood estimates, and $\\mathrm{Var}(\\varepsilon)=1$ for ordered probit and $\\pi^{2}/3$ for ordered logit. Then the $y^*$ standardized coefficient for $x_k$ is $$\\beta_{k}^{Sy^* }=\\frac{\\beta_{k}}{\\sigma_{y^*}}$$ which can be interpreted as follows: For a unit increase in $x_k$,$y^* $ expected to increase by $\\beta_{k}^{\\mathbf{S}y^{*}}$ standard deviations, holding all other variables constant. The fully standardized coefficient is $$\\beta_{k}^{S}=\\frac{\\sigma_{k}\\beta_{k}}{\\sigma_{y^{*}}}$$ which can be interpreted as follows: For a standard deviation increase in $x_k$,$y^* $ is expected to inciease by $\\beta_{k}^{S}$ standard deviations, holding all other variables constant. $y^*$的定义和边际变化： 在有序反应模型（ORM）中，$y^*$表示一个未观测到的变量，可以用以下方程表示： $$ y^* = \\mathbf{x}\\boldsymbol{\\beta} + \\varepsilon $$ 这里，$\\mathbf{x}$是自变量向量，$\\boldsymbol{\\beta}$是参数向量，$\\varepsilon$是误差项。 对于特定自变量$x_k$，$y^*$关于$x_k$的边际变化等于与该变量相关的系数$\\beta_k$： $$ \\frac{\\partial y^*}{\\partial x_k} = \\beta_k $$ $y^*$的标准化和解释： 由于$y^*$的真实尺度未知，需要标准化才能解释其含义。标准化后的系数表示单位变化引起的标准偏差变化。 对于自变量$x_k$，其标准化系数为： $$\\beta_{k}^{Sy^* }=\\frac{\\beta_{k}}{\\sigma_{y^*}}$$ 其中，$\\sigma_{y^* }$是$y^*$的估计标准差。 完全标准化系数： 完全标准化系数考虑了自变量 $x_k$ 和 $y^* $ 的标准差，用于衡量单位标准差变化对$y^* $的影响。 对于自变量$x_k$，其完全标准化系数为： $$\\beta_{k}^{S}=\\frac{\\sigma_{k}\\beta_{k}}{\\sigma_{y^{*}}}$$ 使用listcoef命令计算： 这些系数是通过listcoef命令和std选项计算得出的。 让我们考虑一个学生的考试成绩预测模型的例子： 考试成绩模型： 我们正在研究一个模型，用于预测学生的考试成绩$y^*$。我们假设考试成绩受到学习时间和课外活动的影响。 考试成绩模型是一个线性模型，可以表示为： $$ y^* = \\beta_0 + \\beta_1 \\times \\text{学习时间} + \\beta_2 \\times \\text{课外活动} + \\varepsilon $$ 在这个方程中，$\\beta_0$是截距，$\\beta_1$和$\\beta_2$分别是学习时间和课外活动的系数，$\\varepsilon$是未观测到的因素对考试成绩的影响。 标准化系数的解释： 标准化系数用于比较不同变量对$y^* $的影响。假设我们估计出的$\\beta_1$为5，表示每增加一个单位的学习时间，$y^* $预期会增加5个单位。 但是，学习时间的单位可能与考试成绩的单位不同，因此我们需要将系数标准化。 标准化系数$\\beta_{1}^{Sy^* }$是系数$\\beta_1$除以$y^* $的标准差$\\sigma_{y^* }$。如果$\\beta_1$为5，而$\\sigma_{y^* }$为10，那么： $$ \\beta_{1}^{Sy^* } = \\frac{5}{10} = 0.5 $$ 这意味着每增加一个标准差的学习时间，$y^* $预期会增加0.5个标准差。 完全标准化系数的解释： 完全标准化系数考虑了自变量和$y^* $的标准差。假设我们还知道课外活动的标准差为3，而$y^* $的标准差仍然为10。 完全标准化系数$\\beta_{1}^{S}$是系数$\\beta_1$乘以自变量$x$的标准差，再除以$y^* $的标准差。如果$\\beta_1$为5，学习时间的标准差为2，那么： $$ \\beta_{1}^{S} = \\frac{2 \\times 5}{10} = 1 $$ 这意味着每增加一个标准差的学习时间，$y^*$预期会增加1个标准差。 These coefficients are computed using listcoef with the std option. For example, after fitting the ordered logit model, ologit class i.female i.white i.year i.educ c.age##c.age income, nolog listcoef, std help ologit (N=5620): Unstandardized and standardized estimates Observed SD: 0.6749\rLatent SD: 2.1437\r-------------------------------------------------------------------------------\r| b z P\u003e|z| bStdX bStdY bStdXY SDofX\r-------------+-----------------------------------------------------------------\rfemale |\rfemale | 0.0162 0.298 0.765 0.008 0.008 0.004 0.498\r|\rwhite |\rwhite | 0.2363 3.277 0.001 0.092 0.110 0.043 0.389\r|\ryear |\r1996 | -0.0799 -1.158 0.247 -0.040 -0.037 -0.019 0.498\r2012 | -0.5039 -6.594 0.000 -0.233 -0.235 -0.109 0.463\r|\reduc |\rhs only | 0.3705 4.730 0.000 0.183 0.173 0.085 0.493\rcollege | 1.5656 15.994 0.000 0.670 0.730 0.313 0.428\r|\rage | -0.0488 -5.308 0.000 -0.825 -0.023 -0.385 16.897\r|\rc.age#c.age | 0.0007 7.646 0.000 1.202 0.000 0.561 1695.148\r|\rincome | 0.0116 22.203 0.000 0.770 0.005 0.359 66.258\r-------","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:8:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.8.2 Odds ratios The ordinal logit model (but not the ordinal probit model) can also be interpreted using odds ratios. Equation (7.2) defined the ordered logit model as $$\\Omega_{\\leq m|\u003em}\\left(\\mathbf{x}\\right)=\\exp\\left(\\tau_m-\\mathbf{x}\\boldsymbol{\\beta}\\right)$$ For example, with four outcomes we would simultaneously estimate the three equations $$\\Omega_{\\leq1|\u003e1}\\left(\\mathbf{x}\\right)=\\exp\\left(\\tau_1-\\mathbf{x}\\boldsymbol{\\beta}\\right)$$ $$\\Omega_{\\leq2|\u003e2}\\left(\\mathbf{x}\\right)=\\exp\\left(\\tau_{2}-\\mathbf{x\\beta}\\right)$$ $$\\Omega_{\\leq3|\u003e3}\\left(\\mathbf{x}\\right)=\\exp\\left(\\tau_{3}-\\mathbf{x\\beta}\\right)$$ Using the same approach as shown for binary logit, the effect of a unit change in $x_k$ equals $$\\frac{\\Omega_{\\leq m|\u003em}\\left(\\mathbf{x},x_k+1\\right)}{\\Omega_{\\leq m|\u003em}\\left(\\mathbf{x},x_k\\right)}=e^{-\\beta_k}=\\frac{1}{e^{\\beta_k}}$$ The value of the odds ratio does not depend on the value of $m$, which is why the parallel regression assumption is also known as the proportional-odds assumption. We could interpret the odds ratio as follows: For a unit increase in $x_k$, the odds of a lower outcome compared with a higher outcome are changed by the factor $\\exp(-\\beta_k)$, holding all other variables constant. For a change in $x_k$ of $\\delta$, $$\\frac{\\Omega_{\\leq m|\u003em}\\left(\\mathrm{x},x_k+\\delta\\right)}{\\Omega_{\\leq m|\u003em}\\left(\\mathrm{x},x_k\\right)}=\\exp\\left(-\\delta\\times\\beta_k\\right)=\\frac{1}{\\exp\\left(\\delta\\times\\beta_k\\right)}$$ which we interpret as follows: For an increase of $\\delta$ in $x_k$， the odds of a lower outcome compared with a higher outcome change by a factor of exp$\\left(-\\delta\\times\\beta_{k}\\right)$ , holding all other variables constant. Notice that the odds ratio is derived by changing one variable, $x_k$, while holding all other variables constant. Accordingly, you do not want to compute the odds ratio for a variable that is included as a polynomial (for example, age and age-squared) or is included in an interaction term. 有序logit模型方程式（7.2）： $$ \\Omega_{\\leq m|\u003em}(\\mathbf{x}) = \\exp(\\tau_m - \\mathbf{x}\\boldsymbol{\\beta}) $$ 这个方程描述了一种统计模型，用于预测一系列有序结果的概率。$\\Omega_{\\leq m|\u003em}(\\mathbf{x})$表示在给定一组特征$\\mathbf{x}$的情况下，得到结果小于或等于$m$的概率与得到结果大于$m$的概率之比。$\\tau_m$是模型中的一个参数，$\\mathbf{x}$是包含特征值的向量，$\\boldsymbol{\\beta}$是与特征相关的参数向量。 各个等式的解释： 对于有四个结果的情况，我们需要同时估计三个方程： 第一个方程表示等于或小于1的结果的概率与大于1的结果的概率之比。 $$\\Omega_{\\leq1|\u003e1}\\left(\\mathbf{x}\\right)=\\exp\\left(\\tau_1-\\mathbf{x}\\boldsymbol{\\beta}\\right)$$ 第二个方程表示等于或小于2的结果的概率与大于2的结果的概率之比。 $$\\Omega_{\\leq2|\u003e2}\\left(\\mathbf{x}\\right)=\\exp\\left(\\tau_{2}-\\mathbf{x\\beta}\\right)$$ 第三个方程表示等于或小于3的结果的概率与大于3的结果的概率之比。 $$\\Omega_{\\leq3|\u003e3}\\left(\\mathbf{x}\\right)=\\exp\\left(\\tau_{3}-\\mathbf{x\\beta}\\right)$$ 赔率比的计算公式： $$ \\frac{\\Omega_{\\leq m|\u003em}(\\mathbf{x},x_k+1)}{\\Omega_{\\leq m|\u003em}(\\mathbf{x},x_k)} = e^{-\\beta_k} = \\frac{1}{e^{\\beta_k}} $$ 这个公式告诉我们，当一个特征值$x_k$增加一个单位时，结果小于或等于$m$的概率与结果大于$m$的概率之比会以$e^{-\\beta_k}$的因子改变，而其他特征保持不变。$\\beta_k$是与特征$x_k$相关的参数。 赔率比的解释： 赔率比的值不依赖于结果的具体取值，因此被称为平行回归假设或比例赔率假设。对赔率比的解释是，对于$x_k$的单位增加，结果较低与结果较高之间的赔率会以$\\exp(-\\beta_k)$的因子改变，而其他变量保持不变。 当然，让我们通过一个简单的例子来说明这些概念。 假设我们正在研究一个考试成绩的有序结果：不及格、及格、良好、优秀。我们想知道学生的学习时间（特征$x_k$）对于各个考试成绩的影响。 有序logit模型方程式： $$ \\Omega_{\\leq m|\u003em}(\\mathbf{x}) = \\exp(\\tau_m - \\mathbf{x}\\boldsymbol{\\beta}) $$ 在我们的例子中，$\\Omega_{\\leq m|\u003em}(\\mathbf{x})$表示在给定学习时间$\\mathbf{x}$的情况下，得到结果小于或等于某个等级$m$（比如“及格”）的概率与得到结果大于$m$的概率之比。$\\tau_m$是模型中的一个参数，表示不同等级之间的分界点，$\\mathbf{x}$是学习时间的值，$\\boldsymbol{\\beta}$是与学习时间相关的参数。 赔率比的计算公式： $$ \\frac{\\Omega_{\\leq m|\u003em}(\\mathbf{x},x_k+1)}{\\Omega_{\\leq m|\u003em}(\\mathbf{x},x_k)} = e^{-\\beta_k} = \\frac{1}{e^{\\beta_k}} $$ 这个公式告诉我们，当学习时间增加一个单位时，结果小于或等于某个等级$m$（比如“及格”）的概率与结果大于$m$的概率之比会以$e^{-\\beta_k}$的因子改变，而其他特征保持不变。$\\beta_k$是与学习时间$x_k$相关的参数。 假设我们的模型告诉我们，$\\beta_k = 0.5$。这意味着当学习时间增加一个单位时，结果小于或等于某个等级$m$的概率与结果大于$m$的概率之比会减少一半。 例如，如果学生平均每天学习3小时（$x_k = 3$），那么结果小于或等于“及格”的概率与结果大于“及格”的概率之比为$e^{-0.5} \\approx 0.606$。这意味着考试成绩达到及格的概率是考试成绩高于及格的概率的0.606倍。 当然，让我用更具体的数值来展示计算过程。 假设我们的模型给出的参数是 $\\beta_k = 0.5$，而学生平均每天学习3小时（$x_k = 3$）。现在我们来计算结果小于或等于“及格”的概率与结果大于“及格”的概","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:8:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.9 Interpretations based on predicted probabilities As noted, we usually prefer interpretations based on predicted probabilities. We find these interpretations to be both clearer for our own thinking and more effective with audiences. Probabilities can be estimated with the formula $$\\widehat{\\mathrm{Pr}}\\left(y=m\\mid\\mathbf{x}\\right)=F\\left(\\widehat{\\tau_m}-\\mathbf{x}\\widehat{\\beta}\\right)-F\\left(\\widehat{\\tau}_{m-1}-\\mathbf{x}\\widehat{\\beta}\\right)$$ Cumulative probabilities are computed as $$\\widehat{\\Pr}\\left(y\\leq m\\mid\\mathbf{x}\\right)=\\sum_{k\\leq m}\\widehat{\\Pr}\\left(y=k\\mid\\mathbf{x}\\right)=F\\left(\\widehat{\\tau}_m-\\mathbf{x}\\widehat{\\boldsymbol{\\beta}}\\right)$$ The values of x can be based on observations in the sample or can be hypothetical values of interest. 让我详细解释这两个公式的含义，并给出一个例子。 预测概率公式： $$ \\widehat{\\mathrm{Pr}}\\left(y=m\\mid\\mathbf{x}\\right) = F\\left(\\widehat{\\tau_m}-\\mathbf{x}\\widehat{\\beta}\\right)-F\\left(\\widehat{\\tau}_{m-1}-\\mathbf{x}\\widehat{\\beta}\\right) $$ 这个公式用于计算在给定特征$\\mathbf{x}$的情况下，观测到结果为$m$的概率。$\\widehat{\\mathrm{Pr}}\\left(y=m\\mid\\mathbf{x}\\right)$表示结果等于$m$的概率的估计值。$F$代表累积分布函数，通常是指标函数的累积分布函数，它将一个值映射到概率上。$\\widehat{\\tau_m}$是模型中的一个参数，表示结果为$m$的临界值，$\\widehat{\\beta}$是模型中与特征相关的参数。 累积概率公式： $$ \\widehat{\\Pr}\\left(y\\leq m\\mid\\mathbf{x}\\right) = \\sum_{k\\leq m}\\widehat{\\Pr}\\left(y=k\\mid\\mathbf{x}\\right) = F\\left(\\widehat{\\tau}_m-\\mathbf{x}\\widehat{\\boldsymbol{\\beta}}\\right) $$ 这个公式用于计算在给定特征$\\mathbf{x}$的情况下，观测到结果小于或等于$m$的概率。$\\widehat{\\Pr}\\left(y\\leq m\\mid\\mathbf{x}\\right)$表示结果小于或等于$m$的概率的估计值。这个概率是对从结果等于1到结果等于$m$的所有可能结果的概率进行求和得到的。$\\widehat{\\tau}_m$是模型中的一个参数，表示结果为$m$的临界值，$\\widehat{\\boldsymbol{\\beta}}$是模型中与特征相关的参数向量。 例子： 假设我们正在研究一个学生考试成绩的模型，特征$\\mathbf{x}$包括学习时间和家庭背景等因素。我们想要估计一个学生考试成绩为及格的概率，并计算学生考试成绩小于或等于良好的概率。 假设我们的模型给出以下估计值：$\\widehat{\\tau_1} = 0$，$\\widehat{\\tau_2} = 1$，$\\widehat{\\beta} = 0.5$。现在，我们有一个学生，他每天学习3小时。 首先，我们计算学生考试成绩为及格的概率： $$ \\begin{align*} \\widehat{\\mathrm{Pr}}(y=\\text{及格}|\\mathbf{x}) \u0026= F(\\widehat{\\tau_2} - \\mathbf{x}\\widehat{\\beta}) - F(\\widehat{\\tau_1} - \\mathbf{x}\\widehat{\\beta}) \\ \u0026= F(1 - 3 \\times 0.5) - F(0 - 3 \\times 0.5) \\ \u0026= F(1.5) - F(-1.5) \\end{align*} $$ 这个结果是一个估计值，表示学生考试成绩为及格的概率。 接下来，我们计算学生考试成绩小于或等于良好的概率： $$ \\widehat{\\Pr}(y\\leq \\text{良好}|\\mathbf{x}) = F(\\widehat{\\tau}_3 - \\mathbf{x}\\widehat{\\boldsymbol{\\beta}}) $$ 在这个例子中，我们假设良好的成绩对应的临界值是$\\widehat{\\tau}_3 = 2$。因此： $$ \\begin{align*} \\widehat{\\Pr}(y\\leq \\text{良好}|\\mathbf{x}) \u0026= F(\\widehat{\\tau}_3 - \\mathbf{x}\\widehat{\\boldsymbol{\\beta}}) \\ \u0026= F(2 - 3 \\times 0.5) \\ \u0026= F(0.5) \\end{align*} $$ 这个结果是一个估计值，表示学生考试成绩小于或等于良好的概率。 The following sections use predicted probabilities in a variety of ways. We begin by examining the distribution of predictions for each observation in the estimation sample as a first step in evaluating your model. Next, we show how marginal effects provide an overall assessment of the impact of each variable. To focus on particular types of respondents, we compute predictions for ideal types defined by substantively motivated characteristics for all independent variables. Extending methods from chapter 6, we show how to statistically test differences in the predictions between ideal types. For categorical predictors, tables of predictions computed as these variables change is an effective way to demonstrate the effects of these variables is to use tables of predictions computed as these variables change. An important challenge is to decide where to hold the values of other variables when making predictions. Finally, we plot predictions as a continuous independent variable changes. ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:9:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.10 Predicted probabilities with predict After fitting a model with ologit or oprobit, a useful first step for assessing your model is to compute the in-sample predictions with the command predict newvar1 [ newvar2 [newvar3··· ]] [if] [in] where you specify one new variable name for each category of the dependent variable. For instance, in the following example, predict specifies that the variables prlover, prworking, prmiddle, and prupper be created with predicted values for the four outcome categories: ologit class i.female i.white i.year i.educ c.age##c.age income, nolog predict prlower prworking prmiddle prupper Ordered logistic regression Number of obs = 5,620\rLR chi2(9) = 1453.95\rProb \u003e chi2 = 0.0000\rLog likelihood = -5016.2107 Pseudo R2 = 0.1266\r------------------------------------------------------------------------------\rclass | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rfemale |\rfemale | 0.02 0.05 0.30 0.765 -0.09 0.12\r|\rwhite |\rwhite | 0.24 0.07 3.28 0.001 0.09 0.38\r|\ryear |\r1996 | -0.08 0.07 -1.16 0.247 -0.22 0.06\r2012 | -0.50 0.08 -6.59 0.000 -0.65 -0.35\r|\reduc |\rhs only | 0.37 0.08 4.73 0.000 0.22 0.52\rcollege | 1.57 0.10 15.99 0.000 1.37 1.76\r|\rage | -0.05 0.01 -5.31 0.000 -0.07 -0.03\r|\rc.age#c.age | 0.00 0.00 7.65 0.000 0.00 0.00\r|\rincome | 0.01 0.00 22.20 0.000 0.01 0.01\r-------------+----------------------------------------------------------------\r/cut1 | -2.14 0.23 -2.59 -1.69\r/cut2 | 0.92 0.23 0.47 1.36\r/cut3 | 4.93 0.24 4.46 5.41\r------------------------------------------------------------------------------\r(option pr assumed; predicted probabilities) The message (option pr assumed; predicted probabilities) reflects that predict can compute many different quantities. Because we did not specify an option indicating which quantity to predict, the default option pr for predicted probabilities was assumed. Predictions in the sample are useful for getting a general sense of what is going on in your model and can be useful for uncovering problems in your data. For example, if there are observations where the predicted probability of being in prlower (or any other outcome) are noticeably larger or smaller than the other predictions, you might check whether there are data problems for those observations. The range of predictions can also give you a rough idea of how large marginal effects can be for a given outcome. If the range of probabilities is small within the estimation sample, the effects of the independent variables will also be small. If the distribution of predictions has multiple modes — let’s say, two — it suggests there could be a binary predictor that is important. Although sometimes the distribution of predictions leads to additional data checking or model revision, often it only assures you that the predictions are reasonable and that you are ready for the methods of interpretation that we consider in the remainder of this chapter. 好的，让我详细解释一下。 样本中的预测值：假设我们有一个模型，用来预测学生考试成绩的可能结果。对于每个学生，我们可以使用这个模型来预测他们可能取得的不同等级的考试成绩，比如不及格、及格、良好和优秀。这些预测值告诉我们，根据模型，每个学生将以多大的概率获得不同等级的成绩。 检测数据问题：对于第二条，我们讨论的是样本中的预测值可能帮助我们发现数据中存在的问题。 当我们观察到某些观测的预测概率明显高于或低于其他观测时，这可能表明数据中存在问题。例如，如果某些观测的预测概率远高于其他观测，这可能是因为这些观测包含了异常值，或者数据记录可能存在错误。 举个例子，假设我们正在研究一个模型，用来预测学生考试成绩的可能结果。我们发现，某些学生的预测概率特别高，远高于其他学生。这可能意味着这些学生的数据存在问题，比如他们的成绩可能被错误地记录在了高分段，或者他们的学习时间等特征值可能存在异常值。 通过观察这些异常的预测值，我们可以发现并修正数据中的问题，例如，我们可以重新检查这些学生的成绩记录，或者确认他们的特征值是否正确录入。这样，我们就可以提高模型的准确性，并确保我们的分析结果是可靠的。 边际效应的估计：当我们说预测值的范围可以帮助我们了解模型的边际效应时，我们是指模型对于不同情况的反应程度。边际效应是指独立变量的一个单位变化如何影响因变量的变化。如果模型的预测值范围很小，意味着模型对于不同情况的反应差异不大，这可能表明模型的边际效应较小。换句话说，即使独立变量有所变化，模型对于结果的预测也不会有太大变化。 举个例子来说明，假设我们正在研究一个模型，用来预测学生考试成绩的可能结果。我们发现，无论学生的学习时间是少还是多，模型对于他们的考试成绩的预测都差不多。这就意味着学生的学习时间对于考试成绩的影响并不显著，模型的边际效应较小。 而当预测值的范围很大时，意味着模型对于不同情况的反应差异较大，这可能表明模型的边际效应较大。换句话说，独立变量的变化会导致结果的较大变化。 检测重要预测变量：当我们观察到预测分布中存在多个峰值时，这可能意味着有一个重要的二元预测变量影响着结果。换句话说，这种现象可能暗示着某个二元变量对于结果的影响非常重要。通过观察这些预测分布的特征，我们可以更深入地了解模型中各个变量的影响，并相应地调整我们的分析方法。 总之，通过观察样本中的预测结果，我们可以更好地了","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:10:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.11 Marginal effects The marginal change in the probability of outcome m is computed as $$\\frac{\\partial\\Pr(y=m\\mid\\mathbf{x})}{\\partial x_k}=\\frac{\\partial F(\\tau_m-\\mathbf{x}\\boldsymbol{\\beta})}{\\partial x_k}-\\frac{\\partial F(\\tau_{m-1}-\\mathbf{x}\\boldsymbol{\\beta})}{\\partial x_k}$$ which is the slope of the curve relating $x_k$ to $\\Pr(y=m|\\mathbf{x})$, holding all other variables constant. The value of the marginal change depends on the value of $x_k$ where the change is evaluated, as well as the values of all other $x’s$ Because the marginal change can be misleading when the probability curve is changing rapidly, we usually prefer using discrete change. The discrete change is the change in the probability of $m$ for a change in $x_k$ from the start value $x_k^{\\mathrm{start}}$ to the end value $x_{k}^{\\mathrm{end}}$ (for example, a change from $x_k = 0$ to $x_k = 1$), holding all other $x$’s constant. Formally, $$\\frac{\\Delta\\Pr\\left(y=m\\mid\\mathbf{x}\\right)}{\\Delta x_k\\left(x_k^{\\mathrm{start}}\\to x_k^{\\mathrm{cod}}\\right)}=\\Pr\\left(y=m\\mid\\mathbf{x},x_k=x_k^{\\mathrm{cod}}\\right)-\\Pr\\left(y=m\\mid\\mathbf{x},x_k=x_k^{\\mathrm{start}}\\right)$$ where $\\Pr\\left(y=m\\mid\\mathbf{x},x_k\\right)$ is the probability that $y = m$ given $x$, noting a specific value for $X_k$.The change indicates that when $x_k$ changes from $x_{k}^{\\mathrm{start}}\\text{ to }x_{k}^{\\mathrm{end}}$ the probability of outcome m changes by $\\Delta\\Pr\\left(y=m\\mid\\mathbf{x}\\right)/\\Delta x_{k}$ holding all other variables at the specific values in $x$. The magnitude of the discrete change depends on the value at which $X_k$ starts, the amount of change in $x_k$, and the values of all other variables. For both marginal and discrete change, we can compute average marginal effects (AMEs), marginal effects at the mean (MEMs), or marginal effects at representative values other than the means. As with the BRM, we find AMEs to be the most useful summary of the effects, thus we consider AMEs for the ORM in this section. MEMs are considered briefly in section 7.15. Examining the distribution of effects over observations is also valuable. To save space, we do not illustrate this in the current chapter, but this can be done using the commands presented in section 8.8.1. 当然，以下是你要求的完整答案： 首先，让我们来详细解释这两个公式。 第一个公式： $$ \\frac{\\partial\\Pr(y=m\\mid\\mathbf{x})}{\\partial x_k} = \\frac{\\partial F(\\tau_m-\\mathbf{x}\\boldsymbol{\\beta})}{\\partial x_k} - \\frac{\\partial F(\\tau_{m-1}-\\mathbf{x}\\boldsymbol{\\beta})}{\\partial x_k} $$ 这个公式表示了因变量 $y$ 为 $m$ 的概率 $\\Pr(y=m\\mid\\mathbf{x})$ 对于自变量 $x_k$ 的边际变化。让我们逐步分解这个公式： $\\frac{\\partial\\Pr(y=m\\mid\\mathbf{x})}{\\partial x_k}$：表示因变量 $y$ 为 $m$ 的概率随着 $x_k$ 的变化而变化的速率。 $\\frac{\\partial F(\\tau_m-\\mathbf{x}\\boldsymbol{\\beta})}{\\partial x_k}$：表示 $x_k$ 的微小变化对应的概率值 $F(\\tau_m-\\mathbf{x}\\boldsymbol{\\beta})$ 的变化量。 $\\frac{\\partial F(\\tau_{m-1}-\\mathbf{x}\\boldsymbol{\\beta})}{\\partial x_k}$：类似地，表示 $x_k$ 的微小变化对应的概率值 $F(\\tau_{m-1}-\\mathbf{x}\\boldsymbol{\\beta})$ 的变化量。 换句话说，这个公式告诉我们，当自变量 $x_k$ 变化时，因变量 $y$ 为 $m$ 的概率的变化量，是由两部分组成的：一部分是 $y$ 大于 $m$ 的概率的变化量，另一部分是 $y$ 小于等于 $m$ 的概率的变化量。 第二个公式： $$ \\frac{\\Delta\\Pr\\left(y=m\\mid\\mathbf{x}\\right)}{\\Delta x_k\\left(x_k^{\\mathrm{start}}\\to x_k^{\\mathrm{end}}\\right)} = \\Pr\\left(y=m\\mid\\mathbf{x},x_k=x_k^{\\mathrm{end}}\\right) - \\Pr\\left(y=m\\mid\\mathbf{x},x_k=x_k^{\\mathrm{start}}\\right) $$ 这个公式表示，在自变量 $x_k$ 从起始值 $x_k^{\\mathrm{start}}$ 变化到结束值 $x_{k}^{\\mathrm{end}}$ 的情况下，因变量 $y$ 为 $m$ 的概率的变化量。让我们逐步分解这个公式： $\\frac{\\Delta\\Pr\\left(y=m\\mid\\mathbf{x}\\right)}{\\Delta x_k\\left(x_k^{\\mathrm{start}}\\to x_k^{\\mathrm{end}}\\right)}$：表示自变量 $x_k$ 从起始值到结束值的变化对应的因变量 $y$ 为 $m$ 的概率的变化量。 $\\Pr\\left(y=m\\mid\\mathbf{x},x_k=x_k^{\\mathrm{end}}\\right)$：表示当 $x_k$ 变为结束值时，$y$ 为 $m$ 的概率。 $\\Pr\\left(y=m\\mid\\mathbf{x},x_k=x_k^{\\mathrm{start}}\\right)$：表示当 $x_k$ 是起始值时，$y$ 为 $m$ 的概率。 换句话说，这个公式告诉我们，当自变量 $x_k$ 从起始值变化到结束值时，因变量 $y$ 为 $m$ 的概率的变化量，是结束值时的概率减去起始值时的概率。 现在，让我们来举个例子来说明这两个公式的用法。 假设我们有一个模型，可以预测学生的考试成绩。我们想知道的是，当学生花更多时间学习时，他们的考试成绩会如何变化。 首先，我们使用第一个公式来计算边际变化。我们假设学生学习时间是一个自变量 $x_k","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:11:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.11.1 Plotting marginal effects As we suggested for the BRM, the AME is a valuable tool for examining the effects of your variables, and we often compute these effects for an initial review of the results of a model. Without doubt, AMEs are far more informative than the parameter estimates or odds ratios. There is, however, a lot of information to be absorbed. By default, for each continuous variable, mchange computes the marginal change and discrete changes for a 1-unit and a standard deviation change in continuous variables; for factor variables, mchange computes a discrete change from 0 to 1. One way to limit the amount of information is to only look at discrete changes of a standard deviation for continuous variables. For example, 当我们谈论一个自变量的平均边际效应时，我们实际上是在讨论这个自变量对于因变量的影响，而不是仅仅讨论它们之间的相关性或关系强度。平均边际效应提供了一个量化的指标，告诉我们当自变量的值发生变化时，因变量的预期变化量。 在实际应用中，平均边际效应通常是在拟合了统计模型之后计算得出的。比如，在逻辑回归模型中，我们可以计算出每个自变量对于因变量的平均边际效应，从而了解它们对于结果的实际影响。这种影响可以是因变量的变化概率，也可以是其他感兴趣的结果指标。 举个例子，假设我们在研究人们购买某种产品的决策时，使用了一个逻辑回归模型，并且其中一个自变量是收入水平。通过计算收入水平的平均边际效应，我们可以得知当收入增加一个单位时，购买该产品的概率预计会增加多少。这种量化的解释有助于我们更深入地理解模型结果，并且为决策提供更具体的指导。 ologit class i.female i.white i.year i.educ c.age##c.age income, nolog mchange, amount(sd) brief ologit: Changes in Pr(y) | Number of obs = 5620\rExpression: Pr(class), predict(outcome())\r| lower working middle upper ------------------------+-------------------------------------------\rfemale | female vs male | -0.001 -0.002 0.003 0.000 p-value | 0.766 0.765 0.765 0.765 white | white vs nonwhite | -0.016 -0.031 0.041 0.006 p-value | 0.002 0.001 0.001 0.001 year | 1996 vs 1980 | 0.004 0.012 -0.014 -0.003 p-value | 0.243 0.249 0.246 0.253 2012 vs 1980 | 0.033 0.067 -0.086 -0.014 p-value | 0.000 0.000 0.000 0.000 2012 vs 1996 | 0.029 0.055 -0.073 -0.011 p-value | 0.000 0.000 0.000 0.000 educ | hs only vs not hs grad | -0.029 -0.047 0.070 0.006 p-value | 0.000 0.000 0.000 0.000 college vs not hs grad | -0.079 -0.252 0.286 0.046 p-value | 0.000 0.000 0.000 0.000 college vs hs only | -0.050 -0.205 0.216 0.039 p-value | 0.000 0.000 0.000 0.000 age | +SD | -0.018 -0.071 0.067 0.022 p-value | 0.000 0.000 0.000 0.000 income | +SD | -0.036 -0.119 0.126 0.030 p-value | 0.000 0.000 0.000 0.000 Even so, there are a lot of coefficients. Fortunately, they can be quickly understood by plotting them. To explain how to do this, we begin by examining the AMEs for a standard deviation change in income and age. Because the model includes age and age-squared, when age is increased by a standard deviation, we need to increase age-squared by the appropriate amount. This is done automatically by Stata because we entered age into the model with the factor-variable notation c.age##c.age. To compute the average discrete changes for a standard deviation increase, type mchange age income, amount(sd) brief ologit: Changes in Pr(y) | Number of obs = 5620\rExpression: Pr(class), predict(outcome())\r| lower working middle upper -------------+-------------------------------------------\rage | +SD | -0.018 -0.071 0.067 0.022 p-value | 0.000 0.000 0.000 0.000 income | +SD | -0.036 -0.119 0.126 0.030 p-value | 0.000 0.000 0.000 0.000 mchange leaves these results in memory, and they are used by our mchangeplot command to create the plot. The horizontal axis indicates the magnitude of the effect, with the letters within the graph marking the discrete change for each outcome. For example, the M in the row for income shows that, on average for a standard deviation change in income, the probability of identifying with the middle class increases by 0.126. Overall, it is apparent that the effects of income are larger than those for a standard deviation change in age. For both variables, the effects are in the same directions with the same relative magnitudes. (Before proceeding, you should make sure you see how the graph corresponds to the output from mchange above.) The plot was produced with the following command: mchangeplot age income, symbols(L W M U) /// min(-.15) max(.15) gap(.05) a","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:11:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.11.2 Marginal effects for a quick overview AMEs are a much better way to obtain a quick overview of the magnitudes of effects than are the estimated coefficients. After fitting your model, you can obtain a table of all effects by simply typing mchange, perhaps restricting effects to discrete changes of a standard deviation: ologit class i.female i.white i.year i.educ c.age##c.age income, nolog mchange, amount(sd) brief ologit: Changes in Pr(y) | Number of obs = 5620\rExpression: Pr(class), predict(outcome())\r| lower working middle upper ------------------------+-------------------------------------------\rfemale | female vs male | -0.001 -0.002 0.003 0.000 p-value | 0.766 0.765 0.765 0.765 white | white vs nonwhite | -0.016 -0.031 0.041 0.006 p-value | 0.002 0.001 0.001 0.001 year | 1996 vs 1980 | 0.004 0.012 -0.014 -0.003 p-value | 0.243 0.249 0.246 0.253 2012 vs 1980 | 0.033 0.067 -0.086 -0.014 p-value | 0.000 0.000 0.000 0.000 2012 vs 1996 | 0.029 0.055 -0.073 -0.011 p-value | 0.000 0.000 0.000 0.000 educ | hs only vs not hs grad | -0.029 -0.047 0.070 0.006 p-value | 0.000 0.000 0.000 0.000 college vs not hs grad | -0.079 -0.252 0.286 0.046 p-value | 0.000 0.000 0.000 0.000 college vs hs only | -0.050 -0.205 0.216 0.039 p-value | 0.000 0.000 0.000 0.000 age | +SD | -0.018 -0.071 0.067 0.022 p-value | 0.000 0.000 0.000 0.000 income | +SD | -0.036 -0.119 0.126 0.030 p-value | 0.000 0.000 0.000 0.000 With a simple command, you can plot the effects: mchangeplot, sig(.05) symbols(L W M U) leftmargin(5) A quick review highlights which variables we might, want to examine more closely. ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:11:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.12 Predicted probabilities for ideal types Ideal types define substantively interesting cases in the data by specifying values of the independent variables. Predicted probabilities for these types of individuals (or whatever the unit of analysis may be) can be computed with mtable or margins. Unlike marginal effects, by comparing two or more ideal types, you can compare probabilities as a whole set of independent variables vary, not just a change in a single variable. In our example, ideal types can be used to examine what more and less advantaged individuals look like and how they differ in their class identification. For instance, we might want to compare the following hypothetical individuals surveyed in 2012: A 25-year-old, nonwhite man without a high school diploma and with a household income of $30,000$ per year. A 60-year-old, white woman with a college degree and with a household income of $150,000$ per year. To compute the predictions, we begin by using margins before showing how mtable can simplify the work. We use a at() to specify values of the independent variables. If there are variables whose values are not specified with a at(), we can use the option atmeans to assign them to their means. Otherwise, by default, margins and mtable would compute the average predicted probability over the sample for the unspecified independent variables. We do not want to do this because ideal types should be thought of as hypothetical observations, so averaging predictions over observations for some independent variables muddles the interpretation. Using values we specified for our first ideal type, we run margins: ologit class i.female i.white i.year i.educ c.age##c.age income, nolog margins, at(female=0 white=0 year=3 educ=1 age=25 income=30) Adjusted predictions Number of obs = 5,620\rModel VCE: OIM\r1._predict: Pr(class==1), predict(pr outcome(1))\r2._predict: Pr(class==2), predict(pr outcome(2))\r3._predict: Pr(class==3), predict(pr outcome(3))\r4._predict: Pr(class==4), predict(pr outcome(4))\rAt: female = 0\rwhite = 0\ryear = 3\reduc = 1\rage = 25\rincome = 30\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_predict |\r1 | 0.23 0.02 11.35 0.000 0.19 0.27\r2 | 0.63 0.01 59.17 0.000 0.61 0.65\r3 | 0.13 0.01 10.15 0.000 0.11 0.16\r4 | 0.00 0.00 6.68 0.000 0.00 0.00\r------------------------------------------------------------------------------\rmargins can only compute a prediction for a single outcome. Because we did not specify which outcome, margins used the default prediction, which is described as Pr(class == l), predict(). This is the predicted probability for the first outcome. Hence, we find that the predicted probability of identifying as lower class for our first ideal type is 0.23. margins, at(female=0 white=0 year=3 educ=1 age=25 income=30) predict(outcome(1)) margins, at(female=0 white=0 year=3 educ=1 age=25 income=30) predict(outcome(2)) margins, at(female=0 white=0 year=3 educ=1 age=25 income=30) predict(outcome(3)) margins, at(female=0 white=0 year=3 educ=1 age=25 income=30) predict(outcome(4)) It is easier, however, to use mtable, which computes predictions for all outcome categories and combines them into a single table. The option ci indicates that we want the output to show the confidence interval. mtable, at(female=0 white=0 year=3 educ=1 age=25 income=30) ci Expression: Pr(class), predict(outcome())\r| lower working middle upper\r----------+---------------------------------------\rPr(y) | 0.230 0.634 0.133 0.003\rll | 0.190 0.613 0.108 0.002\rul | 0.270 0.655 0.159 0.004\rSpecified values of covariates\r| female white year educ age income\r----------+-----------------------------------------------------------\rCurrent | 0 0 3 1 25 30\rWe could also compute predicted probabilities for both ideal types at the same time: mtable, atright norownum width(7) /// at(","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:12:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.12.1 (Advanced) Testing differences between ideal types Although we regard this section as extremely useful, we mark it as advanced because it requires a firm grasp of using loops and local macros in Stata. If you are still getting used to these, you might want to skip this section until you are comfortable with both. 同We may want to know whether a difference between ideal types is statistically significant for the same reason that we may perform a significance test for a set of coefficients: we are considering a change that involves multiple variables, and we want to evaluate how likely it is that we would observe a difference this large just by chance. Although the differences between predictions for our two ideal types are almost certainly significant, we can test this by extending methods used for binary outcomes. 同To test differences in predictions, we need to overwrite the estimation results from ologit with the predictions generated by margins. An inherent limitation in margins is that posting can only be done for a single outcome. That is, we cannot post the predictions for our four outcomes at one time. (We hope this will be addressed in future versions of margins.) To deal with this inconvenience, we will use a forvalues loop to repeat the tests for each outcome. First, we list the model and store the estimates, because we will have to restore the model results after we post the predictions for a particular outcome: ologit class i.female i.white i.year i.educ c.age##c.age income, nolog estimates store olm Next, we compute tests for each of the four outcome categories by using the following commands: mlincom, clear forvalues iout = 1/4 { // start loop quietly { mtable, out(`iout') post /// at(female=0 white=0 year=3 ed=1 age=25 income=30) /// at(female=1 white=1 year=3 ed=3 age=60 income=150) mlincom 1 - 2, stats(est pvalue) rowname(outcome `iout') add estimates restore olm } } // end loop We start with mlincom, clear to erase previous results from mlincom before we accumulate the new results that we will use to make our table. The forvalues {...} loop runs the code between braces once for each outcome. We use quietly {...} so that the output from mtable and mlincom is not displayed. Instead, we will list results after the loop is completed. Within the loop, the mtable option post saves the estimates for outcome ‘iout’ to the matrix e(b) so that mlincom can test the difference between the predictions for the first and second ideal types. The mlincom command uses option add to collect the results to display later. After the loop, running mlincom without options displays the results we just computed. The column named lincom has the linear combination of the estimates—in this case, the difference in predictions—while column pvalue is the p-value for testing that the difference is 0: mlincom | lincom pvalue ————-+——————- outcome 1 | 0.222 0.000 outcome 2 | 0.496 0.000 outcome 3 | -0.626 0.000 outcome 4 | -0.092 0.000 ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:12:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.13 Tables of predicted probabilities When there are substantively important categorical predictors in the model, examining tables of predicted probabilities over values of these variables can be an effective way to interpret the results. In this example, we use mtable to look at predictions over the values of the year of the survey, which correspond to 1980, 1996, and 2012: ologit class i.female i.white i.year i.educ c.age##c.age income, nolog mtable, at(year=(1 2 3)) atmeans norownum Expression: Pr(class), predict(outcome())\ryear lower working middle upper\r------------------------------------------------\r1 0.049 0.473 0.462 0.016\r2 0.053 0.489 0.443 0.015\r3 0.078 0.565 0.347 0.010\rSpecified values of covariates\r| 1. 1. 2. 3. | female white educ educ age income\r----------+-----------------------------------------------------------\rCurrent | .549 .814 .582 .241 45.2 68.1\rThe atmeans option holds other variables at their means in the estimation sample. We conclude the following: Changing only the year of the survey, and with income measured in 2012 dollars for all survey years, the probability of a respondent identifying as working class increased from 0.47 in 1980 to 0.57 in 2012, while the probability of identifying as middle class declined from 0.46 to 0.35. To obtain confidence intervals for the predictions, we use the option stat(ci), which could be abbreviated simply as ci: mtable, at(year=(1 2 3)) atmeans stat(ci) Expression: Pr(class), predict(outcome())\r| year lower working middle upper\r----------+-------------------------------------------------\rPr(y) | 1 0.049 0.473 0.462 0.016\rll | 1 0.042 0.447 0.433 0.013\rul | 1 0.056 0.499 0.491 0.020\rPr(y) | 2 0.053 0.489 0.443 0.015\rll | 2 0.046 0.468 0.421 0.012\rul | 2 0.059 0.510 0.466 0.018\rPr(y) | 3 0.078 0.565 0.347 0.010\rll | 3 0.068 0.543 0.321 0.008\rul | 3 0.088 0.587 0.372 0.012\rSpecified values of covariates\r| 1. 1. 2. 3. | female white educ educ age income\r----------+-----------------------------------------------------------\rCurrent | .549 .814 .582 .241 45.2 68.1\rThe lower and upper bounds of the intervals print on separate rows beneath each prediction. For example: Holding independent variables at their sample means, respondents in 2012 had a 0.078 probability of identifying as lower class (95% CI: [0.068, 0.088]). We might also want to generate tables for a combination of categorical independent variables. For example, how does class affiliation vary by race for the three years of our survey? While we are considering the probabilities implied by having year and white in the model as separate independent variables, we could also fit a model in which the interaction term i.year#i.white is included. mtable, at(year=(1 2 3) white=(0 1)) atmeans norownum Expression: Pr(class), predict(outcome())\rwhite year lower working middle upper\r----------------------------------------------------------\r0 1 0.059 0.511 0.417 0.013\r0 2 0.063 0.526 0.399 0.012\r0 3 0.093 0.593 0.306 0.008\r1 1 0.047 0.464 0.472 0.017\r1 2 0.051 0.480 0.454 0.016\r1 3 0.075 0.558 0.356 0.010\rSpecified values of covariates\r| 1. 2. 3. | female educ educ age income\r----------+-------------------------------------------------\rCurrent | .549 .582 .241 45.2 68.1\rThe predictions vary by year within a given value of white. The way in which variables vary is determined by the order in which the variables are specified with ologit, not by the order of variables within the at() statement. The table might be clearer if predictions were arranged to vary by white within each value of year (in practice, we often have to try it both ways before deciding which is clearer for the purpose at hand). We can refit the ologit model with year listed before white, or we can specify the values of year within three separate at() statements: mtable, atmeans norownum /// at(year=1 white=(0 1)) /// 1980 at(year=2 white=(0 1)) /// 1996 at(year=3 white=(0 1)) // 2012 Expression: Pr(class), predict(outcome())\rwhite year lower working m","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:13:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.14 Plotting predicted probabilities Plotting predicted probabilities for each outcome can also be useful for the ORM. These plots illustrate how predicted probabilities change as a continuous, independent variable changes. With the BRM, we showed two approaches for making plots: directly with marginsplot or in two steps with margins and graph. Plotting multiple outcomes, however, can only be done using the latter technique because marginsplot is limited to plotting a single outcome. To illustrate graphing predictions, we consider how the probability of class affiliation changes as household income changes, holding all other variables at their sample means. Of course, the plot could also be constructed for other sets of characteristics.The option at(inc=0(25)250) tells mgen to generate predictions as income changes from 0 to 250 in increments of 25, leading to 11 sets of predictions. The option atmeans holds other variables to their means. We use stub(CL_) to add CL_ (indicating predictions for class) to the names of variables generated by mgen: ologit class i.female i.white i.year i.educ c.age##c.age income, nolog mgen, at(income=(0(25)250)) stub(CL_) atmeans Predictions from: margins, at(income=(0(25)250)) atmeans predict(outcome())\rVariable Obs Unique Mean Min Max Label\r--------------------------------------------------------------------------------------\rCL_pr1 11 11 .0439624 .0074601 .1207294 pr(y=lower) from margins\rCL_ll1 11 11 .0385464 .0057745 .1067216 95% lower limit\rCL_ul1 11 11 .0493784 .0091458 .1347373 95% upper limit\rCL_income 11 11 125 0 250 household income\rCL_Cpr1 11 11 .0439624 .0074601 .1207294 pr(y\u003c=lower)\rCL_pr2 11 11 .377087 .1301718 .6238775 pr(y=working) from margins\rCL_ll2 11 11 .3565982 .1081524 .6070259 95% lower limit\rCL_ul2 11 11 .3975758 .1521912 .6407292 95% upper limit\rCL_Cpr2 11 11 .4210494 .137632 .744607 pr(y\u003c=working)\rCL_pr3 11 11 .5424338 .2492696 .7612023 pr(y=middle) from margins\rCL_ll3 11 11 .5212693 .2296227 .7417022 95% lower limit\rCL_ul3 11 11 .5635983 .2689165 .7807024 95% upper limit\rCL_Cpr3 11 11 .9634833 .8988342 .9938766 pr(y\u003c=middle)\rCL_pr4 11 11 .0365167 .0061234 .1011657 pr(y=upper) from margins\rCL_ll4 11 11 .030147 .0047639 .0828273 95% lower limit\rCL_ul4 11 11 .0428865 .0074829 .1195042 95% upper limit\rCL_Cpr4 11 10 1 1 1 pr(y\u003c=upper)\r-------------------------------------------------------------------------------------\rSpecified values of covariates\r1. 1. 2. 3. 2. 3. female white year year educ educ age ---------------------------------------------------------------------------\r.5491103 .8140569 .4510676 .3099644 .5818505 .2414591 45.15712 Each variable has 11 observations corresponding to different values of income. Variables containing predicted probabilities are stored in variables named CL_pr#. For example, CL_pr2 is the predicted probability of identifying as working class, the second category of our outcome. Variables containing cumulative probabilities—that is, the probability of observing a given category or lower—are stored as variables CL_Cpr#. For example, CL_Cpr2 is the predicted probability of a respondent identifying as either lower class or working class. Although mgen assigns variable labels to the variables it generates, we can change these to improve the look of the plot that we are creating. Specifically, we use label var CL_pr1 \"Lower\" label var CL_pr2 \"Working\" label var CL_pr3 \"Middle\" label var CL_pr4 \"Upper\" label var CL_Cpr1 \"Lower\" label var CL_Cpr2 \"Lower/Working\" label var CL_Cpr3 \"Lower/Working/Middle\" Next, we plot the probabilities of individual outcomes by using graph. Here we plot the four probabilities against values of income. graph twoway connected CL_pr1 CL_pr2 CL_pr3 CL_pr4 CL_income, /// title(\"Panel A: Predicted Probabilities\") /// xtitle(\"Household income (2012 dollars)\") /// xlabel(0(50)250) ylabel(0(.25)1, grid gmin gmax) /// ytitle(\"\") name(tmpprob, replace) Standard options for graph are used to specify the axes and labels. The ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:14:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.15 Probability plots and marginal effects Having considered various methods of interpretation, we now show the link between marginal effects and plots of predicted probabilities to hopefully provide you with new insights on the nature of ordinal models. The following graph, based on section 7.14, shows how the probabilities of class affiliation change with income, holding all other variables at their means. The mean of income is indicated with a dashed, vertical line: The slope of each probability curve evaluated at the mean of income, indicated by where the probability curves intersect the vertical line, is the marginal change in the probability of a given class affiliation with respect to income, with all variables held at their means. We can compute these changes by using mchange, atmeans to estimate MEMs: ologit class i.female i.white i.year i.educ c.age##c.age income, nolog mchange income, atmeans amount(marginal) dec(4) ologit: Changes in Pr(y) | Number of obs = 5620\rExpression: Pr(class), predict(outcome())\r| lower working middle upper -------------+-------------------------------------------\rincome | Marginal | -0.0006 -0.0022 0.0027 0.0002 p-value | 0.0000 0.0000 0.0000 0.0000 Predictions at base value\r| lower working middle upper -------------+-------------------------------------------\rPr(y|base) | 0.0586 0.5107 0.4173 0.0134 Base values of regressors\r| 1. 1. 2. 3. 2. 3. | female white year year educ educ age income -------------+---------------------------------------------------------------------------------------\rat | .5491 .8141 .4511 .31 .5819 .2415 45.16 68.08 1: Estimates with margins option atmeans.\rThe marginal changes are in row Marginal, with the significance level for the change listed in row p-value. These changes correspond to the probability curves at the point of intersection with the vertical line. For the slope for middle class, shown with squares, is 0.0027. The magnitude of the marginal changes would differ if we computed the marginal effects at different values of the independent variables. For example, we can compute the effects with income equal to $250,000$, with all other variables still kept at their means: mchange income, at(income=250) atmeans amount(marginal) dec(4) ologit: Changes in Pr(y) | Number of obs = 5620\rExpression: Pr(class), predict(outcome())\r| lower working middle upper -------------+-------------------------------------------\rincome | Marginal | -0.0001 -0.0013 0.0003 0.0011 p-value | 0.0000 0.0000 0.0378 0.0000 Predictions at base value\r| lower working middle upper -------------+-------------------------------------------\rPr(y|base) | 0.0075 0.1302 0.7612 0.1012 Base values of regressors\r| 1. 1. 2. 3. 2. 3. | female white year year educ educ age income -------------+---------------------------------------------------------------------------------------\rat | .5491 .8141 .4511 .31 .5819 .2415 45.16 250 1: Estimates with margins option atmeans.\rThe marginal change for the probability of identifying with the middle class is much smaller, corresponding to the leveling off of the curve (shown with ♦’s) on the right side of the graph. In this example, the signs of the marginal effects for each outcome are the same throughout the range of income. This, however, does not need to be true. In the ORM, not only does the magnitude of the effect change as the values of the independent variables change, but even the sign can change. That is to say, the effect of a variable can be positive at one point and can be negative at other points, even if we have not included polynomial terms or interactions in the model. In a model without interaction or polynomials for a given independent variable, the sign of that variable’s regression coefficient will always be the same as the direction of changes in the probability of the highest outcome category as the independent variable increases. In our example of subjective social class, because the coefficient for income is positive, increases in inc","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:15:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.16 Less common models for ordinal outcomes Stata can also fit several less commonly used models for ordinal outcomes. In concluding this chapter, we describe these models briefly and note their commands for estimation. Long (Forthcoming) provides further details. SPost commands do not work with all these models, but our m* commands do work with the estimation commands that support margins. ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:16:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.16.1 The stereotype logistic model The stereotype logistic model, also referred to as the stereotype ORM, was proposed by Anderson (1984) in response to the restrictive assumption of parallel regressions in the ORM. The stereotype logistic model is a compromise between allowing the coefficients for each independent variable to vary by outcome category (as is the case with the multinomial logit model, considered in the next chapter) and restricting the coefficients to be identical across all outcomes (as was the case with the ordered logit model). The stereotype logistic model can be fit in Stata by using the slogit command (see [R] slogit). The one-dimensional version of the model is defined as 这段话是在讨论一种名为\"stereotype logistic model\"（刻板逻辑模型）的统计模型。它是对传统的有序Logit模型的改进，旨在克服其对于所有结果都采用相同系数的限制，同时又不像多项式Logit模型那样允许每个自变量的系数随着结果类别的变化而变化。该模型可以在Stata中使用**slogit**命令来拟合。 $$\\ln\\frac{\\Pr\\left(y=q\\mid\\mathbf{x}\\right)}{\\Pr\\left(y=r\\mid\\mathbf{x}\\right)}=\\left(\\theta_{q}-\\theta_{r}\\right)-\\left(\\phi_{q}-\\phi_{r}\\right)\\left(\\mathbf{x}\\boldsymbol{\\beta}\\right)$$ where $\\beta$ is a vector of coefficients associated with the independent variables, the $\\alpha$’s are intercepts, and the $\\gamma$’s are scale factors that mediate the effects of the x’s. This one-dimensional model is ordinal as defined in section 7.15 and often produces very similar predictions to the ORM. When additional dimensions are added, it is no longer an ordinal model. Indeed, with enough dimensions, it is equivalent to the multinomial logit model. Accordingly, we postpone further discussion until chapter 8. ","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:16:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.16.2 The generalized ordered logit model The parallel regression assumption results from assuming the same coefficient vector ($ \\beta $) in the $ J - 1 $ logit equations $$\\ln\\Omega_{\\leq m|\u003em}\\left(\\mathbf{x}\\right)=\\tau_{m}-\\mathbf{x}\\boldsymbol{\\beta}$$ where $\\Omega_{\\leq m|\u003em}\\left(\\mathbf{x}\\right)=\\mathrm{Pr}\\left(y\\leq m\\mid\\mathbf{x}\\right)/\\mathrm{Pr}\\left(y\u003em\\mid\\mathbf{x}\\right)$ The generalized ordered logit model allows $ \\beta $ to differ for each of the $ J - 1 $ comparisons. That is $$\\ln\\Omega_{\\leq m|\u003em}\\left(\\mathrm{x}\\right)=\\tau_m-\\mathbf{x}\\beta_m\\quad\\mathrm{for~}j=1\\mathrm{~to~}J-1$$ where predicted probabilities are computed as $$\\Pr\\left(y=1\\mid\\mathbf{x}\\right)=\\frac{\\exp\\left(\\tau_1-\\mathbf{x\\beta}_1\\right)}{1+\\exp\\left(\\tau_1-\\mathbf{x\\beta}_1\\right)}$$ $$\\Pr\\left(y=j\\mid\\mathbf{x}\\right)=\\frac{\\exp\\left(\\tau_j-\\mathbf{x}\\beta_j\\right)}{1+\\exp\\left(\\tau_j-\\mathbf{x}\\boldsymbol{\\beta_j}\\right)}-\\frac{\\exp\\left(\\tau_{j-1}-\\mathbf{x}\\beta_{j-1}\\right)}{1+\\exp\\left(\\tau_{j-1}-\\mathbf{x}\\boldsymbol{\\beta}_{j-1}\\right)}\\quad\\mathrm{for~}j=2\\mathrm{~to~}J-1$$ $$\\Pr\\left(y=J\\mid\\mathbf{x}\\right)=1-\\frac{\\exp\\left(\\tau_{J-1}-\\mathbf{x\\beta_{J-1}}\\right)}{1+\\exp\\left(\\tau_{J-1}-\\mathbf{x\\beta}_{J-1}\\right)}$$ No formal constraint precludes negative predicted probabilities. Discussions of this model can be found in Clogg and Shihadeh (1994, 146-147), Fahrmeir and Tutz (1994, 91), and McCullagh and Nelder (1989, 155). A critical view of the model can be found in Greene and Hensher (2010, 189-192), who highlight that the model can predict negative “probabilities” and that it cannot be formulated in terms of a continuous latent dependent variable. Further, as noted by Long (Forthcoming), the generalized ordered logit model is not an ordinal regression model because, like the multinomial logit model, it does not necessarily make predictions that maintain the ordinality of the outcome. Parallel Regression Assumption: 这个假设源自于多项logit模型，其中假设不同类别之间的回归系数是相同的。具体地，在你提供的公式中，$\\boldsymbol{\\beta}$表示回归系数向量。这个假设认为在$J - 1$个logit方程中，这个系数向量是相同的。例如，如果我们用一个多项logit模型来预测学生通过不同课程的可能性，这个假设就是认为不同课程对学生通过考试的影响是相似的。 Generalized Ordered Logit Model: 广义有序logit模型允许不同类别之间的回归系数不同。在你提供的公式中，$\\beta_m$表示第$m$个比较的系数向量。这意味着在预测不同的类别之间，可以使用不同的回归系数。例如，假设我们要预测一家餐厅的顾客评价，有“不满意”、“一般”和“满意”三个等级，而这些等级之间的影响可能是不同的，比如食物质量对“满意”和“不满意”的影响可能是不同的。 Predicted Probabilities: 预测概率是在给定自变量的情况下，某个类别的发生概率。在你的公式中，$\\Pr\\left(y=j\\mid\\mathbf{x}\\right)$表示在给定自变量$\\mathbf{x}$的情况下，因变量$y$取值为$j$的概率。这个概率通过使用模型中的参数$\\tau_j$和系数向量$\\beta_j$进行计算。举个例子，假设我们要预测一名学生通过一门课程的可能性，$\\Pr\\left(y=1\\mid\\mathbf{x}\\right)$表示学生通过课程的概率，$\\Pr\\left(y=2\\mid\\mathbf{x}\\right)$表示学生可能通过但也可能不通过的概率，$\\Pr\\left(y=3\\mid\\mathbf{x}\\right)$表示学生不通过课程的概率。 Negative Predicted Probabilities: 负的预测概率是指在模型中计算出的某个类别的发生概率为负数。这在实际应用中是不合理的，因为概率应该在0到1之间。如果模型产生负的预测概率，可能表示模型存在问题，需要进行调整或者修正。例如，在学生考试成绩预测的例子中，负的预测概率可能表示模型对学生考试成绩的预测不准确，需要重新调整模型参数或者考虑其他因素。 首先，我们来看你给出的第一个公式： $$ \\ln\\Omega_{\\leq m|\u003em}\\left(\\mathbf{x}\\right)=\\tau_{m}-\\mathbf{x}\\boldsymbol{\\beta} $$ 这个公式是广义有序logit模型的一个基本方程。让我们来分解它： $\\ln\\Omega_{\\leq m|\u003em}\\left(\\mathbf{x}\\right)$表示的是两个概率的比值的自然对数。这个比值是$y\\leq m$的概率与$y\u003em$的概率之比，其中$y$是因变量，$m$是一个类别标记。 $\\tau_{m}$是一个阈值参数，用来划分不同的类别。对于不同的$m$，有不同的阈值。 $\\mathbf{x}$是自变量的向量。 $\\boldsymbol{\\beta}$是系数向量，表示自变量对因变量的影响。 这个方程的含义是：在给定自变量$\\mathbf{x}$的情况下，$y$属于不同类别的概率之比的对数，等于一个类别的阈值参数$\\tau_{m}$减去自变量$\\mathbf{x}$与系数向量$\\boldsymbol{\\beta}$的内积。 接下来，我们来看第二个公式： $$ \\Pr\\left(y=1\\mid\\mathbf{x}\\right)=\\frac{\\exp\\left(\\tau_1-\\mathbf{x\\beta}_1\\right)}{1+\\exp\\left(\\tau_1-\\mathbf{x\\beta}_1\\right)} $$ 这个公式是预测$y$等于第一个类别的概率的方程。让我们来解释它： $\\Pr\\left(y=1\\mid\\mathbf{x}\\right)$表示的是在给定自变量$\\mathbf{x}$的情况下，$y$等于第一个类别的概率。 $\\exp$是指数函数，用来计算参数$\\tau_1-\\mathbf{x\\beta}_1$的指数。 分母$1+\\exp\\left(\\tau_1-\\mathbf{x\\beta}_1\\right)$确保了概率的范围在0到1之间。 这个方程的含义是：在给定自变量$\\mathbf{x}$的情况下，$y$等于第一个类别的概率，是阈值参数$\\tau_1$和自变量$\\mathbf{x}$与系数向量$\\boldsymbol{\\beta}_1$的内积的指数函数，除以1加上这个指数函数。 第三个公式: $$ \\Pr\\left(y=j\\mid\\mathbf{x}\\right)=\\frac{\\exp\\left(\\tau_j-\\mathbf{x}\\beta_j\\right)}{1+\\exp\\left(\\tau_j-\\mathbf{x}\\boldsymbol{\\be","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:16:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"16.3 (Advanced) Predictions without using factor-variable notation Factor variables make it much simpler to make predictions when there are linked variables, such as age and age-squared. Because gologit2 does not support factor-variable notation, we use this model to illustrate how to make the correct predictions. The most important point for most readers is likely that you want to use factor variables whenever possible! If you use factor-variable notation in your models, you do not need to worry about the issues discussed in this section. However, you might still find the section useful to deepen your understanding of predictions in nonlinear models. We can compute predicted probabilities for given values of observations as we did with the ordered logit model and use the same approach to interpretation. For example, here are results using the same ideal types that we used in section 7.12. mtable, atright norownum width(7) /// at(female=0 white=0 year1996=0 year2012=1 educ_hs=0 educ_col=0 /// age=25 agesq=625 income=30) /// at(female=1 white=1 year1996=0 year2012=1 educ_hs=0 educ_col=1 /// age=60 agesq=3600 income=150) Expression: Pr(class), predict(outcome())\rlower working middle upper female white educ_col age agesq income\r-----------------------------------------------------------------------------------------\r0.155 0.701 0.136 0.008 0 0 0 25 625 30\r0.000 0.122 0.809 0.069 1 1 1 60 3600 150\rSpecified values of covariates\r| year1996 year2012 educ_hs\r----------+----------------------------\rCurrent | 0 1 0\rComparing the results from ologit that were computed earlier in the chapter, mtable, atright norownum width(7) /// at(female=0 white=0 year=3 ed=1 age=25 income=30) /// at(female=1 white=1 year=3 ed=3 age=60 income=150) Expression: Pr(class), predict(outcome())\rlower working middle upper female white educ age income\r-------------------------------------------------------------------------------\r0.230 0.634 0.133 0.003 0 0 1 25 30\r0.008 0.138 0.759 0.095 1 1 3 60 150\rSpecified values of covariates\r| year\r----------+--------\rCurrent | 3\rThe main difference between the generalized and the ordered logit models is that the predicted probabilities for the categories with the highest probabilities (working class for the first ideal type and middle class for the second) are about 0.06 higher in the generalized ordered logit model. We can also use mchange to obtain changes in the predicted probability for particular values of the independent variables, which provides an opportunity to illustrate how to deal with polynomial terms, such as age-squared, when you are not using factor-variable notation. Suppose that we want the discrete change for white, which is a binary variable. If factor-variable notation had been used, mchange would know that it is a binary variable. Because we are not using factor-variable notation, we must tell mchange to compute the change from 0 to 1 with the option amount (binary). It is tempting, but incorrect, to compute the change like this: mchange white, amount(binary) atmeans // incorrect method! gologit2: Changes in Pr(y) | Number of obs = 5620\rExpression: Pr(class), predict(outcome())\r| lower working middle upper -------------+-------------------------------------------\rwhite | 0 to 1 | -0.001 -0.066 0.072 -0.005 p-value | 0.403 0.001 0.000 0.336 Predictions at base value\r| lower working middle upper -------------+-------------------------------------------\rPr(y|base) | 0.011 0.503 0.466 0.020 Base values of regressors\r| female white year1996 year2012 educ_hs educ_col age agesq -------------+----------------------------------------------------------------------------------------\rat | .549 .814 .451 .31 .582 .241 45.2 2325 | income -------------+----------\rat | 68.1 Because the mean of age is 45.16, agesq should be held at $45.16 \\times 45.16 = 2039$, not 2325, which is the mean of agesq. The correct way to compute marginal effects is to specify the value of agesq in at(): mchange white, amount(binary) at","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:16:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.16.4 The sequential logit model Some ordinal outcomes represent the progress of events or stages in some process through which an individual can advance. For example, the outcome could be faculty rank, where the stages are assistant professor, associate professor, and full professor. The key characteristic of the process is th at an individual must pass through each stage. The outcome is thus the result of a sequence of potential transitions: an assistant professor may or may not make the transition to associate professor, and an associate professor may or may not make the transition to full professor The most straightforward way to model an outcome like this is as a series of BRMs. Consider the binary logit model from chapter 5: $$\\ln\\frac{\\Pr\\left(y=1\\mid\\mathbf{x}\\right)}{\\Pr\\left(y=0\\mid\\mathbf{x}\\right)}=\\alpha+\\mathbf{x}\\boldsymbol{\\beta}$$ where we have made the intercept explicit rather than including it in $ \\beta $. To extend this to multiple transitions, we estimate for each transition the log odds of having made the transition (y \u003e m) versus not having made the transition (y = m). For example, we estimate the log odds of being an associate or a full professor (y \u003e 1) versus being an assistant professor (y = 1). We allow separate coefficients ($ \\beta_m $) for each transition from $ y = m $: $$\\ln\\frac{\\Pr(y\u003em\\mid\\mathbf{x})}{\\Pr(y=m\\mid\\mathbf{x})}=\\alpha_m+\\mathbf{x}\\beta_m\\quad\\mathrm{for~}m=1\\mathrm{~to~}J-1$$ where J is the number of stages. This is an example of a broader group of models called sequential logit models (for example, Liao [1994, 26-28]). This model differs importantly from the generalized ordered logit model in that observations in which $ y \u003c m $ are not used in the estimation of $ \\beta_m $. For example, assistant professors are not used when modeling the transition from associate professor to full professor. To demonstrate how to fit this model, we use the variable educ in the gssclass4 dataset as our outcome. The three values of educ represent two transitions: students may or may not graduate from high school, and high school graduates may or may not graduate from college. To fit the model, we first use recode to create dummy variables representing whether or not respondents at each stage made the transition to the next. The variable educ has the distribution use gssclass4, clear tab educ, miss educational |\rattainment | Freq. Percent Cum.\r------------+-----------------------------------\rnot hs grad | 993 17.67 17.67\rhs only | 3,270 58.19 75.85\rcollege | 1,357 24.15 100.00\r------------+-----------------------------------\rTotal | 5,620 100.00\rWe create the variable gradcollege to indicate if someone with a high school diploma graduated from college, where those who did not graduate from high school (educ=1) are recoded as missing, not as 0. Those who did not graduate from high school are not included in the analysis of the transition to college graduation. recode educ (1=0) (2 3=1), gen(gradhs) label var gradhs \"Graduate high school?\" recode educ (1=.) (2=0) (3=1), gen(gradcollege) label var gradcollege \"Graduate college?\" Next, we use logit to fit separate models for each transition, using race and sex as independent variables. * HS degree vs not logit gradhs i.white i.female, or nolog Logistic regression Number of obs = 5,620\rLR chi2(2) = 25.40\rProb \u003e chi2 = 0.0000\rLog likelihood = -2608.1189 Pseudo R2 = 0.0048\r------------------------------------------------------------------------------\rgradhs | Odds ratio Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rwhite |\rwhite | 1.53 0.13 5.09 0.000 1.30 1.80\r|\rfemale |\rfemale | 0.97 0.07 -0.48 0.631 0.84 1.11\r_cons | 3.39 0.29 14.35 0.000 2.87 4.00\r------------------------------------------------------------------------------\rNote: _cons estimates baseline odds.\r* College degree vs HS degree logit gradcollege i.white i.female, or nolog Logistic regression Number of obs = 4,627","date":"2024-02-25","objectID":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/:16:4","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_7 ：Models for ordinal outcomes","uri":"/chapter_7-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"In th is chapter, we discuss methods for interpreting results from models for binary outcom es.","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"The challenge of interpreting results, then, is to find a summary of how changes in the independent variables are associated with changes in the outcome that best reflects critical substantive processes without overwhelming yourself or your readers with distracting detail. **Using odds ratios to interpret the logit model is very common but rarely is it sufficient for understanding the results of the model. Nonetheless, it is important to understand what odds ratios mean for several reasons.**For one, odds ratios are used a lot, and you need to understand what they can and cannot tell you. Also, odds ratios are useful for understanding the structure of the ordinal regression model in chapter 7 and the multinomial logit model in chapter 8. Interpretation based only on (y^*) parallels interpretation in the linear regression model, but it is not often used for binary outcomes. It is, however, sometimes useful for models for ordinal outcomes, considered in chapter 7. We begin in section 6.2 with marginal effects, which we find more informative than the more commonly used odds ratios as scalar measures to assess the magnitude of a variable’s effect. In section 6.3, we consider computing predictions based on substantively motivated profiles of values for the independent variables, also referred to as ideal types. Thinking about the types of individuals represented in your sample is a valuable way to gain an intuitive sense of which configurations of variables are substantively important. Tables of predictions, which are discussed in section 6.4, can effectively highlight the impact of categorical independent variables. We end our discussion of interpretation in section 6.6 by considering graphical methods to show how probabilities change as a continuous independent variable changes. When using logit or probit, or any nonlinear model, we suggest that you try a variety of methods of interpretation with the goal of finding an elegant way to present the results that does justice to the complexities of the nonlinear model and the substantive application. No one method works in all situations, and often the only way to determine which method is most effective is to try them all. Fortunately, the methods we consider in this chapter can be readily extended to models for ordinal, nominal, and count outcomes, which are considered in chapters 7-9. ","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:0:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1 Interpretation using regression coefficients Interpretation of regression models involves examining how a change in an independent variable is associated with a change in the outcome. In the nonlinear binary regression model (BRM), a regression coefficient indicates the direction of a variable’s effect. In our model of labor force participation, the coefficients for (k5) and (k618) are both negative, which implies that higher numbers of children are associated with a lower probability of being in the labor force. What is harder to interpret from the coefficient is the magnitude of the effect. The logit model, for example, can be written as $$\\ln\\Omega\\left(\\mathbf{x}\\right)=\\mathbf{x}\\beta $$ The ($\\beta$) coefficients indicate the effect of the independent variable on the log odds of the outcome, where the log odds is also known as the logit. We can interpret the ($\\beta$)’s as follows: For a unit change in ($x_k$), we expect the log of the odds of the outcome to change by ($\\beta_k$) units, holding all other variables constant. This interpretation does not depend on the level of ($X_k$) or the levels of the other variables in the model. In this regard, it is just like the linear regression model. The problem is that a change of ($\\beta_k$) in the log odds has little substantive meaning for most people. Consequently, tables of logit coefficients typically have little value for conveying the magnitude of effects. As an alternative, odds ratios can be used to explain the effects of independent variables on the odds, which we consider in the next section. ","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:1:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.1 Interpretation using odds ratios Effects for the logit model (but not the probit model) can be interpreted in terms of changes in the odds. For binary outcomes, we typically consider the odds of observing a positive outcome, coded 1, versus a negative outcome, coded 0: $$\\Omega\\left(\\mathbf{x}\\right)=\\frac{\\Pr\\left(y=1\\mid\\mathbf{x}\\right)}{\\Pr\\left(y=0\\mid\\mathbf{x}\\right)}=\\frac{\\Pr\\left(y=1\\mid\\mathbf{x}\\right)}{1-\\Pr\\left(y=1\\mid\\mathbf{x}\\right)}$$ In the logit model, the log odds are a linear combination of the (x)’s and $beta’s$. For example, consider a model with three independent variables: The problem with interpreting these ( \\beta )’s directly is that changes in log odds are not substantively meaningful to most audiences. To make the interpretation more meaningful, we can transform the log odds to the odds by taking the exponential of both sides of the equation. This leads to a model that is multiplicative instead of linear but in which the outcome is the odds: $$\\Omega\\left(\\mathbf{x},x_{3}\\right)=e^{\\beta_{0}}e^{\\beta_{1}x_{1}}e^{\\beta_{2}x_{2}}e^{\\beta_{3}x_{3}}$$ Our notation emphasizes the value of (X_3), which we want to increase by 1: \\begin{align*} \\Omega\\left(\\mathbf{x},x_3+1\\right) \u0026= e^{\\beta_0}e^{\\beta_1x_1}e^{\\beta_2x_2}e^{\\beta_3\\left(x_3+1\\right)} \\ \\end{align*} \\begin{align*} \\qquad\\qquad\\qquad= e^{\\beta_0}e^{\\beta_0}e^{\\beta_1x_1}e^{\\beta_2x_2}e^{\\beta_3x_3}e^{\\beta_3} \\end{align*} This leads to the odds ratio $$\\frac{\\Omega\\left(\\mathbf{x},x_{3}+1\\right)}{\\Omega\\left(\\mathbf{x},x_{3}\\right)}=\\frac{e^{\\beta_{0}}e^{\\beta_{1}x_{1}}e^{\\beta_{2}x_{2}}e^{\\beta_{3}x_{3}}e^{\\beta_{3}}}{e^{\\beta_{0}}e^{\\beta_{1}x_{1}}e^{\\beta_{2}x_{2}}e^{\\beta_{3}x_{3}}}=e^{\\beta_{3}}$$ Accordingly, we can interpret the exponential of the logit coefficient as follows: For a unit change in $x_k$, the odds are expected to change by a factor of $\\exp(\\beta_k)$, holding other variables constant. For $\\exp(\\beta_k) \u003e 1$, you could say that the odds are “exp($\\beta_k$) times larger,” and for $\\exp(\\beta_k) \u003c 1$, you could say that the odds are $\\exp(\\beta_k)$ times smaller. If $\\exp(\\beta_k) = 1$, then $X_k$ does not affect the odds. We can evaluate the effect of a standard deviation change in $x_k$ instead of a unit change: **The odds ratio is computed by changing one variable, while holding all other variables constant. This means that the formula in (6.1) cannot be used when the variable being changed is mathematically linked to another variable. For example, if $x_1$ is age and $x_2$ is age-squared, you cannot increase $x_1$ by 1 while holding $x_2$ constant.**In such cases, the odds ratio computed as $\\exp(\\beta_k)$ should not be interpreted. Although odds ratios are a common method of interpretation for logit models, it is essential to understand their limitations. Most importantly, they do not indicate the magnitude of the change in the probability of the outcome. We begin with an example from our model of labor force participation, followed by a few words of caution. The output from logit with the or option shows the odds ratios instead of the estimated $\\beta$’s: logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog Logistic regression Number of obs = 753\rLR chi2(8) = 124.30\rProb \u003e chi2 = 0.0000\rLog likelihood = -452.72367 Pseudo R2 = 0.1207\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5 | -1.39 0.19 -7.25 0.000 -1.77 -1.02\rk618 | -0.07 0.07 -0.96 0.336 -0.20 0.07\r|\ragecat |\r40-49 | -0.63 0.21 -3.00 0.003 -1.04 -0.22\r50+ | -1.28 0.26 -4.92 0.000 -1.79 -0.77\r|\rwc |\rcollege | 0.80 0.23 3.48 0.001 0.35 1.25\r|\rhc |\rcollege | 0.14 0.21 0.66 0.508 -0.27 0.54\rlwg | 0.61 0.15 4.04 0.000 0.31 0.91\rinc | -0.04 0.01 -4.24 0.000 -0.05 -0.02\r_cons | 1.01 0.29 3.54 0.000 0.45 1.57\r------------------------------------------------------------------------------\rHere ","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:1:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.2 (Advanced) Interpretation using y* Binary logit and probit models are rarely interpreted in terms of the latent variable $y^*$. Accordingly, this section is primarily useful to provide a deeper understanding of identification and why logit coefficients are generally larger than probit coefficients. As discussed in section 5.1.1, the logit and probit models can be derived from regression of a latent variable $y^*$. $$y^{*}=\\mathbf{x}\\beta+\\varepsilon $$ where $\\varepsilon$ is a random error. For the probit model, we assume $\\varepsilon$ is normal with $\\operatorname{Var}(\\varepsilon)=1.$ For logit, we assume $\\varepsilon$ is distributed logistically with $\\mathrm{Var}(\\varepsilon)=\\pi^2/3$ As with the linear regression model, the marginal change in $y^*$ with respect to $xk$ is $$\\frac{\\partial y^*}{\\partial x_k}=\\beta_k$$ However, because $y^*$ is latent, its true metric is unknown and depends on the identification assumption we make about the variance of the errors As we saw in section 5.2.2, the coefficients produced by logit and probit cannot be directly compared with one another. The logit coefficients will typically be about 1.7 times larger than the probit coefficients, simply as a result of the arbitrary assumption about the variance of the error. Consequently, the marginal change in y* cannot be interpreted without standardizing by the estimated standard deviation of y*, which is computed as $$\\widehat{\\sigma}_{y^{*}}^{2}=\\widehat{\\boldsymbol{\\beta}}^{\\prime}\\widehat{\\mathrm{Var}}\\left(\\mathbf{x}\\right)\\widehat{\\boldsymbol{\\beta}}+\\mathrm{Var}\\left(\\varepsilon\\right)$$ where $\\operatorname{Var}\\left(\\mathbf{x}\\right)$ is the covariance matrix for the observed $x’s$,$\\widehat\\beta$ contains maximum likelihood estimates, and $\\operatorname{Var}(\\varepsilon)=1$ for probit and $\\mathrm{Var}(\\varepsilon)=\\pi^2/3$ for logit. Then the $y^*$-standardized coefficient for $xk$ is which can be interpreted as follows: For a unit increase in $x_k$, $y*$ is expected to increase by $\\beta_{k}^{S}y^{*}$ standard deviations,holding all other variables constant. The fully standardized coefficient is $$\\beta_{k}^{S}=\\frac{\\sigma_{k}\\beta_{k}}{\\sigma_{y^{*}}}$$ which can be interpreted as follows: For each standard deviation increase in $x_k$ , $y*$ is expected to increase by $\\beta_{k}^{S}$ standard deviations, holding all other variables constant. These coefficients are computed by listcoef with the std option: logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog listcoef, std help logit (N=753): Unstandardized and standardized estimates Observed SD: 0.4956\rLatent SD: 2.0474\r-------------------------------------------------------------------------------\r| b z P\u003e|z| bStdX bStdY bStdXY SDofX\r-------------+-----------------------------------------------------------------\rk5 | -1.3916 -7.250 0.000 -0.729 -0.680 -0.356 0.524\rk618 | -0.0657 -0.961 0.336 -0.087 -0.032 -0.042 1.320\r|\ragecat |\r40-49 | -0.6268 -3.003 0.003 -0.305 -0.306 -0.149 0.487\r50+ | -1.2791 -4.924 0.000 -0.529 -0.625 -0.259 0.414\r|\rwc |\rcollege | 0.7977 3.481 0.001 0.359 0.390 0.175 0.450\r|\rhc |\rcollege | 0.1359 0.661 0.508 0.066 0.066 0.032 0.488\rlwg | 0.6099 4.045 0.000 0.358 0.298 0.175 0.588\rinc | -0.0351 -4.238 0.000 -0.408 -0.017 -0.199 11.635\rconstant | 1.0140 3.545 0.000 . . . .\r-------------------------------------------------------------------------------\rb = raw coefficient\rz = z-score for test of b=0\rP\u003e|z| = p-value for z-test\rbStdX = x-standardized coefficient\rbStdY = y-standardized coefficient\rbStdXY = fully standardized coefficient\rSDofX = standard deviation of X\rThe y*-standardized* coefficients are in the column labeled bStdY*, and the fully standardized coefficients are in the column bStdXY. We could interpret these coefficients as follows: For each additional young child, the propensity of a woman to join the labor force decreases by 0.68 standard deviations, holding all other variables constant. For every standard deviation increase in family income, ","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:1:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2 Marginal effects: Changes in probabilities A marginal effect measures the change in the probability of an outcome for a change in Xk, holding all other independent variables constant at specific values. The critical idea is that one variable is changing while the other variables are not. There are two varieties of marginal effects. A marginal change computes the effect of an instantaneous or infinitely small change in Xk. A discrete change computes the effect of a discrete or finite change in Xk. (See section 4.5 for an introductory discussion of marginal effects.) Marginal change and discrete change in the BRM\rA marginal change, shown by the tangent to the probability curve at x = 1 in figure 6.1, is the rate of change in the probability for an infinitely small change in $x_k$ holding other variables at specific values: $$\\frac{\\partial\\Pr(y=1\\mid\\mathbf{x}=\\mathbf{x}^*)}{\\partial x_k}$$ Because the effect is computed with a partial derivative, some authors refer to this as the partial change or partial effect. In this formula, x* contains specific values of the independent variables. For example, x* could equal x7 with the observed values for the ith observation, it could equal the means x of all variables, or it could equal any other values. When the meaning is clear, we will refer to x without specifying x*. The important thing is that the value of the marginal effect depends on the specific values of the xk s where the change is computed. In the BRM , the marginal change has the simple formula $$\\frac{\\partial\\Pr\\left(y_i=1\\mid\\mathbf{x}\\right)}{\\partial x_k}=f\\left(\\mathbf{x}\\beta\\right)\\beta_k$$ where $f$ is the normal probability distribution function (PDF) for probit and the logistic PDF for logit. In logit models, the marginal change has a particularly convenient form: $$\\frac{\\partial\\Pr\\left(y_i=1\\mid\\mathbf{x}\\right)}{\\partial x_k}=\\Pr\\left(y_i=1\\mid\\mathbf{x}\\right)\\left[1-\\Pr\\left(y_i=1\\mid\\mathbf{x}\\right)\\right]\\beta_k$$ From this formula, we see that the change must be greatest when $\\Pr\\left(y=1\\mid\\mathbf{x}\\right)=0.5$,where the marginal change is $(0.5)(0.5)\\beta_{k}=\\beta_{k}/4$.Accordingly, dividing a binary logit coefficient by 4 indicates the maximum marginal change in the probability (Cramer 1991, 8). As long as the model does not include power or interaction terms, the marginal change for xk has the same sign as $\\beita_k$ for all values of x because the PDF is always positive. (Computing marginal effects when powers and interactions are in the model is discussed in section 6.2.1.) The formula also shows that marginal changes for different independent variables differ by a scale factor. For example, the ratio of the marginal effect of $x_j$ to the effect of $x_k$ is $$\\frac{\\partial\\Pr\\left(y_i=1\\mid\\mathbf{x}\\right)/\\partial x_j}{\\partial\\Pr\\left(y_i=1\\mid\\mathbf{x}\\right)/\\partial x_k}=\\frac{f\\left(\\mathbf{x}\\boldsymbol{\\beta}\\right)\\beta_j}{f\\left(\\mathbf{x}\\boldsymbol{\\beta}\\right)\\beta_k}=\\frac{\\beta_j}{\\beta_k}$$ for all values of x. Consequently, while does not tell you the magnitude of $x_k’s$ effect, it can tell you how much larger or smaller it is than the effects of other variables. A discrete change, sometimes called a first difference, is the actual change in the predicted probability for a given change in $x_k$, holding other variables at specific values. For example, the discrete change for an increase in age from 30 to 40 is the change in the probability of being in the labor force as age increases from 30 to 40, holding other variables at specified values. Defining $x_k^{\\mathrm{start}}$ as the starting value of $x_k$ and $x_k^{\\mathrm{end}}$ as the ending value,the discrete change equals $$\\frac{\\Delta\\Pr\\left(y=1\\mid\\mathbf{x}\\right)}{\\Delta x_k\\left(x_k^{\\mathrm{start}}\\to x_k^{\\mathrm{ond}}\\right)}=\\Pr\\left(y=1\\mid\\mathbf{x},x_k=x_k^{\\mathrm{end}}\\right)-\\Pr\\left(y=1\\mid\\mathbf{x},x_k=x_k^{\\mathrm{start}}\\right)$$ For binary variables, such as having attended college, the obvi","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:2:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.1 Linked variables Fundamental to the concept of a marginal effect is the idea that only one variable changes while holding all other variables at specified values. An exception must be made for variables that are linked mathematically. For example, if $x_{\\text{age}}$ is age and $X_{\\text{agesq}} = x_{\\text{age}} \\times x_{\\text{age}}$, you cannot change $x_{\\text{age}}$ while holding $x_{\\text{agesq}}$ constant. The change in $x_{\\text{age}}$ must be matched by a corresponding change in $x_{\\text{agesq}}$. This is easy to illustrate with a discrete change in age from 20 to 30: $$\\begin{aligned}\\frac{\\Delta\\operatorname{Pr}\\left(y=1\\mid\\mathbf{x}\\right)}{\\Delta\\operatorname{gre}\\left(20\\to30\\right)}\u0026=\u0026\\operatorname{Pr}\\left(y=1\\mid\\mathbf{x},x_{\\mathrm{age}}=30,x_{\\mathrm{agesq}}=30^2\\right)\\end{aligned}$$ $$\\begin{aligned}\\quad\\quad\\qquad\\qquad\\qquad\u0026-\\operatorname{Pr}\\left(y=1\\mid\\mathbf{x},x_{\\mathrm{age}}=20,x_{\\mathrm{agesq}}=20^2\\right)\\end{aligned}$$ Linked variables must also be considered for the variables being held constant. For example, if we are computing the marginal effect of $x_k$ while holding age at its mean, we need to hold $x_{\\text{age}}$ at $\\text{mean}(x_{\\text{age}})$ and $x_{\\text{agesq}}$ at $[\\text{mean}(x_{\\text{age}}) × \\text{mean}(x_{\\text{age}})]$ not at $\\text{mean}(x_{\\text{agesq}})$. Similarly, if your model includes $X_{\\text{female}}$, $x_{\\text{age}}$, and the interaction $$x_{\\mathrm{female}\\times\\mathrm{age}}=x_{\\mathrm{female}}\\times x_{\\mathrm{age}},$, you cannot change $x_{\\mathrm{age}}$ while holding $x_{\\mathrm{female}\\times\\mathrm{age}}$ constant. Categorical regressors that enter a model as a set of indicators are also linked. Suppose that education has three categories: no high school degree, high school diploma as the highest degree, and college diploma as the highest degree. Let $x_{\\text{hs}} = 1$ if high school is the highest degree and equal 0 otherwise; and let $x_{\\text{College}} = 1$ if college is the highest degree and equal 0 otherwise. If $x_{\\text{hs}} = 1$, then $x_{\\text{College}} = 0$. You cannot increase $x_{\\text{College}}$ from 0 to 1 while holding $x_{\\text{hs}}$ at 1. Computing the effect of having college as the highest degree ($x_{\\text{hs}} = 0$, $x_{\\text{College}} = 1$) compared with high school as the highest degree ($x_{\\text{hs}} = 1$, $x_{\\text{College}} = 0$) involves changing two variables: When discussing marginal effects with linked variables, we will say “holding other variables constant” with the implicit understanding that appropriate adjustments for linked variables are being made. A major benefit of using factor-variable notation when specifying a regression model is that margins, mchange, mtable, and mgen keep track of which variables are linked, and compute predictions and marginal effects correctly. 边际效应概念： 边际效应是指当我们微小地改变一个变量时，观察这个变化对概率的影响有多大。这里强调了在计算边际效应时，其他变量都被保持在特定的值上，保证我们只观察一个变量的变化对概率的影响。 相关变量的例外情况： 文中提到，如果有数学上相关的变量，需要特别注意。比如，如果 $x_{\\text{age}}$ 表示年龄，而 $X_{\\text{agesq}} = x_{\\text{age}} \\times x_{\\text{age}}$ 表示年龄的平方，那么在改变 $x_{\\text{age}}$ 的同时，需要相应地调整 $x_{\\text{agesq}}$。这是因为它们是数学上相关的。 具体例子： 通过公式 $\\frac{\\Delta\\operatorname{Pr}\\left(y=1\\mid\\mathbf{x}\\right)}{\\Delta\\operatorname{gre}\\left(20\\to30\\right)}$ 的计算，我们可以看到对于年龄从20到30的变化，事件 $y=1$ 的概率变化是通过两个部分的减法来计算的。第一部分是在年龄为30岁和 $x_{\\text{agesq}}$ 为 $30^2$ 的情况下，事件发生的概率；第二部分是在年龄为20岁和 $x_{\\text{agesq}}$ 为 $20^2$ 的情况下，事件发生的概率。两者之差即为由于年龄变化引起的边际效应。 变量的数学关系和离散变化： 文中还提到，对于有数学关系的变量，需要适当调整。比如，如果模型中包含了交互项 $x_{\\text{female}\\times\\text{age}}$，在改变 $x_{\\text{age}}$ 时需要注意。类别型变量也需要特殊处理，确保在改变某一个变量时，与之相关的其他变量也得到相应调整。 ","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:2:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.2 Summary measures of change The marginal effect of a variable depends on the specific values of all independent variables. Because the effect of $x_k$ differs for each observation (unless, of course, multiple observations have identical values), there is a distribution of marginal effects in the sample. For interpretation, we seek a simple, informative summary of this distribution of effects. There are three basic approaches: Marginal effect at the mean (MEM). Compute the marginal effect of $x_k$ with all variables held at their means. Marginal effect at representative values (MER). Compute the marginal effect of $X_k$ with variables held at specific values that are selected for being especially instructive for the substantive questions being considered. The MEM is a special case of the MER. Average marginal effect (AME). Compute the marginal effect of $X_k$ for each observation at its observed values $x_i$, and then compute the average of these effects. We consider each measure before discussing how to decide which measure is appropriate for your application. MEMs and MERs The MEM is computed with all variables held at their means. For a marginal change, this is $$\\frac{\\partial\\Pr\\left(y=1\\mid\\overline{\\mathbf{x}},x_k=\\overline{x}_k\\right)}{\\partial x_k}$$ which can be interpreted as follows: For someone who is average on all characteristics, the marginal change of $x_k$ is The discrete change equals For someone who is average on all characteristics, increasing $x_k$ by 8 changes the probability by … The MER would replace “who is average” with a description of the values of the covariates. AMEs The AME is the mean of the marginal effect computed at the observed values for all observations in the estimation sample. For a marginal change, this is which can be interpreted as follows: The average marginal effect of $x_k$ is … The average discrete change equals which is interpreted as follows: On average, increasing $x_k$ by $\\delta $ increases the probability by … For factor variables or changes from one fixed value to another (for example, from the maximum to the maximum), we say On average, increasing $X_k$ from start-value to end-value increases the probability by… Standard errors of marginal effects For each of these measures of change, standard errors can be computed using the delta method (Agresti 2013, 72-75; Wooldridge 2010, 576-577; Xu and Long 2005; [r] margins). The standard errors allow you to test whether a marginal effect is 0, to add a confidence interval to the estimated effect, and to test such things as whether marginal effects are equal at different values of the independent variables. ","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:2:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.3 Should you use the AME, the MEM , or the MER? The popularity of the MEM is probably because of the ease of computation. Computing an AME, in principle, involves N times more computation than the corresponding MEM. With the rapid growth in computing power, this is a trivial issue compared with having readily available software that easily computes the AME. For example, with our prchange command in SPost9, computing MEMs was trivially easy. Although you could compute the AME with prchange, you needed to write your own program to collect and summarize the computations for each observation. Few people, ourselves included, bothered to do that. With Stata’s margins and our mchange, it is as easy to compute AMEs as MEMs. These computational advances do not, however, imply that the AME is always the best way to assess the effect of a variable. There are several issues to consider when deciding which measure of change to use. Does the marginal effect computed at the mean of all variables provide useful information about the overall effect of that variable? This is relevant not only in deciding what to do in current analyses but when evaluating past research that used the MEM. A common criticism of the MEM is that typically there is no actual case in the dataset for which all variables equal the mean. Most obviously, with binary independent variables, the mean does not correspond to a possible value of an observation. For example, a variable like “pregnant” is measured as 0 and 1 without it being possible to observe someone with a value equal to a sample mean of intermediate value. This issue alone leads some to disfavor the MEM (Hanmer and Kalkan 2013). We are not ourselves as concerned about this point because holding a binary variable at its mean is, roughly speaking, taking a weighted mean of effects for each group. If the groups are a focus of the analysis, you can compute MERs for each group by using group-specific means. Alternatively, effects can be computed at the modal values of the binary variables, but this ignores everyone who is in a less well-represented group. Sometimes, it is argued that the MEM is a reasonable approximation to the AME. Although Greene and Hensher (2010, 36) correctly observed that the AME and MEM are often similar, they incorrectly suggest that this is especially true in large samples. Although the two measures will often be similar, they can differ in substantively meaningful ways, and whether this is the case has little to do with whether a sample is bigger or smaller. Bartus (2005) and Verlinda (2006) explain more precisely when $MEM$ and $AME$ differ and which is larger. For the binary logit and probit models, the difference between the $AME$ and $MEM$ for depends on three things: the probability that $y = 1$ when all $\\beta$’s are held to their means, the variance of $x_k$, and the size of $\\beta_k$ (Bartus 2005; Hanmer and Kalkan 2013, SI). The sign of the difference between the $AME$ and $MEM$ depends on $Pr(y = 1 | x)$, with the $AME$ being larger at lower and higher probabilities. In the middle, the $MEM$ is larger, with the largest difference occurring when $Pr(y = 1 | x) = 0.5$. The $AME$ and $MEM$ will be equal when the probability is about 0.21 and 0.79 for the binary logit model, and about 0.15 and 0.85 for the binary probit model. The $AME, MEM,$ and $MER$ are each summary measures, and no single summary of effects is ideal for all situations. Broadly speaking, we believe that the $AME$ is the best summary of the effect of a variable. Because it averages the effects across all cases in the sample, it can be interpreted as the average size of the effect in the sample. The $MEM$ is computed at values of the independent variables that might not be representative of anyone in the sample. However, both $AME$ and $MEM$ are limited because they are based on averages. If the average value of each regressor is a substantively interesting location in the data, the $MEM$ is useful because it te","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:2:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.4 Examples of marginal effects In this section, we use our model of labor force participation to illustrate the computation and interpretation of marginal effects with mchange. The mchange command makes it simple to compute marginal effects for different amounts of changes, either averaging effects over the sample or computing them at fixed values. mchange uses margins to compute the effects, which are then collected into a compact table. For example, running mchange after fitting our baseline model creates a 30-line table that summarizes 500 lines of output from a dozen margins commands. If you want to learn more about margins, you can add the option details to mchange to see how to use margins output. Information on using margins to compute marginal effects is given in section 6.2.6. We begin by fitting our model and storing the estimates so that they can be restored later: use binlfp4, clear logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog Logistic regression Number of obs = 753\rLR chi2(8) = 124.30\rProb \u003e chi2 = 0.0000\rLog likelihood = -452.72367 Pseudo R2 = 0.1207\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5 | -1.39 0.19 -7.25 0.000 -1.77 -1.02\rk618 | -0.07 0.07 -0.96 0.336 -0.20 0.07\r|\ragecat |\r40-49 | -0.63 0.21 -3.00 0.003 -1.04 -0.22\r50+ | -1.28 0.26 -4.92 0.000 -1.79 -0.77\r|\rwc |\rcollege | 0.80 0.23 3.48 0.001 0.35 1.25\r|\rhc |\rcollege | 0.14 0.21 0.66 0.508 -0.27 0.54\rlwg | 0.61 0.15 4.04 0.000 0.31 0.91\rinc | -0.04 0.01 -4.24 0.000 -0.05 -0.02\r_cons | 1.01 0.29 3.54 0.000 0.45 1.57\r------------------------------------------------------------------------------\restimates store base estat summarize, labels Estimation sample logit Number of obs = 753\r---------------------------------------------------------------------------------------\rVariable | Mean Std. dev. Min Max Label\r-------------+-------------------------------------------------------------------------\rlfp | .5683931 .4956295 0 1 In paid labor force?\rk5 | .2377158 .523959 0 3 # kids \u003c 6\rk618 | 1.353254 1.319874 0 8 # kids 6-18\ragecat | Wife's age group\r40-49 | .3851262 .4869486 0 1\r50+ | .2191235 .4139274 0 1\rwc | Wife attended college?\rcollege | .2815405 .4500494 0 1\rhc | Husband attended college?\rcollege | .3917663 .4884694 0 1\rlwg | 1.097115 .5875564 -2.05412 3.21888 Log of wife's estimated wages\rinc | 20.12897 11.6348 -.029 96 Family income excluding wife's\r---------------------------------------------------------------------------------------\rWe will next show how to compute and interpret AMEs for continuous and factor variables, before examining the corresponding MEMs. Marginal effects in models with powers and interactions are then considered. Finally, we show how to compute the distribution of effects for observations in the estimation sample. 2.4.1 AMEs for continuous variables For continuous independent variables, mchange computes the average marginal change and average discrete change of 1 and a standard deviation. To assess the effects of income and wages, type: mchange inc lwg logit: Changes in Pr(y) | Number of obs = 753\rExpression: Pr(lfp), predict(pr)\r| Change p-value -------------+---------------------\rinc | +1 | -0.007 0.000 +SD | -0.086 0.000 Marginal | -0.007 0.000 lwg | +1 | 0.120 0.000 +SD | 0.072 0.000 Marginal | 0.127 0.000 Average predictions\r| not in LF in LF -------------+---------------------\rPr(y|base) | 0.432 0.568 The average predictions, listed below the table of changes, show that in the sample the average predicted probability of being in the labor force is 0.432. This is the same value you would obtain by first running predict and then computing the mean of the predictions. In later examples, we often suppress this result by adding the brief option. Summarizing the AMEs for a standard deviation change, we can say Holding other variables at their observ","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:2:4","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.5 The distribution of marginal effects The value of a marginal effect depends on the level of all variables in the model. Because each observation can have different values of the independent variables, there is a distribution of marginal effects within the sample where the AME is the mean of this distribution. Although the mean tells you where the center of the distribution is, it does not reflect variation within the distribution. Just as the means of the independent variables used to compute the MEM might not correspond even approximately to anyone in the sample, the AME might not correspond to the magnitude of the marginal effect for anyone in the sample. For this reason, we believe that examining the distribution of marginal effects provides valuable substantive insights. We consider two approaches for learning about the distribution of marginal effects. First, we compute effects for each observation and create a histogram of the effects. Although there is no Stata command for this, we provide simple programs that you can adapt to your needs. Second, we compute marginal effects at strategic locations in the data space by using MERs. This approach is presented in section 6.3. A third approach that we do not consider here estimates the quantiles of the effects in the population; see Firpo (2007) and Cattaneo (2010) for seminal papers, and see Cattaneo, Drukker, and Holland (2013) and Drukker (2014) for intuition, Stata commands, and extensions to survival data. For example, a training program that boosts the income of low-income participants and has no effect on higher-income participants could have the 0.25 quantile effect be significant and the 0.75 quantile effect be insignificant. These quantiles of effects provide the researcher with a more nuanced picture of the effect of a treatment than the one provided by the mean effect. The marginal change for the BRM, assuming no interactions or power terms, equals: $$\\frac{\\partial\\Pr\\left(y_i=1\\mid\\mathbf{x}_i\\right)}{\\partial x_k}=f\\left(\\mathbf{x}_i\\boldsymbol{\\beta}\\right)\\beta_k$$ The shape of the distribution of marginal changes for each observation is determined by $ f(x_i\\beta) $, where $ \\beta_k $ simply rescales $ f(x_i\\beta) $ to create the distribution of effects for $ x_k $. The distribution is more spread out if $ \\beta_k $ is larger in absolute value and is more condensed if $ \\beta_k $ is smaller. Although the shape of the distribution of discrete changes will be similar for different variables, they are not a simple rescaling of each other. 当我们谈论计算每个观察值的边际变化时，我们想知道当某个自变量微小变化时，因变量的变化量是多少。在 logistic 回归模型中，我们可以使用一个简单的公式来计算这个边际变化。 首先，我们有一个 logistic 回归模型，其预测某个事件发生的概率。假设我们有一个自变量 $ x_i $，我们想要知道当 $ x_i $ 发生微小变化时，事件发生的概率会如何变化。 公式中的 $ f(x_i\\boldsymbol{\\beta}) $ 是 logistic 函数，它描述了自变量 $ x_i $ 和事件发生的概率之间的关系。这个函数会给出一个在 0 到 1 之间的值，表示事件发生的概率。 $ (1 - f(x_i\\boldsymbol{\\beta})) $ 则表示事件不发生的概率，因为 logistic 函数的性质保证了事件发生和不发生的概率之和为 1。 因此，$ f(x_i\\boldsymbol{\\beta}) \\cdot (1 - f(x_i\\boldsymbol{\\beta})) $ 就是事件发生和不发生的联合概率密度函数，也可以看作是事件发生的概率密度函数（Probability Density Function, PDF）。 当我们计算出每个观察值的 $ \\text{Pr}(y_i = 1 | x_i) $（即事件发生的概率）后，我们可以将其代入上述公式中，得到对应的边际变化。这个边际变化告诉我们，当 $ x_i $ 发生微小变化时，事件发生的概率会如何变化。 举个例子来说明这个过程：假设我们有一个 logistic 回归模型，想要预测一个学生是否通过了考试，而自变量包括学习时间 $ x_1 $ 和学习资料数量 $ x_2 $。我们可以使用上述公式来计算每个学生通过考试的概率，并进一步分析当学习时间或学习资料数量微小变化时，通过考试的概率会如何变化。这样，我们就可以评估这些因素对考试结果的影响程度。 There are several ways to compute the marginal changes for each observation. For the logit model, the simplest approach is to use the formula where $ \\text{Pr}(y_i = 1 | x_i) 1 - \\text{Pr}(y_i = 1 | x_i) $ is the PDF for the logistic distribution. After predict computes $ \\text{Pr}(y_i = 1 | x_i) $ for each observation, it is easy to create a variable containing the marginal effects: 给定一个具有多个自变量 $ x_1, x_2, \\ldots, x_n $ 的模型，我们想要计算当某个自变量 $ x_k $ 微小变化时，因变量 $ y $ 的变化量。这个变化量称为边际变化（marginal change）。 边际变化可以使用以下公式计算： $$ \\frac{\\partial\\Pr(y_i=1|\\mathbf{x}_i)}{\\partial x_k} = f(\\mathbf{x}_i\\boldsymbol{\\beta}) \\cdot \\beta_k $$ 这里的 $ \\","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:2:5","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.6 (Advanced) Algorithm for computing the distribution of effects In this section, we use margins and slightly more advanced programming techniques to create a general algorithm for plotting the distribution of effects. Although the programming is more complicated, the code works with any model that is compatible with margins, even if your model includes interactions and product terms. We suggest you read this section after you have mastered other materials in this chapter. Instead of using generate to compute marginal effects based on the formula for a specific model, this algorithm uses margins to compute the effect for each observation. Although this is computationally slow, it works very generally for creating a histogram of any marginal effect that can be computed by margins or by predictions made by margins. We begin with a review of using margins to compute marginal effects (see section 4.5 for related information) 2.6.1 Using margins to compute marginal effects The option dydx (varname) tells margins to compute marginal effects. If varname is a factor variable, such as i.wc in our example, margins computes the discrete change as varname changes from 0 to 1. If varname is not a factor variable, margins computes the marginal change (that is, partial derivative) for varname. We begin by fitting the model and storing the results. We must store them because we will use the post option with margins, which replaces the regression estimates in memory with the results from margins. use binlfp4, clear logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog estimates store mymodel Next, we compute the marginal change with margins,dydx (inc), leaving in the return r(b). margins, dydx(wc) matlist r(b) Average marginal effects Number of obs = 753\rModel VCE: OIM\rExpression: Pr(lfp), predict()\rdy/dx wrt: 1.wc\r------------------------------------------------------------------------------\r| Delta-method\r| dy/dx std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rwc |\rcollege | 0.16 0.04 3.69 0.000 0.08 0.25\r------------------------------------------------------------------------------\rNote: dy/dx for factor levels is the discrete change from the base level.\rWe can also use margins to compute discrete changes for continuous variables, but this takes two steps. First, we make two predictions and post the results. Second, we use lincom or mlincom to compute the discrete change. For example, suppose that we want to compute the change in the probability of labor force participation as the number of young children increases from 0 to 3. We compute predictions with two atspecs, one for k5=0 and the other for k5=3: | 0b. 1.\r| wc wc -------------+---------------------\ry1 | 0 .1624037 margins, at(k5=0) at(k5=3) post Predictive margins Number of obs = 753\rModel VCE: OIM\rExpression: Pr(lfp), predict()\r1._at: k5 = 0\r2._at: k5 = 3\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_at |\r1 | 0.64 0.02 34.97 0.000 0.60 0.67\r2 | 0.04 0.02 2.09 0.036 0.00 0.07\r------------------------------------------------------------------------------\rBecause we used the post option, the predictions are saved to e(b), which allows us to use mlincom (or lincom) to compute the average discrete change: mlincom 2 - 1 | lincom pvalue ll ul -------------+---------------------------------------\r1 | -0.599 0.000 -0.656 -0.541 The linear combination in the column lincom is returned to r(est). We can also compute a change of a fixed amount from the observed values, for example, the average change as inc increases by 1 from its observed values. To do this, we will want to use a single margins command to produce two different predictions: one in which predictions are computed at the observed values and one in which inc is increased by 1. To get predic","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:2:6","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3 Ideal types An ideal type is a hypothetical observation with substantively illustrative values. A table of probabilities for ideal types of people, countries, cows, or whatever you are studying can quickly summarize the effects of key variables. In our example of labor force participation, we want to examine four ideal types of respondents: A young family with lower income, no college education, and young children. A young family with college education and young children. A middle-aged family with college education and teenage children. An older family with college education and adult children. We find ideal types to be particularly illustrative for interpretation when independent variables are substantially correlated. In the above example, we first consider the contrast between lower income and no college education and higher income and college education, because these indicators of SES covary strongly enough that it is easy to envision them as low- and high-SES prototypes. Across the latter three examples, we construct ideal types reflecting that the age of parents and their children change together. We use mtable to estimate the probabilities for each of these ideal types. To introduce the command and explain some options, we begin with an example that combines two sets of predictions. (See section 4.4 for an introduction to mtable.) We then illustrate two approaches for creating a table of ideal types. For our first ideal type, we define a young, lower-class family as having the values specified as at (agecat=1 k5=2 k618=0 inc=10 lwg=.75 hc=0 wc=0). lwg equals the log of the federal minimum wage for 1975, the year the data were collected. We use mtable to make predictions, using the rowname() option to label the results. The option ci, a synonym for statistics (ci), requests confidence intervals along with the predicted probability. Because this is the first step in constructing a table of predictions, we use the clear option to remove from memory any prior predictions saved by mtable. logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog mtable, rowname(1 Young low SES young kids) ci clear /// at(agecat=1 k5=2 k618=0 inc=10 lwg=.75 hc=0 wc=0) Expression: Pr(lfp), predict()\r| Pr(y) ll ul\r----------------------------+-----------------------------\r1 Young low SES young kids | 0.159 0.068 0.251\rSpecified values of covariates\r| k5 k618 agecat wc hc lwg inc\r----------+---------------------------------------------------------------------\rCurrent | 2 0 1 0 0 .75 10\rWe conclude the following: For a young, lower SES family with two young children, the estimated probability of being in the labor force is 0.16 with a 95% confidence interval from 0.07 to 0.25. For our next ideal type, we define a young, college-educated family with young children by using at(agecat=1 k5=2 k618=0 wc=1 hc=1), which specifies the values for all the independent variables except lwg and inc. Because we used the atmeans option, these variables are set to the means in the estimation sample. To place the new prediction below the prediction from the last mtable command, we use the below option. Below the table of predictions is a table showing the levels of the covariates when the predictions were made. Although you can suppress its display with the brief option, we find it useful for knowing exactly how the ideal types were defined. Set 1 refers to the first predictions in the table, which we numbered as 1. The row Current contains values of the at() variables from the current or most recent mtable command. We add two more ideal types that show what happens to the probability of being in the labor force as women and children get older. We use quietly to suppress output until the last mtable command, which displays the complete table. quietly mtable, rowname(3 Midage college with teens) ci below /// at(agecat==2 k5==0 k618==2 wc==1 hc==1) atmeans mtable, rowname(4 Older college with adult kids) ci below /// at(agecat==3 k5==0 k618==0 wc==1 hc==1) atmeans Expr","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:3:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3.1 Using local means with ideal types The last three rows of the table were constructed using the atmeans option to specify the values of inc and lwg to the sample means. We refer to means based on the entire estimation sample as global means. Although using global means for each ideal type is simple, it often is not realistic. For example, it is reasonable to assume that levels of income and wages would be higher for college-educated respondents than for those who have not attended college and that they would change with age, which is not reflected in the global means. To address this problem, we can use local means that are defined based on the characteristics specified in the at() statements. To do this, we create a selection variable that equals 1 if an observation is part of the group defined by the conditions of an atspec and equals 0 otherwise. In other words, a selection variable indicates whether an observation is part of the group defined for the ideal type. To create these variables, we use the generate command with if conditions that correspond to the atspecs used for an ideal type: capture drop _sel* gen _selYC = agecat==1 \u0026 k5==2 \u0026 k618==0 \u0026 wc==1 \u0026 hc==1 label var _selYC \"Select Young college young kids\" gen _selMC = agecat==2 \u0026 k5==0 \u0026 k618==2 \u0026 wc==1 \u0026 hc==1 label var _selMC \"Select Midage college with teens\" gen _selOC = agecat==3 \u0026 k5==0 \u0026 k618==0 \u0026 wc==1 \u0026 hc==1 label var _selOC \"Select Older college with adult kids\" Once these variables are created, we can make a table of predictions containing local means for variables not explicitly set by the atspec. The first row of the table is unchanged from before because all variables for that ideal type were explicitly specified in the at() option: quietly mtable, rowname(1 Young low SES young kids) ci clear /// at(agecat=1 k5=2 k618=0 inc=10 lwg=.75 hc=0 wc=0) In the next command, we add if _selYC==1 to the mtable command so that predictions are based only on observations defined by _selYC: quietly mtable if _selYC==1, rowname(2 Young college young kids) /// atmeans ci below The if condition selects observations where agecat==1 \u0026 k5==2 \u0026 k618==0 \u0026 wc==1 \u0026 hc==1, which define our ideal type. The means of these variables will equal their specified values (for example, agecat will equal 1 and k5 will equal 2), while those variables not used to define the selection variable will equal the local mean defined by selection variables. For example, lwg will equal the average log of wages for young families with college education. Accordingly, the if condition makes it easy to specify the values we wanted to define our ideal type. In the same way, we add the last two ideal types to the table: quietly mtable if _selMC==1, rowname(3 Midage college with teens) /// atmeans ci below mtable if _selOC==1, rowname(4 Older college with adult kids) /// atmeans ci below Expression: Pr(lfp), predict()\r| Pr(y) ll ul\r----------------------------+-----------------------------\r1 Young low SES young kids | 0.159 0.068 0.251\r2 Young college young kids | 0.394 0.234 0.554\r3 Midage college with teen | 0.739 0.659 0.820\r4 Older college with adult | 0.631 0.528 0.734\rSpecified values of covariates\r| k5 k618 agecat wc hc lwg inc\r----------+---------------------------------------------------------------------\rSet 1 | 2 0 1 0 0 .75 10\rSet 2 | 2 0 1 1 1 1.62 16.6\rSet 3 | 0 2 2 1 1 1.16 24.4\rCurrent | 0 0 3 1 1 1.38 27.9\rAn advantage of using local means with ideal types is that the values of variables not specified in the type are held to values more consistent with what is actually observed, so the ideal type more accurately resembles the actual cases in our dataset that share the key features of the ideal type. ","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:3:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3.2 Comparing ideal types with statistical tests The predicted probabilities of labor force participation vary among the four ideal types. Before concluding, for example, that the probability of being in the labor force is greater for a young, college-educated family with children than for a family with no college education, we need to test whether the predictions are significantly different. Essentially, this involves testing whether a discrete change is 0 when the starting values and ending values vary on multiple variables. To show how this is done, we compute two ideal types in the same mtable command and post the results so that we can evaluate them with mlincom. Because we are posting the results, we begin with estimates store so that we can later restore the estimation results from logit after they have been replaced by the posted predictions. logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog estimates store base mtable, atmeans post /// at(agecat=1 k5=2 k618=0 wc=0 hc=0 lwg=.75 inc=10) /// ideal type 1 at(agecat=1 k5=2 k618=0 wc=1 hc=1 lwg=1.62 inc=16.64) // ideal type 2 Expression: Pr(lfp), predict()\r| wc hc lwg inc Pr(y)\r----------+-------------------------------------------------\r1 | 0 0 .75 10 0.159\r2 | 1 1 1.62 16.6 0.394\rSpecified values of covariates\r| k5 k618 agecat\r----------+-----------------------------\rCurrent | 2 0 1\rNow, we estimate the difference in the predictions and end by restoring the estimation results from logit: mlincom 1 - 2 | lincom pvalue ll ul -------------+---------------------------------------\r1 | -0.234 0.000 -0.340 -0.129 We conclude the following: A wife from a young, lower SES family with young children is significantly less likely to be in the labor force than a wife from a young family with college education (p \u003c 0.001). ","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:3:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3.3 (Advanced) Using macros to test differences between ideal types In this section, we discuss using local macros and returns to automate the process of computing predictions at multiple fixed values of the at() variables. If you rarely test the equality of predictions, the methods from the last section should meet your needs. If you often test the equality of predictions, this section can save you time. It is tedious and error-prone to specify the atspecs for multiple ideal types to test the equality of predictions. To automate this process, we can use the returned results from mtable. When mtable is run with a single at(), it returns the local r(atspec) as a string that contains the specified values of the covariates. This is easiest to understand with an example: mtable, atmeans at(agecat=1 k5=2 k618=0 wc=0 hc=0 lwg=.75 inc=10) Expression: Pr(lfp), predict()\rPr(y)\r--------\r0.159\rSpecified values of covariates\r| k5 k618 agecat wc hc lwg inc\r----------+---------------------------------------------------------------------\rCurrent | 2 0 1 0 0 .75 10\rThe values shown in the Specified values of covariates table are saved in the return r(atspec): display \"`r(atspec)'\" . display \"`r(atspec)'\"\rk5=2 k618=0 1b.agecat=1 2.agecat=0 3.agecat=0 0b.wc=1 1.wc=0 0b.hc=1 1.hc=0 lwg=.75 inc=10 We create a local macro that is used to specify the atspec for mtable: local myatspec `r(atspec)' mtable, atmeans at(`myatspec') Expression: Pr(lfp), predict()\rPr(y)\r--------\r0.159\rSpecified values of covariates\r| k5 k618 agecat wc hc lwg inc\r----------+---------------------------------------------------------------------\rCurrent | 2 0 1 0 0 .75 10\rThe results match those we obtained earlier. Using this strategy and the selection variables created before (see page 273), we create local macros with the atspecs for our four ideal types: quietly mtable, atmeans at(agecat=1 k5=2 k618=0 inc=10 lwg=.75 hc=0 wc=0) local YngLow `r(atspec)' quietly mtable if _selYC == 1, atmeans local YngCol `r(atspec)' quietly mtable if _selMC == 1, atmeans local MidCol `r(atspec)' quietly mtable if _selOC == 1, atmeans local OldCol `r(atspec)' We use these locals to compute four predictions with a single mtable: mtable, at(`YngLow') at(`YngCol') at(`MidCol') at(`OldCol') post Expression: Pr(lfp), predict()\r| k5 k618 agecat wc hc lwg inc Pr(y)\r----------+-------------------------------------------------------------------------------\r1 | 2 0 1 0 0 .75 10 0.159\r2 | 2 0 1 1 1 1.62 16.6 0.394\r3 | 0 2 2 1 1 1.16 24.4 0.739\r4 | 0 0 3 1 1 1.38 27.9 0.631\rSpecified values where .n indicates no values specified with at()\r| No at()\r----------+---------\rCurrent | .n\rBecause the values of all independent variables were specified for each prediction, their values appear in the table of predictions rather than in a table of values of covariates below the predictions. Because there are no values to place in the table, .n is shown. Because the predictions were posted, we can use mlincom for each comparison: mlincom 1 - 2 mlincom 1 - 3 mlincom 1 - 4 . mlincom 1 - 2\r| lincom pvalue ll ul -------------+---------------------------------------\r1 | -0.235 0.000 -0.340 -0.129 . mlincom 1 - 3\r| lincom pvalue ll ul -------------+---------------------------------------\r1 | -0.580 0.000 -0.720 -0.440 . mlincom 1 - 4\r| lincom pvalue ll ul -------------+---------------------------------------\r1 | -0.471 0.000 -0.622 -0.320 Alternatively, we can take advantage of the pwcompare() option in margins, which is not available with the mtable command. We specify the values at which we want to make predictions, just like we did with mtable. We suppress the lengthy listing of the titlegend and request pairwise comparisons of the estimates: estimates restore base margins, at(`YngLow') at(`YngCol') at(`MidCol') at(`OldCol') /// noatlegend pwcompare(effects) Pairwise comparisons of adjusted predictions Number of obs = 753\rModel VCE: OIM\rExpression: Pr(lfp), predict()\r----------------------------------------------------","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:3:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3.4 Marginal effects for ideal types Given our cautions about relying on a single value to summarize marginal effects, ideal types are an excellent way to examine variation in the size of effects at different locations in the data space. Here we are taking different hypothetical observations and describing how the predicted probability changes as the value of one of the independent variables for that observation changes. To do this, we use local macros to specify the values at which the change is to be computed, just as we did with mtable. First, we com pute discrete changes for wc and k5 for a young, low SES family: mchange wc k5, atmeans amount(one) at(`YngLow') logit: Changes in Pr(y) | Number of obs = 753 Expression: Pr(lfp), predict(pr)\r| Change p-value ---------------+---------------------\rwc | college vs no | 0.137 0.008 k5 | +1 | -0.114 0.000 Predictions at base value\r| not in LF in LF -------------+---------------------\rPr(y|base) | 0.841 0.159 Base values of regressors\r| k5 k618 agecat wc hc lwg inc -------------+----------------------------------------------------------------------------\rat | 2 0 1 0 0 .75 10 1: Estimates with margins option atmeans.\rmatrix YngLow = r(table) mchange leaves the marginal effects in the r (table) matrix, which we copy to the matrix YngLow so that we can combine it with estimates of effects for other ideal types. mchange wc k5, atmeans amount(one) at(`YngCol') matrix YngCol = r(table) mchange wc k5, atmeans amount(one) at(`MidCol') matrix MidCol = r(table) mchange wc k5, atmeans amount(one) at(`OldCol') matrix OldCol = r(table) Next, we select the first column of each matrix, which contains the effects, and concatenate them into a single matrix we name me: matrix me = YngLow[1...,1], YngCol[1...,1], MidCol[1...,1], OldCol[1...,1] matrix colnames me = YngLow YngCol MidCol OldCol matlist me, format(%9.2f) twidth(15) | YngLow YngCol MidCol OldCol ----------------+-------------------------------------------\rwc | college vs no | 0.14 0.19 0.19 0.19 ----------------+-------------------------------------------\rk5 | +1 | -0.11 -0.32 -0.32 -0.32 The effects of the wife going to college are within 0.06 across the four ideal types. The effects of having one more young child in the family, however, increase in magnitude from -0.11 for young families without college education to -0.33 for older families that attended college. The differences in discrete changes for k5 reflect the variation in the size of effects within the sample. ","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:3:4","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4 Tables of predicted probabilities When you are interested in the effects of one or more categorical independent variables, a table of predictions can be very effective. For example, our analysis thus far highlights the importance of attending college and having young children. To see how these variables jointly affect the probability of being in the labor force, we can use a simple mtable command: use binlfp4, clear logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog estimates store base mtable, at(wc=(0 1) k5=(0 1 2 3)) atmeans Expression: Pr(lfp), predict()\r| k5 wc Pr(y)\r----------+-----------------------------\r1 | 0 0 0.604\r2 | 0 1 0.772\r3 | 1 0 0.275\r4 | 1 1 0.457\r5 | 2 0 0.086\r6 | 2 1 0.173\r7 | 3 0 0.023\r8 | 3 1 0.049\rSpecified values of covariates\r| 2. 3. 1. | k618 agecat agecat hc lwg inc\r----------+-------------------------------------------------------------\rCurrent | 1.35 .385 .219 .392 1.1 20.1\rAlthough this is the information we want, it is not an effective table. We can improve it by using two at()’s along with atvars(wc k5) to list values of wc in the first column followed by values of k5. The option names(columns) removes the row numbers (see help matlist for details on the names() option). mtable, at(wc=0 k5=(0 1 2 3)) at(wc=1 k5=(0 1 2 3)) atmeans /// atvars(wc k5) names(columns) Expression: Pr(lfp), predict()\r1. wc k5 Pr(y)\r----------------------------\r0 0 0.604\r0 1 0.275\r0 2 0.086\r0 3 0.023\r1 0 0.772\r1 1 0.457\r1 2 0.173\r1 3 0.049\rSpecified values of covariates\r| 2. 3. 1. | k618 agecat agecat hc lwg inc\r----------+-------------------------------------------------------------\rCurrent | 1.35 .385 .219 .392 1.1 20.1\rThe table shows the strong effect of education and how the size of the effects differ by the number of young children, but the information still is not presented well. Our next step is to compute the discrete change for college education conditional on the number of young children: $$\\frac{\\Delta\\Pr\\left(y=1\\mid\\mathbf{x},\\mathbf{k}5\\right)}{\\Delta\\text{wc}\\left(0\\to1\\right)}$$ Because wc was entered into the model as a factor variable, we can compute the discrete change by using dydx(wc). In the process, let’s create an even more effective table that gets close to what we might include in a paper. First, we compute the predictions for wc=0: quietly mtable, estname(NoCol) at(wc=0 k5=(0 1 2 3)) atmeans brief Next, we make predictions for wc=1. We use the right option to place the predictions to the right of those from the prior mtable command, and we use atvars(_none) because we do not want the column with k5 included again. quietly mtable, estname(College) at(wc=1 k5=(0 1 2 3)) atmeans /// atvars(_none) right Now, we use the dydx(wc) option to compute discrete changes. We place these along with the p-value for testing whether the change is 0 to the right: mtable, estname(Change) dydx(wc) at(k5=(0 1 2 3)) atmeans /// atvars(_none) right stats(estimate p) names(columns) brief Expression: Pr(lfp), predict()\rk5 NoCol NoCol College Change p\r----------------------------------------------------------\r0 0.772 0.604 0.772 0.168 0.000\r1 0.457 0.275 0.457 0.182 0.001\r2 0.173 0.086 0.173 0.087 0.013\r3 0.049 0.023 0.049 0.027 0.085\rWe can summarize these findings: For someone who is average on all characteristics and has no young children, having attended college significantly increases the predicted probability of being in the labor force by 0.17. The size of the effect decreases with the number of young children. For example, for someone with two young children, the increase is only 0.09, which is significant at the 0.01 level. Although this table shows clearly how education and children affect labor force participation, it assumes that it is reasonable to change wc and k5 while holding other variables at their global means. This is unrealistic. For example, it is likely that women with three young children will be in the youngest age group, while few people with three young children will be over 50. Each cel","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:4:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"5 Second differences comparing marginal effects We can compute AMEs based on a subset of observations. For example, suppose that we are interested in ways in which the wife’s and the husband’s educations interact to affect labor force participation. Because we are focusing on the joint effects of these two variables, we fit a new model that includes the interaction between wc and hc: logit lfp k5 k618 i.agecat wc##hc lwg inc, nolog estimates store base Logistic regression Number of obs = 753\rLR chi2(9) = 125.57\rProb \u003e chi2 = 0.0000\rLog likelihood = -452.08908 Pseudo R2 = 0.1219\r----------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-----------------+----------------------------------------------------------------\rk5 | -1.40 0.19 -7.27 0.000 -1.78 -1.02\rk618 | -0.07 0.07 -0.96 0.337 -0.20 0.07\r|\ragecat |\r40-49 | -0.62 0.21 -2.97 0.003 -1.03 -0.21\r50+ | -1.27 0.26 -4.88 0.000 -1.78 -0.76\r|\rwc |\rcollege | 1.19 0.43 2.75 0.006 0.34 2.05\r|\rhc |\rcollege | 0.25 0.23 1.08 0.281 -0.20 0.69\r|\rwc#hc |\rcollege#college | -0.56 0.51 -1.11 0.269 -1.55 0.43\r|\rlwg | 0.61 0.15 4.05 0.000 0.31 0.90\rinc | -0.03 0.01 -4.19 0.000 -0.05 -0.02\r_cons | 0.98 0.29 3.41 0.001 0.42 1.54\r----------------------------------------------------------------------------------\rWe want to know whether the effect of a women going to college is the same for a women whose husband did go to college as for a woman whose husband did not go to college: $$H_0{:\\frac{\\Delta\\Pr\\left(y=1\\mid\\mathrm{x,hc}=0\\right)}{\\Delta\\mathrm{wc}}}=\\frac{\\Delta\\Pr\\left(y=1\\mid\\mathrm{x,hc}=1\\right)}{\\Delta\\mathrm{wc}}$$ To test this hypothesis, we compute the AME of wc averaging over only those cases where hc is 0 and compare it with the AME for those cases where hc is 1. Although we could compute these discrete changes by using mchange wc if hc==1 and mchange wc if hc==0, this will not allow us to test whether the effects are equal because the estimates cannot be posted for testing with mlincom. To test the hypothesis, we use mtable with the dydx(wc) option to compute the discrete change for wc and the over(hc) option to request the changes be computed with the subgroups defined by hc: mtable, dydx(wc) over(hc) post Expression: Pr(lfp), predict()\r| d Pr(y)\r----------+---------\rno | 0.233\rcollege | 0.128\rSpecified values where .n indicates no values specified with at()\r| No at()\r----------+---------\rCurrent | .n\rThe row labeled no contains the discrete change of wc for those women whose husbands did not attend college (no is the value label for hc= 0), and the row college contains the change for those whose husbands attended college. The post option saves the estimates to e(b), which allows us to use mlincom to test whether the marginal effects are equal: test _b[1.wc:0.hc]=_b[1.wc:1.hc] mlincom 1 - 2 ( 1) [1.wc]0bn.hc - [1.wc]1.hc = 0\rchi2( 1) = 1.42\rProb \u003e chi2 = 0.2329\r. mlincom 1 - 2\r| lincom pvalue ll ul -------------+---------------------------------------\r1 | 0.105 0.233 -0.068 0.279 We conclude the following: Although the average effect of the wife going to college is 0.10 larger when the husband did not go to college than when he did, this difference is not significant (p \u003e 0.10). ","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:5:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"6 Graphing predicted probabilities With a continuous independent variable, you can plot the predicted probabilities over the range of the variable. For example, to examine the effects of inc, we might plot the predicted probability of labor force participation as income changes, holding other variables at fixed values. We offer two approaches for making such graphs. First, Stata’s marginsplot command uses predictions from margins to create plots. As you will see, it quickly produces effective graphs. The second approach uses our mgen command to generate variables with the values to be plotted, which are then plotted with graph. This is essentially what marginsplot does behind the scenes. Although marginsplot is simpler, mgen is more flexible in ways that often justify the greater effort that it requires. The advantage will be particularly apparent in subsequent chapters when we create plots for multiple outcomes that cannot be created with marginsplot. We begin by showing you how to create plots where one variable changes while all other variables are held constant. ","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:6:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"6.1 Using marginsplot The first step is to use margins to compute predicted probabilities as income increases from 0 to 100, while holding other variables at their means: use binlfp4, clear logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog estimates store base margins, at(inc=(0(10)100)) atmeans Adjusted predictions Number of obs = 753\rModel VCE: OIM\rExpression: Pr(lfp), predict()\r1._at: k5 = .2377158 (mean)\rk618 = 1.353254 (mean)\r1.agecat = .3957503 (mean)\r2.agecat = .3851262 (mean)\r3.agecat = .2191235 (mean)\r0.wc = .7184595 (mean)\r1.wc = .2815405 (mean)\r0.hc = .6082337 (mean)\r1.hc = .3917663 (mean)\rlwg = 1.097115 (mean)\rinc = 0\r11._at: k5 = .2377158 (mean)\rk618 = 1.353254 (mean)\r1.agecat = .3957503 (mean)\r2.agecat = .3851262 (mean)\r3.agecat = .2191235 (mean)\r0.wc = .7184595 (mean)\r1.wc = .2815405 (mean)\r0.hc = .6082337 (mean)\r1.hc = .3917663 (mean)\rlwg = 1.097115 (mean)\rinc = 100\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_at |\r1 | 0.73 0.04 20.36 0.000 0.66 0.81\r2 | 0.66 0.03 25.32 0.000 0.61 0.71\r3 | 0.58 0.02 29.40 0.000 0.54 0.62\r4 | 0.49 0.03 17.17 0.000 0.44 0.55\r5 | 0.41 0.04 9.20 0.000 0.32 0.49\r6 | 0.32 0.06 5.70 0.000 0.21 0.44\r7 | 0.25 0.06 3.94 0.000 0.13 0.38\r8 | 0.19 0.07 2.95 0.003 0.06 0.32\r9 | 0.14 0.06 2.33 0.020 0.02 0.26\r10 | 0.11 0.06 1.92 0.055 -0.00 0.21\r11 | 0.08 0.05 1.63 0.103 -0.02 0.17\r------------------------------------------------------------------------------\rThe atlegend shows the values of the independent variables for each of the 11 predictions in the table, which are automatically saved in the matrix r(b). Because the atlegend can be quite long, we often use noatlegend to suppress it. Then, we use margins for a more compact summary. margins, at(inc=(0(10)100)) atmeans noatlegend Adjusted predictions Number of obs = 753\rModel VCE: OIM\rExpression: Pr(lfp), predict()\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_at |\r1 | 0.73 0.04 20.36 0.000 0.66 0.81\r2 | 0.66 0.03 25.32 0.000 0.61 0.71\r3 | 0.58 0.02 29.40 0.000 0.54 0.62\r4 | 0.49 0.03 17.17 0.000 0.44 0.55\r5 | 0.41 0.04 9.20 0.000 0.32 0.49\r6 | 0.32 0.06 5.70 0.000 0.21 0.44\r7 | 0.25 0.06 3.94 0.000 0.13 0.38\r8 | 0.19 0.07 2.95 0.003 0.06 0.32\r9 | 0.14 0.06 2.33 0.020 0.02 0.26\r10 | 0.11 0.06 1.92 0.055 -0.00 0.21\r11 | 0.08 0.05 1.63 0.103 -0.02 0.17\r------------------------------------------------------------------------------\rmlistat at() values held constant\r2. 3. 1. 1. k5 k618 agecat agecat wc hc lwg --------------------------------------------------------------------\r.238 1.35 .385 .219 .282 .392 1.1 at() values vary\r_at | inc -------+---------\r1 | 0 2 | 10 3 | 20 4 | 30 5 | 40 6 | 50 7 | 60 8 | 70 9 | 80 10 | 90 11 | 100 Either way, marginsplot uses the predictions in r(b) along with other returns from margins, and it graphs the predictions including the 95% confidence intervals. marginsplot The graph shows how the probability of being in the labor force decreases with family income. It also shows that the confidence intervals are smaller near the center of the data (the mean of inc is 20.1) and increase as we move to the extremes. Although marginsplot does an excellent job of creating the graph without requiring options, you can fully customize the graph. Use help marginsplot for full details. For example, to suppress the confidence interval, type marginsplot, noci. To use shading to show the confidence interval (illustrated below), type marginsplot, recast(line) recastci(rarea). If you are only interested in plotting a single type of prediction from one model, there is little reason to use anything but marginsplot. But, if you want to plot multiple outcomes, such as for multinomial l","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:6:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"6.2 Using mgen with the graph command To create the same graph as above by using mgen, our first step is to generate variables for plotting: mgen, atmeans at(inc=(0(10)100)) stub(PLT) predlabel(Pr(LFP)) Predictions from: margins, atmeans at(inc=(0(10)100)) predict(pr)\rVariable Obs Unique Mean Min Max Label\r-------------------------------------------------------------------------------------------\rPLTpr1 11 11 .3608011 .0768617 .7349035 Pr(LFP)\rPLTll1 11 11 .2708139 -.0156624 .6641427 95% lower limit\rPLTul1 11 11 .4507883 .1693859 .8056643 95% upper limit\rPLTinc 11 11 50 0 100 Family income excluding wife's\r-------------------------------------------------------------------------------------------\rSpecified values of covariates\r1. 3. 1. 1. k5 k618 agecat agecat wc hc lwg ---------------------------------------------------------------------------\r.2377158 1.353254 .3851262 .2191235 .2815405 .3917663 1.097115 The option stub() specifies the prefix for variables that are generated. If stub() is not specified, the default stub(_) is used. If you want to replace existing plot variables (perhaps while debugging your do-file), add the option replace. The option predlabel() customizes the variable label for PLTprl, which is handy because by default graph uses this label for the y axis. If we list the values for the first 13 observations, we see the variables created by mgen: list PLTinc PLTpr PLTll PLTul lfp in 1/13, clean PLTinc PLTpr1 PLTll1 PLTul1 lfp 1. 0 .7349035 .6641427 .8056643 not in LF 2. 10 .6613024 .6101217 .7124832 not in LF 3. 20 .5789738 .5403737 .6175739 not in LF 4. 30 .4920058 .4358374 .5481742 not in LF 5. 40 .405519 .3191012 .4919367 not in LF 6. 50 .324523 .2129492 .4360968 not in LF 7. 60 .2528245 .1272066 .3784425 not in LF 8. 70 .1924535 .0644926 .3204144 not in LF 9. 80 .1437253 .0227563 .2646942 not in LF 10. 90 .1057196 -.0023663 .2138055 not in LF 11. 100 .0768617 -.0156624 .1693859 not in LF 12. . . . . not in LF 13. . . . . not in LF Column 1 contains the 11 values of income from variable PLTinc that will define the x coordinates. The next column contains predicted probabilities computed at the values of income with other variables held at their means. The negative effect of income is shown by the increasingly small probabilities. The next two columns contain the upper and lower bounds of the confidence intervals for the predictions. The first four variables have missing values beginning in rows 12 and 13 because our atspec requested only 11 predictions. The last column shows the observed variable lfp, which does not have missing values. This is being shown to remind you that the variables created for graphing are added to the dataset used for estimation. You can also create a basic graph w ithout confidence intervals: scatter PLTpr PLTinc Next, we want to add the 95% confidence interval around the predictions. This requires more complicated graph options. To explain these, let’s start by looking at the graph we want to create: twoway /// (rarea PLTul PLTll PLTinc, color(gs12)) /// (connected PLTpr PLTinc, msymbol(i)) /// , title(\"Adjusted Predictions\") /// caption(\"Other variables held at their means\") /// ytitle(Pr(LFP)) ylabel(0(.25)1, grid gmin gmax) legend(off) Here is the twoway command that we will explain: The first thing to realize is that the twoway command includes two plots: a plot and a connected plot. These are overlaid to make a single graph. First, the shaded confidence intervals are created with a rarea plot where the area on the y axis is shaded between the values of PLTul for the upper level or bound and PLTll for the lower bound. We chose color(gs12) to make the shading grayscale level 12, a matter of personal preference. Second, the line with predicted probabilities is created with a connected plot, where msymbol(i) specifies that the symbols (shown as solid circles in our prior graph) that are connected should be invisible—that is, draw the line without symbols. We defined the rarea p","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:6:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"6.3 Graphing multiple predictions An effective way to show the effects of two variables is to graph predictions at various levels of one variable as the other variable changes. This can be done with either marginsplot or mgen. 6.3.1 Using marginsplot We can plot the effects of income for each of the age groups. First, we compute the predictions with margins, where margins agecat indicates that we want predictions for each level of the factor variable agecat. at(inc = (0(10)100))atmeans specifies predictions as income increases from 0 to 100 by 10s, with all variables except agecat at their means: margins agecat, at(inc=(0(10)100)) atmeans noatlegend Adjusted predictions Number of obs = 753\rModel VCE: OIM\rExpression: Pr(lfp), predict()\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_at#agecat |\r1#30-39 | 0.82 0.03 26.36 0.000 0.76 0.88\r1#40-49 | 0.71 0.05 15.79 0.000 0.63 0.80\r1#50+ | 0.57 0.06 9.20 0.000 0.44 0.69\r2#30-39 | 0.77 0.03 25.73 0.000 0.71 0.83\r2#40-49 | 0.64 0.04 17.04 0.000 0.56 0.71\r2#50+ | 0.48 0.05 9.42 0.000 0.38 0.58\r3#30-39 | 0.70 0.03 21.88 0.000 0.64 0.76\r3#40-49 | 0.55 0.03 17.13 0.000 0.49 0.62\r3#50+ | 0.39 0.04 8.95 0.000 0.31 0.48\r4#30-39 | 0.62 0.04 14.75 0.000 0.54 0.70\r4#40-49 | 0.47 0.04 12.72 0.000 0.39 0.54\r4#50+ | 0.31 0.04 7.27 0.000 0.23 0.40\r5#30-39 | 0.53 0.06 9.22 0.000 0.42 0.65\r5#40-49 | 0.38 0.05 8.08 0.000 0.29 0.47\r5#50+ | 0.24 0.04 5.40 0.000 0.15 0.33\r6#30-39 | 0.45 0.07 6.01 0.000 0.30 0.59\r6#40-49 | 0.30 0.06 5.34 0.000 0.19 0.41\r6#50+ | 0.18 0.05 4.01 0.000 0.09 0.27\r7#30-39 | 0.36 0.09 4.19 0.000 0.19 0.53\r7#40-49 | 0.23 0.06 3.81 0.000 0.11 0.35\r7#50+ | 0.14 0.04 3.09 0.002 0.05 0.22\r8#30-39 | 0.29 0.09 3.10 0.002 0.11 0.47\r8#40-49 | 0.18 0.06 2.89 0.004 0.06 0.30\r8#50+ | 0.10 0.04 2.48 0.013 0.02 0.18\r9#30-39 | 0.22 0.09 2.42 0.016 0.04 0.40\r9#40-49 | 0.13 0.06 2.30 0.021 0.02 0.24\r9#50+ | 0.07 0.04 2.05 0.040 0.00 0.14\r10#30-39 | 0.17 0.08 1.96 0.049 0.00 0.33\r10#40-49 | 0.10 0.05 1.91 0.056 -0.00 0.20\r10#50+ | 0.05 0.03 1.75 0.080 -0.01 0.11\r11#30-39 | 0.12 0.07 1.65 0.099 -0.02 0.27\r11#40-49 | 0.07 0.04 1.63 0.104 -0.01 0.15\r11#50+ | 0.04 0.02 1.52 0.128 -0.01 0.09\r------------------------------------------------------------------------------\rmlistat k5 k618 agecat agecat wc hc lwg --------------------------------------------------------------------\r.238 1.35 .385 .219 .282 .392 1.1 at() values vary\r_at | inc -------+---------\r1 | 0 2 | 10 3 | 20 4 | 30 5 | 40 6 | 50 7 | 60 8 | 70 9 | 80 10 | 90 11 | 100 The labeling of the predictions from margins can be confusing. The left column of the prediction table is labeled _at#agecat, which indicates that the information in this column begins with a number corresponding to the 11 values of inc used for making predictions; these are referred to as the _at values. For example, 1 is the prediction with inc=0 while 11 is the prediction with inc=100. After that, the value or value label for agecat is listed. For example, the row labeled l#30-39 contains the predictions when all variables except agecat are held at the first _at value, with agecat=1 as indicated by the value label 30-39. The command marginsplot, noci automatically understands what these predictions are and creates the plot we want. marginsplot, noci legend(cols(3)) This example shows that marginsplot can plot multiple curves for the same outcome from the same model. Unfortunately, it cannot plot curves for multiple outcomes (for example, the probability of categories 1, 2, and 3 in an ordinal model) or predictions from different models (for example, showing how the predictions differ from two specifications of the model). For this, you need to use mgen. 6.3.2 Using mgen with graph We can create the same graph as in the previous section by running mgen once for each level of agecat: mgen, atmeans at(in","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:6:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"6.3 Overlapping confidence intervals We find that researchers sometimes conclude that estimates are significantly different only if confidence intervals for two estimates do not overlap. That is, if the confidence intervals overlap, the hypothesis that the estimates are equal is accepted. Although this might have been a useful approximation when computation was very expensive, it often leads to incorrect conclusions because it ignores the covariances of the estimators that need to be taken into account when testing equality. To illustrate the problem, as well as show how discrete changes and marginal changes can be graphed, we use the techniques above to plot the probability of labor force participation by income for women who attended college and those who did not. We start with the graph before showing how we created it. We use one mgen command for each level of wc: mgen, atmeans at(inc=(0(5)100) wc=0) stub(PLTWC0) predlab(NoCollege) Predictions from: margins, atmeans at(inc=(0(5)100) wc=0) predict(pr)\rVariable Obs Unique Mean Min Max Label\r-------------------------------------------------------------------------------------\rPLTWC0pr1 21 21 .3177494 .0623648 .6889161 NoCollege\rPLTWC0ll1 21 21 .2309727 -.0151898 .6107005 95% lower limit\rPLTWC0ul1 21 21 .4045261 .1399194 .7671317 95% upper limit\rPLTWC0inc 21 21 50 0 100 Family income excluding wife's\r-------------------------------------------------------------------------------------\rSpecified values of covariates\r1. 3. 1. k5 k618 agecat agecat wc hc lwg ---------------------------------------------------------------------------\r.2377158 1.353254 .3851262 .2191235 0 .3917663 1.097115 mgen, atmeans at(inc=(0(5)100) wc=1) stub(PLTWC1) predlab(College) twoway /// (rarea PLTWC0ul PLTWC0ll PLTWC0inc, col(gs12)) /// (rarea PLTWC1ul PLTWC1ll PLTWC0inc, col(gs12)) /// (connected PLTWC0pr PLTWC1pr PLTWC1inc, msym(i i)) /// , ytitle(Pr(In Labor Force)) legend(order(4 3)) Judging by the overlap of confidence intervals, we might mistakenly conclude that the probability of labor force participation was significantly higher for women who attended college when family income was between $5,000$ and $40,000$ but not at other incomes. To see how poorly this “approximation” works, we compute the discrete change conditional on income with mgen. The option dydx(wc) specifies that we want to predict the marginal effect of wc. Because wc was entered into the model as the factor variable i.wc, mgen computes a discrete change. mgen, dydx(wc) atmeans at(inc=(0(5)100)) stub(PLTWCDC) /// predlab(Discrete change in LFP by attending college) Predictions from: margins, dydx(wc) atmeans at(inc=(0(5)100)) predict(pr)\rVariable Obs Unique Mean Min Max Label\r------------------------------------------------------------------------------------------------------\rPLTWCDCd_pr1 21 21 .1507267 .0663191 .1967745 Discrete change in LFP by attending college\rPLTWCDCll1 21 21 .0556941 -.0111785 .0895455 95% lower limit\rPLTWCDCul1 21 21 .2457593 .1438166 .3049388 95% upper limit\rPLTWCDCinc 21 21 50 0 100 Family income excluding wife's\r------------------------------------------------------------------------------------------------------\rSpecified values of covariates\r1. 3. 1. 1. k5 k618 agecat agecat wc hc lwg ---------------------------------------------------------------------------\r.2377158 1.353254 .3851262 .2191235 .2815405 .3917663 1.097115 Plotting the results along with those for the probabilities leads to figure 6.2, which shows that women who attended college have significantly higher probabilities of labor force participation over almost the entire income distribution, excepting only incomes above $95,000$ (where there are very few cases). What is remarkable about margins is that it allows you to test just about anything you might want to say about your predictions! twoway /// (rarea PLTWC0ul PLTWC0ll PLTWC0inc, col(gs12)) /// (rarea PLTWC1ul PLTWC1ll PLTWC0inc, col(gs12)) /// (connected PLTWC0pr PLTWC1pr PLTWC1in","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:6:4","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"6.5 Adding power terms and plotting predictions As shown in section 6.2.1, squared terms can be included in models by using factor-variable notation. For example, income and income-squared can be included in the model by adding the term c.inc##c.inc. Although you can obtain the same parameter estimates by generating a new variable for income-squared, margins or our m* commands will not compute predictions correctly. With factor-variable notation, however, power terms and interaction terms do not pose any special problems. When mgen makes predictions, it automatically increases income-squared appropriately as income changes. To illustrate how this works, we compare predictions from a model that is linear in income with a model that adds the squared term c.inc#c.inc. First, we fit the model that includes income (but not income-squared) and make predictions: logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog mgen, predlabel(linear) atmeans at(inc=(0(10)100)) stub(_lin) Predictions from: margins, atmeans at(inc=(0(10)100)) predict(pr)\rVariable Obs Unique Mean Min Max Label\r------------------------------------------------------------------------------------------\r_linpr1 11 11 .3608011 .0768617 .7349035 linear\r_linll1 11 11 .2708139 -.0156624 .6641427 95% lower limit\r_linul1 11 11 .4507883 .1693859 .8056643 95% upper limit\r_lininc 11 11 50 0 100 Family income excluding wife's\r------------------------------------------------------------------------------------------\rSpecified values of covariates\r1. 3. 1. 1. k5 k618 agecat agecat wc hc lwg ---------------------------------------------------------------------------\r.2377158 1.353254 .3851262 .2191235 .2815405 .3917663 1.097115 Next, we fit th e model that adds income-squared and make predictions: logit lfp k5 k618 i.agecat i.wc i.hc lwg c.inc##c.inc, nolog mgen, predlabel(quadratic) atmeans at(inc=(0(10)100)) stub(_quad) Predictions from: margins, atmeans at(inc=(0(10)100)) predict(pr)\rVariable Obs Unique Mean Min Max Label\r-------------------------------------------------------------------------------------\r_quadpr1 11 11 .4410442 .2887324 .8078035 quadratic\r_quadll1 11 11 .2613509 -.1501807 .7207593 95% lower limit\r_quadul1 11 11 .6207375 .4265932 .9690264 95% upper limit\r_quadinc 11 11 50 0 100 Family income excluding wife's\r-------------------------------------------------------------------------------------\rSpecified values of covariates\r1. 3. 1. 1. k5 k618 agecat agecat wc hc lwg ---------------------------------------------------------------------------\r.2377158 1.353254 .3851262 .2191235 .2815405 .3917663 1.097115 Then, we plot the predictions: graph twoway connected _linpr _quadpr _lininc, /// title(\"Comparing income specifications\") /// caption(\"Other variables at their means\") /// msym(Oh Dh) msiz(*1.5 *1) mcol(black black) lpat(solid dash) /// ytitle(\"Pr(In Labor Force)\") /// ylabel(0(.25)1, grid gmin gmax) Overlapping confidence intervals compared with discrete change\rAlthough the differences at higher incomes are suggestive and dramatic, the evidence for preferring the quadratic model is mixed. BIC provides positive support for the linear model, while AIC supports the quadratic model. The coefficient for income-squared is significant at the 0.046 level. The confidence intervals around the predictions at high income levels (not shown) are wide. Based on these results, we are not convinced to abandon our baseline model. ","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:6:5","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"6.6 (Advanced) Graphs with local means When plotting predictions over the range of a variable, you must decide where to hold the values of other variables. With the atmeans option in mgen, as the plotted variable changes, the other variables stay at the same global means. Following our previous discussion of local means, in this section we show you how to allow the values of the other variables to change as the variable being plotted changes. This requires using mtable with the over() option and moving predictions from the matrix that mtable returns. These steps require more data management than other parts of the book, but they can provide valuable insights into how robust your plot and conclusions are to assumptions about the levels of other variables. When demonstrating tables of predictions, we suggested caution before holding other variables at their global means because changing one variable while holding all other variables at the same values might not be realistic. For example, suppose that we included age in our model as a continuous variable ranging from 20 to 90. Plotting predictions as age changes while holding the number of young children constant is unrealistic because older respondents are unlikely to have any young children in the family. Note that we have the same problem if we used a subscribed instead of atmeans here; in that case, we would be including in our average predictions those cases for which the hypothetical value of age is implausible given the observed numbers of children. One alternative approach, which we will not explore further here, is to forgo using global means in favor of a set of representative values that are substantively plausible for all values of age (that is, a family with no children). In any event, if you are plotting predictions in regions of your data where it is impossible or very unlikely that observations will exist, the predictions might be misleading. You can determine whether global means are reasonable by exploring how other values affect the results. We will consider what happens when we use local means instead of global in generating the plot. To illustrate how this is done, we start with the example used above, where we plotted labor force participation by income. mgen, at(inc=(0(10)100)) atmeans stub(GLOBAL) predlabel(Global means) Predictions from: margins, at(inc=(0(10)100)) atmeans predict(pr)\rVariable Obs Unique Mean Min Max Label\r-------------------------------------------------------------------------------------\rGLOBALpr1 11 11 .4410442 .2887324 .8078035 Global means\rGLOBALll1 11 11 .2613509 -.1501807 .7207593 95% lower limit\rGLOBALul1 11 11 .6207375 .4265932 .9690264 95% upper limit\rGLOBALinc 11 11 50 0 100 Family income excluding wife's\r-------------------------------------------------------------------------------------\rSpecified values of covariates\r1. 3. 1. 1. k5 k618 agecat agecat wc hc lwg ---------------------------------------------------------------------------\r.2377158 1.353254 .3851262 .2191235 .2815405 .3917663 1.097115 Plotting the predictions produces th e following plot: These predictions were made by increasing family incomes from $0$ to $100,000$, holding wc, he, lwg, and other variables at their global means. This implies that those with no income have the same education and wages as those with $100,000$. As noted, before accepting this graph as a reasonable summary of the effect of family income on labor force participation, we want to determine how sensitive the predictions are to the values at which we held the other variables. In particular, what would happen if we held the other variables at levels more typical of those with a given income? Because inc is continuous, we cannot compute means for the non-income variables conditional on a single value of income, because these means might be based on very few observations. Instead, we begin by generating the variable inc10k, which divides income into groups of $10,000$: gen inc10k = trunc(i","date":"2024-01-30","objectID":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/:6:6","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_6 ：Models for binary outcomes——Interpretation","uri":"/chapter_6-models-for-binary-outcomesinterpretation-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"This chapter shows how to fit the binary regression model, how to test hypotheses, how to compute residuals and influence statistics, and how to calculate scalar measures of model fit. ","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"This chapter focuses on the two most often used models for binary outcomes, the binary logit and binary probit models, referred to jointly as the binary regression model (BRM). The BRM allows a researcher to explore how each explanatory variable affects the probability of the event occurring. The BRM is also the foundation from which more complex models for ordinal, nominal, and count models are derived. Ordinal and nominal regression models are equivalent to simultaneously fitting a set of BRMs. Although the link is less direct in count models, the Poisson distribution can be derived as the outcome of many binary trials. Consequently, the principles of fitting, testing, and interpreting binary models provide essential tools that are used in later chapters. Although each chapter of the book is largely self-contained, the two chapters on binary outcomes provide more detailed explanations than later chapters. **We begin the chapter by reviewing the mathematical structure of the binary regression model. We then examine statistical testing and fit. These discussions are brief, and much of it is intended either as a simple overview or as a review for those who are familiar with the models.**For a complete discussion, see Agresti (2013), Hosmer, Lemeshow, and Sturdivant (2013), or Long (1997). Although the material in this chapter is fundamental to working with these models, we anticipate that the more important contribution of this book will be in helping you interpret and present results. The issues involved in effective interpretation are extensive enough that we devote a chapter of its own to the topic, to which this chapter might be considered the prelude. ","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:0:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1 The statistical model There are three ways to derive the BRM, with each method leading to the same statistical model. First, a latent variable can be hypothesized along with a measurement model relating the latent variable to the observed binary outcome. Second, the model can be constructed as a probability model. Third, the model can be generated as a random utility or discrete choice model. This last approach is not considered in our review; see Long (1997, 155-156) for an introduction and Train (2009) for a detailed discussion. ","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:1:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.1 A latent-variable model Assume a latent or unobserved variable $y*$ ranging from $-\\infty $ to $\\infty $ that is related to the observed independent variables by the structural model $$y_i^*=\\mathbf{x}_i\\boldsymbol{\\beta}+\\varepsilon_i$$ where $i$ indicates the observation and $\\varepsilon$ is a random error. For a single independent variable, we can simplify the notation to $$y_{i}^{*}=\\alpha+\\beta x_{i}+\\varepsilon_{i}$$ These equations are identical to those for the linear regression model except —— and this is a big exception —— that the dependent variable is unobserved. The observed binary dependent variable has two values, typically coded as 0 for a negative outcome (that is, the event did not occur) and 1 for a positive outcome (that is, the event did occur). A measurement equation defines the link between the binary observed variable $y$ and the continuous latent variable $y*$. Cases with positive values of $y*$ are observed as $y$ = 1, while cases with negative or 0 values of $y*$ are observed as $y$ = 0. To give a concrete example, imagine a survey item that asks respondents if they agree or disagree with the proposition that “a working mother can establish just as warm and secure a relationship with her children as a mother who does not work”. Obviously, respondents will vary greatly in their opinions. Some people adamantly agree with the proposition, some adamantly disagree, and still others have weak opinions one way or the other. *Imagine an underlying continuum $y$ of feelings about this item, with each respondent having a specific value on the continuum.*Those respondents with positive values for y will answer “agree” to the survey question (y = 1) and those with negative values will “disagree” (y = 0). A shift in a respondent’s opinion might move her from agreeing strongly with the position to agreeing weakly with the position, which would not change the response we observe. Or, the respondent might move from weakly agreeing to weakly disagreeing, in which case, we would observe a change from y = 1 to y = 0. Consider a second example, which we use throughout this chapter. Let y = 1 if a woman is in the paid labor force and let y = 0 if she is not. The independent variables include age, number of children, education, family income, and expected wages. Not all women in the labor force (y = 1) are there with the same certainty. One woman might be close to leaving the labor force, whereas another woman could be firm in her decision.to work. In both cases, we observe y = 1. The idea of a latent y* is that an underlying propensity to work generates the observed state. Although we cannot directly observe the propensity, at some point a change in y* results in a change in what we observe, namely, whether the woman is in the labor force. Relationship between latent variable y* and P r(y = 1) for the BRM\rThe latent-variable model for a binary outcome with a single independent variable is shown in figure above. For a given value of $x$ $$\\Pr(y=1\\mid x)=\\Pr(y^{*}\u003e0\\mid x)$$ Substituting the structural model and rearranging terms $$\\Pr(y=1\\mid x)=\\Pr(\\varepsilon\u003e-[\\alpha+\\beta x]\\mid x)$$ which shows how the probability depends on the distribution of the error $\\varepsilon$. Two distributions of $\\varepsilon$ are commonly used, both with an assumed mean of 0. First, $\\varepsilon$ is assumed to be normal with $\\mathrm{Var}(\\varepsilon)=1$. This leads to the binary probit model in which (5.1) becomes $$\\Pr(y=1\\mid x)=\\int_{-\\infty}^{\\alpha+\\beta x}\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{t^{2}}{2}\\right)dt$$ Alternatively, $\\varepsilon$ is assumed to be distributed logistically with $\\mathrm{Var}(\\varepsilon)=\\pi^2/3$, leading to the binary logit model with the simpler equation The peculiar value assumed for $\\mathrm{Var}(\\varepsilon)$ in the logit model illustrates a basic point about the identification of models with latent outcomes. In the linear regression model, $\\mathrm{Var}(\\varepsilon)$ can be estimated because y is","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:1:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.2 A nonlinear probability model Can all binary dependent variables be conceptualized as observed manifestations of some underlying latent propensity? Although philosophically interesting, perhaps, the question is of little practical importance, because the BRM can also be derived without appealing to a latent variable. This is done by specifying a nonlinear model relating the $x’s$ to the probability of an event. Following Theil (1970), the logit model can be derived by constructing a model in which the predicted $\\Pr\\left(y=1\\mid\\mathbf{x}\\right)$ is forced to be within the range 0 to 1. For example, in the linear probability model. $$\\Pr\\left(y=1\\mid\\mathbf{x}\\right)=\\mathbf{x}\\boldsymbol{\\beta}+\\varepsilon $$ the predicted probabilities can be greater than 1 and less than 0. To constrain the predictions to the range 0 to 1, we first transform the probability into the odds, $$\\Omega\\left(\\mathbf{x}\\right)=\\frac{\\Pr\\left(y=1\\mid\\mathbf{x}\\right)}{\\Pr\\left(y=0\\mid\\mathbf{x}\\right)}=\\frac{\\Pr\\left(y=1\\mid\\mathbf{x}\\right)}{1-\\Pr\\left(y=1\\mid\\mathbf{x}\\right)}$$ which indicate how often something happens $(y = 1)$ relative to how often it does not happen $(y = 0)$. The odds range from 0 when $\\mathrm{Pr}\\left(y=1\\mid\\mathbf{x}\\right)=0$ to ∞ when $\\mathrm{Pr}\\left(y=1\\mid\\mathbf{x}\\right)=1$. The log of the odds, often referred to as the logit, ranges from $-∞$ to $∞$. This range suggests a model that is linear in the logit: $$\\ln\\Omega\\left(\\mathbf{x}\\right)=\\mathbf{x}\\beta $$ This equation is equivalent to the logit model (5.2). Interpretation of this form of the model often focuses on factor changes in the odds, which are discussed below. Other binary regression models are created by choosing functions of $x\\beta$ that range from 0 to 1. Cumulative distribution functions (CDFs) have this property and readily provide several examples. For example, the CDF for the standard normal distribution results in the probit model. ","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:1:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2 Estimation using logit and probit commands Logit and probit models can be fit with the following commands and their basic options: logit depvar [indepvars] [if] [in] [weight] [, noconstant asis or vce(vcetype)] probit depvar [indepvars] [if] [in] [weight] [, noconstant asis vce(vcetype)] Variable lists depvar is the dependent variable, indepvars is a list of independent variables. If indepvars is not included, Stata fits a model with only an intercept. Warning about dependent variable. In binary models, all nonmissing, nonzero values of depvar are classified as positive outcomes, traditionally referred to as successes. Only zero values are considered negative outcomes, which are referred to as failures. Because negative values are nonzero, they are considered to be positive outcomes. To avoid possible confusion, we recommend that you explicitly create a 0/1 variable for use as depvar. Specifying the estimation sample if and in qualifiers. can be used to restrict the estimation sample. For example, if you want to fit a logit model for only women who went to college, as indicated by the variable wc, you could specify logit lfp k5 k618 age he lwg if wc==1. Listwise deletion. Stata excludes cases in which there are missing values for any of the variables in the model. Accordingly, if two models are fit using the same dataset but have different independent variables, the models may have different samples. We recommend that you use mark and markout (discussed in section 3.1.6) to explicitly remove cases with missing data. Weights and complex samples Both logit and probit can be used with fweight, pweight, and iweight. Survey estimation can be done using svy: logit or svy: probit. See section 3.1.7 for details. Options noconstant specifies that the model not have a constant term asis specifies that estimates for variables that have perfect prediction should be included in the results table. For details, see page 197. or (for logit only) reports the odds ratios defined as $\\mathrm{exp}(\\widehat{\\beta})$. Standard errors and confidence intervals are similarly transformed. Alternatively, our listcoef command can be used. vce(vcetype) specifies the type of standard errors to be computed. See section 3.1.9 for details. ","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:2:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.1 Example of logit model Our example is from Mroz’s (1987) study of the labor force participation of women, using data from the 1976 Panel Study of Income Dynamics. The sample consists of 753 white, married women between the ages of 30 and 60 years. The dependent variable lfp equals 1 if a woman is in the labor force and equals 0 otherwise. We use codebook, compact to list information about the variables we plan to include in our model: use binlfp4, clear codebook lfp k5 k618 agecat wc hc lwg inc, compact Variable Obs Unique Mean Min Max Label\r-------------------------------------------------------------------------------------------\rlfp 753 2 .5683931 0 1 In paid labor force?\rk5 753 4 .2377158 0 3 # kids \u003c 6\rk618 753 9 1.353254 0 8 # kids 6-18\ragecat 753 3 1.823373 1 3 Wife's age group\rwc 753 2 .2815405 0 1 Wife attended college?\rhc 753 2 .3917663 0 1 Husband attended college?\rlwg 753 676 1.097115 -2.054124 3.218876 Log of wife's estimated wages\rinc 753 621 20.12897 -.0290001 96 Family income excluding wife's\r-------------------------------------------------------------------------------------------\rAlthough the meaning of most of the variables is clear from the label, lwg is the log of an estimate of what the wife’s wages would be if she was employed, given her other characteristics. Because the outcome is labor force participation, it is important to include what the wife might be expected to earn if she was employed. Following the same reasoning, inc is family income excluding whatever the wife earns; this is, therefore, a measure of what the family income would be if the wife was not employed. We consider interpretation later, but it may also help bearing in mind that the data are from 1976. In the United States, prices have risen by just over a factor of 4 between 1976 and 2014, so a change in income of $5,000$ in 1974 is similar to a change in income of $20,000$ in 2014. Because agecat is ordinal, we use tabulate to examine the distribution among the age groups: tabulate agecat, missing Next, we want to fit the logit model. To be consistent with the naming practice Stata will use, we use 2.agecat and 3.agecat to refer to dummy variables indicating whether agecat==2 and whether agecat==3, respectively. By fitting the logit model, $$\\begin{aligned}\\Pr(1\\mathbf{f}\\mathbf{p}=1)\u0026=F(\\beta_0+\\beta_\\mathbf{kf}\\mathbf{k}\\mathbf{5}+\\beta_\\text{kG19}\\mathbf{k}\\text{618}+\\beta_\\text{2 agecat}2.\\text{agecat}\\\u0026+\\beta_\\text{3.agecat}3.\\text{agecat}+\\beta_\\text{we}\\mathbf{w}\\mathbf{c}+\\beta_\\text{he}\\mathbf{hc}+\\beta_\\text{lwg}1\\text{wg}+\\beta_\\text{lac}\\mathbf{inc})\\end{aligned}$$ we obtain the following results: logit lfp k5 k618 i.agecat i.wc i.hc lwg inc Iteration 0: Log likelihood = -514.8732 Iteration 1: Log likelihood = -453.10297 Iteration 2: Log likelihood = -452.72408 Iteration 3: Log likelihood = -452.72367 Iteration 4: Log likelihood = -452.72367 Logistic regression Number of obs = 753\rLR chi2(8) = 124.30\rProb \u003e chi2 = 0.0000\rLog likelihood = -452.72367 Pseudo R2 = 0.1207\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5 | -1.39 0.19 -7.25 0.000 -1.77 -1.02\rk618 | -0.07 0.07 -0.96 0.336 -0.20 0.07\r|\ragecat |\r40-49 | -0.63 0.21 -3.00 0.003 -1.04 -0.22\r50+ | -1.28 0.26 -4.92 0.000 -1.79 -0.77\r|\rwc |\rcollege | 0.80 0.23 3.48 0.001 0.35 1.25\r|\rhc |\rcollege | 0.14 0.21 0.66 0.508 -0.27 0.54\rlwg | 0.61 0.15 4.04 0.000 0.31 0.91\rinc | -0.04 0.01 -4.24 0.000 -0.05 -0.02\r_cons | 1.01 0.29 3.54 0.000 0.45 1.57\r------------------------------------------------------------------------------\rThe information in the header and table of coefficients is in the same form as discussed in chapter 3. The iteration log begins with Iteration 0: log likelihood = -514.8732 and ends with Iteration 4: log likelihood = -452.72367, with the intermediate iterations showing the steps take","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:2:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.2 comparing logit and probit Above, we fit the model with logit, but we could have used probit instead. An easy way to show how the results would differ is to put them side by side in a single table. We can do this by using estimates table (see [R] estimates table), which is more generally useful for combining results from multiple models into one table. After fitting the logit model, we use estimates store to save the estimates with the name Mlogit: logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog estimates store Mlogit We then fit a probit model and store the results: probit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog estimates store Mprobit Next, we combine the results with estimates table. Option b() sets the format for displaying the coefficients. b(%9.3f) lists the estimates in nine columns with five decimal places. Option t requests test statistics for individual coefficients—either z tests or f tests depending on the model that was fit. Varlable labels uses variable labels rather than variable names to label coefficients (the option was named label before Stata 13), with varwidth 0 indicating how many columns should be used for the labels. Variable names and value labels are used with factor variables. estimates table Mlogit Mprobit, b(%9.3f) t varlabel varwidth(30) The estimates table output labels the test statistic as t regardless of whether z tests or t are used. --------------------------------------------------------\rVariable | Mlogit Mprobit -------------------------------+------------------------\r# kids \u003c 6 | -1.392 -0.840 | -7.25 -7.50 # kids 6-18 | -0.066 -0.041 | -0.96 -1.01 | Wife's age group | 40-49 | -0.627 -0.382 | -3.00 -3.06 50+ | -1.279 -0.780 | -4.92 -5.00 | Wife attended college? | college | 0.798 0.482 | 3.48 3.55 | Husband attended college? | college | 0.136 0.074 | 0.66 0.60 Log of wife's estimated wages | 0.610 0.371 | 4.04 4.21 Family income excluding wife's | -0.035 -0.021 | -4.24 -4.37 Constant | 1.014 0.622 | 3.54 3.69 --------------------------------------------------------\rLegend: b/t\rComparing results, the estimated logit coefficients are about 1.7 times larger than the probit estimates. For example, the ratios for k5 and inc are 1.66. This illustrates how the magnitudes of the coefficients are affected by the assumed $\\operatorname{Var}(\\varepsilon)$. The ratio of estimates for hc is larger because of the large standard errors for these estimates. Values of the z tests for logit and probit are quite similar because they are not affected by the assumed $\\operatorname{Var}(\\varepsilon)$, but they are not exactly the same because the models assume different distributions of the errors. ","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:2:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.3 (Advanced) Observations predicted perfectly We mark this section as advanced because if you work with large samples where your outcome variable is not rare, you may never encounter perfect prediction. If you have smaller samples with binary predictors, you may encounter it regularly. Maximum likelihood estimation is not possible when the dependent variable does not vary within one of the categories of an independent variable. This is referred to as perfect prediction or quasiincomplete separation. To illustrate this, suppose that we are treating k5 as categorical rather than continuous in our model of labor force participation. To do this, we regress lfp on indicator variables for the number of children.’* Variable k5_l equals 1 if a person had one young child and equals 0 otherwise, and so on for k5_2 and k5_3. Only three respondents had three young children, and none of these women were in the paid labor force: tab lfp k5 In paid |\rlabor | # kids \u003c 6\rforce? | 0 1 2 3 | Total\r-----------+--------------------------------------------+----------\rnot in LF | 231 72 19 3 | 325 in LF | 375 46 7 0 | 428 -----------+--------------------------------------------+----------\rTotal | 606 118 26 3 | 753 We find that lfp is 0 every time k5_3 is 0. A logit model predicting lfp with the binary variables k5_l, k5_2, and k5_3 (with no children being the excluded category) cannot be estimated because the observed coefficient for k5_3 is effectively infinite. Think of it this way: The observed odds of being in the labor force for those with no children is 375/231 = 1.62, while the observed odds for those with three young children is 0/3 = 0. The odds ratio is 0/1.62 = 0. For the odds ratios to be 0, $\\widehat{\\beta}{k5.3}$ must be negative infinity. As the likelihood is maximized, estimates of $\\beta{k5.3}$ get more and more negative until Stata realizes that the parameter cannot be estimated and reports the following: logit lfp k5_1 k5_2 k5_3, or nolog note: k5_3 != 0 predicts failure perfectly;\rk5_3 omitted and 3 obs not used.\rLogistic regression Number of obs = 750\rLR chi2(2) = 31.05\rProb \u003e chi2 = 0.0000\rLog likelihood = -496.82164 Pseudo R2 = 0.0303\r------------------------------------------------------------------------------\rlfp | Odds ratio Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5_1 | 0.39 0.08 -4.52 0.000 0.26 0.59\rk5_2 | 0.23 0.10 -3.30 0.001 0.09 0.55\rk5_3 | 1.00 (omitted)\r_cons | 1.62 0.14 5.79 0.000 1.38 1.91\r------------------------------------------------------------------------------\rNote: _cons estimates baseline odds.\rnote: k5_3 != 0 predicts failure perfectly;\rk5_3 omitted and 3 obs not used.\rThe message can be interpreted as follows. If someone in the sample has three young children (that is, if k5_3 is not equal to 0), then she is never in the labor force (that is, lfp equals 0), which is considered a “failure” in the terminology of the model. At this point, Stata removes the three cases where k5_3 is 0 and also removes k5_3 from the model. In the output, the coefficient for k5_3 is shown as 1 followed by (omitted). If you use the asis option, Stata keeps k5_3 and the observations in the model and shows the estimate at convergence. logit lfp k5_1 k5_2 k5_3, or asis nolog Logistic regression Number of obs = 753\rLR chi2(2) = 36.10\rProb \u003e chi2 = 0.0000\rLog likelihood = -496.82164 Pseudo R2 = 0.0351\r------------------------------------------------------------------------------\rlfp | Odds ratio Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5_1 | 0.39 0.08 -4.52 0.000 0.26 0.59\rk5_2 | 0.23 0.10 -3.30 0.001 0.09 0.55\rk5_3 | 0.00 . . . . .\r_cons | 1.62 0.14 5.79 0.000 1.38 1.91\r------------------------------------------------------------------------------\rNote: _cons estimates baseline odds.\rNote: 3 failures and 0 successes completely determined.\rThe estimated odds ratio of 4.43e-","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:2:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3 Hypothesis testing Hypothesis tests of regression coefficients can be conducted with the $z$ statistics from the estimation output, using the test command for Wald tests of simple and complex hypotheses, and with the lrtest command for the corresponding likelihood-ratio (LR) tests. We discuss using each to test hypotheses involving a single coefficient and then show how test and lrtest can be used for hypotheses involving multiple coefficients. See section 3.2 for general information on hypothesis testing using Stata. While often in this book, we show how to conduct both Wald and LR tests of the same hypothesis, in practice, you would want to test a hypothesis with only one type of test. ","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:3:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3.1 Testing individual coefficients Most often, we are interested in testing the hypothesis $H_{0}\\colon\\beta_{k}=0$, which corresponds to results in column z in the output from logit and probit. For example, consider the results for variables k5 and wc from the logit output generated in section 5.2.1: logit lfp k5 i.wc i.hc k618 i.agecat lwg inc, nolog Logistic regression Number of obs = 753\rLR chi2(8) = 124.30\rProb \u003e chi2 = 0.0000\rLog likelihood = -452.72367 Pseudo R2 = 0.1207\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5 | -1.39 0.19 -7.25 0.000 -1.77 -1.02\r|\rwc |\rcollege | 0.80 0.23 3.48 0.001 0.35 1.25\r|\rhc |\rcollege | 0.14 0.21 0.66 0.508 -0.27 0.54\rk618 | -0.07 0.07 -0.96 0.336 -0.20 0.07\r|\ragecat |\r40-49 | -0.63 0.21 -3.00 0.003 -1.04 -0.22\r50+ | -1.28 0.26 -4.92 0.000 -1.79 -0.77\r|\rlwg | 0.61 0.15 4.04 0.000 0.31 0.91\rinc | -0.04 0.01 -4.24 0.000 -0.05 -0.02\r_cons | 1.01 0.29 3.54 0.000 0.45 1.57\r------------------------------------------------------------------------------\rWe conclude the following: Having young children has a significant effect on the probability of being in the labor force $z = -7.25, p \u003c 0.01$ for a two-tailed test. The effect of the wife attending college is significant at the 0.01 level 3.1.1 Testing single coefficients using test The $z$ test included in the output of estimation commands is a Wald test, which can also be computed as a chi-squared test by using test. For example, to test $H_{0}\\colon\\beta_{\\mathrm{k5}}=0$ test k5 ( 1) [lfp]k5 = 0\rchi2( 1) = 52.57\rProb \u003e chi2 = 0.0000\rStata refers to the coefficient for k5 as [lfp ]k5 because the dependent variable is lfp . We conclude the following: The effect of having young children on entering the labor force is significant at the 0.01 level $(\\chi^{2}=52.57,\\mathrm{~df}=1,p\u003c0.01)$ The value of the $z$ test is identical to the square root of the corresponding chi-squared test with 1 degree of freedom. For example, using display as a calculator display sqrt(52.57) This corresponds to —7.25 from the logit output shown above. Aside: Using returns . Using returned results is a better way to show this. When you use the test command, the chi-squared statistic is returned as the scalar r(chi2). The command display sqrt(r(chi2)) then provides the same result more elegantly and with slightly more accuracy. 3.1.2 Testing single coefficients using Irtest An LR test is computed by comparing the log likelihood from a full model with that of a restricted model. To test a single coefficient, we begin by fitting the full model and storing the results: logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog estimates store Mfull Then, we fit the model without k5 and store the results: logit lfp k618 i.agecat i.wc i.hc lwg inc, nolog estimates store Mnok5 Next, we run Irtest : lrtest Mfull Mnok5 Likelihood-ratio test\rAssumption: Mnok5 nested within Mfull\rLR chi2(1) = 62.55\rProb \u003e chi2 = 0.0000\rThe LR test shows the following: The effect of having young children is significant at the 0.01 level $(\\operatorname{LR}\\chi^2=62.55,\\mathrm{~df}=1,p\u003c0.01)$. If you want to run an LR test comparing a model stored by using “estimates store” with the last model fit, you can use a single period to represent the last model. For example, instead of “lrtest Mfull Mnok5,” you could use “lrtest Mfull .,” where “.” represents the last model. ","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:3:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3.2 Testing mutiple coefficiencts You might want to test complex hypotheses that involve more than one coefficient. For example, we have two variables that reflect education in the family: “he” and “wc.” The conclusion that education has (or does not have) a significant effect on labor force participation cannot be based on separate tests of single coefficients. A joint hypothesis can also be tested using either “test” or “lrtest.” Similarly, to test the effect of “agecat,” requires testing the coefficients of all indicator variables. 3.2.1 Testing multiple coefficients using test To test that the effect of the wife attending college and of the husband attending college on labor force participation are simultaneously equal to 0 (that is, $H_{0}\\colon\\beta_{\\mathbf{wc}}=\\beta_{\\mathbf{hc}}=0$), we fit the full model and test the two coefficients. We must use “i.he” and “i.wc,” not “hc” and “wc.” estimates restore Mfull test 1.hc 1.wc ( 1) [lfp]1.hc = 0\r( 2) [lfp]1.wc = 0\rchi2( 2) = 17.83\rProb \u003e chi2 = 0.0001\rWe reject the hypothesis that the effects of the husband’s and the wife’s education are simultaneously equal to 0 $(\\chi^{2}=17.83,\\mathrm{df}=2,p\u003c0.01)$. test can also be used to test the equality of coefficients. For example, to test that the effect of the wife attending college on labor force participation is equal to the effect of the husband attending college (that is, Ha: βVC = βhc), we type: test 1.hc = 1.wc ( 1) - [lfp]1.wc + [lfp]1.hc = 0\rchi2( 1) = 3.24\rProb \u003e chi2 = 0.0719\rHere, the test translated $\\beta_{\\mathbf{uc}}=\\beta_{\\mathbf{hc}}$ into the equivalent expression $-\\beta_{\\mathbf{wc}}+\\beta_{\\mathbf{hc}}=0$. The null hypothesis that the effects of husband’s and wife’s education are equal is marginally significant. We might conclude the following: There is weak evidence that the effects of husband’s and wife’s education are equal $(\\chi^{2}=3.24,\\mathrm{df}=1,p=0.072)$ We can test that the effect of agecat is 0 by specifying the two indicator variables that were created from the factor variable i.agecat: test 2.agecat 3.agecat ( 1) [lfp]2.agecat = 0\r( 2) [lfp]3.agecat = 0\rchi2( 2) = 24.27\rProb \u003e chi2 = 0.0000\rTo avoid having to specify each of the automatically created indicators, we can use testparm: testparm i.agecat ( 1) [lfp]2.agecat = 0\r( 2) [lfp]3.agecat = 0\rchi2( 2) = 24.27\rProb \u003e chi2 = 0.0000\rThe advantage of “testparm” is that it works no matter how many indicator variables have been created by “i.catvar.” 3.2.2 Testing multiple coefficients using Irtest To compute an LR test of multiple coefficients, we start by fitting the full model and saving the results with “estimates store estname.” To test the hypothesis that the effect of the wife attending college and of the husband attending college on labor force participation are both equal to 0 (that is, $H_{0}\\colon\\beta_{\\mathbf{vc}}=\\beta_{\\mathbf{hc}}=0)$, we fit the model that excludes these two variables and then run lrtest: logit lfp k5 k618 i.agecat lwg inc, nolog estimates store Mnowchc lrtest Mfull Mnowchc Likelihood-ratio test\rAssumption: Mnowchc nested within Mfull\rLR chi2(2) = 18.68\rProb \u003e chi2 = 0.0001\rWe conclude the following: The hypothesis that the effects of the husband’s and the wife’s education are simultaneously equal to 0 can be rejected at the 0.01 level. $(\\mathrm{LR}\\chi^{2}=18.68,\\mathrm{df}=2,p\u003c0.01)$ This logic can be extended to exclude other variables. Say that we wish to test the hypothesis that the effects of all the independent variables are simultaneously 0. We do not need to fit the full model again because the results are still saved from our use of “estimates store Mfull” above. We fit the model with no independent variables and then run “lrtest”: logit lfp, nolog estimates store Mconstant lrtest Mfull Mconstant Logistic regression Number of obs = 753\rLR chi2(0) = 0.00\rProb \u003e chi2 = .\rLog likelihood = -514.8732 Pseudo R2 = 0.0000\r------------------------------------------------------------------------------\rlfp |","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:3:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3.3 Comparing LR and Wald tests Although the LR and Wald tests are asymptotically equivalent, their values differ in finite samples. In our example, Statistical theory is unclear on whether the LR or Wald test is to be preferred in models for categorical outcomes, although many statisticians, ourselves included, prefer the LR test. The choice of which test to use is often determined by convenience, personal preference, and convention within an area of research. Recall from chapter 3 that if robust standard errors or svy estimation is used, only Wald tests are available. For Wald tests of a single coefficient, some disciplines prefer to use chi-squared tests, while others prefer the corresponding $z$ test. ","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:3:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4 Predicted probabilities, residuals, and influential observations For a given set of values of the independent variables, the predicted probability can be computed from the estimated coefficients. $$\\mathrm{Logit:~}\\widehat{\\Pr}\\left(y=1\\mid\\mathbf{x}\\right)=\\Lambda\\left(\\mathbf{x}\\widehat{\\beta}\\right)\\quad\\mathrm{Probit:~}\\widehat{\\Pr}\\left(y=1\\mid\\mathbf{x}\\right)=\\Phi\\left(\\mathbf{x}\\widehat{\\beta}\\right)$$ where $\\Lambda$ is the CDF for the logistic distribution with variance $π^2/3$, and $\\Phi$ is the CDF for the normal distribution with variance 1. For any set of values of the independent variables, whether occurring in the sample or not, the predicted probability can be computed. In this section, we consider predictions for each observation in the dataset along with residuals and measures of influence based on these predictions. In sections 6.2-6.6, we use predicted probabilities for interpretation. ","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:4:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4.1 Predicted probabilities using predict After running logit or probit, predict newvar [if] [in] computes the predicted probability of a positive outcome for each observation, given the observed values on the independent variables, and saves them in the new variable newvar. Predictions are computed for all cases in memory that do not have missing values for any variables in the model, regardless of whether “if” and “in” were used to restrict the estimation sample. For example, if you fit “logit lfp k5 i.age catif wc==l,” only 212 cases are used when fitting the model. But “predict newvar” computes predictions for all 753 cases in the dataset. If you want predictions only for the estimation sample, you can use the command “predict newvar if e(sample)==1,” where “e(sample)” is the variable created by “logit” or “probit” to indicate whether a case was used when fitting the model. We can use predict to examine the range of predicted probabilities from our model. For example, we start by computing the predictions: logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog estimates store logit predict prlogit predict prlogit\r(option pr assumed; Pr(lfp))\rBecause we did not specify which quantity to predict, the default option “pr” for the probability of a positive outcome was assumed, and the new variable “prlogit” was given the default variable label “Pr(lfp).” In general, and especially when fitting multiple models, we suggest adding your own variable label to the prediction to avoid having multiple variables with the same label. Here we add a variable label and compute summary statistics: label var prlogit \"Logit: Pr(lfp | X)\" codebook prlogit, compact Variable Obs Unique Mean Min Max Label\r----------------------------------------------------------------------------\rprlogit 753 753 .5683931 .0135618 .9512301 Logit: Pr(lfp | X)\r----------------------------------------------------------------------------\rThe predicted probabilities range from 0.014 to 0.951 with a mean of 0.568. We use “dotplot” to examine the distribution of predictions. dotplot prlogit, ylabel(0(.2)1, grid gmin gmax) In this example of “dotplot,” the option “ylabel(0(.2)1, grid gmin gmax)” sets the range of the axis from 0 to 1 with grid lines in increments of 0.2, where the “gmin” and “gmax” suboptions add lines at the minimum and maximum values. Even if the actual range of the predictions is smaller than 0 to 1, we find it useful to see the distribution relative to the entire potential range of probabilities. Examining the distribution of predictions is a valuable first step after fitting your model to get a general sense of your data and possibly detect problems. Our plot shows that there are individuals with predicted probabilities that span almost the entire range from 0 to 1, with roughly two-thirds of the observations between 0.40 and 0.80. The large range reflects that our sample contains individuals with both very large and very small probabilities of labor force participation. Examining the characteristics of these individuals could be useful for guiding later analysis. If the distribution was bimodal, it would suggest the importance of a binary independent variable or the possibility of two types of individuals, perhaps with shared characteristics on many variables. 4.1.1 Comparing logit to probit predictions predict can also be used to show that the predictions from logit and probit models are nearly identical. Although the two models make different assumptions about the distribution of $\\varepsilon,$, these differences are absorbed in the relative magnitudes of the estimated coefficients. To see this, we begin by fitting comparable logit and probit models and computing their predicted probabilities. First, we fit the logit model, store the estimates, and compute predictions: use binlfp4, clear logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog estimates store logit predict prlogit Next, we fit the probit model: probit lfp k5 k618 i.agecat i.wc i.hc lwg in","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:4:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4.2 Residuals and influential observations using predict After you have fit your baseline model, we suggest that you examine residuals and look for influential observations before beginning postestimation analyses for interpretation. Residuals and influential observations can help you discover problems with your data and sometimes suggest problems in your model specification. Residuals are the difference between a model’s predicted and observed outcomes for each observation in the sample. Cases that have large residuals are known as outliers. When an observation has a large effect on the estimated parameters, it is said to be influential. We illustrate these ideas with the linear regression model in figure below. The distinction between an outlier and an influential observation\rNot all outliers are influential, as the figure shows by using simulated data for the linear regression of y on x. The residual for an observation is its vertical distance from the regression line. In the top panel, the observation highlighted with a solid circle has a large residual and is considered an outlier. Even so, it is not very influential on the slope of the regression line. That is, the slope of the regression line is very close to what it would be if the highlighted observation was dropped from the sample and the model was fit again. In the bottom panel, the only observation whose value has changed is the highlighted observation marked with a square. The residual for this observation is small, but the observation is very influential; its presence is entirely responsible for the slope of the new regression line being positive instead of negative. Building on the analysis of residuals and influence in the linear regression model (see Fox [2008] and Weisberg [2005, chap. 5]), Pregibon (1981) extended these ideas to the BRM. 4.2.1 Residuals The predicted probability for a given set of independent variables is $$\\pi_{i}=\\Pr\\left(y_{i}=1\\mid\\mathbf{x}_{i}\\right)$$ The deviations $y_i-\\pi_i$ are heteroskedastic because the variance depends on the probability $\\pi_i$ of a positive outcome: $$\\mathrm{Var}\\left(y_{i}-\\pi_{i}\\mid\\mathbf{x}{i}\\right)=\\pi{i}\\left(1-\\pi_{i}\\right)$$ The variance is greatest when $π_i = 0.5$ and decreases as $π_i$ approaches 0 or 1. That is, a fair coin is the most unpredictable, with a variance of $0.5 (1 - 0.5) = 0.25$. A coin that has a very small probability of landing head up (or tail up) has a small variance, for example, $0.01(1 — 0.01) = 0.0099$. The Pearson residual divides the residual $y-\\widehat{\\pi}$ by its standard deviation: $$r_i=\\frac{y_i-\\widehat{\\pi}_i}{\\sqrt{\\widehat{\\pi}_i\\left(1-\\widehat{\\pi}_i\\right)}}$$ Large values of $r$ suggest a failure of the model to fit a given observation Pregibon (1981) showed that the variance of $r$ is not 1 because $\\operatorname{Var}(y_{i}-\\hat{\\pi}_{i})$ is not exactly equal to the estimate $\\widehat{\\pi}_i\\left(1-\\widehat{\\pi}_i\\right)$ He proposed the standardized Pearson residual $$r_{i}^{\\mathrm{std}}=\\frac{r_{i}}{\\sqrt{1-h_{\\mathrm{ii}}}}$$ where Although $r^{\\mathrm{std}}$ is preferred over $r$ because of its constant variance, we find that the two residuals are often similar in practice. However, because $r$ is simple to compute in Stata, you should use this measure. 4.2.2 Example An index plot is an easy way to examine residuals by plotting them against the observation number. Standardized residuals are computed by specifying the rstandard option with predict. For example, logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog predict rstd, rstandard label var rstd \"Standardized Residual\" sort inc, stable generate index = _n label var index \"Observation Number\" After computing the standardized residuals that are saved in the new variable rstd, we sorted the cases by inc so that observations are ordered from lowest to highest income in the plot that follows. The next line creates the variable index equal to the observation’s row number in the dataset, where _n on t","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:4:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4.3 Least likely observations The command leastlikely (Freese 2002) will list the least likely observations. For example, for a binary model, leastlikely will list both the observations with the smallest $\\widehat{\\Pr}\\left(y=0\\mid\\mathbf{x}\\right)$ when y$ = 0$ and the observations with smallest $\\widehat{\\Pr}\\left(y=1\\mid\\mathbf{x}\\right)$ when $y = 1$. In addition to being used after logit and probit, leastlikely can be used after most binary models in which the option pr for predict generates the predicted probabilities of a positive outcome (for example, cloglog, scobit, and hetprobit) and after many models for ordinal or nominal outcomes in which the option outcome(#) for predict generates the predicted probability of outcome# (for example, ologit, oprobit, mlogit, mprobit, and slogit). leastlikely is not appropriate for models in which the probabilities produced by predict are probabilities within groups or panels (for example, such as clogit, nlogit, and asmprobit). Syntax The syntax for leastlikely is as follows: leastlikely [varlist] [if] [in] [, n(#) generate(varname) [nodisplay | display] nolabel noobs] where varlist contains any variables whose values are to be listed in addition to the observation numbers and probabilities. Options n(#) specifies the number of observations to be listed for each level of the outcome variable. The default is n(5). For multiple observations with identical predicted probabilities, all observations will be listed. generate(varname) specifies that the probabilities of observing the outcome that was observed be stored in varname. Options controlling the list of values [no]display forces the format into display or tabular (nodisplay) format. If you do not specify one of these options, Stata chooses the one it decides will be most readable. nolabel causes numeric values rather than labels to be displayed noobs suppresses printing of the observation numbers Example We can use leastlikely to identify the least likely observations from our model of labor force participation and to list the values of the variables k5, k618, and wc for these observations. Based on our model logit lfp k5 k618 i.age cat i.wc i.hc lwg inc, use binlfp4, clear logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog leastlikely k5 k618 wc agecat Outcome: 0 (not in LF)\r+------------------------------------------+\r| Prob k5 k618 wc agecat |\r|------------------------------------------|\r60. | .14033509 0 1 college 30-39 |\r172. | .1291429 0 2 college 30-39 |\r221. | .17652958 0 2 college 30-39 |\r252. | .11741232 0 0 college 30-39 |\r262. | .16508454 0 3 college 30-39 |\r+------------------------------------------+\rOutcome: 1 (in LF)\r+------------------------------------------+\r| Prob k5 k618 wc agecat |\r|------------------------------------------|\r427. | .17668873 0 5 no 50+ |\r496. | .18095707 1 0 no 50+ |\r534. | .10392637 1 2 no 30-39 |\r635. | .11522446 1 3 college 30-39 |\r662. | .11338183 2 0 no 30-39 |\r+------------------------------------------+\rAmong women not in the labor force, we find that the lowest predicted probability of not being in the labor force occurs for those who have young children, attended college, and are younger. For women in the labor force with the lowest probabilities of being in the labor force, all but one individual have young children, most have more than one older child, and one attends college. This suggests further consideration of how labor force participation is affected by having children in the family. ","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:4:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"5 Measures of fit As discussed in chapter 3, a scalar measure of fit can be useful when comparing competing models. Information criteria such as the Bayesian information criterion (BIC) and Akaike’s information criterion (AIC) can be used to select among models and are often very useful. There are many pseudo-R^2 statistics that are inspired by the coefficient of determination R^2 in the linear regression model, but we find them less informative, even though they are often used. Finally, the Hosmer Lemeshow statistic is a popular way to assess the overall fit of the model, but we do not recommend it for reasons we explain below. ","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:5:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"5.1 Information criteria To illustrate scalar measures of fit and information criteria, we consider two models. Model 1 (M1) contains our original specification of independent variables k5, k618, agecat, wc, he, lwg, and inc. Model 2 (M2) drops variables k618, he, and lwg, and adds income-squared with c. in c##c. The models are fit, and estimates are stored. quietly logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog estimates store model1 quietly logit lfp k5 i.agecat i.wc c.inc##c.inc, nolog estimates store model2 We can list the estimates by using the option stats(N BIC AIC R2_p) to include the sample size, BIC, AIC, and pseudo-R^2 that are normally included in models fit with maximum likelihood. Recall that the formulas for these statistics are given in section 3.3.2. estimates table model1 model2, b(%9.3f) p(%9.3f) stats(N bic aic r2_p) --------------------------------------\rVariable | model1 model2 -------------+------------------------\rk5 | -1.392 -1.369 | 0.000 0.000 k618 | -0.066 | 0.336 |\ragecat |\r40-49 | -0.627 -0.512 | 0.003 0.011 50+ | -1.279 -1.137 | 0.000 0.000 |\rwc |\rcollege | 0.798 1.119 | 0.001 0.000 |\rhc |\r1 | 0.136 | 0.508 |\rlwg | 0.610 | 0.000 inc | -0.035 -0.060 | 0.000 0.001 |\rc.inc#c.inc | 0.000 | 0.083 |\r_cons | 1.014 1.743 | 0.000 0.000 -------------+------------------------\rN | 753 753 bic | 965.064 968.574 aic | 923.447 936.206 r2_p | 0.121 0.104 --------------------------------------\rLegend: b/p\rModel 2 (M2) modifies Model 1 (M1) by removing one statistically significant variable and two non-significant variables from M1, while adding a variable that is significant at the 0.10 level. Since the models are not nested, they cannot be compared with a likelihood ratio (LR) test, but we can use the BIC and AIC statistics. In this example, both the BIC and AIC statistics are smaller for M1, providing support for that model. Following Raftery’s (1995) guidelines, we can say that there is positive (neither weak nor strong) support for M1. You can obtain information criteria in two other ways. You can use the ic option to fitstat, which shows multiple versions of the AIC and BIC measures (see section 3.3 for the formula for these measures): estimates restore model1 quietly fitstat, save ic estimates restore model2 fitstat, diff ic | Current Saved Difference -------------------------+--------------------------------------\rAIC | AIC | 936.206 923.447 12.759 (divided by N) | 1.243 1.226 0.017 -------------------------+--------------------------------------\rBIC | BIC(df=7/9/-2) | 968.574 965.064 3.511 BIC (based on deviance) | -4019.347 -4022.857 3.511 BIC' (based on LRX2) | -67.796 -71.307 3.511 Difference of 3.511 in BIC provides positive support for saved model.\rThe results match those from the estimates table output and even tell you the strength of support for the preferred model. You can also use the estat ic command: logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog estat ic Akaike's information criterion and Bayesian information criterion\r-----------------------------------------------------------------------------\rModel | N ll(null) ll(model) df AIC BIC\r-------------+---------------------------------------------------------------\r. | 753 -514.8732 -452.7237 9 923.4473 965.0639\r-----------------------------------------------------------------------------\rNote: BIC uses N = number of observations. See [R] IC note.\rlogit lfp k5 i.agecat i.wc c.inc##c.inc, nolog estat ic Akaike's information criterion and Bayesian information criterion\r-----------------------------------------------------------------------------\rModel | N ll(null) ll(model) df AIC BIC\r-------------+---------------------------------------------------------------\r. | 753 -514.8732 -461.103 7 936.206 968.5745\r-----------------------------------------------------------------------------\rNote: BIC uses N = number of observations. See [R] IC note.\rThese results match those from fitstat ","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:5:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"5.2 $Pseudo-R^2’s$ Within a substantive area, $pseudo-R^2s$ might provide a rough index of whether a model is adequate. For example, if prior models of labor force participation routinely have values of 0.4 for a particular $pseudo-R^2$, you would expect that new analyses with a different sample or with revised measures of the variables would result in a similar value for that measure. But there is no convincing evidence that selecting a model that maximizes the value of a $pseudo-R^2$ results in a model that is optimal in any sense other than the model has a larger value of that measure. We use the same models estimated in the last section and use fitstat to compute the scalar measures of fit (see section 3.3 for the formula for these measures): logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog quietly fitstat, save logit lfp k5 i.agecat i.wc c.inc##c.inc, nolog fitstat, diff | Current Saved Difference -------------------------+--------------------------------------\rLog-likelihood | Model | -461.103 -452.724 -8.379 Intercept-only | -514.873 -514.873 0.000 -------------------------+--------------------------------------\rChi-square | D(df=746/744/2) | 922.206 905.447 16.759 LR(df=6/8/-2) | 107.540 124.299 -16.759 p-value | 0.000 0.000 0.000 -------------------------+--------------------------------------\rR2 | McFadden | 0.104 0.121 -0.016 McFadden(adjusted) | 0.091 0.103 -0.012 McKelvey \u0026 Zavoina | 0.183 0.215 -0.032 Cox-Snell/ML | 0.133 0.152 -0.019 Cragg-Uhler/Nagelkerke | 0.179 0.204 -0.026 Efron | 0.135 0.153 -0.018 Tjur's D | 0.135 0.153 -0.018 Count | 0.672 0.676 -0.004 Count(adjusted) | 0.240 0.249 -0.009 -------------------------+--------------------------------------\rIC | AIC | 936.206 923.447 12.759 AIC divided by N | 1.243 1.226 0.017 BIC(df=7/9/-2) | 968.574 965.064 3.511 -------------------------+--------------------------------------\rVariance of | e | 3.290 3.290 0.000 y-star | 4.026 4.192 -0.165 Note: Likelihood-ratio test assumes current model nested in saved model.\rDifference of 3.511 in BIC provides positive support for saved model.\rAfter fitting our first model with logit, we used quietly to suppress the output from fitstat with the save option to retain the results in memory. After fitting the second model, fitstat, diff displays the fit statistics for both models, fitstat, diff computes differences between all measures, shown in the column labeled Difference, even if the models are not nested. As with the lrtest command, you must determine if it makes sense to interpret the computed difference. What do the fit statistics show? The values of the $pseudo-R^2s$ are slightly larger for M1, which is labeled “Saved logit” in the table. If you take the $pseudo-R^2s$ as evidence for the best model, which we do not, there is some evidence preferring M1. ","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:5:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"5.3 (Advanced) Hosmer-Lemeshow statistic We only recommend reading this section if you are considering using the Hosmer-Lemeshow statistic. After reviewing how the measure is computed, we illustrate that the statistic is highly dependent upon an arbitrary decision on the number of groups used. As a result, we do not recommend this measure. The idea of the Hosmer-Lemeshow (HL) test is to compare predicted probabilities with the observed data (Hosmer and Lemeshow 1980; Lemeshow and Hosmer 1982). This popular test can be computed using the estat gof command after fitting a logit or probit model. Unlike the measures presented above, this command also works with models fit by using complex survey data with the svy prefix. To explain how the test works, we review the steps that are used to compute it. Fit the regression model Compute the predicted probabilities $\\widehat{\\pi}_i$ Sort the data from the smallest $\\widehat{\\pi}_i$ to the largest. Divide the observations into G groups, where 10 groups are often used. Each group will have $n_{g}\\approx\\frac{N}{G}$ The first group will have the $n_1$ smallest values of $\\widehat{\\pi}_i$, and so on. If $G$ does not divide equally into $N$, the group sizes will differ slightly. WTithin each group, compute the mean prediction: $$\\overline{\\pi_g}=\\sum_\\text{i in group g}{ \\widehat { \\pi }_i/n_g}$$ Also compute th e mean number of observed cases where $y = 1$: $$\\overline{y_g}=\\sum_{i\\text{ in group g}} y _ i / n _ g$$ The test statistic is $$HL=\\sum_{g=1}^{G}\\frac{\\left(n_g\\overline{y}_g-n_g\\overline{\\pi}_g\\right)^2}{n_g\\overline{\\pi}_g\\left(1-\\overline{\\pi}_g\\right)}$$ Hosmer, Lemeshow, and Sturdivant (2013) ran simulations that suggest that HL is distributed approximately as $χ^2$ with $G – 2$ degrees of freedom if the model is correctly specified. If the p-value is large, the model is considered to fit the data. To give an example, we fit the model we have used as a running example, and we use estat gof to compute the HL statistic: logit lfp k5 k618 i.agecat i.wc i.hc lwg c.inc, nolog estat gof, group(10) note: obs collapsed on 10 quantiles of estimated probabilities.\rGoodness-of-fit test after logistic model\rVariable: lfp\rNumber of observations = 753\rNumber of groups = 10\rHosmer–Lemeshow chi2(8) = 13.76\rProb \u003e chi2 = 0.0881\rThe p-value is greater than 0.05, indicating that the model fits based on the criterion provided for the HL. Unfortunately, we do not find conclusions from the HL test to be convincing. First, as Hosmer and Lemeshow point out, the HL test is not a substitute for examining individual predictions and residuals as discussed in the last section. Second, Allison (2012b. 67) raised concerns that the test is not very powerful. In a simple simulation with 500 cases, the HL test failed to reject an incorrect model 75% of the time. Third, and most critically, the choice of the number of groups is arbitrary, even though 10 is most often used. The results of the Hosmer-Lemeshow test are sensitive to the arbitrary choice of the number of groups. In our experience, this is often the case, and for this reason, we do not recommend the test. We can illustrate the sensitivity of the Hosmer-Lemeshow test by varying the number of groups used to compute the test from 5 to 15 in the model fit above: quietly logit lfp k5 k618 i.agecat i.wc i.hc lwg c.inc, nolog capture matrix drop hl forvalues numgroups = 5(1)15 { quietly estat gof, group(`numgroups') local rm = r(m) matrix r = r(chi2), r(df), chi2tail(r(df),r(chi2)) matrix rownames r = \"`rm' groups\" matrix hl = nullmat(hl) \\ r } matrix colnames hl = chi2 df prob matlist hl, format(%8.3f) | chi2 df prob -------------+-----------------------------\r5 groups | 4.043 3.000 0.257 6 groups | 8.762 4.000 0.067 7 groups | 10.424 5.000 0.064 8 groups | 13.831 6.000 0.032 9 groups | 15.503 7.000 0.030 10 groups | 13.763 8.000 0.088 11 groups | 17.980 9.000 0.035 12 groups | 24.055 10.000 0.007 13 groups | 15.230 11.000 0.172 14 groups | 19.360 12.000 ","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:5:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"6 Other commands for binary outcomes Logit and probit models are the most commonly used models for binary outcomes and are the only ones that we consider in this book, but other models exist that can be fit in Stata. Among them are the following: cloglog assumes a complementary log-log distribution for the errors instead of a logistic or normal distribution. scobit fits a logit model that relaxes the assumption that the marginal change in the probability is greatest when Pr($y$ = 1) = 0.5. hetprobit allows the assumed variance of the errors in the probit model to vary as a function of the independent variables, which is one approach to comparing logit and probit coefficients across groups (Williams 2009). ivprobit fits a probit model where one or more of the regressors are endogenously determined. biprobit simultaneously fits two binary probits and can be used when errors are correlated with each other, as in the estimation of seemingly unrelated regression models for continuous dependent variables. mvprobit (Cappellari and Jenkins 2003) extends this idea to more than two binary probits. Binary outcomes that reflect an event that is expected to happen eventually for all cases are often handled using survival analysis, which is not covered in this book. Cleves et al. (2010) provides a detailed introduction to survival analysis in Stata focusing on the st* commands. Likewise, we do not consider Stata’s extensive commands for working with panel and multilevel data, including Stata’s xt* and me* commands, but these are discussed extensively in Rabe-Hesketh and Skrondal (2012) and Cameron and Trivedi (2010). ","date":"2024-01-25","objectID":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/:6:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_5 ：Models for binary outcomes Estimation, testing, and fit","uri":"/chapter_5-models-for-binary-outcomes-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"introduce methods of interpretation that will be used throughout the rest of the book. ","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"introduce methods of interpretation that will be used throughout the rest of the book. Models for categorical outcomes are nonlinear, and this nonlinearity is th e fundamental challenge th at must be addressed for effective interpretation. Most simply, this means th at you cannot effectively interpret your model by presenting a list of the estimated parameters. Instead, we believe that the most effective way to interpret these models is by first fitting the model and then computing and examining postestimation predictions of the outcomes. ","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:0:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1 Comparing linear and nonlinear models ","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:1:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.1 linear models Consider a linear regression model where $y$ is the dependent variable, $x$ is a continuous independent variable, and $d$ is a binary independent variable. The model is $$y=\\alpha+\\beta x+\\delta d+\\varepsilon $$ Given the usual assumption th at $E(\\varepsilon\\mid x,d)=0$, it follows that $$E(y|x,d)=\\alpha+\\beta x+\\delta d$$ which is graphed in figure 4.1. The solid line plots $E(\\varepsilon\\mid x,d)$ as $x$ changes, holding $d$ = 0;that is, $E(y|x,d)=\\alpha+\\beta x$ ,The dashed line plots $E(\\varepsilon\\mid x,d)$ as $x$ changes when $d$ = 1, which has the effect of changing the intercept :$E(y|x,d)=\\alpha+\\beta x+\\delta1=(\\alpha+\\delta)+\\beta x.$ A simple linear model\rThe effect of $x$ on $y$ can be computed as the partial derivative of $E(\\varepsilon\\mid x,d)$ with respect to $x$. This is sometimes called the marginal change: $$\\frac{\\partial E(y|x,d)}{\\partial x}=\\frac{\\partial\\left(\\alpha+\\beta x+\\delta d\\right)}{\\partial x}=\\beta $$ The marginal change is the ratio of the change in the expected value of y to the change in x , when the change in x is infinitely small, holding d constant. In linear models, the marginal change equals the discrete change in $E(\\varepsilon\\mid x,d)$ as x changes by one unit, holding other variables constant. In our notation, we indicate that x is changing by a discrete amount with $\\Delta x$ using $(x\\rightarrow x+1)$ to indicate that x changes from its current value to be 1 larger (for example, from 10 to 11 or from 9.3 to 10.3): $$\\frac{\\Delta E(y|x,d)}{\\Delta x(x\\to x+1)}={\\alpha+\\beta(x+1)+\\delta d}-(\\alpha+\\beta x+\\delta d)=\\beta $$ When x increases by 1, $E(\\varepsilon\\mid x,d)$ increases by $\\beta$ regardless of the values for $x$ and $d$ at the point where change is measured. This is shown by the four small triangles in figure above with bases of length 1 and heights of $\\beta$. The effect of d cannot be computed as a partial derivative because $d$ is discrete. Instead, we measure the change in $E(y\\mid x,d)$ with a discrete change from 0 to 1 indicated as $\\Delta d\\left(0\\rightarrow1\\right)$: $$\\frac{\\Delta E(y\\mid x,d)}{\\Delta d(0\\rightarrow1)}=(\\begin{array}{c}\\alpha+\\beta x+\\delta1\\end{array})-(\\begin{array}{c}\\alpha+\\beta x+\\delta0\\end{array})=\\delta $$ When d changes from 0 to 1,$E(\\varepsilon\\mid x,d)$ changes by $\\delta$ units regardless of the level of $x$. This is shown by the two arrows labeled $\\delta$ in figure above marking the distance between the solid and dashed lines. The distinguishing feature for interpretation in linear models is th at the effect of a given change in an independent variable is the same regardless of the value of that variable at the start of its change and regardless of the level of the other variables in the model. Interpretation only needs to specify which variable is changing, by how much, and that other variables are being held constant. Given the simple structure of linear models, such as regress, most interpretations require only reporting the estimates. There are, however, important exceptions. In our discussion, we assumed that the model does not include polynomial terms such as $x^2$ or interactions such as $xd$. When such terms are included, the linear model becomes nonlinear in the sense we consider in the next section. ","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:1:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.2 Nonlinear models We use a logit model to illustrate the idea of nonlinearity. Let $y = 1$ if the outcome occurs, say, if a person is in the labor force, and otherwise y = 0. T he curves are from the logit equation $$\\Pr(y=1|x,d)=\\frac{\\exp{(\\alpha+\\beta x+\\delta d)}}{1+\\exp{(\\alpha+\\beta x+\\delta d)}}$$ where the $\\alpha$,$\\beta$, and $\\delta$ parameters in this equation are unrelated to those for the linear model. Once again, $x$ is continuous and $d$ is binary. The model is shown in figure A simple nonlinear model\rThe nonlinearity of the model makes it more difficult to interpret the effects of x and d on the probability of y occurring. For example, neither the marginal change $\\partial\\Pr\\left(y=1|x,d\\right)/\\partial x$ nor the discrete change $\\Delta\\Pr\\left(y=1\\mid x,d\\right)/\\Delta d(0\\rightarrow1)$ are constant, but instead depend on the values of $ x$ and $d$ . Consider the effect of changing d from 0 to 1 for a given value of $x$. This effect is the distance between the solid curve for d = 0 and the dashed curve for $d$ = 1. Because the curves are not parallel, the magnitude of the difference in the predicted probability at d = 1 compared with d = 0 depends on the value of x where the difference is computed. Accordingly, $\\Delta_{d1}\\neq\\Delta_{d2}$ Similarly, the magnitude of the effect of $ x$ depends on the values of $x$ and d where the effect is evaluated so that $\\Delta_{x1}\\neq\\Delta_{x2}\\neq\\Delta_{x3}\\neq\\Delta_{x4}$ . In nonlinear models, the effect of a change in a variable depends on the values of all variables in the model and is no longer simply equal to a parameter of the model. Accordingly, the methods of interpretation that we recommend for nonlinear models are largely based on the use of predictions, which we consider in the next section. ","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:1:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2 Approaches to interpretation The primary methods of interpretation presented in this book are based on predictions from the model. The model is fit and the estimated parameters are used to make predictions at values of the independent variables that are (hopefully) useful for understanding the substantive implications of the nonlinear model. These methods depend critically on Stata’s predict and margins commands, which are the foundation for the SPost commands m table, mchange, and mgen (referred to collectively as the m* commands). Although the basic use of these commands is straightforward, they have many—sometimes subtle—features that are valuable for fully interpreting your model. This chapter provides an overview of general principles and syntax for these commands. Details on why you would use each feature are explained fully when the commands are used in later chapters to interpret specific models. ","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:2:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.1 Method of interpretation based on predictions We use predictions in four basic ways Predictions for each observation. Most fundamentally, predictions can be computed for each observation by using predict. Predictions include the probabilities of outcomes as well as rates for count models. We often start our analysis by examining the distribution of predictions in the estimation sample. Predictions at specified values. Predicted values at specific values of the independent variables can be computed using the commands margins and mtable. These commands can compute predictions at substantively interesting combinations of values of the independent variables, which we refer to as profiles or ideal types. In some cases, tables of predictions are arranged by the level of one or more explanatory variables and can succinctly summarize processes affecting the outcomes Marginal effects. An important way to examine the effects of a variable is to compute how changes in the variable are associated with changes in the outcomes, holding other variables constant. These changes, known as marginal effects, can be computed as a marginal change when a regressor changes by an infinitely small amount or as a discrete change when a regressor changes by a fixed amount. Marginal effects are computed by margins, mtable. and mchange. which can easily compute average marginal effects and marginal effects at the mean. **Graphs of predictions.**For continuous independent variables, graphs often effectively summarize effects. Stata’s m arginsplot elegantly plots a single outcome category based on predictions from margins. Just as m argins can only compute predictions for one outcome at a time, marginsplot does not allow you to plot multiple outcomes. Because this is essential for models with nominal and ordinal outcomes, we wrote mgen to generate variables with predictions for all outcomes. These variables containing predictions can be plotted using Stata’s graph command ","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:2:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.2 Method of interpretation using parameters Although the predictions used for each of these methods are computed using the model’s estimated param eters, in some cases the parameters themselves can be used for interpretation. Examples include odds ratios for binary models, standardized coefficients for latent outcomes, and factor changes in rates for count models. These are considered in detail in later chapters. ","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:2:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.3 Stata and SPost commands for interpretation The most fundamentally important command for sophisticated interpretation using predictions is Stata’s ‘margins’ command. This command is incredibly powerful, flexible, and general. As a consequence, it can be rather intimidating to use. To make ‘margins’ simpler to use, we wrote a series of “wrappers” that use ‘margins’ for their computations; they simplify the process of specifying the predictions you want and produce output that is easier to interpret. Nonetheless, there are times when you might need to use ‘margins,’ either because our commands did not anticipate something you want to do or because we encountered technical issues that made using ‘margins’ the only option. Accordingly, even if our ’m*’ commands seem to do everything you want, you should have some familiarity with what ‘margins’ does and how it works. This will also give you a better understanding of what our commands are doing. ","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:2:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3 Predictions for each observation The ‘predict’ command computes predicted values for each observation in the current dataset. ‘predict’ has many options that depend on the model that was fit. Here we consider only the options that provide information we use regularly in later chapters. If you type ‘help estimation-command’ (for example, ‘help logit’), you can click on the ‘Also See’ tab in the upper-right corner of the window and then select the postestimation entry for the command (for example, ‘[R] logit postestimation’); the postestimation entry includes details on how ‘predict’ works for that estimation command. The simplest syntax for predict is predict newvarlist where newvarlist contains the name or names of the variables that are generated to hold the predictions. How many variables and what is predicted depends on the model. The defaults for estimation commands used in this book are listed in the following table. As an example, we compute predicted probabilities for a logit model of women’s labor force participation. Below, ‘predict’ generates the variable ‘prob’ (a name we chose) containing the probabilities of a woman being in the labor force. use binlfp4, clear logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog Logistic regression Number of obs = 753\rLR chi2(8) = 124.30\rProb \u003e chi2 = 0.0000\rLog likelihood = -452.72367 Pseudo R2 = 0.1207\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5 | -1.39 0.19 -7.25 0.000 -1.77 -1.02\rk618 | -0.07 0.07 -0.96 0.336 -0.20 0.07\r|\ragecat |\r40-49 | -0.63 0.21 -3.00 0.003 -1.04 -0.22\r50+ | -1.28 0.26 -4.92 0.000 -1.79 -0.77\r|\rwc |\rcollege | 0.80 0.23 3.48 0.001 0.35 1.25\r|\rhc |\rcollege | 0.14 0.21 0.66 0.508 -0.27 0.54\rlwg | 0.61 0.15 4.04 0.000 0.31 0.91\rinc | -0.04 0.01 -4.24 0.000 -0.05 -0.02\r_cons | 1.01 0.29 3.54 0.000 0.45 1.57\r------------------------------------------------------------------------------\rpredict prob summarize prob Variable | Obs Mean Std. dev. Min Max\r-------------+---------------------------------------------------------\rprob | 753 .5683931 .1945282 .0135618 .9512301\rThe summary statistics show that in the sample of 753, the probabilities range from 0.014 to 0.951 with an average of 0.568. A detailed discussion of predicted probabilities for binary models is provided in chapter 6 For models with ordinal or nominal outcomes (chapters 7 and 8), ‘predict’ computes the predicted probability of an observation falling into each of the outcome categories. So, instead of providing a single variable name for predictions, you specify as many names as there are categories. For example, after fitting a model for a nominal dependent variable with four categories, you can use ‘predict prob1 prob2 prob3 prob4’. The new variables contain the predicted probabilities of being in the first, second, third, and fourth categories. For count models, by default predict computes the rate or expected count. Alternatively, predict newvarname, pr(#) computes the predicted probabilities of the specified counts. For example, predict prob0, pr(0) generates the variable ‘prob0’ containing estimates of Pr(y = 0 | x). Also, ‘predict pr(#iow \u003e #high)’ computes probabilities of contiguous counts. For example, predict prob1to3, pr(1,3) generates the variable ‘prob1to3’ containing estimates of $Pr(1 \u003c y \u003c 3 | x).$ ","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:3:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4 Predictions at specified values Interpreting predictions at substantively interesting values of the regressors is an essential method of interpretation. Such predictions can be made with m argins and with the m* commands we have written that are based upon margins. We focus on several aspects of these commands: Specifying values of the independent variables. Explaining how factor variables are handled. Using a $numlist$ for predictions at multiple values. Making predictions by the levels of a variable defining groups. Predicting quantities th at are not the default for margins We will explain how to use the margins command to make predictions, and then we will show how the same things (and more) can be done using the m* command mtable. ","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:4:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4.1 Why use the m* commands instead of margins? Our m* commands mtable, mchange, and mgen are “wrappers” for margins. By wrapper, we mean that the m commands translate your specification into a series of margins commands that actually do the computations.* Although we think margins is an extraordinary command, it can be difficult to use, and the output can be difficult to interpret. It does difficult things with amazing ease and also makes you work hard to do some simple things. In some ways, frankly, margins is more suited for a programmer than for a data analyst. For example, if you are fitting models with ordinal, nominal, or count outcomes, you have to run margins once for each outcome. Then, you face the tedious and error-prone task of combining the output from several margins commands. The learning curve for margins can also be steep. Our commands make it easier—sometimes much easier. The output is more compact, and if you want to plot the predictions, variables are automatically generated. ","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:4:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4.2 Using margins for predictions The margins command allows you to predict many quantities and compute summary measures of your predictions. To begin, it is helpful to see how margins is related to predict. Consider the example in section 4.3, where predict computed the probability of labor force participation for each observation in the sample. Using summarize to analyze the variable generated by predict, we found that the mean probability was 0.568. We can obtain exactly the same mean prediction with margins: logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog margins Logistic regression Number of obs = 753\rLR chi2(8) = 124.30\rProb \u003e chi2 = 0.0000\rLog likelihood = -452.72367 Pseudo R2 = 0.1207\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5 | -1.39 0.19 -7.25 0.000 -1.77 -1.02\rk618 | -0.07 0.07 -0.96 0.336 -0.20 0.07\r|\ragecat |\r40-49 | -0.63 0.21 -3.00 0.003 -1.04 -0.22\r50+ | -1.28 0.26 -4.92 0.000 -1.79 -0.77\r|\rwc |\rcollege | 0.80 0.23 3.48 0.001 0.35 1.25\r|\rhc |\rcollege | 0.14 0.21 0.66 0.508 -0.27 0.54\rlwg | 0.61 0.15 4.04 0.000 0.31 0.91\rinc | -0.04 0.01 -4.24 0.000 -0.05 -0.02\r_cons | 1.01 0.29 3.54 0.000 0.45 1.57\r------------------------------------------------------------------------------\rPredictive margins Number of obs = 753\rModel VCE: OIM\rExpression: Pr(lfp), predict()\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_cons | 0.57 0.02 34.24 0.000 0.54 0.60\r------------------------------------------------------------------------------\rWhen no options are specified, margins calculates the mean of the default quantity computed by predict for the estimation command. Earlier, we used predict to generate a variable with the probabilities of $y = 1$ for each observation, and we used summarize to compute the mean probability. Behind the scenes, this is what margins does. An advantage of using margins to compute the average predicted probability is that it provides the 95% confidence interval along with a test of the null hypothesis that the average prediction is 0. Stata does this using the delta method to compute standard errors (see [R] margins or Agresti [2013, 72]). In this example, testing that the mean prediction is 0 is not useful; but, when we later use margins to compute marginal effects, testing whether estimates differ from 0 is very useful. In addition to computing average predictions over the sample, margins allows us to compute predictions at specified values of the independent variables, whether those values occur in the sample or not. The most common example of this is computing the prediction with all variables at their mean by using the atmeans option: margins, atmeans Adjusted predictions Number of obs = 753\rModel VCE: OIM\rExpression: Pr(lfp), predict()\rAt: k5 = .2377158 (mean)\rk618 = 1.353254 (mean)\r1.agecat = .3957503 (mean)\r2.agecat = .3851262 (mean)\r3.agecat = .2191235 (mean)\r0.wc = .7184595 (mean)\r1.wc = .2815405 (mean)\r0.hc = .6082337 (mean)\r1.hc = .3917663 (mean)\rlwg = 1.097115 (mean)\rinc = 20.12897 (mean)\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_cons | 0.58 0.02 29.33 0.000 0.54 0.62\r------------------------------------------------------------------------------\rThe output, begins by listing the values of the independent variables at which the prediction was calculated, called the atlegend, where (mean) lets you know that these values are the means. 4.2.1 The at() option for specifying values The at() option allows us to set specific values of the independent variables at which pre","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:4:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4.3 (Advanced) Nondefault predictions using margins Although the section heading seems esoteric, this is an important topic. The default predictions computed by margins or the m* commands are often the predictions you want, which is why they are the default. But you might want to predict some other quantity. Using the options described in this section, you can predict arbitrarily complex functions of any quantity computed by predict. Several useful applications of this powerful feature of margins are illustrated in later chapters. By default, margins predicts whatever predict would predict by default for the last estimation command. For instance, the default prediction for regress is the predicted value $\\widehat{E(y|\\mathbf{x})}$, whereas for logit the default prediction is $\\widehat{\\mathrm{Pr}}(y=1|\\mathbf{x})$. For most estimation commands, you can predict other quantities by adding an option to predict. For example, after logit, the command predict myxb, xb generates the variable myxb with the linear combination of the log-odds. To determine the default prediction and what other types of predictions are available for a given estimation command, type help estimation-command postestimation (for example, help logit postestimation). The margins command can estimate any of the quantities computed by predict, as well as arbitrarily complex functions of these quantities with the predict() and expression() options. 4.3.1 The predict() option With the predict(statistic) option, the margins command makes predictions for any statistic that can be computed by predict. For example, in the ordered logit model considered in chapter 7, the default prediction is the probability of the first outcome. Suppose that our outcome categories are numbered 1, 2, 3, and 4. Running margins without predict() computes the average of $\\widehat{\\mathrm{Pr}}(y_{i}=1|\\mathbf{x}_{i})$. Because predict prob2, outcome(2) generates the variable prob2 containing $\\widehat{\\Pr}(y_i=2|\\mathbf{x}_i)$, to estimate the average probability that y equals 2, we use margins, predict(outcome(2)): use gssclass4, clear ologit class i.fem i.white i.year i.ed age inc, nolog margins, predict(outcome(2)) Predictive margins Number of obs = 5,620\rModel VCE: OIM\rExpression: Pr(class==2), predict(outcome(2))\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_cons | 0.46 0.01 73.93 0.000 0.45 0.47\r------------------------------------------------------------------------------\rIn the output, Expression: Pr(class==2), predict(outcome(2)) indicates the quantity being estimated. Because margins can only predict one outcome at a time, we must either run margins once for each outcome or use mtable to automate this process, as we describe shortly. 4.3.2 The expression() option The expression() option lets you estimate transformations of what is computed by predict(). To show how this works, imagine that after fitting an ordered logit model on an outcome with four categories, we want the predicted probability that y is 2, 3, or 4. That is, we want to compute $\\begin{aligned}\\Pr(y\\ne1|\\mathbf{x})=1-\\Pr(y=1|\\mathbf{x})\\end{aligned}$. The option is expression(1 - predict(outcome(1))): margins, expression(1-predict(outcome(1))) Predictive margins Number of obs = 5,620\rModel VCE: OIM\rExpression: 1-predict(outcome(1))\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_cons | 0.93 0.00 274.89 0.000 0.92 0.94\r------------------------------------------------------------------------------\rA similar application is computing the probability of a 0 after fitting a binary model. We cannot obtain this prediction with the predict() option because the predict command doe","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:4:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4.4 Tables of predictions using mtable mtable makes tables from the predictions computed by margins. You do not need to run margins because mtable does this for you, using most of the options for margins that we just considered. In addition, mtable has options to customize how the results appear by adding labels, selecting statistics, and combining results from multiple mtable commands. There are, however, some features in margins that will not work with mtable. Most notably, perhaps, margins allows a varlist with factor variables, but mtable does not. But as we showed on page 151, results that can be computed with a varlist can be computed using at(), so this does not limit what you can do with mtable. To explain how m table works, we start by creating a table of predicted probabilities that vary by wc and he from a logit model. We will talk at length about how to interpret these predictions in chapter 6 use binlfp4, clear logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog mtable, at(wc=(0 1) hc=(0 1)) atmeans Expression: Pr(lfp), predict()\r| wc hc Pr(y)\r----------+-----------------------------\r1 | 0 0 0.509\r2 | 0 1 0.543\r3 | 1 0 0.697\r4 | 1 1 0.725\rSpecified values of covariates\r| 2. 3. | k5 k618 agecat agecat lwg inc\r----------+-------------------------------------------------------------\rCurrent | .238 1.35 .385 .219 1.1 20.1\rIn the header, Expression echoes the description that margins uses to describe the predictions it is making. The column Pr(y) contains predicted probabilities that lfp is 1. The first row of the prediction table, numbered 1, shows that the probability of being in the labor force is 0.509 for a woman who did not go to college (wc=0) and whose husband did not go to college (hc=0), holding other variables at their means as specified with the atmeans option. Rows 2, 3, and 4 show predictions for other combinations of hc and wc. Values of the independent variables that are held constant are displayed below the predictions. To convince you (we hope) of the advantages of mtable. let’s look at the corresponding output from margins. We show all the output because if you use noatlegend, you risk not knowing which predictions correspond to which values of the variables that vary. margins, at(wc=(0 1) hc=(0 1)) atmeans Adjusted predictions Number of obs = 753\rModel VCE: OIM\rExpression: Pr(lfp), predict()\r1._at: k5 = .2377158 (mean)\rk618 = 1.353254 (mean)\r1.agecat = .3957503 (mean)\r2.agecat = .3851262 (mean)\r3.agecat = .2191235 (mean)\rwc = 0\rhc = 0\rlwg = 1.097115 (mean)\rinc = 20.12897 (mean)\r2._at: k5 = .2377158 (mean)\rk618 = 1.353254 (mean)\r1.agecat = .3957503 (mean)\r2.agecat = .3851262 (mean)\r3.agecat = .2191235 (mean)\rwc = 0\rhc = 1\rlwg = 1.097115 (mean)\rinc = 20.12897 (mean)\r3._at: k5 = .2377158 (mean)\rk618 = 1.353254 (mean)\r1.agecat = .3957503 (mean)\r2.agecat = .3851262 (mean)\r3.agecat = .2191235 (mean)\rwc = 1\rhc = 0\rlwg = 1.097115 (mean)\rinc = 20.12897 (mean)\r4._at: k5 = .2377158 (mean)\rk618 = 1.353254 (mean)\r1.agecat = .3957503 (mean)\r2.agecat = .3851262 (mean)\r3.agecat = .2191235 (mean)\rwc = 1\rhc = 1\rlwg = 1.097115 (mean)\rinc = 20.12897 (mean)\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_at |\r1 | 0.51 0.03 18.54 0.000 0.46 0.56\r2 | 0.54 0.04 12.21 0.000 0.46 0.63\r3 | 0.70 0.05 14.23 0.000 0.60 0.79\r4 | 0.73 0.04 19.92 0.000 0.65 0.80\r------------------------------------------------------------------------------\rThe margins output has additional information about the predictions, such as the confidence interval, that was missing from the mtable output. We can include the confidence interval in the mtable output by adding the options statistics(ci). At the same time, we show how to customize the label for predictions by using estname(): mtable, at(wc=(0 1) hc=(0 1)) atmeans estname(Pr_LFP) statistics(ci) Expression: Pr(lfp), predict()","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:4:4","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4.5 Marginal effects: Changes in predictions Marginal effects are estimates of the change in an outcome for a change in one independent variable, holding all other variables constant. Here we provide an overview of the commands and basic concepts for computing marginal effects. A detailed discussion of marginal effects, along with substantive applications of alternative measures of change, is given in later chapters, especially chapter 6. We begin by discussing margins, which computes marginal effects with the dydx() option, and we then show how mtable can do the same thing. Because marginal effects are such a useful summary of effects in nonlinear models, we created mchange to easily compute many types of changes and present them in a compact table. 4.5.1 Marginal effects using margins margins can calculate the change in a predicted quantity as an independent variable changes, holding other variables constant. The prediction can be anything that margins can estimate. **The variables for which changes are calculated are specified using the dydx(varlist) option, where dydx(*) indicates that changes for all independent variables are to be computed.**For example, use binlfp4, clear logit lfp k5 i.agecat i.wc inc, nolog margins, dydx(*) Average marginal effects Number of obs = 753\rModel VCE: OIM\rExpression: Pr(lfp), predict()\rdy/dx wrt: k5 2.agecat 3.agecat 1.wc inc\r------------------------------------------------------------------------------\r| Delta-method\r| dy/dx std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5 | -0.29 0.03 -8.35 0.000 -0.36 -0.22\r|\ragecat |\r40-49 | -0.11 0.04 -2.72 0.007 -0.19 -0.03\r50+ | -0.24 0.05 -5.30 0.000 -0.33 -0.15\r|\rwc |\rcollege | 0.22 0.04 6.16 0.000 0.15 0.29\rinc | -0.01 0.00 -4.30 0.000 -0.01 -0.00\r------------------------------------------------------------------------------\rNote: dy/dx for factor levels is the discrete change from the base level.\rThe amount of change in a regressor that is used to calculate the change in the prediction depends on whether the variable is a continuous or a factor variable, where Stata assumes variables are continuous unless specified as factor variables with the i. notation. In our example, k5 and inc are continuous while agecat and wc are factor variables. For a continuous variable, margins estimates the marginal change, which is the partial derivative or instantaneous rate of change in the estimated quantity with respect to a given variable, holding other variables constant. For factor variables, margins calculates the discrete change, which is the difference in the prediction when the factor variable is 1 compared with the prediction when the variable is 0. For the binary variable wc, this is the change in the probability of being in the labor force if the wife attended college compared with if she did not attend college. For multiple-category factor variables, the change is from the base category to the value listed in column dy/dx. For i.agecat in this example, the row labeled 40-49 is the change in the probability for a change in agecat from the excluded base category 30-39 to the category 40-49. It bears repeating that margins only calculates the discrete change for variables specified with the i. factor-variable notation. For example (using underlining to highlight the differences between the two commands), although logit lfp k5 i.agecat wc inc and logit lfp k5 i.agecat i.wc inc yield the same estimates of the regression coefficients, the values of dydx() computed by margins will differ. In the first specification, wc is not a factor variable, so margins computes the partial derivative with respect to wc; in the second specification, i.wc is a factor variable, so margins computes the discrete change. Almost certainly in this context, you want the discrete change, and so factor-variable notation must be used when fitting the model. 4.5.2 Marginal effects using mtable Showing how mtable c","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:4:5","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4.6 Plotting predictions For continuous variables, graphs can effectively summarize effects. The Stata command marginsplot plots the predictions from the most recently run margins. Our mgen command can also be used to plot results from margins. The major difference between the commands is that marginsplot creates plots, while mgen generates variables that can be used with Stata’s graphing commands. The former approach is convenient, but ultimately limited because it allows you to plot only a single outcome category from a single model in a graph. 4.6.1 Plotting predictions with marginsplot marginsplot uses results from the preceding margins command. For example, here we plot the predicted probabilities of labor force participation over the ages 20 to 80 for women who attended college and those who did not: use binlfp4, clear logit lfp k5 k618 age i.wc i.hc lwg inc, nolog margins, at(age=(20(10)80) wc=(0 1)) atmeans marginsplot 4.6.2 Plotting predictions with marginsplot The mgen command generates variables that can be plotted using Stata’s graph commands. Like mtable and mchange, mgen runs margins for you and accepts most of the options that can be used with margins. The most important options for graphing are at(), which is used to specify the range of the variable on the x-axis and the levels of other variables, and atmeans, if you want to hold other variables at the mean. Here is a simple example that uses mgen to create a variable containing predictions as income increases from $0$ to $100,000$ in increments of $10,000$: use binlfp4, clear logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog mgen, at(inc=(0(10)100)) stub(A) atmeans Predictions from: margins, at(inc=(0(10)100)) atmeans predict(pr)\rVariable Obs Unique Mean Min Max Label\r-----------------------------------------------------------------------------------------------------\rApr1 11 11 .3608011 .0768617 .7349035 pr(y=in LF) from margins\rAll1 11 11 .2708139 -.0156624 .6641427 95% lower limit\rAul1 11 11 .4507883 .1693859 .8056643 95% upper limit\rAinc 11 11 50 0 100 Family income excluding wife's\r-----------------------------------------------------------------------------------------------------\rSpecified values of covariates\r1. 3. 1. 1. k5 k618 agecat agecat wc hc lwg ---------------------------------------------------------------------------\r.2377158 1.353254 .3851262 .2191235 .2815405 .3917663 1.097115 The option stub(stubname) provides the first letters to be used in the names of the variables that are generated. We recommend a stub that differs from the starting letters of variables in the dataset; then, afterward, the variables can be easily deleted by typing drop stubname*. If the variable names in your dataset are all lowercase, an uppercase stub works well for this purpose. If you do not use the stub() option, the default stub is an underscore, leading to variable names such as _pr1. If you want to overwrite existing variables, perhaps while debugging the command, you can include the option replace. In our example, mgen generated four variables: Aprl with the predicted probabilities, Alll and Aull with the lower and upper bounds of the confidence interval for the prediction, and Ainc with values of inc for each prediction. The values of Ainc are determined by the at() option. The summary statistics for generated variables show that inc ranges from 0 to 100, with predicted probabilities ranging from 0.08 to 0.73. We can list these values: list Apr Ainc in 1/12, clean graph twoway connected Apr Ainc Apr1 Ainc 1. .7349035 0 2. .6613024 10 3. .5789738 20 4. .4920058 30 5. .405519 40 6. .324523 50 7. .2528245 60 8. .1924535 70 9. .1437253 80 10. .1057196 90 11. .0768617 100 12. . . We can run mgen multiple times to generate variables with predictions at different levels of variables that are not varying. Here we use quietly to suppress the output from mgen, and we create variables with predictions at each level of agecat. The results are then plotted using a sin","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:4:6","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4.7 Interpretation of parameters Although the primary methods of interpretation in this book are based on predictions from the model, some methods of interpretation involve simple transformations of the model’s parameters. For some estimation commands, there are options to list transformations of the estimates, such as the or option to list odds ratios for logit or the beta option to list standardized coefficients for regress. Although Stata is commendably clear in explaining the meaning of the estimated parameters, in some models it is easy to be confused about proper interpretations. For example, the zip model (discussed in chapter 9) simultaneously fits a binary and a count model, and it is easy to be confused regarding the direction of the effects. For the estimation commands considered in this book, plus some not considered here, our listcoef command lists estimated coefficients in ways that facilitate interpretation. You can list coefficients selected by name or by significance level, list transformations of the coefficients, and request help on interpretation. In fact, often you will not need the normal output from the estimation. You could suppress this output with the prefix quietly (for example, quietly logit lf pk5 wche) and then use the listcoef command. 4.7.1 The listcoef command The listcoef command listcoef [varlist] [, [factor|percent|std] adjacent gt lt negative positive pvalue (#) nolabel constant off help] **where varlist indicates that coefficients for only these variables are to be listed. If no varlist is given, then coefficients for all variables are listed. The varlist should not use factor-variable notation.**For example, for the model logit lfp i.age cat i.wc lwg, the command listcoef agecat will show the coefficients for 2.agecat and 3.agecat. If agecat##c.lw g was in the model, estimates for all coefficients that include agecat would be listed. 4.7.1.1 Options for types of coefficients Depending on the model and the specified options, **listcoef computes standardized coefficients, factor changes in the odds or expected counts, or percentage changes in the odds or expected counts.**More information on these types of coefficients is provided below, as well as in the chapters that deal with specific types of outcomes. factor requests factor change coefficients indicating how many times larger or smaller the outcome is. In some cases, these coefficients are odds ratios. percent requests percentage change coefficients indicating the percentage change in the outcome. std requests that coefficients be standardized to a unit variance for the independent variables or the dependent variable. For models that can be derived from a latent-dependent variable (for example, the binary logit model), the variance of the latent outcome is estimated. The following options (details on these options are given below) are available for each estimation command. If an option is the default, it does not need to be specified. 4.7.1.2 Options for mlogit, m probit, and slogit For the mlogit, mprobit, and slogit commands discussed in chapter 8, listcoef can show the coefficients for each pair of outcome categories. When these models are used with ordered outcomes, it is helpful to look at a subset of these coefficients. The following options are for this purpose: adjacent specifies that only the coefficients from comparisons in which the two category values are adjacent will be printed (for example, comparing outcome 1 versus 2, and 2 versus 1, but not 1 versus 3). This option can be combined with gt or lt. gt specifies that only the coefficients from comparisons in which the first category has a larger value than the second will be printed (for example, comparing outcome 2 versus 1, but not 1 versus 2). lt specifies that only the coefficients from comparisons in which the first category has a smaller value than the second will be printed (for example, comparing outcome 1 versus 2, but not 2 versus 1). negative specifies that onl","date":"2024-01-21","objectID":"/chapter_4-methods-of-interpretation/:4:7","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_4 ：Methods of interpretation","uri":"/chapter_4-methods-of-interpretation/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"Reviews Stata commands for fitting models,testing hypotheses,and computing measures of model fit","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"Reviews Stata commands for fitting models,testing hypotheses,and computing measures of model fit* Our book deals with what we think are the most fundamental and useful cross-sectional regression models for categorical and count outcomes: binary logit and probit, ordinal logit and probit, multinomial logit, Poisson regression, and negative binomial regression. We also explore several less common models, such as the stereotype logistic regression model and the zero-inflated and zero-truncated count models. Although these models differ in many respects, they generally share common features: Each model is fit by maximum likelihood, and many can be fit when data is collected using a complex sample survey design. Hypotheses about the parameters can be tested with Wald and likelihood-ratio tests. Measures of fit can be computed. The models can be interpreted by examining predicted values of the outcomes, a topic that is considered in chapter 4. Because of these similarities, the same principles and many of the same commands can be applied to each model. ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:0:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1 Estimation Each of the models we consider is fit using maximum likelihood (ML). ML estimates are the values of the parameters that have the greatest likelihood of generating the observed sample of data if the assumptions of the model are true. To obtain the ML estimates, a likelihood function calculates how likely it is that w e would observe the set of outcome values we actually observed if a given set of parameter estimates were the true parameters. If we imagine a surface in which the range of possible values of a makes up one axis and the range of $\\beta$ makes up another axis, the resulting graph of the likelihood function would look like a hill; the ML estimates would be the parameter values corresponding to the top of this hill. The variance of the estim ates corresponds roughly to how quickly the slope is changing near the top of the hill. ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:1:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.1 Stata’s output for ML estimation The process of iteration is reflected in the initial lines of Stata’s output. Below are the first lines of the output from the logit model of labor force participation th at we use as an example in chapters 5 and 6: use binlfp4, clear logit lfp k5 k618 agecat wc hc lwg inc Iteration 0: Log likelihood = -514.8732 Iteration 1: Log likelihood = -453.09301 Iteration 2: Log likelihood = -452.72688 Iteration 3: Log likelihood = -452.72649 Iteration 4: Log likelihood = -452.72649 Logistic regression Number of obs = 753\rLR chi2(7) = 124.29\rProb \u003e chi2 = 0.0000\rLog likelihood = -452.72649 Pseudo R2 = 0.1207\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5 | -1.39 0.19 -7.39 0.000 -1.76 -1.02\rk618 | -0.07 0.07 -0.96 0.338 -0.20 0.07\ragecat | -0.64 0.13 -4.92 0.000 -0.89 -0.38\rwc | 0.80 0.23 3.48 0.001 0.35 1.25\rhc | 0.14 0.21 0.66 0.510 -0.27 0.54\rlwg | 0.61 0.15 4.06 0.000 0.32 0.91\rinc | -0.03 0.01 -4.25 0.000 -0.05 -0.02\r_cons | 1.66 0.37 4.45 0.000 0.93 2.39\r------------------------------------------------------------------------------\rThe results begin with the iteration log, where the first, line, iteration 0, reports the value of the log likelihood at the start value. Whereas earlier we talked about maximizing the likelihood function, in practice, programs maximize the log of the likelihood, which simplifies the computations and yields the same result. Here the log likelihood at the start is —514.8732. The next four lines show the progress in maximizing the log likelihood, converging to the value of —452.72649. The rest of the output, which is omitted here, is discussed later in this section. ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:1:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.2 ML and sample size Under the usual assumptions, the ML estimator is consistent, efficient, and asymptotically normal. These properties hold as the sample size approaches infinity (see Cameron and Trivedi [2005]; Cramer [1986]; and Eliason [1993] for details). Although ML estimators are not necessarily bad estimators in small samples, the small-sample behavior of ML estim ators for the models we consider is largely unknown. Except for the logit and Poisson regression, which can be fit using exact perm utation methods with exlogistic or expoisson, alternative estimators with known small-sample properties are generally not available. With this in mind, Long (1997, 54) proposed the following guidelines for the use of ML in small samples: It is risky to use ML with samples smaller than 100, while samples over 500 seem adequate. These values should be raised depending on characteristics of the model and the data. First, if there are many parameters, more observations are needed A rule of at least 10 observations per parameter seems reasonable This does not imply that a minimum of 100 is not needed if you have only two parameters. Second, if the data are ill-conditioned (for example, independent variables are highly collinear) or if there is little variation in the dependent variable (for example, nearly all the outcomes are 1), a larger sample is required. Third, some models seem to require more observations (such as the ordinal regression model or the zero-inflated count models). ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:1:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.3 Problems in obtaining ML estimates Although the numerical methods used by S tata to fit models with ML are highly refined and generally work extremely well, you can encounter problems. If your sample size is adequate, but you cannot get a solution or you get estimates th at appear to not make substantive sense, one common cause is th at the data have not been properly “cleaned”. In addition to mistakes in constructing variables and selecting observations, the scaling of variables can cause problems. The larger the ratio between the largest and smallest standard deviations among variables in the model, the more problems you are likely to encounter with numerical m ethods due to rounding. For example, if income is measured in units of $1$, income is likely to have a very large standard deviation relative to other variables. Recoding income to units of $1,000$ can solve the problem. For a detailed technical discussion of maximum likelihood estim ation in Stata, see Gould, Pitblado, and Poi (2010) Overall, however, numerical methods for ML estimation work well when your model is appropriate for your data. Still, Cramer’s (1986, 10) advice about the need for care in estimation should be taken seriously: Check the data, check their transfer into the computer, check the actual computations (preferably by repeating at least a sample by a rival program), and always remain suspicious of the results, regardless of the appeal. ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:1:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.4 Syntax of estimation commands All single-equation estimation commands that we consider in this book have the same syntax: command depvar [ indepvars ] [if] [ in ] [ weight ] [ , options ] Here are a few examples for a lo g it model with lfp as the dependent variable: logit lfp k5 k618 age wc lwg logit lfp k5 k618 age wc lwg if hc == 1 logit lfp k5 k618 age wc lwg if hc == 1, level(90) The syntax diagram here uses 1) variable lists, 2) i f and in conditions, 3) weights, and 4) options. ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:1:4","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.5 Variable lists Stata automatically corrects some mistakes you may make when specifying independent variables. For example, if you include wc as an independent variable when th e sample is restricted to a single value of wc, such as logit lfp k5 age he wc if wc==l. then Stata drops wc from the model. Or suppose that you recode a k-category variable into a set of k indicator variables. Recall that one of the indicator variables must be excluded to avoid perfect collinearity. If you included all k indicator variables in indepvars, Stata automatically excludes one of them. Using factor-variable notation in the variable list In Stata 11 and later, you can specify a k-category variable as a set of indicator variables using Stata’s factor-variable notation. Prefixing a variable name with i.tells Stata to do this. In our previous example, suppose th at instead of age being measured in years, it was measured using three age groups with the variable agecat: tabulate agecat, missing Wife's age |\rgroup | Freq. Percent Cum.\r------------+-----------------------------------\r30-39 | 298 39.58 39.58\r40-49 | 290 38.51 78.09\r50+ | 165 21.91 100.00\r------------+-----------------------------------\rTotal | 753 100.00\rVariable agecat equals 1 for ages 30- 39, 2 for 40 49, and 3 for 50 or older. If we were not using factor variables, we could recode the three categories of ag ecat to generate three dummy variables: generate age3039 = (agecat==1) if agecat \u003c . label var age3039 \"Age 30 to 39?\" generate age4049 = (agecat==2) if agecat \u003c . label var age4049 \"Age 40 to 49?\" generate age50plus = (agecat==3) \u0026 agecat \u003c . label var age50plus \"Age 50 or older?\" Next, we fit a model using these variables, where age3039 is the excluded base category: logit lfp k5 k618 age4049 age50plus wc hc lwg inc, nolog Using factor-variable notation, we can fit the exact same model but let S tata automatically create the indicator variables: logit lfp k5 k618 i.agecat wc hc lwg inc, nolog By default, Stata uses the lowest value of the source variable as the base or omitted category in the model. Accordingly, logit lfp k5 k618 i.agecat wc hc lwg inc, nolog If you want a different base category, specify the base with the prefix ib#. , where # is the value of the base category. In our example, to treat women ages 50 or older as our base category instead of women ages 30-39, we would specify the model as follows: logit lfp k5 k618 ib3.agecat wc hc lwg inc, nolog By default, any variable not specified with i. is treated as a continuous variable. For example, in the specification logit lfp k5 k618 agecat wc hc lwg inc, nolog More on factor-variable notation sometimes factor variables can be confusing. First, the name of the variables automatically generated by Stata when fitting a model using factor variables might not be obvious. This is important because some postestimation commands, such as test and lincom, require the exact, symbolic name of the variable associated with a coefficient. To obtain the names associated with each coefficient, referred to as symbolic names, one can simply type the name of the last estimation command with the option coeflegend, such as logit, coefleg end.The model is not fit again, but the names associated with the estimates are listed. For example, fitting a model where factor-variable notation creates indicator variables and interactions produces output like this: logit lfp i.agecat c.age##c.age, nolog Logistic regression Number of obs = 753\rLR chi2(4) = 11.17\rProb \u003e chi2 = 0.0248\rLog likelihood = -509.29034 Pseudo R2 = 0.0108\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\ragecat |\r40-49 | -0.37 0.35 -1.05 0.296 -1.05 0.32\r50+ | -0.56 0.56 -1.00 0.319 -1.66 0.54\r|\rage | 0.26 0.14 1.77 0.077 -0.03 0.54\r|\rc.age#c.age | -0.00 0.00 -1.74 0.082 -0.01 0.00\r|\r_cons | -4.90 3.03 -1.62 ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:1:5","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.6 Specifying the estimation sample if and in restrictions can be used to define the sample of observations used to fit the model, referred to as the estimation sample, where the syntax for if and in conditions follows the guidelines in chapter 2, page 45. 1.6.1 Missing data Estimation commands use listwise deletion to exclude cases in which there are missing values for any of the variables in the model. Accordingly, if two models are fit using the same dataset but have different sets of independent variables, it is possible to have different samples. Suppose th at among the 753 cases in the sample, 23 have missing data for at least one variable. If we fit a model using all variables, we would obtain use binlfp4-missing, clear logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog Logistic regression Number of obs = 730\rLR chi2(8) = 113.28\rProb \u003e chi2 = 0.0000\rLog likelihood = -441.33862 Pseudo R2 = 0.1137\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5 | -1.40 0.19 -7.22 0.000 -1.78 -1.02\rk618 | -0.07 0.07 -0.95 0.340 -0.20 0.07\r|\ragecat |\r40-49 | -0.66 0.21 -3.11 0.002 -1.07 -0.24\r50+ | -1.26 0.26 -4.76 0.000 -1.77 -0.74\r|\rwc |\rcollege | 0.74 0.23 3.23 0.001 0.29 1.20\r|\rhc |\rcollege | 0.16 0.21 0.77 0.439 -0.25 0.57\rlwg | 0.59 0.15 3.88 0.000 0.29 0.88\rinc | -0.03 0.01 -3.82 0.000 -0.05 -0.02\r_cons | 1.02 0.29 3.51 0.000 0.45 1.60\r------------------------------------------------------------------------------\rSuppose that seven of the missing cases were missing only for k618 and that we fit a second model excluding k618: logit lfp k5 i.agecat i.wc i.hc lwg inc, nolog Logistic regression Number of obs = 737\rLR chi2(7) = 116.53\rProb \u003e chi2 = 0.0000\rLog likelihood = -444.78964 Pseudo R2 = 0.1158\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5 | -1.38 0.19 -7.23 0.000 -1.75 -1.01\r|\ragecat |\r40-49 | -0.60 0.21 -2.92 0.003 -1.00 -0.20\r50+ | -1.15 0.24 -4.87 0.000 -1.62 -0.69\r|\rwc |\rcollege | 0.78 0.23 3.38 0.001 0.33 1.23\r|\rhc |\rcollege | 0.14 0.21 0.67 0.505 -0.27 0.54\rlwg | 0.61 0.15 4.03 0.000 0.31 0.90\rinc | -0.03 0.01 -3.89 0.000 -0.05 -0.02\r_cons | 0.87 0.25 3.48 0.000 0.38 1.36\r------------------------------------------------------------------------------\rThus we cannot, use a likelihood ratio test or information criteria to compare the two models (see sections 3.2 and 3.3 for details), because changes in the estimates could be due either to changes in the model specification or to the use of different samples to fit the models. When you compare coefficients across models, you want the samples to be the same. Although Stata uses listwise deletion when fitting models, this is rarely the best way to handle missing data. We recommend that you make explicit decisions about which cases to include in your analyses rather than let cases be dropped implicitly. Indeed, we would prefer th at Stata issue an error rather than automatically drop cases. The mark and markout commands make it simple to explicitly exclude missing data, mark markvar generates the new variable markvar that equals 1 for all cases, markout mnrkvar varlist changes the values of markvar to 0 for any cases in which values of any of the variables in varlist are missing. The following example, where we have artificially created the missing data, illustrates how this works: use binlfp4-missing, clear mark nomiss markout nomiss lfp k5 k618 agecat wc hc lwg inc tab nomiss nomiss | Freq. Percent Cum.\r------------+-----------------------------------\r0 | 23 3.05 3.05\r1 | 730 96.95 100.00\r------------+-----------------------------------\rTotal | 753 100.00\rBecause nomiss is 1 for cases where none of the variables in our models is missing, to u","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:1:6","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.7 Weights and survey data Weights indicate that some observations should be given more weight than others when computing estimates. The syntax for specifying weights is [type=vamame] , where the square brackets are part of the command, type is the type of weight to be used, and vamame is the variable containing the weights. Stata recognizes four types of weights: fweights (frequency weights) indicate that an observation represents multiple observations with identical values. For example, if an observation has an fweight of 5, this is equivalent to having five identical, duplicate observations. If you do not include a weight modifier in your estim ation command, this is equivalent to specifying [fweight= l]. pweights (sampling weights) denote the inverse of the probability that the observation is included because of the sampling design. For example, if a case has a pweight of 1,200, that case had a 1 in 1,200 chance of being selected into the sample and in th at sense represents 1,200 observations in the population. aweights (analytic weights) are inversely proportional to the variance of an observation. The variance of the jth observation is assumed to be $\\frac{\\sigma^2}{\\omega_{j}}$, where $\\omega_{j}$ is the analytic weight. Analytic weights are used most often when observations are averages and the weights are the num ber of elements that gave rise to the average. For example, if each observation is the cell mean from a larger dataset, the data are heteroskedastic because the variance of the means decreases as the number of observations used to calculate them increases. iw eights (importance weights) have no formal statistical definition. They are used by programmers to facilitate certain types of computations Frequency weights differ notably from the other types because a dataset th at includes an fw eight variable can be used to create a new dataset that yields equivalent results without frequency weights by simply repeating observations with duplicate values. As a result, frequency weights pose no issues for various techniques we consider in this book The use of weights is a complex topic, and it is easy to apply weights incorrectly. If you need to use weights, we encourage you to read the discussions in [u] 11.1.6 weight and [u] 20.23 Weighted estimation . Winship and Radbill (1994) have an accessible introduction to weights in the linear regression model. Heeringa, West, and Berglund (2010) provide an in-depth treatment along with examples using Stata in their excellent book on complex survey design, a topic we consider next. 1.7.1 complex survey desighs Complex survey designs have three major features. First, samples can be divided into strata within which observations are selected separately. For example, a sample might be stratified by region of the country so th at the researchers can achieve precisely the number of respondents they want from each region. Second, samples can use clustering in which higher levels of aggregation, called primary sampling units, are selected first and then individuals are sampled from within these clusters. A survey of adolescents might use schools as its prim ary sampling unit and then sample students from within each school. Observations within clusters often share similarities leading to violations of the assumption of independent observations. Accordingly, when there is clustering, the usual standard errors will be incorrect because they do not adjust for the lack of independence. Third, individuals can have different probabilities of selection. For example, the design might oversample minority populations. Such oversampling allows more precise estimates of subgroup characteristics, but probability weights must be used to obtain accurate estimates for the population as a whole. Stata’s svy commands for samples with complex survey designs (see the Stata Survey Data Manual for details) provide estimates where the standard errors are adjusted for stratification, clustering, and wei","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:1:7","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.8 Options for regression models The following options apply to most regression models. Unique options for specific models are considered in later chapters. noconstant constrains the intercept to equal 0. For example, in a linear regression, the command regress y xl x2, noconstant would fit the model $y = \\beta_{1} X_{1} + \\beta_{2} X_{2} + \\varepsilon$ nolog suppresses the iteration history, which shortens the output. If you use this option, which we often do, and have problems obtaining estimates, it is a good idea to refit the model w ithout this option and with the trace option. trace lets you see the values of the parameters for each step of th e iteration. This can be useful for determining which variables may be causing a problem if your model has difficulty converging. level(#) specifies the level of the confidence interval. By default, Stata provides 95% confidence intervals for estimated coefficients, meaning that the interval around the estimated $\\hat{\\beta}$ would capture the true value of $\\beta$ 95% of the time if repeated samples were drawn, level () allows you to specify other intervals. For example, level (90) specifies a 90% interval. You can also change the default level with the command set level. For example, set level 90 specifies 90% confidence intervals. vce (cluster cluster-variable) specifies that the observations are independent across the clusters th at are defined by unique values of cluster-variable but are not necessarily independent within clusters. Specifying this option leads to robust standard errors, as discussed below, with additional corrections for the effects of clustered data. See Hosmer, Lemeshow, and Sturdivant (2013, chap. 9) for a detailed discussion of logit models with clustered data. Using vce ( cluster cluster-variable) does not affect the coefficient estimates but can have a large impact on the standard errors. vce(vcetype) specifies the type of standard errors th at are reported, vce (robust) replaces traditional standard errors with robust standard errors, which are also known as Huber, White, or sandwich standard errors. These are discussed further next, in section 3.1.9. Gould, Pitblado, and Poi (2010) provide details on how robust standard errors are computed in Stata. Robust standard errors are automatically used if the vce ( cluster cluster-variable) option is specified, if probability weights are used, or if a model is fit using svy. In earlier versions of Stata, this option was simply robust. Option vce (bootstrap) estimates the variance-covariance matrix by bootstrap, which involves repeated reestimation on samples drawn with replacement from the original estimation sample. Option vce(jackknife) uses the jackknife method, which involves refitting the model N times, each time leaving out a single observation. Type help vce option for further details. vsquish eliminates the blank lines in output that are inserted when factor-variable notation is used. We sometimes use nolog and vsquish in this book to save space. ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:1:8","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.9 Robust standard errors Robust standard errors, which are computed by Stata when the robust option is specified, go by a variety of names, including Huber-Eicker-White, clustered, White, heteroskedasticity-consistent, HCCM, and sandwich standard errors. In the last decade, their use has become increasingly common. For example, King and Roberts (2014) conducted a survey of articles in the American Political Science Review and found that 66% of the articles using regression models reported robust standard errors. Robust standard errors are considered robust in the sense that they are correct in the presence of some types of violations of the assumptions of the model. For example, if the correct model is a binary logit model but a binary probit model is fit, the model has been misspecified. The estimates obtained by fitting a logit model cannot be maximum likelihood estimates because an incorrect likelihood function is being used (that is, a logistic probability density is used instead of the correct normal density). When a model is misspecified in this way, the usual standard errors are incorrect (White 1982). For this reason, Arminger (1995) argues that robust standard errors should be broadly used. He writes: “If one keeps in mind that most researchers misspecify the model …, it is obvious that their estimated parameters can usually be interpreted only as minimum ignorance estimators and that the standard errors and test statistics may be far away from the correct asymptotic values, depending on the discrepancy between the assumed density and the actual density that generated the data.” In some cases, robust standard errors are likely to work quite well. If violations of the underlying model are minor, as we would argue is the case if the true model is logit and you fit a probit model, then the robust standard errors are preferred, but the differences are likely to be quite small. In our informal simulations, they are trivially different. On the other hand, if you fit a Poisson regression model (see chapter 9) in the presence of overdispersion, Cameron and Trivedi (2013, 72 80) provide convincing evidence that robust standard errors provide a more accurate assessment of statistical significance. If there is clustering in the data, robust standard errors should be used, ideally by specifying vce(cluster cluster-variablc) or by using svy estimation. Arguments for robust standard errors are compelling. Some argue they should be used nearly always in practice. At the same time, robust standard errors are not a general solution to problems of misspecification, and they have important limitations. Kauermann and Carroll (2001) show that even when the model is correct, robust standard errors have more sampling variability, and sometimes far more, than the usual standard errors. This is “the price that one pays to obtain consistency”. These theoretical results are consistent with simulations by Long and Ervin (2000), who found that in the linear regression model robust standard errors often did worse than the usual standard errors in samples smaller than 500. They recommended using small-sample versions that can be computed in Stata for regress with the options hc2 or hc3. Among nonlinear models, Kauermann and Carroll (2001) consider the Poisson regression model and the logit model. They showed that the loss of efficiency when using robust standard errors can be worse than th at occurring in normal models. However, we are unaware of small-sample versions of robust standard errors for nonlinear models There is a second and potentially very serious problem. If robust standard errors are used because a model is misspecified, it is im portant to consider what other implications misspecification may have. Freedman (2006) is dismissive of robust standard errors for many of the models discussed in this book for this reason, writing pointedly: “It remains unclear why applied workers should care about the variance of an estimator for the wro","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:1:9","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.10 Reading the estimation output Because we have already discussed the iteration log, in the following example we suppress it with the nolog option and consider other parts of the output from estimation commands. Although the sample output is from logit, our discussion applies generally to other regression models fit by maximum likelihood. We comment briefly below on changes to the estimation output for svy estimation. The following output from logit illustrates how Stata displays results from regression commands: use binlfp4, clear logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog Logistic regression Number of obs = 753\rLR chi2(8) = 124.30\rProb \u003e chi2 = 0.0000\rLog likelihood = -452.72367 Pseudo R2 = 0.1207\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5 | -1.39 0.19 -7.25 0.000 -1.77 -1.02\rk618 | -0.07 0.07 -0.96 0.336 -0.20 0.07\r|\ragecat |\r40-49 | -0.63 0.21 -3.00 0.003 -1.04 -0.22\r50+ | -1.28 0.26 -4.92 0.000 -1.79 -0.77\r|\rwc |\rcollege | 0.80 0.23 3.48 0.001 0.35 1.25\r|\rhc |\rcollege | 0.14 0.21 0.66 0.508 -0.27 0.54\rlwg | 0.61 0.15 4.04 0.000 0.31 0.91\rinc | -0.04 0.01 -4.24 0.000 -0.05 -0.02\r_cons | 1.01 0.29 3.54 0.000 0.45 1.57\r------------------------------------------------------------------------------\r1.10.1 Header Log likelihood = -452.72367 is the value of the log likelihood at convergence. Number of obs is the number of observations, excluding those with missing values and those excluded with if and in conditions. LR chi2(8) is the value of a likelihood-ratio chi-squared for the test of the null hypothesis that all the coefficients associated with independent variables are simultaneously equal to 0 (see page 119 for details). The number in parentheses is the degrees of freedom for the test. When robust standard errors or probability weights are used, results from a Wald test of the same null hypothesis are shown instead. Prob \u003e chi2 indicates the p-value. Pseudo R2 is the measure of fit also known as McFadden’s (1974) R^2. Details on how this measure is computed are given on page 126. 1.10.2 Estimates and standard errors The leftmost column lists the variables in the model, with the dependent variable at the top. The independent variables are in the same order as they were typed on the command line. The constant, labeled _cons, is last. With Stata 13 and later, factor variables are labeled with their value labels. For example, the indicator variable for agecat==2 is labeled as agecat followed by 40-49, which is the value label for category 2. In Stata 11 and 12, or with the nofvlabel option in Stata 13 and later, the indicator variable for agecat==2 is labeled as agecat followed on the next line by 2. Retyping the estim ation command followed by coef legend will list the symbolic names of each regression parameter. Column Coef . contains estimates of the regression coefficients Column Std. Err . contains the standard errors of the estimates. With the vce(robust) option, these are labeled Robust Std . Err. Column z contains the $z$ test equal to the estimate divided by its standard error. Column P\u003e|z| is the two-tailed significance level. A significance level listed as 0.000 means that p \u003c 0.001. For example, p = 0.00049 is rounded to 0.000. Column [95% Conf. Interval] contains the confidence interval for each estimate. Instead of testing a specific hypothesis (for example, $H_0: (\\beta = 0$), we can use a confidence interval that contains the true parameter with a chosen probability, known as the confidence level. For a given confidence level, the estimated upper and lower bounds define the confidence interval. 1.10.3 Differences in output for svy estimation With svy estimation, the output differs, reflecting that the estimates are no longer ML estimates. In addition to the sample size, an estim ate of the population size is shown. The likelih","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:1:10","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.11 Storing estimation results Stata considers the results of a model that has just been fit to be the active estimates.After fitting a model, you can typee return list to see a summary of the informationth at Stata stores about the active estimates. Postestimation commands are based onth e active estimates. When a new model is fit, its results become the active estimates,replacing the previous model’s estimates. The estimates store and estimates save commands preserve the active estimation results so that they can be retrieved and used even after a new model is fit.estimates store saves the active estimates to memory, while estimates save savesthem to a file. Storing estimation results is extremely useful for several reasons. For one,commands like lrtest and estimates table use results from more than one model.Because only one set of estimates can be active at a time, stored estimates are the way we can refer to multiple sets of estimates. Additionally, the margins command, usedextensively in later chapters, makes predictions based on estimates from a model thathas already been fit, meaning the active estimates. For some applications, however, we will need to overwrite the active estimates from our regression model with estimatesfrom margins by using the post option. Once this is done, the estimates from the regression model are no longer active and need to be restored (discussed below) as theactive estimates if you want to do additional postestimation analysis of the model. After running any estimation command, the syntax is estimates store name For example, to store the estimation results with the name logit 1. type use binlfp4, clear logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog estimates store logit1 After running estimates restore , the estimation results in memory are the same as if we had just fit the model, even though we may have fit other models in the interim. Of course, we need to be careful about changes made to the data after fitting the model, but the same caveat about not changing the data between fitting a model and postestimation analysis applies even when estimates store and estimates restore are not used. 1.11.1 (Advanced) Saving estimates to a file if you are fitting a model that takes hours to estimate, you may want to save the result to a file so you can use them later without refitting the model. Because estimates store holds the estimates in memory, estimates stored in one Statasession are not available in the next. Even within a S tata session, the command clear all erases stored estimates. To use estimates in a later session or after clearing memory,you can use estimates save to save results to a disk file: estimates save filename, replace For example, estimates save model1, replace will create the file model1.ster. We can load previously saved estimation results with estim ates use: estimates use filename estimates use restores the estimates almost as if we had just fit the model, and the“almost” here is very important. **As described earlier, when we fit a model, Stata creates the variable e(sample) to indicate which observations were used when fitting the model. Some postestimation commands need e (sample) to produce proper results.However, estimates use does not require that the data in memory are the data used to estimate the saved results. You can even run estimates use without data in memory.**Accordingly, estima tes use does not restore the e(sample) variable. Although this prevents some postestimation commands from working, this is better than having them give wrong answers because the wrong dataset is in memory. To deal with this issue, you can reset e(sample). When doing this, you are responsible for making sure that the data loaded to memory are the same as the data when the model was originally fit. Assuming the proper data are in memory, you use the estimates esample command to respecify the outcome and independent variables, thei if and in conditions, and the weights that were ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:1:11","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"1.12 Reformatting output with estimates table estimates table reformats the results from an estimation command to look more like the tables that are seen in articles and books. estimates table also makes it easier to move estimation results into a word processor or spreadsheet to make presentation-quality tables. We strongly recommend using this command or some other automated procedure rather than retyping results to make tables. Not only is this less tedious, but it diminishes the possibility of errors. Also, if you revise your model and used estimates table in your do-file, then you automatically have the corrected tables. The syntax is estimates table [ model-name1 [model-name2 … ] ] [ , options ] where model-name# is the name of a model whose results were stored using estim ates store . If model-name# is not specified, the estim ation results in memory are used. Here is a simple example that lets us compare estimates from similarly specified logit and probit models, a topic considered in detail in chapter 5. We start by fitting the two models and using estimates store to save the estimates: use binlfp4, clear logit lfp k5 i.agecat i.wc, nolog estimates store logit_model Logistic regression Number of obs = 753\rLR chi2(4) = 85.93\rProb \u003e chi2 = 0.0000\rLog likelihood = -471.9082 Pseudo R2 = 0.0834\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5 | -1.35 0.19 -7.27 0.000 -1.72 -0.99\r|\ragecat |\r40-49 | -0.62 0.20 -3.18 0.001 -1.01 -0.24\r50+ | -1.19 0.23 -5.27 0.000 -1.63 -0.75\r|\rwc |\rcollege | 0.83 0.18 4.53 0.000 0.47 1.19\r_cons | 0.89 0.16 5.40 0.000 0.57 1.21\r------------------------------------------------------------------------------\rprobit lfp k5 i.agecat i.wc, nolog estimates store probit_model Probit regression Number of obs = 753\rLR chi2(4) = 85.57\rProb \u003e chi2 = 0.0000\rLog likelihood = -472.08881 Pseudo R2 = 0.0831\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5 | -0.82 0.11 -7.56 0.000 -1.03 -0.61\r|\ragecat |\r40-49 | -0.37 0.12 -3.17 0.001 -0.61 -0.14\r50+ | -0.72 0.14 -5.29 0.000 -0.99 -0.46\r|\rwc |\rcollege | 0.50 0.11 4.57 0.000 0.29 0.71\r_cons | 0.54 0.10 5.50 0.000 0.35 0.73\r------------------------------------------------------------------------------\rWe combine the estimates by using estimates table : estimates table logit_model probit_model, b(%12.3f) t varlabel --------------------------------------------------------\rVariable | logit_model probit_model -------------------------+------------------------------\r# kids \u003c 6 | -1.351 -0.820 | -7.27 -7.56 | Wife's age group | 40-49 | -0.624 -0.374 | -3.18 -3.17 50+ | -1.190 -0.723 | -5.27 -5.29 | Wife attended college? | college | 0.832 0.500 | 4.53 4.57 Constant | 0.889 0.540 | 5.40 5.50 --------------------------------------------------------\rLegend: b/t\restimates table provides great flexibility for what you include in your table. Although you should check the Stata Base Reference Manual or type help estimates table for complete information, here are some of the most basic and helpful options: b(format) specifies the format used to print the coefficients. For example, b(%9.3f) indicates the estimates are to be in a column nine characters wide with three decimal places. For more information on formats, see help format or the Stata User’s Guide. varwidth (#) specifies the width of the column th at includes variable names and labels on the left side of the table. This is often needed when variable labels are used. keep (varlist) or drop (varlist) specify which of the independent variables to include in or exclude from the table. se[(format)], t[(format)], and p[C(format)] request standard errors, t or z statistics, and p-values, respect","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:1:12","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2 Testing If the assumptions of the model hold, ML estimators are distributed asymptotically normally: $$\\hat{\\beta_k}\\stackrel{a}{\\sim}N\\left(\\beta_{k},\\sigma_{\\stackrel{\\lambda}{\\beta_{k}}}^{2}\\right)$$ 当我们谈论最大似然估计（MLE）时，我们实际上是在说一种通过观察到的数据来猜测模型参数的方法。最大似然估计的目标是找到一组参数值，使得观察到的数据在这组参数下出现的可能性最大。 让我们用一个更具体的例子来解释： 想象你有一个魔术硬币，但你不知道它是如何工作的。你想知道抛一次硬币，它正面朝上的概率是多少。我们用一个字母 (p) 来表示这个概率。现在，你开始做实验，抛硬币多次，记录每次是正面还是反面。 我们把所有实验的结果称为观察到的数据，比如说你连续抛了10次硬币，结果是 7 次正面（H）和 3 次反面（T）。这组数据就是 (D)。 现在，MLE 的核心思想是：我们要找到一个 (p) 的值，使得在这个 (p) 下，观察到这组数据的概率最大。 在硬币的例子中，我们可以用二项分布来表示抛硬币的概率。假设硬币正面朝上的概率是 (p)，那么观察到 7 次正面和 3 次反面的概率可以用下面的公式表示： $ P(D|p) = p^7 \\cdot (1-p)^3 $ 这里，(P(D|p)) 表示在给定 (p) 的情况下，观察到数据 (D) 的概率。 然后，MLE 就是要找到使这个概率最大的 (p)。你可以把它想象成在 (p) 的可能取值范围内找到一个使得实验结果最有可能出现的 (p)。 数学上，我们可以通过求解导数为零的方程或者使用计算工具找到最大值。最终，我们得到了一个估计值 $ \\hat{p} $，它是使得观察到这组数据的概率最大的 (p)。 简而言之，MLE 就是通过数学方法找到一个最有可能解释观察到的数据的参数值。在硬币的例子中，它告诉我们硬币正面朝上的概率可能是多少，以最好地解释我们实验的结果。 The hypothesis $H_0 :{\\beta}_k = \\beta^*$ can be tested with the $ z $ statistic: z\r=\rβ\r^\rk\r−\rβ\r∗\rσ\r^\rβ\r^\rk\rIf Ho is true, then 2: is distributed approximately normally with a mean of 0 and a variance of 1 for large samples. The sampling distribution is shown in the following figure, where the shading shows the rejection region for a two-tailed test at the 0.05 level For some estimators, such as linear regression implemented by regress and with survey estimation, the estimators have at distribution rather than a normal distribution. The general principles of testing are, however, the same. ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:2:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.1 One-tailed and two-tailed tests The probability levels in the Stata output for estim ation commands are for two-tailed tests. When past research or theory suggests the sign of the coefficient, a one-tailed test might be used, and $H_0$ is rejected only when $t$ or $z$ is in the expected tail. You should divide P \u003e |t| (or P\u003e|z| ) by 2 only when the estimated coefficient is in the expected direction. Disciplines vary in their preferences for using one-tailed or two-tailed tests. Consequently, it is im portant to be explicit about whether p-values are for one-tailed or two-tailed tests. Unless stated otherwise, all the p-values we report in this book are for two-tailed tests. ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:3:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.2 Wald and likelihood-ratio tests For models fit by ML, hypotheses can be tested with Wald tests by using test and with likelihood-ratio (LR) tests by using lrtest. Only Wald tests are available for coefficients estimated using survey estimation. For both types of tests, there is a null hypothesis $H_0$ that implies constraints on the model’s parameters. For example, $H_0: \\beta_vc — \\beta_hc = 0$ hypothesizes that two of the parameters are 0 in the population. The Wald test assesses $H_0$ by considering two pieces of information. First, all else being equal, the greater the distance between the estimated coefficients and the hypothesized values, the less support we have for $H_0$. Second, the greater the curvature of the log-likelihood function, the more certainty we have about our estimates. This means that smaller differences between the estimates and hypothesized values are required to reject $H_0$. The LR test assesses $H_0$ by comparing the log likelihood from th e full model that does not include the constraints implied by $H_0$ with a restricted model th at does impose those constraints. If the constraints significantly reduce the log likelihood, then $H_0$ is rejected. Thus the LR test requires fitting two models. Although the LR and Wald tests are asymptotically equivalent, they have different values in finite samples, particularly in small samples. In general, it is unclear which test is to be preferred. Cameron and Trivedi (2005, 238) review the literature and conclude that neither test is uniformly superior. Nonetheless, many statisticians prefer the LR when both are suitable. We do recommend com puting only one test or the other; that is, we see no reason why you would want to compute or report both tests for a given hypothesis ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:4:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.3 Wald tests with test and testparm test computes Wald tests for linear hypotheses about parameters from the last model that was fit. Here we consider the most useful features of this powerful command. Features for multiple-equation models, such as mlogit, zip, and zinb, are discussed in chapters 8 and 9. Use help test for more features and help testnl for Wald tests of nonlinear hypotheses.” test varlist [ , accumulate ] The first syntax for test allows you to test that one or more coefficients from the last model are simultaneously equal to 0: where varlist contains names of independent variables from the last estimation. Some examples of test after fitting the model logit lfp k5 k618 i.agecat i.wc i.hclwg inc should make this first syntax clear. With one variable listed—here, k5—we are testing $H0: \\beta_k5 = 0$. use binlfp4, clear logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog Logistic regression Number of obs = 753\rLR chi2(8) = 124.30\rProb \u003e chi2 = 0.0000\rLog likelihood = -452.72367 Pseudo R2 = 0.1207\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5 | -1.39 0.19 -7.25 0.000 -1.77 -1.02\rk618 | -0.07 0.07 -0.96 0.336 -0.20 0.07\r|\ragecat |\r40-49 | -0.63 0.21 -3.00 0.003 -1.04 -0.22\r50+ | -1.28 0.26 -4.92 0.000 -1.79 -0.77\r|\rwc |\rcollege | 0.80 0.23 3.48 0.001 0.35 1.25\r|\rhc |\rcollege | 0.14 0.21 0.66 0.508 -0.27 0.54\rlwg | 0.61 0.15 4.04 0.000 0.31 0.91\rinc | -0.04 0.01 -4.24 0.000 -0.05 -0.02\r_cons | 1.01 0.29 3.54 0.000 0.45 1.57\r------------------------------------------------------------------------------\r* Wald tests test k5 ( 1) [lfp]k5 = 0\rchi2( 1) = 52.57\rProb \u003e chi2 = 0.0000\rThe resulting chi-squared test with 1 degree of freedom equals the square of the $z$ test statistic in the logit output. The results indicate that we can reject the null hypothesis. If we list all the regressors in the model, we can test that all the coefficients except the constant are simultaneously equal to 0. When factor-variable notation is used, variables must be specified with the value.variable-name syntax, such as 2.agecat. Recall that if you are not sure what name to use, you can replay the results by using the coef legend option (for example, logit, coef legend). test k5 k618 2.agecat 3.agecat 1.wc 1.hc lwg inc ( 1) [lfp]k5 = 0\r( 2) [lfp]k618 = 0\r( 3) [lfp]2.agecat = 0\r( 4) [lfp]3.agecat = 0\r( 5) [lfp]1.wc = 0\r( 6) [lfp]1.hc = 0\r( 7) [lfp]lwg = 0\r( 8) [lfp]inc = 0\rchi2( 8) = 95.90\rProb \u003e chi2 = 0.0000\rAs noted above, an LR test of the same hypothesis is part of the standard output ol estim ation commands, labeled as LR chi2 in the header ot the estimation output. To test all the coefficients associated with a factor variable with more than two categories, you can use testparm . For example, to test that all the coefficients for agecat are 0, we can use test: test 2.agecat 3.agecat ( 1) [lfp]2.agecat = 0\r( 2) [lfp]3.agecat = 0\rchi2( 2) = 24.27\rProb \u003e chi2 = 0.0000\rThe same results are obtained with testparm : ( 1) [lfp]2.agecat = 0\r( 2) [lfp]3.agecat = 0\rchi2( 2) = 24.27\rProb \u003e chi2 = 0.0000\rBecause agecat has only two categories, the advantage of testpaxm is not great. But when there are many categories, it is much simpler to use. The second syntax for test allows you to test hypotheses about linear combinations of coefficients: test [exp = exp] [ , accumulate ] For example, to test th at two coefficients are equal— say, $H_0:\\beta = \\beta_k618$: test k5=k618 ( 1) [lfp]k5 - [lfp]k618 = 0\rchi2( 1) = 45.07\rProb \u003e chi2 = 0.0000\rBecause the test statistic is significant, we reject the null hypothesis that the effect of having young children on labor force participation is equal to the effect of having older children. As before, testing hypotheses involving indicator variables requires us to specify both the value and the variable. For example, to test that the coeff","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:5:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.3.1 The accumulate option The accumulate option allows you to build more complex hypotheses based 011 the prior test command. For example, we begin with a test of $H_0:\\beta_k5 = \\beta_k618$: test k5=k618 ( 1) [lfp]k5 - [lfp]k618 = 0\rchi2( 1) = 45.07\rProb \u003e chi2 = 0.0000\rNext, add the constraint that $\\beta_wc = \\beta_hc$ test 1.wc=1.hc, accumulate ( 1) [lfp]k5 - [lfp]k618 = 0\r( 2) [lfp]1.wc - [lfp]1.hc = 0\rchi2( 2) = 47.63\rProb \u003e chi2 = 0.0000\rThis results in a test of $H_0:\\beta_k5 = \\beta_k618$, $\\beta_wc = \\beta_hc$. Instead of using the accumulate option, we could have used a single test command with multiple restrictions: test (k5=k618) ( 1.wc = 1.hc ). ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:5:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.4 LR tests with Irtest lrtest compares nested models by using an LR test. The syntax is where model-one and model-two are the names of estimation results stored by estimates store. When model-two is not specified, the most recent estimation results are used in its place. Typically, we begin by fitting the full or unconstrained model, and then we store the results. For example, Irtest model-one [ model-two] logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog estimates store full Logistic regression Number of obs = 753\rLR chi2(8) = 124.30\rProb \u003e chi2 = 0.0000\rLog likelihood = -452.72367 Pseudo R2 = 0.1207\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5 | -1.39 0.19 -7.25 0.000 -1.77 -1.02\rk618 | -0.07 0.07 -0.96 0.336 -0.20 0.07\r|\ragecat |\r40-49 | -0.63 0.21 -3.00 0.003 -1.04 -0.22\r50+ | -1.28 0.26 -4.92 0.000 -1.79 -0.77\r|\rwc |\rcollege | 0.80 0.23 3.48 0.001 0.35 1.25\r|\rhc |\rcollege | 0.14 0.21 0.66 0.508 -0.27 0.54\rlwg | 0.61 0.15 4.04 0.000 0.31 0.91\rinc | -0.04 0.01 -4.24 0.000 -0.05 -0.02\r_cons | 1.01 0.29 3.54 0.000 0.45 1.57\r------------------------------------------------------------------------------\rwhere full is the name we chose for the estimation results from the full model.9 After we store the results, we fit a model that is nested in the full model. A nested model is one that can be created by imposing constraints on the coefficients in the prior model. Most commonly, some of the variables from the full model are excluded, which in effect constrains the coefficients for these variables to be 0. For example, if we drop k5 and k618 from the last model, this produces logit lfp i.agecat i.wc i.hc lwg inc, nolog estimates store nokidvars Logistic regression Number of obs = 753\rLR chi2(6) = 61.59\rProb \u003e chi2 = 0.0000\rLog likelihood = -484.07589 Pseudo R2 = 0.0598\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\ragecat |\r40-49 | -0.02 0.18 -0.10 0.922 -0.36 0.33\r50+ | -0.48 0.21 -2.36 0.018 -0.89 -0.08\r|\rwc |\rcollege | 0.66 0.22 3.06 0.002 0.24 1.09\r|\rhc |\rcollege | 0.03 0.20 0.18 0.859 -0.35 0.42\rlwg | 0.61 0.15 4.18 0.000 0.32 0.89\rinc | -0.03 0.01 -4.37 0.000 -0.05 -0.02\r_cons | 0.22 0.22 1.01 0.312 -0.21 0.65\r------------------------------------------------------------------------------\rWe stored the results for the nested models as nokidvars. Next, we com pute the test: lrtest full nokidvars Likelihood-ratio test\rAssumption: nokidvars nested within full\rLR chi2(2) = 62.70\rProb \u003e chi2 = 0.0000\rThe output indicates that the LR test assumes that nokidvars is nested in full. It is up to the user to ensure that the models are nested. Because our models are nested, the result is an LR test of the hypothesis Ho: $H_0:\\beta_k5 = \\beta_k618 = 0$. The significant chi-squared statistic means that we reject the null hypothesis that these two coefficients are simultaneously equal to 0. Although we fit the full model first followed by the constrained model, lrtest allows the constrained model to be fit first followed by the full model. The output for all models fit by maximum likelihood includes an LR test that all the coefficients except the intercept(s) are 0. For our full model above, this is listed as LR chi2(8) = 124.30. The results can be computed with lrtest as follows: logit lfp, nolog Logistic regression Number of obs = 753\rLR chi2(0) = 0.00\rProb \u003e chi2 = .\rLog likelihood = -514.8732 Pseudo R2 = 0.0000\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_cons | 0.28 0.07 3.74 0.000 0.13 0.42\r--------------------------------------","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:6:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"2.4.1 Avoiding invalid LR tests lrtest does not always prevent you from computing an invalid test. There are two things that you must check: that the two models are nested and that the two models were fit using the same sample. In general, if either of these conditions is violated, the results of lrtest are meaningless. Although lrtest exits with an error message if the number of observations differs in the two models, this check does not catch those cases in which the number of observations is the same but the samples are different. One exception to the requirement of equal sample sizes is when perfect prediction removes some observations. In such a case, the apparent sample sizes for nested models differ, but an LR test is still appropriate (see section 5.2.3 for details). When this occurs, the force option can be used to force lrtest to compute the seemingly invalid test. For details on ensuring the same sample size, see our discussion of mark and markout in section 3.1.6. ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:6:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3 Measures of fit Assessing fit involves both the analysis of the fit of individual observations and the evaluation of scalar measures of fit for the model as a whole. Many scalar measures have been developed to summarize the overall goodness of fit of regression models. A scalar measure can in some cases be useful in comparing competing models and. ultimately, in selecting a final model. W ithin a substantive area, measures of fit might provide a rough index of whether a model is adequate. However, there is no convincing evidence that selecting a model th a t maximizes the value of a given measure results in a model th at is optimal in any sense other than the m odel’s having a larger (or, in some instances, smaller) value of th a t measure. Measures of fit provide some information, but it is partial information th at must be assessed within the context of the theory motivating the analysis, past research, and the estim ated parameters of the model being considered. ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:7:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3.1 Syntax of fitstat The SPost fitstat command calculates many fit statistics for the estimation commands in this book. We should mention again that we often find these measures of limited utility in our own research, with the exception of the information criteria BIC and AIC. When we do use these measures, we find it helpful to compare multiple measures, fitstat makes this simple. The options diff, saving (), and using () facilitate the comparison of measures across two models. Although fitstat duplicates some measures computed by other Stata commands (for example, the pseudo-R² in standard Stata output and the information criteria from estat ic), fitstat adds many more measures and makes it convenient to compare measures across models. The syntax is fitstat[, saving(name) using(name) ic force diff] fitstat terminates with an error if the last estimation command does not return a value for the log-likelihood function for a model with only an intercept (that is, if e(ll_0) is missing). This occurs, for example, if the noconstant option is used to fit a model. Although fitstat can be used when models are fit with weighted data, there are two limitations. First, some measures cannot be computed with some types of weights and none can be computed after svy estimation. Second, when pweights or robust standard errors are used to fit the model, fitstat uses the “pseudolikelihood” rather than the likelihood to compute measures of fit. Given the heuristic nature of the various measures of fit, we see no reason why the resulting measures would be inappropriate. Options fitstat[, saving(name)] saves the computed measures in a matrix, _fitstat_name, for later comparisons. When the saving() option is not used, fitstat saves results to the matrix _fitstat_0. using (name) compares the measures for the model in memory, referred to in the output as the current model, with those of the model saved as name. diff compares the current model to the prior model. ic presents only the Bayesian information criterion (BIC) and Akaike’s information criterion (AlC). When comparing two models, fitstat reports Raftery’s (1995) guidelines for assessing the strength of one model over another with BIC. force is required to compare information criteria when the number of observations or the estimation m ethod varies between the two models, or to conduct a likelihood ratio test under circumstances in which Stata’s lrtest command would require the force option. ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:7:1","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3.2 Methods and formulas used by fitstat In this section, we provide brief descriptions of each measure computed by fitstat. Full details for most measures along with citations to original sources are in Long (1997). We begin with formulas for several quantities that are used in the computation of other measures. We then consider the information criteria BIC and AIC. Again, these are the measures that we find most useful in practice. We then review the coefficient of determination $R^2$ for the linear regression model followed by numerous pseudo-R²s. 3.2.1 Quantities used in other measures Log-likelihood based measures. Stata begins maximum likelihood iterations by computing the log likelihood of the model with all parameters but the intercept constrained to 0, referred to as $\\ln L\\left(M_{\\text {Intercept }}\\right)$ The log likelihood upon convergence, referred to as $\\ln L\\left(M_{\\text {Full }}\\right)$, is also listed. This information is presented in the iteration log and in the head er for the estim ation results. LR chi-square test of all coefficients. An LR test of the hypothesis that all coefficients except the intercepts are 0 can be computed by comparing the log likelihoods: $LR = 2 \\ln L\\left(M_{\\text {Full }}\\right)-2 \\ln L\\left(M_{\\text {Intercept }}\\right)$LR is reported by Stata as $ LR chi2 (df) = #$, where the degrees of freedom in parentheses are the number of constrained parameters. For the zip and zinb models discussed in chapter 9, LR tests that the coefficients in the count portion (not the binary portion) of the model are 0. Deviance. The deviance compares the given model with a model that has one parameter for each observation so th at the model reproduces the observed d ata perfectly. The deviance is defined as $D=-2 \\ln L\\left(M_{\\text {Full }}\\right)$, where the degrees of freedom equals $N$ m inus the number of param eters. $D$ does not have a. chi-squared distribution. 3.2.2 Information criteria Information measures can be used to compare both nested and nonnested models. AIC. The formula for Akaike’s information criterion (1973) used by fitstat and Stata’s estat ic command is $$ \\mathrm{AIC}=-2 \\ln \\widehat{L}\\left(M_k\\right)+2 P_k $$ where $\\widehat{L}\\left(M_k\\right)$ is the likelihood of model $ M_k$ and $P_k$ is the number of parameters in the model (for example, $K$ + 1 in the binary regression model, where $K$ is the number of regressors). All else being equal, the model with the smaller AIC is considered the better-fitting model. Another definition of AIC is equal to the value in (3.1) divided by $N$ . We include this quantity in the $fitstat$ output as AIC divided by N. BIC. The Bayesian information criterion (BIC) was proposed by Raftery (1995) and others as a means to compare nested and nonnested models. Because BIC imposes a greater penalty for the number of parameters in a model, it favors a simpler mode, compared with the AIC measure. The BIC statistic is defined in at least three ways. Although this can be confusing the choice of which version to use is not important, as we show after presenting thvarious definitions. Stata defines the BIC for model $M_k$ as $$\\mathrm{BlC_k} = -2:\\ln\\hat{L}(M_{k})+\\mathrm{df}_{k}\\ \\ln N$$ where $df_{k}$ is the number of param eters in $M_k$, including auxiliary parameters such as a in the negative binomial regression model. As with AIC, the smaller or more negativ the BIC, the better the fit. A second definition of BIC is computed using the deviance $$\\mathrm{BIC_k}^{D}=D(M_{k})+\\mathrm{df}_{k}^{D}\\ \\ln N$$ where $df_{k}$ is the degrees of freedom associated with the deviance, fitstat labels this as BIC (based on deviance). The third version, sometimes denoted as BIC’, uses the LR chi-squared with $df_{k} equal to the number of regressors (not parameters) in the model. $$\\mathrm{BIC_k}^{\\prime}=-G^{2}(M_{k})+\\mathrm{df}_{k}^{\\prime}\\ \\ln N$$ The difference in the BICs from two models indicates which model is preferred. Because $\\mathrm{BIC_1}-\\mathr","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:7:2","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"3.3 Example of fitstat To examine all the measures of fit, we repeat our example for information criteria, but this time we use fitstat without the ic option. We fit our base model and save the fitstat results with the name basemodel: use binlfp4, clear logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog fitstat, ic saving(basemodel) Next, we fit a model that includes the variable kid s, which is the sum of k5 and k618. and drops k5 and k618. fitstat compares this model with the saved model: generate kids = k5 + k618 label var kids \"Number of kids 18 or younger\" logit lfp kids i.agecat i.wc i.hc lwg inc, nolog fitstat,using(basemodel) | Current Saved Difference -------------------------+--------------------------------------\rLog-likelihood | Model | -478.684 -452.724 -25.960 Intercept-only | -514.873 -514.873 0.000 -------------------------+--------------------------------------\rChi-square | D(df=745/744/1) | 957.368 905.447 51.921 LR(df=7/8/-1) | 72.378 124.299 -51.921 p-value | 0.000 0.000 0.000 -------------------------+--------------------------------------\rR2 | McFadden | 0.070 0.121 -0.050 McFadden(adjusted) | 0.055 0.103 -0.048 McKelvey \u0026 Zavoina | 0.125 0.215 -0.090 Cox-Snell/ML | 0.092 0.152 -0.061 Cragg-Uhler/Nagelkerke | 0.123 0.204 -0.081 Efron | 0.090 0.153 -0.063 Tjur's D | 0.091 0.153 -0.063 Count | 0.633 0.676 -0.042 Count(adjusted) | 0.151 0.249 -0.098 -------------------------+--------------------------------------\rIC | AIC | 973.368 923.447 49.921 AIC divided by N | 1.293 1.226 0.066 BIC(df=8/9/-1) | 1010.361 965.064 45.297 -------------------------+--------------------------------------\rVariance of | e | 3.290 3.290 0.000 y-star | 3.761 4.192 -0.431 Note: Likelihood-ratio test assumes current model nested in saved model.\rDifference of 45.297 in BIC provides very strong support for saved model.\rIn this example, the two models are nested because the second model is in effect imposing the constraint $\\mathrm{t~}\\beta_{\\mathrm{k5}}=\\beta_{\\mathrm{k618}}$ on the first model. ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:7:3","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Regression Models for Categorical Dependents variables Using Stata"],"content":"4 estat postestimation commands estat is a set of subcommands that provide different statistics about the model whose estimates are active. Each is invoked using estat subcommand. Here we provide an overview of some of the most useful subcommands, which we use in later chapters estat summarize estat summarize provides descriptive statistics for the variables in the model by using the estimation sample. For example, use binlfp4, clear logit lfp k5 k618 i.agecat i.wc i.hc lwg inc, nolog Logistic regression Number of obs = 753\rLR chi2(8) = 124.30\rProb \u003e chi2 = 0.0000\rLog likelihood = -452.72367 Pseudo R2 = 0.1207\r------------------------------------------------------------------------------\rlfp | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rk5 | -1.39 0.19 -7.25 0.000 -1.77 -1.02\rk618 | -0.07 0.07 -0.96 0.336 -0.20 0.07\r|\ragecat |\r40-49 | -0.63 0.21 -3.00 0.003 -1.04 -0.22\r50+ | -1.28 0.26 -4.92 0.000 -1.79 -0.77\r|\rwc |\rcollege | 0.80 0.23 3.48 0.001 0.35 1.25\r|\rhc |\rcollege | 0.14 0.21 0.66 0.508 -0.27 0.54\rlwg | 0.61 0.15 4.04 0.000 0.31 0.91\rinc | -0.04 0.01 -4.24 0.000 -0.05 -0.02\r_cons | 1.01 0.29 3.54 0.000 0.45 1.57\r------------------------------------------------------------------------------\restat summarize Estimation sample logit Number of obs = 753\r-------------------------------------------------------------------\rVariable | Mean Std. dev. Min Max\r-------------+-----------------------------------------------------\rlfp | .5683931 .4956295 0 1\rk5 | .2377158 .523959 0 3\rk618 | 1.353254 1.319874 0 8\r|\ragecat |\r40-49 | .3851262 .4869486 0 1\r50+ | .2191235 .4139274 0 1\r|\rwc |\rcollege | .2815405 .4500494 0 1\r|\rhc |\rcollege | .3917663 .4884694 0 1\rlwg | 1.097115 .5875564 -2.054124 3.218876\rinc | 20.12897 11.6348 -.0290001 96\r-------------------------------------------------------------------\rThe output is equivalent to the results from summarize modelvars if e(sample) == 1, where modelvars is the list of variables in your model. Several options are useful: labels displays variable labels rather than the names of the variables. noheader suppresses the header. noweights ignores the weights if they have been used in estimation. estat ic estat ic lists the inform ation criteria AIC and BIC for the last model. See page 123 for details estat vce estat vce lists the variance-covariance matrix for the coefficient estimates. For further details, see help estat vce ","date":"2024-01-16","objectID":"/chapter_3-estimation-testing-and-fit/:8:0","tags":["Categorical dependent variables","stata"],"title":"[Categorical Dependent Var] Chapter_3 ：Estimation, testing, and fit","uri":"/chapter_3-estimation-testing-and-fit/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"The chapter briefly illustrates the mechanics of using these commands in the context of a complex survey","date":"2024-01-16","objectID":"/19.chapter19complex-survey-data/","tags":["Interaction","stata"],"title":"Chapter19 ：Complex survey data","uri":"/19.chapter19complex-survey-data/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"The chapter briefly illustrates the mechanics of using these commands in the context of a complex survey The example dataset used in this chapter is the nhanes2.dta dataset. This is one of the Stata example datasets and is used via the Internet with the webuse command, shown below. webuse nhanes2 svyset The svyset command has already been used to declare the design for this survey, naming the primary sampling unit, the person weight, and the strata. Sampling weights: finalwgt\rVCE: linearized\rSingle unit: missing\rStrata 1: strata\rSampling unit 1: psu\rFPC 1: \u003czero\u003e\rLet’s now perform a regression analysis using this dataset. Let’s predict systolic blood pressure from the person’s age (in six age groups), sex, and weight. We use the svy prefix before the regress command to account for the survey design as specified by the svyset command. svy:regress bpsystol i.agegrp i.sex c.weight Survey: Linear regression\rNumber of strata = 31 Number of obs = 10,351\rNumber of PSUs = 62 Population size = 117,157,513\rDesign df = 31\rF(7, 25) = 328.16\rProb \u003e F = 0.0000\rR-squared = 0.3087\r------------------------------------------------------------------------------\r| Linearized\rbpsystol | Coefficient std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\ragegrp |\r30–39 | 1.20 0.57 2.11 0.043 0.04 2.37\r40–49 | 6.88 0.72 9.57 0.000 5.41 8.35\r50–59 | 16.04 0.71 22.44 0.000 14.58 17.49\r60–69 | 23.38 0.77 30.26 0.000 21.81 24.96\r70+ | 30.28 0.90 33.83 0.000 28.46 32.11\r|\rsex |\rFemale | -0.65 0.54 -1.20 0.239 -1.75 0.45\rweight | 0.43 0.02 24.85 0.000 0.39 0.46\r_cons | 88.07 1.32 66.49 0.000 85.36 90.77\r------------------------------------------------------------------------------\rWe can use the contrast, pwcompare, margins, and marginsplot commands to interpret these results. The use of these commands is briefly illustrated below. The contrast command can be used to make comparisons among the groups formed by a factor variable. The contrast command below tests the equality of the adjusted means for the six age groups. The test shows that the average systolic blood pressure is not equal among the six age groups. contrast agegrp Contrasts of marginal linear predictions\rDesign df = 31\rMargins: asbalanced\r------------------------------------------------\r| df F P\u003eF\r-------------+----------------------------------\ragegrp | 5 297.86 0.0000\rDesign | 31\r------------------------------------------------\rNote: F statistics are adjusted for the survey\rdesign.\rThe output of the contrast command indicates the $F$ test is adjusted for the survey design. If you wanted to omit the adjustment for the design degrees of freedom, youcould add the nosvyadjust option, as shown below. (See [R] contrast for more details about this option.) contrast agegrp,nosvyadjust Contrasts of marginal linear predictions\rDesign df = 31\rMargins: asbalanced\r------------------------------------------------\r| df F P\u003eF\r-------------+----------------------------------\ragegrp | 5 341.99 0.0000\r|\rDesign | 31\r------------------------------------------------\rThe pwcompare command can also be used to form pairwise comparisons among the different age groups. In the example below, the mcompare(sidak) option is included to adjust for multiple comparisons. pwcompare agegrp,pveffects mcompare(sidak) Pairwise comparisons of marginal linear predictions\rDesign df = 31\rMargins: asbalanced\r---------------------------\r| Number of\r| comparisons\r-------------+-------------\ragegrp | 15\r---------------------------\r--------------------------------------------------------\r| Sidak\r| Contrast Std. err. t P\u003e|t|\r----------------+---------------------------------------\ragegrp |\r30–39 vs 20–29 | 1.20 0.57 2.11 0.482\r40–49 vs 20–29 | 6.88 0.72 9.57 0.000\r50–59 vs 20–29 | 16.04 0.71 22.44 0.000\r60–69 vs 20–29 | 23.38 0.77 30.26 0.000\r70+ vs 20–29 | 30.28 0.90 33.83 0.000\r40–49 vs 30–39 | 5.68 0.65 8.76 0.000\r50–59 vs 30–39 | 14.83 0.76 19.51 0.000\r60–69 vs 30–39 | 22.","date":"2024-01-16","objectID":"/19.chapter19complex-survey-data/:0:0","tags":["Interaction","stata"],"title":"Chapter19 ：Complex survey data","uri":"/19.chapter19complex-survey-data/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"The chapter begins with a discussion of logistic regression models(most detailed)","date":"2024-01-15","objectID":"/18.chapter18nonlinear-models/","tags":["Interaction","stata"],"title":"Chapter18 ：Nonlinear models","uri":"/18.chapter18nonlinear-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"The chapter begins with a discussion of logistic regression models(most detailed) ","date":"2024-01-15","objectID":"/18.chapter18nonlinear-models/:0:0","tags":["Interaction","stata"],"title":"Chapter18 ：Nonlinear models","uri":"/18.chapter18nonlinear-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 Binary logistic regression ","date":"2024-01-15","objectID":"/18.chapter18nonlinear-models/:1:0","tags":["Interaction","stata"],"title":"Chapter18 ：Nonlinear models","uri":"/18.chapter18nonlinear-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.1 A logistic model with one categorical predictor Let’s consider a simple logistic regression model that predicts whether a person smokes (smoke) by the person’s self-reported social class (class). The variable class is a categorical variable that is coded: 1 = lower class, 2 = working class, 3 = middle class, and 4 = upper class. logit smoke i.class,nolog Logistic regression Number of obs = 15,464\rLR chi2(3) = 198.45\rProb \u003e chi2 = 0.0000\rLog likelihood = -9904.5707 Pseudo R2 = 0.0099\r--------------------------------------------------------------------------------\rsmoke | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r---------------+----------------------------------------------------------------\rclass |\rworking class | -0.32 0.07 -4.37 0.000 -0.46 -0.18\rmiddle class | -0.72 0.07 -9.80 0.000 -0.86 -0.58\rupper class | -0.92 0.12 -7.54 0.000 -1.16 -0.68\r|\r_cons | -0.13 0.07 -1.85 0.064 -0.26 0.01\r--------------------------------------------------------------------------------\rWe can interpret and visualize the results of this model using the contrast, pwcompare, margins, and marginsplot commands, as described in the following sections. 1.1.1 Using the contrast command If we want to test the overall equality of the four social class groups in terms of their log odds of smoking. logit smoke i.class,nolog Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------\r| df chi2 P\u003echi2\r-------------+----------------------------------\rclass | 3 196.71 0.0000\r------------------------------------------------\rWe can also apply contrast operators to form specific comparisons among the levels of the social class. contrast ar.class,nowald pveffects //(adjacent (previous) level) Contrasts of marginal linear predictions\rMargins: asbalanced\r-------------------------------------------------------------------------\r| Contrast Std. err. z P\u003e|z|\r---------------------------------+---------------------------------------\rclass |\r(working class vs lower class) | -0.32 0.07 -4.37 0.000\r(middle class vs working class) | -0.40 0.04 -11.26 0.000\r(upper class vs middle class) | -0.20 0.10 -1.92 0.055\r-------------------------------------------------------------------------\rThis test shows that the four social class groups are not all equal in terms of their log odds of smoking We can add the or option to the contrast command to display the results as odds ratios. contrast ar.class,nowald pveffects or Contrasts of marginal linear predictions\rMargins: asbalanced\r-------------------------------------------------------------------------\r| Odds ratio Std. err. z P\u003e|z|\r---------------------------------+---------------------------------------\rclass |\r(working class vs lower class) | 0.73 0.05 -4.37 0.000\r(middle class vs working class) | 0.67 0.02 -11.26 0.000\r(upper class vs middle class) | 0.82 0.09 -1.92 0.055\r-------------------------------------------------------------------------\rthe results can now be interpreted using odds ratios. For example, the odds of smoking for a person who identifies as middle class is 0.669 times the odds of smoking for someone who identifies as working class. 1.1.2 Using the pwcompare command We can also use the pwcompare command to form comparisons among the levels of the social class. Like the contrast command, these comparisons are made in the logodds metric. pwcompare class,pveffects Pairwise comparisons of marginal linear predictions\rMargins: asbalanced\r-----------------------------------------------------------------------\r| Unadjusted\r| Contrast Std. err. z P\u003e|z|\r-------------------------------+---------------------------------------\rsmoke |\rclass |\rworking class vs lower class | -0.32 0.07 -4.37 0.000\rmiddle class vs lower class | -0.72 0.07 -9.80 0.000\rupper class vs lower class | -0.92 0.12 -7.54 0.000\rmiddle class vs working class | -0.40 0.04 -11.26 0.000\rupper class vs working class | -0.60 0.10 -5.80 0.000\rupper class vs middle class | -0.20 0.10 -1.92 0.055\r----","date":"2024-01-15","objectID":"/18.chapter18nonlinear-models/:1:1","tags":["Interaction","stata"],"title":"Chapter18 ：Nonlinear models","uri":"/18.chapter18nonlinear-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.2 A logistic model with one continuous predictor Let’s now briefly consider a model with one continuous predictor, predicting whether a person smokes (smoke) from his or her education level. use gss_ivrm.dta logit smoke educ,nolog Logistic regression Number of obs = 16,332\rLR chi2(1) = 174.04\rProb \u003e chi2 = 0.0000\rLog likelihood = -10483.854 Pseudo R2 = 0.0082\r------------------------------------------------------------------------------\rsmoke | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\reduc | -0.07 0.01 -13.10 0.000 -0.08 -0.06\r_cons | 0.22 0.07 3.36 0.001 0.09 0.35\r------------------------------------------------------------------------------\rLet’s use the margins and marginsplot commands to visualize the relationship between education and the log odds of smoking. (Note the inclusion of the predict(xb) option on the margins command to specify the use of the log-odds metric.) margins,at(educ=(5(1)20))predict(xb) Adjusted predictions Number of obs = 16,332\rModel VCE: OIM\rExpression: Linear prediction (log odds), predict(xb)\r1._at: educ = 5\r2._at: educ = 6\r3._at: educ = 7\r4._at: educ = 8\r5._at: educ = 9\r6._at: educ = 10\r7._at: educ = 11\r8._at: educ = 12\r9._at: educ = 13\r10._at: educ = 14\r11._at: educ = 15\r12._at: educ = 16\r13._at: educ = 17\r14._at: educ = 18\r15._at: educ = 19\r16._at: educ = 20\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_at |\r1 | -0.12 0.04 -2.95 0.003 -0.20 -0.04\r2 | -0.19 0.04 -5.22 0.000 -0.26 -0.12\r3 | -0.26 0.03 -8.13 0.000 -0.32 -0.20\r4 | -0.33 0.03 -11.91 0.000 -0.38 -0.27\r5 | -0.40 0.02 -16.84 0.000 -0.44 -0.35\r6 | -0.46 0.02 -23.09 0.000 -0.50 -0.42\r7 | -0.53 0.02 -30.20 0.000 -0.57 -0.50\r8 | -0.60 0.02 -36.37 0.000 -0.63 -0.57\r9 | -0.67 0.02 -39.31 0.000 -0.70 -0.64\r10 | -0.74 0.02 -38.80 0.000 -0.78 -0.70\r11 | -0.81 0.02 -36.50 0.000 -0.85 -0.76\r12 | -0.88 0.03 -33.82 0.000 -0.93 -0.82\r13 | -0.94 0.03 -31.36 0.000 -1.00 -0.88\r14 | -1.01 0.03 -29.27 0.000 -1.08 -0.94\r15 | -1.08 0.04 -27.52 0.000 -1.16 -1.00\r16 | -1.15 0.04 -26.08 0.000 -1.24 -1.06\r------------------------------------------------------------------------------\rmarginsplot Note how the relationship between education and the log odds of smoking is linear. For every additional year of education, the log odds of smoking decreases by 0.07. Log odds of smoking by education level\rLet’s now visualize this relationship in terms of the probability of smoking. margins,at(educ=(5(1)20)) Adjusted predictions Number of obs = 16,332\rModel VCE: OIM\rExpression: Pr(smoke), predict()\r1._at: educ = 5\r2._at: educ = 6\r3._at: educ = 7\r4._at: educ = 8\r5._at: educ = 9\r6._at: educ = 10\r7._at: educ = 11\r8._at: educ = 12\r9._at: educ = 13\r10._at: educ = 14\r11._at: educ = 15\r12._at: educ = 16\r13._at: educ = 17\r14._at: educ = 18\r15._at: educ = 19\r16._at: educ = 20\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_at |\r1 | 0.47 0.01 45.90 0.000 0.45 0.49\r2 | 0.45 0.01 50.26 0.000 0.44 0.47\r3 | 0.44 0.01 55.77 0.000 0.42 0.45\r4 | 0.42 0.01 62.72 0.000 0.41 0.43\r5 | 0.40 0.01 71.30 0.000 0.39 0.41\r6 | 0.39 0.00 81.08 0.000 0.38 0.40\r7 | 0.37 0.00 90.01 0.000 0.36 0.38\r8 | 0.35 0.00 93.68 0.000 0.35 0.36\r9 | 0.34 0.00 88.76 0.000 0.33 0.35\r10 | 0.32 0.00 77.69 0.000 0.32 0.33\r11 | 0.31 0.00 65.44 0.000 0.30 0.32\r12 | 0.29 0.01 54.74 0.000 0.28 0.30\r13 | 0.28 0.01 46.16 0.000 0.27 0.29\r14 | 0.27 0.01 39.41 0.000 0.25 0.28\r15 | 0.25 0.01 34.10 0.000 0.24 0.27\r16 | 0.24 0.01 29.87 0.000 0.22 0.26\r------------------------------------------------------------------------------\rmarginsplot Predicted probability of smoking by education level\r","date":"2024-01-15","objectID":"/18.chapter18nonlinear-models/:1:2","tags":["Interaction","stata"],"title":"Chapter18 ：Nonlinear models","uri":"/18.chapter18nonlinear-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.3 A logistic model with covariates Let’s add some covariates to this model, predicting smoking from class as well as education, age, and year of interview logit smoke i.class educ age yrint,nolog Logistic regression Number of obs = 15,375\rLR chi2(6) = 742.33\rProb \u003e chi2 = 0.0000\rLog likelihood = -9580.0723 Pseudo R2 = 0.0373\r--------------------------------------------------------------------------------\rsmoke | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r---------------+----------------------------------------------------------------\rclass |\rworking class | -0.26 0.07 -3.47 0.001 -0.41 -0.11\rmiddle class | -0.46 0.08 -5.89 0.000 -0.61 -0.30\rupper class | -0.52 0.13 -4.11 0.000 -0.77 -0.27\r|\reduc | -0.09 0.01 -13.88 0.000 -0.10 -0.08\rage | -0.02 0.00 -18.12 0.000 -0.02 -0.02\ryrint | -0.03 0.00 -9.48 0.000 -0.04 -0.03\r_cons | 65.46 6.72 9.74 0.000 52.29 78.63\r--------------------------------------------------------------------------------\rcontrast class //test the overall effect of class Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------\r| df chi2 P\u003echi2\r-------------+----------------------------------\rclass | 3 49.68 0.0000\r------------------------------------------------\rWe can use the margins command to help us interpret this effect by computing the predictive margins of the probability of smoking by class, as shown below. margins class,nopvalues Predictive margins Number of obs = 15,375\rModel VCE: OIM\rExpression: Pr(smoke), predict()\r----------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r---------------+------------------------------------------------\rclass |\rlower class | 0.43 0.02 0.39 0.46\rworking class | 0.37 0.01 0.36 0.38\rmiddle class | 0.32 0.01 0.31 0.34\rupper class | 0.31 0.02 0.27 0.35\r----------------------------------------------------------------\rWe can use marginsplot command to graph these predictive margins marginsplot,xlabel(,angle(45)) The predictive marginal probability of smoking by class(adding covariance)\rIn the predicted probability metric, the size of the effect of a variable can (and will) vary as a function of the value of the covariates. By comparison, in the logit metric (like any linear model), the size of the effect of a variable remains constant regardless of the values of the covariate. Let’s explore this point by using the contrast command to estimate the effect of class. contrast ar.class,nowald pveffects //Compare each level of class with the previous level of class Contrasts of marginal linear predictions\rMargins: asbalanced\r-------------------------------------------------------------------------\r| Contrast Std. err. z P\u003e|z|\r---------------------------------+---------------------------------------\rclass |\r(working class vs lower class) | -0.26 0.07 -3.47 0.001\r(middle class vs working class) | -0.19 0.04 -5.14 0.000\r(upper class vs middle class) | -0.07 0.11 -0.64 0.523\r-------------------------------------------------------------------------\rThese differences are computed and expressed in the log-odds metric, the natural (linear) metric for the model. The magnitude of these group differences and their significance would remain constant at any level of the covariates. Let’s form these same comparisons but instead using the margins command, forming the comparisons using the predicted probability metric. margins ar.class,contrast(nowald pveffects) Contrasts of predictive margins Number of obs = 15,375\rModel VCE: OIM\rExpression: Pr(smoke), predict()\r-------------------------------------------------------------------------\r| Delta-method\r| Contrast std. err. z P\u003e|z|\r---------------------------------+---------------------------------------\rclass |\r(working class vs lower class) | -0.06 0.02 -3.41 0.001\r(middle class vs working class) | -0.04 0.01 -5.14 0.000\r(upper class vs middle class) | -0.01 0.02 -0.65 0.519\r--------------------------------------------------","date":"2024-01-15","objectID":"/18.chapter18nonlinear-models/:1:3","tags":["Interaction","stata"],"title":"Chapter18 ：Nonlinear models","uri":"/18.chapter18nonlinear-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2 Multinomial logistic regression Let’s now consider a multinomial logistic regression model, focusing on how the commands contrast, pwcompare, margins, and marginsplot can be used after fitting such a model. Let’s model this happiness rating as a function of gender, class, education, and year of interview. The mlogit command chooses the most frequent outcome (which was the second outcome, pretty happy) as the base outcome. use gss_ivrm.dta mlogit haprate i.gender i.class educ yrint,nolog Multinomial logistic regression Number of obs = 48,409\rLR chi2(12) = 2076.07\rProb \u003e chi2 = 0.0000\rLog likelihood = -44799.143 Pseudo R2 = 0.0226\r--------------------------------------------------------------------------------\rhaprate | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r---------------+----------------------------------------------------------------\rnot_too_happy |\rgender |\rFemale | 0.02 0.03 0.67 0.502 -0.04 0.08\r|\rclass |\rworking class | -0.99 0.05 -20.60 0.000 -1.08 -0.90\rmiddle class | -1.19 0.05 -23.33 0.000 -1.29 -1.09\rupper class | -0.89 0.10 -8.65 0.000 -1.09 -0.69\r|\reduc | -0.08 0.00 -16.13 0.000 -0.09 -0.07\ryrint | 0.00 0.00 2.55 0.011 0.00 0.01\r_cons | -6.38 2.65 -2.41 0.016 -11.59 -1.18\r---------------+----------------------------------------------------------------\rpretty_happy | (base outcome)\r---------------+----------------------------------------------------------------\rvery_happy |\rgender |\rFemale | 0.07 0.02 3.47 0.001 0.03 0.11\r|\rclass |\rworking class | 0.33 0.06 5.82 0.000 0.22 0.44\rmiddle class | 0.74 0.06 12.91 0.000 0.63 0.85\rupper class | 1.18 0.08 15.25 0.000 1.03 1.33\r|\reduc | -0.00 0.00 -0.04 0.966 -0.01 0.01\ryrint | -0.01 0.00 -6.59 0.000 -0.01 -0.00\r_cons | 11.27 1.88 5.99 0.000 7.58 14.96\r--------------------------------------------------------------------------------\rI find it hard to interpret the model using the coefficients. I find it far easier to interpret the results using the contrast, pwcompare, margins, and marginsplot commands. We can use the contrast command to obtain the overall effect of class. By default, this test is performed for the first equation (that is, for outcome 1, not too happy). This test is significant. contrast class Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------\r| df chi2 P\u003echi2\r-------------+----------------------------------\rnot_too_ha~y |\rclass | 3 558.71 0.0000\r------------------------------------------------\rAdding the equation(3) option performs the contrast with respect to the third outcome (that is, very happy). This test is also significant. contrast class,equation(3) Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------\r| df chi2 P\u003echi2\r-------------+----------------------------------\rvery_happy |\rclass | 3 564.26 0.0000\r------------------------------------------------\rThe atequations option can be used to apply the contrast command with respect to all the equations. The output of this command matches what we saw in the previous two contrast commands. contrast class,atequations Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------\r| df chi2 P\u003echi2\r-------------+----------------------------------\rnot_too_ha~y |\rclass | 3 558.71 0.0000\r-------------+----------------------------------\rpretty_happy |\rclass | (omitted)\r-------------+----------------------------------\rvery_happy |\rclass | 3 564.26 0.0000\r------------------------------------------------\rWe can use contrast operators with the contrast command to make specific comparisons among groups. In the example below, the ar. contrast operator is used to compare adjacent levels of class for the third equation. I also included the rrr option to interpret the results in terms of relative-risk ratios. Each of these contrasts is significant. contrast ar.class,equation(3) nowald pveffects rrr Contrasts of marginal linear predictions\rMargins: a","date":"2024-01-15","objectID":"/18.chapter18nonlinear-models/:2:0","tags":["Interaction","stata"],"title":"Chapter18 ：Nonlinear models","uri":"/18.chapter18nonlinear-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3 Ordinal logistic regression Let’s use the variable haprate from the previous section as the outcome but now model it using an ordinal logistic regression. Let’s use the ologit command to model the three-level variable haprate (1 = not too happy, 2 = pretty happy, and 3 = very happy) as a function of gender, class, education, and year of interview. use gss_ivrm.dta ologit haprate i.gender i.class educ yrint,nolog Ordered logistic regression Number of obs = 48,409\rLR chi2(6) = 1799.46\rProb \u003e chi2 = 0.0000\rLog likelihood = -44937.445 Pseudo R2 = 0.0196\r--------------------------------------------------------------------------------\rhaprate | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r---------------+----------------------------------------------------------------\rgender |\rFemale | 0.06 0.02 3.34 0.001 0.02 0.10\r|\rclass |\rworking class | 0.96 0.04 23.15 0.000 0.88 1.04\rmiddle class | 1.36 0.04 32.12 0.000 1.28 1.44\rupper class | 1.66 0.06 25.75 0.000 1.54 1.79\r|\reduc | 0.03 0.00 10.55 0.000 0.03 0.04\ryrint | -0.01 0.00 -7.84 0.000 -0.01 -0.00\r---------------+----------------------------------------------------------------\r/cut1 | -13.41 1.65 -16.64 -10.19\r/cut2 | -10.60 1.65 -13.82 -7.37\r--------------------------------------------------------------------------------\rI will bypass interpreting the coefficients and briefly illustrate the use of the contrast, pwcompare, margins, and marginsplot commands. First, let’s consider the contrast command. The contrast command below tests the overall effect of class. contrast class //test the overall effect of class Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------\r| df chi2 P\u003echi2\r-------------+----------------------------------\rhaprate |\rclass | 3 1286.31 0.0000\r------------------------------------------------\rWe can further dissect the overall effect of class through the use of contrast operators. The contrast command below uses the ar. contrast operator to compare each level of class with the previous level. contrast ar.class,nowald pveffects eform //Compare each level of class with the previous level. Contrasts of marginal linear predictions\rMargins: asbalanced\r-------------------------------------------------------------------------\r| exp(b) Std. err. z P\u003e|z|\r---------------------------------+---------------------------------------\rhaprate |\rclass |\r(working class vs lower class) | 2.61 0.11 23.15 0.000\r(middle class vs working class) | 1.50 0.03 20.95 0.000\r(upper class vs middle class) | 1.35 0.07 5.86 0.000\r-------------------------------------------------------------------------\rThe exponentiated coefficients can be very abstract. Instead, let’s compute the predictive marginal probability of being very happy (the third response) as a function of self-identified social class. The predictive marginal probability of rating oneself as very happy was 43.7% for those identifying themselves as upper class. margins class,predict(pr outcome(3)) Predictive margins Number of obs = 48,409\rModel VCE: OIM\rExpression: Pr(haprate==3), predict(pr outcome(3))\r--------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. z P\u003e|z| [95% conf. interval]\r---------------+----------------------------------------------------------------\rclass |\rlower class | 0.13 0.00 28.33 0.000 0.12 0.14\rworking class | 0.28 0.00 97.08 0.000 0.27 0.28\rmiddle class | 0.37 0.00 115.21 0.000 0.36 0.37\rupper class | 0.44 0.01 35.61 0.000 0.41 0.46\r--------------------------------------------------------------------------------\rBy applying the ar. contrast operator, we can obtain comparisons among the adjacent levels of social class. margins ar.class,predict(pr outcome(3)) contrast(pveffects nowald) //Comparisons among the adjacent levels of social class. Contrasts of predictive margins Number of obs = 48,409\rModel VCE: OIM\rExpression: Pr(haprate==3), predict(pr outcome(3))\r---------------------------------","date":"2024-01-15","objectID":"/18.chapter18nonlinear-models/:3:0","tags":["Interaction","stata"],"title":"Chapter18 ：Nonlinear models","uri":"/18.chapter18nonlinear-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4 Poisson regression Let’s now briefly consider a Poisson model, showing the use of the contrast, pwcompare, margins, and marginsplot commands following the use of the poisson command. Let’s fit a model predicting the number of children a person has from gender, class, education, and year of interview use gss_ivrm.dta poisson children i.gender i.class educ yrint Iteration 0: Log likelihood = -95914.71 Iteration 1: Log likelihood = -95914.709 Poisson regression Number of obs = 51,417\rLR chi2(6) = 5773.21\rProb \u003e chi2 = 0.0000\rLog likelihood = -95914.709 Pseudo R2 = 0.0292\r--------------------------------------------------------------------------------\rchildren | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r---------------+----------------------------------------------------------------\rgender |\rFemale | 0.13 0.01 20.47 0.000 0.12 0.14\r|\rclass |\rworking class | -0.09 0.01 -7.18 0.000 -0.12 -0.07\rmiddle class | -0.05 0.01 -3.58 0.000 -0.07 -0.02\rupper class | 0.06 0.02 2.87 0.004 0.02 0.10\r|\reduc | -0.07 0.00 -68.73 0.000 -0.07 -0.07\ryrint | -0.00 0.00 -3.75 0.000 -0.00 -0.00\r_cons | 3.68 0.58 6.38 0.000 2.55 4.82\r--------------------------------------------------------------------------------\rAs we have seen before, the contrast command can be used to test the overall effect of class. Contrasts of marginal linear predictions Margins: asbalanced\r------------------------------------------------\r| df chi2 P\u003echi2\r-------------+----------------------------------\rclass | 3 129.31 0.0000\r------------------------------------------------\rThe ar. contrast operator is used to compare adjacent levels of class, comparing each class with the previous class. contrast ar.class,nowald pveffects //compare the adjacent levels of class,comparing each class with the previous class Contrasts of marginal linear predictions\rMargins: asbalanced\r-------------------------------------------------------------------------\r| Contrast Std. err. z P\u003e|z|\r---------------------------------+---------------------------------------\rclass |\r(working class vs lower class) | -0.09 0.01 -7.18 0.000\r(middle class vs working class) | 0.05 0.01 6.69 0.000\r(upper class vs middle class) | 0.11 0.02 6.00 0.000\r-------------------------------------------------------------------------\rWe can use the pwcompare command to form pairwise comparisons among the four class groups. pwcompare class,pveffects Pairwise comparisons of marginal linear predictions\rMargins: asbalanced\r-----------------------------------------------------------------------\r| Unadjusted\r| Contrast Std. err. z P\u003e|z|\r-------------------------------+---------------------------------------\rchildren |\rclass |\rworking class vs lower class | -0.09 0.01 -7.18 0.000\rmiddle class vs lower class | -0.05 0.01 -3.58 0.000\rupper class vs lower class | 0.06 0.02 2.87 0.004\rmiddle class vs working class | 0.05 0.01 6.69 0.000\rupper class vs working class | 0.15 0.02 8.48 0.000\rupper class vs middle class | 0.11 0.02 6.00 0.000\r-----------------------------------------------------------------------\rWhen using the margins command, the default is to compute the predicted number of events. The margins command below computes the predicted number of children by class. margins class,nopvalues //compute the predicted number of children class Predictive margins Number of obs = 51,417\rModel VCE: OIM\rExpression: Predicted number of events, predict()\r----------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r---------------+------------------------------------------------\rclass |\rlower class | 2.08 0.03 2.03 2.12\rworking class | 1.89 0.01 1.87 1.91\rmiddle class | 1.98 0.01 1.96 2.00\rupper class | 2.21 0.04 2.13 2.28\r----------------------------------------------------------------\rThe margins command is used to compute the predicted number of children for those with 5 to 18 years of education. margins,at(educ=(5(1)18)) marginsplot Predictive margins Number of obs = 51,417\rMod","date":"2024-01-15","objectID":"/18.chapter18nonlinear-models/:4:0","tags":["Interaction","stata"],"title":"Chapter18 ：Nonlinear models","uri":"/18.chapter18nonlinear-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"5 More applications of nonlinear models ","date":"2024-01-15","objectID":"/18.chapter18nonlinear-models/:5:0","tags":["Interaction","stata"],"title":"Chapter18 ：Nonlinear models","uri":"/18.chapter18nonlinear-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"5.1 Categorical by categorical interaction This section illustrates a categorical by categorical interaction using a logistic regression. For this example, let’s use the variable fepol as the outcome variable. The respondent was asked if they believed that women are not suited for politics. The variable fepol is coded: 1 = yes and 0 = no. Thus using fepol as our outcome, we will model endorsement of the statement as a function of two categorical predictors: gender (gender) and a three-level measure of education (educ3). This analysis is restricted to interviews conducted between 1972 and 1980. use gss_ivrm keep if yrint \u003c= 1980 logit fepol i.educ3##gender age,nolog Logistic regression Number of obs = 5,014\rLR chi2(6) = 310.52\rProb \u003e chi2 = 0.0000\rLog likelihood = -3313.7435 Pseudo R2 = 0.0448\r------------------------------------------------------------------------------\rfepol | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\reduc3 |\rHS | -0.23 0.10 -2.26 0.024 -0.43 -0.03\rColl | -0.65 0.13 -4.99 0.000 -0.91 -0.40\r|\rgender |\rFemale | 0.19 0.10 1.94 0.052 -0.00 0.39\r|\reduc3#gender |\rHS#Female | -0.14 0.13 -1.06 0.287 -0.39 0.12\rColl#Female | -0.57 0.19 -2.97 0.003 -0.95 -0.20\r|\rage | 0.02 0.00 11.48 0.000 0.02 0.02\r_cons | -0.84 0.12 -6.99 0.000 -1.07 -0.60\r------------------------------------------------------------------------------\rThe contrast command is used to test the overall interaction of educ3 by gender. contrast educ3#gender //test the overall interaction of educ3 by gender Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------\r| df chi2 P\u003echi2\r-------------+----------------------------------\reduc3#gender | 2 8.85 0.0120\r------------------------------------------------\rTo help understand this interaction, let’s use the margins command to estimate the log odds of believing that women are not suited for politics by educ3 and gender. Then, let’s make a graph of these predicted logits using the marginsplot command. margins educ3#gender,nopvalues predict(xb) Predictive margins Number of obs = 5,014\rModel VCE: OIM\rExpression: Linear prediction (log odds), predict(xb)\r----------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r---------------+------------------------------------------------\reduc3#gender |\rnot hs#Male | 0.07 0.08 -0.08 0.22\rnot hs#Female | 0.27 0.07 0.14 0.40\rHS#Male | -0.16 0.06 -0.28 -0.03\rHS#Female | -0.10 0.05 -0.20 0.00\rColl#Male | -0.58 0.11 -0.79 -0.37\rColl#Female | -0.96 0.13 -1.21 -0.71\r----------------------------------------------------------------\rmarginsplot,legend(subtitle(Gender)) Predicted log odds of believing women are not suited for politics by gender and education\rAlthough the log-odds metric is not easy to interpret, we can still glean the trends implied by the gender by education interaction. The graph suggests that the log odds of agreeing with this statement declines with increasing education, and that this decline appears to be stronger for females than for males. test by interaction gender with comparisons of adjacent education levels，test partial interaction by applying the ar.contrast operator to educ3 and interacting that with gender. contrast ar.educ3#gender Contrasts of marginal linear predictions\rMargins: asbalanced\r-----------------------------------------------------------\r| df chi2 P\u003echi2\r------------------------+----------------------------------\reduc3#gender |\r(HS vs not hs) (joint) | 1 1.13 0.2874\r(Coll vs HS) (joint) | 1 5.60 0.0180\rJoint | 2 8.85 0.0120\r-----------------------------------------------------------\rLet’s now assess the gender difference at each level of education. We can do this by testing the simple effect of gender at each level of education using the contrast command below. contrast gender@educ3 Contrasts of marginal linear predictions\rMargins: asbalanced\r----","date":"2024-01-15","objectID":"/18.chapter18nonlinear-models/:5:1","tags":["Interaction","stata"],"title":"Chapter18 ：Nonlinear models","uri":"/18.chapter18nonlinear-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"5.2 Categorical by continuous interaction This section illustrates a categorical by continuous interaction using a binary logistic regression. The outcome for this example is the variable fepres, which is coded: 1 = would vote for a woman president and 0 = would not vote for a woman president. Let’s model this as a function of time (that is, year of interview) and education to see if the linear change in this attitude over time differed by education level. With respect to the year of interview, this question was asked in 17 different years ranging from 1972 to 1998, then it was asked again in 2008 and 2010. Because of the large 10-year gap between 1998 and 2008, we will omit the data for 2008 onward. With respect to education, let’s use the three-level categorical variable educ3, which is coded: 1 = non–high school graduate, 2 = high school graduate, and 3 = college graduate. use gss_ivrm.dta drop if yrint\u003e=2008 logit fepres i.yrint72##educ3 age gender Let’s begin by assessing the trend in the log odds of the outcome across years.we first fit a model using fepres as the outcome predicted by i.yrint72##educ3 as well as age and gender. logit fepres i.yrint72##educ3 age gender Iteration 0: Log likelihood = -9680.7297 Iteration 1: Log likelihood = -8870.722 Iteration 2: Log likelihood = -8766.7906 Iteration 3: Log likelihood = -8765.9168 Iteration 4: Log likelihood = -8765.9147 Iteration 5: Log likelihood = -8765.9147 Logistic regression Number of obs = 23,926\rLR chi2(52) = 1829.63\rProb \u003e chi2 = 0.0000\rLog likelihood = -8765.9147 Pseudo R2 = 0.0945\r-------------------------------------------------------------------------------\rfepres | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r--------------+----------------------------------------------------------------\ryrint72 |\r2 | 0.17 0.14 1.25 0.211 -0.10 0.43\r3 | 0.13 0.13 0.95 0.340 -0.13 0.39\r5 | 0.09 0.13 0.70 0.486 -0.17 0.35\r6 | 0.13 0.14 0.96 0.339 -0.14 0.40\r10 | 0.54 0.14 3.99 0.000 0.28 0.81\r11 | 0.72 0.15 4.67 0.000 0.42 1.02\r13 | 0.23 0.14 1.58 0.114 -0.05 0.51\r14 | 0.60 0.15 3.94 0.000 0.30 0.89\r16 | 0.68 0.18 3.75 0.000 0.33 1.04\r17 | 0.31 0.18 1.70 0.089 -0.05 0.67\r18 | 0.51 0.20 2.50 0.012 0.11 0.91\r19 | 0.89 0.20 4.35 0.000 0.49 1.29\r21 | 0.98 0.21 4.58 0.000 0.56 1.39\r22 | 1.17 0.18 6.36 0.000 0.81 1.52\r24 | 1.10 0.19 5.92 0.000 0.73 1.46\r26 | 1.16 0.19 6.02 0.000 0.78 1.54\r|\reduc3 |\rHS | 0.05 0.13 0.43 0.666 -0.19 0.30\rColl | 0.85 0.24 3.60 0.000 0.39 1.31\r|\ryrint72#educ3 |\r2#HS | 0.44 0.19 2.27 0.023 0.06 0.81\r2#Coll | -0.05 0.33 -0.15 0.884 -0.70 0.60\r3#HS | 0.46 0.19 2.41 0.016 0.08 0.83\r3#Coll | 0.22 0.35 0.63 0.529 -0.47 0.91\r5#HS | 0.36 0.19 1.95 0.051 -0.00 0.73\r5#Coll | 0.53 0.36 1.47 0.142 -0.18 1.23\r6#HS | 0.59 0.19 3.08 0.002 0.22 0.97\r6#Coll | 0.19 0.34 0.55 0.582 -0.48 0.86\r10#HS | 0.61 0.19 3.12 0.002 0.23 0.99\r10#Coll | 0.06 0.35 0.16 0.870 -0.62 0.74\r11#HS | 0.20 0.21 0.97 0.330 -0.20 0.61\r11#Coll | -0.16 0.34 -0.47 0.641 -0.83 0.51\r13#HS | 0.48 0.20 2.45 0.014 0.10 0.86\r13#Coll | 0.31 0.34 0.91 0.365 -0.36 0.99\r14#HS | 0.34 0.21 1.63 0.104 -0.07 0.75\r14#Coll | 0.54 0.39 1.39 0.163 -0.22 1.31\r16#HS | 0.45 0.25 1.78 0.075 -0.04 0.93\r16#Coll | 0.39 0.43 0.89 0.372 -0.46 1.23\r17#HS | 0.83 0.25 3.36 0.001 0.34 1.31\r17#Coll | 0.41 0.41 1.02 0.309 -0.38 1.21\r18#HS | 0.88 0.27 3.20 0.001 0.34 1.41\r18#Coll | 0.78 0.44 1.76 0.078 -0.09 1.64\r19#HS | 0.54 0.27 2.00 0.045 0.01 1.08\r19#Coll | 0.51 0.47 1.09 0.274 -0.41 1.43\r21#HS | 0.39 0.27 1.42 0.156 -0.15 0.92\r21#Coll | 0.31 0.43 0.72 0.470 -0.54 1.16\r22#HS | 0.40 0.23 1.70 0.089 -0.06 0.86\r22#Coll | 0.04 0.35 0.11 0.909 -0.65 0.73\r24#HS | 0.61 0.24 2.53 0.011 0.14 1.08\r24#Coll | 0.58 0.39 1.49 0.137 -0.18 1.34\r26#HS | 0.70 0.25 2.77 0.006 0.21 1.19\r26#Coll | 0.53 0.40 1.32 0.186 -0.25 1.31\r|\rage | -0.03 0.00 -23.33 0.000 -0.03 -0.02\rgender | -0.02 0.04 -0.50 0.617 -0.10 0.06\r_cons | 2.19 0.12 17.85 0.000 1.95 2.43\r-------------------------------------------------------------------------------\rWe then use t","date":"2024-01-15","objectID":"/18.chapter18nonlinear-models/:5:2","tags":["Interaction","stata"],"title":"Chapter18 ：Nonlinear models","uri":"/18.chapter18nonlinear-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"5.3 Piecewise modeling This section illustrates modeling a continuous predictor fit using piecewise modeling in the context of a logistic regression model. Let’s begin by inspecting the nature of the relationship between education and smoking status. we first fit a model predicting smoking status from education, treating education as a categorical variable. Age is also included in the model as a covariate. The output of the logit command is omitted to save space. use gss_ivrm.dta logit smoke i.educ age // treating education as a categorical variable. Iteration 0: Log likelihood = -10539.7 Iteration 1: Log likelihood = -10121.92 Iteration 2: Log likelihood = -10117.862 Iteration 3: Log likelihood = -10117.858 Iteration 4: Log likelihood = -10117.858 Logistic regression Number of obs = 16,274\rLR chi2(21) = 843.69\rProb \u003e chi2 = 0.0000\rLog likelihood = -10117.858 Pseudo R2 = 0.0400\r------------------------------------------------------------------------------\rsmoke | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\reduc |\r1 | 0.05 0.66 0.08 0.940 -1.24 1.34\r2 | -0.33 0.57 -0.58 0.563 -1.44 0.78\r3 | 0.63 0.42 1.49 0.136 -0.20 1.46\r4 | -0.05 0.41 -0.11 0.912 -0.85 0.76\r5 | 0.16 0.40 0.39 0.694 -0.63 0.94\r6 | -0.08 0.39 -0.22 0.828 -0.84 0.67\r7 | 0.05 0.38 0.12 0.901 -0.69 0.79\r8 | -0.01 0.36 -0.03 0.975 -0.73 0.70\r9 | 0.35 0.37 0.95 0.342 -0.37 1.07\r10 | 0.44 0.37 1.21 0.227 -0.27 1.16\r11 | 0.35 0.36 0.97 0.333 -0.36 1.07\r12 | -0.17 0.36 -0.47 0.639 -0.88 0.54\r13 | -0.36 0.36 -0.97 0.330 -1.07 0.36\r14 | -0.40 0.36 -1.09 0.276 -1.11 0.32\r15 | -0.43 0.37 -1.15 0.248 -1.15 0.30\r16 | -0.84 0.36 -2.31 0.021 -1.56 -0.13\r17 | -1.16 0.38 -3.06 0.002 -1.91 -0.42\r18 | -1.14 0.38 -3.00 0.003 -1.88 -0.39\r19 | -0.86 0.40 -2.15 0.032 -1.65 -0.08\r20 | -0.93 0.39 -2.36 0.018 -1.70 -0.16\r|\rage | -0.02 0.00 -18.58 0.000 -0.02 -0.02\r_cons | 0.49 0.37 1.33 0.185 -0.23 1.20\r------------------------------------------------------------------------------\rLet’s now make a graph that shows the predicted logit of smoking as a function of education, adjusting for age. margins educ,predict(xb) marginsplot,xline(12 16) Log odds of smoking by education\rThe graph in figure above includes vertical lines at 12 and 16 years of education. These junctures seem like excellent candidates for the placement of knots where there is a change in slope and a change in intercept. Let’s fit such a model below mkspline edprehsm 12 edhsm 16 edcom = educ,marginal logit smoke c.edprehsm c.edhsm c.edcom hsgrad cograd age,nolog Logistic regression Number of obs = 16,274\rLR chi2(6) = 806.95\rProb \u003e chi2 = 0.0000\rLog likelihood = -10136.225 Pseudo R2 = 0.0383\r------------------------------------------------------------------------------\rsmoke | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\redprehsm | 0.05 0.01 3.68 0.000 0.03 0.08\redhsm | -0.16 0.03 -5.83 0.000 -0.21 -0.10\redcom | 0.06 0.04 1.40 0.161 -0.02 0.14\rhsgrad | -0.60 0.06 -9.60 0.000 -0.72 -0.48\rcograd | -0.30 0.10 -3.18 0.001 -0.49 -0.12\rage | -0.02 0.00 -19.10 0.000 -0.02 -0.02\r_cons | 0.27 0.15 1.76 0.078 -0.03 0.57\r------------------------------------------------------------------------------\rBefore interpreting the results, let’s create a graph of the predicted log odds of smoking as a function of education. margins,at(edprehsm=0 edhsm=0 edcom=0 hsgrad=0 cograd=0) /// at(edprehsm=12 edhsm=0 edcom=0 hsgrad=0 cograd=0) /// at(edprehsm=12 edhsm=0 edcom=0 hsgrad=1 cograd=0) /// at(edprehsm=16 edhsm=4 edcom=0 hsgrad=1 cograd=0) /// at(edprehsm=16 edhsm=4 edcom=0 hsgrad=1 cograd=1) /// at(edprehsm=20 edhsm=8 edcom=4 hsgrad=1 cograd=1) /// predict(xb) noatlegend mat yhat = r(b)' mat educ = (0 \\ 12 \\ 12 \\ 16 \\ 16\\ 20) svmat yhat svmat educ graph twoway line yhat1 educ1,xline(12 16) title(\"Piecewise Model\") Predicted log odds of smoking from education fit using a piecew","date":"2024-01-15","objectID":"/18.chapter18nonlinear-models/:5:3","tags":["Interaction","stata"],"title":"Chapter18 ：Nonlinear models","uri":"/18.chapter18nonlinear-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter considers models where time is treated as a categorical variable","date":"2024-01-14","objectID":"/17.chapter17time-as-a-categorical-predictor/","tags":["Multilevel","Interaction","stata"],"title":"Chapter17 ：Time as a categorical predictor","uri":"/17.chapter17time-as-a-categorical-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter considers models where time is treated as a categorical variable The models presented in this chapter will use the mixed command to fit a model that is a hybrid of a traditional repeated-measures analysis of variance (ANOVA) and a mixed model. Like the repeated-measures ANOVA, there will be one fixed intercept (rather than having random intercepts that we commonly see when using the mixed command). By specifying the noconstant option in the random-effects portion of the mixed command, a fixed intercept will be estimated. To account for the nonindependence of residuals across time points, we will use the residuals() option within the random-effects portion of the mixed command. This allows us to model the structure of the residual covariances across time points. The following examples will use an unstructured residual covariance, which estimates a separate residual variance for each time point and a separate residual correlation among each pair of the time points. ","date":"2024-01-14","objectID":"/17.chapter17time-as-a-categorical-predictor/:0:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter17 ：Time as a categorical predictor","uri":"/17.chapter17time-as-a-categorical-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 Example 1: Time treated as a categorical variable Sleep was measured at three time points, the first a control (baseline) condition and the second and third measurements while taking a sleep medication. here are two aims of this study. The first aim is to assess the initial impact of sleep medication on duration of sleep.So the second aim is to the assess the sustained effectiveness of the medication on sleep, comparing the amount of sleep in the second and third months use sleep_cat3.dta summarize Note! Wide and long datasets Data for this kind of study might be stored with one observation per person and three variables representing the different time points. Sometimes, this is called a multivariate format, and Stata would call this a wide format. If your dataset is in that kind of form, you can use the reshape command to convert it to a long format. Let’s now use the mixed command to predict sleep from month, treating month as a categorical variable. Specifying || id: introduces the random-effects part of the model and indicates that the observations are nested within id. Next, the noconstant option is specified in the random-effects options so only one fixed intercept is fit. Furthermore, the residuals() option is included to specify covariance structure of the residuals between months (within each level of the person’s ID). The residuals() option specifies an unstructured residual covariance among the different months within each person. This accounts for the nonindependence of the observations among time points for each person. mixed sleep i.month || id:,noconstant residuals(unstructured,t(month))nolog Mixed-effects ML regression Number of obs = 300\rGroup variable: id Number of groups = 100\rObs per group:\rmin = 3\ravg = 3.0\rmax = 3\rWald chi2(2) = 35.94\rLog likelihood = -1437.3712 Prob \u003e chi2 = 0.0000\r------------------------------------------------------------------------------\rsleep | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rmonth |\r2 | 21.07 3.83 5.50 0.000 13.56 28.58\r3 | 18.43 3.81 4.84 0.000 10.97 25.89\r|\r_cons | 348.24 3.14 110.73 0.000 342.08 354.40\r------------------------------------------------------------------------------\r------------------------------------------------------------------------------\rRandom-effects parameters | Estimate Std. err. [95% conf. interval]\r-----------------------------+------------------------------------------------\rid: (empty) |\r-----------------------------+------------------------------------------------\rResidual: Unstructured |\rvar(e1) | 989.04 139.87 749.61 1304.95\rvar(e2) | 928.55 131.32 703.77 1225.14\rvar(e3) | 731.36 103.43 554.31 964.96\rcov(e1,e2) | 224.17 98.42 31.27 417.06\rcov(e1,e3) | 135.43 86.12 -33.37 304.22\rcov(e2,e3) | 107.72 83.11 -55.17 270.61\r------------------------------------------------------------------------------\rLR test vs. linear model: chi2(5) = 11.61 Prob \u003e chi2 = 0.0405\rNote: The reported degrees of freedom assumes the null hypothesis is not on the boundary of the parameter\rspace. If this is not true, then the reported test is conservative.\rBefore we interpret the coefficients from this model, let’s use the margins command to compute the predicted mean of sleep for each level of month, as shown below. margins month,nopvalues Adjusted predictions Number of obs = 300\rExpression: Linear prediction, fixed portion, predict()\r--------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r-------------+------------------------------------------------\rmonth |\r1 | 348.24 3.14 342.08 354.40\r2 | 369.31 3.05 363.34 375.28\r3 | 366.67 2.70 361.37 371.97\r--------------------------------------------------------------\rWe can use the marginsplot command to create a graph showing the predicted mean of sleep across the three months, as shown below. marginsplot Estimated minutes of sleep at night by month\rBefore making those spec","date":"2024-01-14","objectID":"/17.chapter17time-as-a-categorical-predictor/:1:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter17 ：Time as a categorical predictor","uri":"/17.chapter17time-as-a-categorical-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2 Example 2: Time (categorical) by two groups This study includes 100 participants in the treatment group and 100 participants in the control group. The main predictors for this example are group (a two-level categorical variable) and month (a three-level categorical variable). use sleep_catcat23.dta list in 1/6,sepby(id) summarize mixed sleep i.group##i.month || id:,noconstant residuals(un,t(month))nolog Mixed-effects ML regression Number of obs = 600\rGroup variable: id Number of groups = 200\rObs per group:\rmin = 3\ravg = 3.0\rmax = 3\rWald chi2(5) = 68.47\rLog likelihood = -2879.78 Prob \u003e chi2 = 0.0000\r-------------------------------------------------------------------------------\rsleep | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r--------------+----------------------------------------------------------------\rgroup |\rMedication | -7.08 4.38 -1.62 0.106 -15.66 1.50\r|\rmonth |\r2 | -3.73 3.75 -1.00 0.319 -11.07 3.61\r3 | -0.57 4.26 -0.13 0.893 -8.91 7.77\r|\rgroup#month |\rMedication#2 | 28.06 5.30 5.30 0.000 17.67 38.45\rMedication#3 | 24.94 6.02 4.14 0.000 13.14 36.74\r|\r_cons | 350.63 3.10 113.24 0.000 344.56 356.70\r-------------------------------------------------------------------------------\r------------------------------------------------------------------------------\rRandom-effects parameters | Estimate Std. err. [95% conf. interval]\r-----------------------------+------------------------------------------------\rid: (empty) |\r-----------------------------+------------------------------------------------\rResidual: Unstructured |\rvar(e1) | 958.68 95.87 788.05 1166.26\rvar(e2) | 721.30 72.13 592.92 877.48\rvar(e3) | 980.26 98.03 805.78 1192.50\rcov(e1,e2) | 138.07 59.61 21.25 254.90\rcov(e1,e3) | 63.24 68.69 -71.40 197.87\rcov(e2,e3) | 119.72 60.06 2.01 237.43\r------------------------------------------------------------------------------\rLR test vs. linear model: chi2(5) = 15.70 Prob \u003e chi2 = 0.0078\rNote: The reported degrees of freedom assumes the null hypothesis is not on the boundary of the parameter\rspace. If this is not true, then the reported test is conservative.\rWe can estimate the mean sleep by group and month using the margins command below. margins month#group,nopvalues marginsplot,noci Adjusted predictions Number of obs = 600\rExpression: Linear prediction, fixed portion, predict()\r---------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r--------------+------------------------------------------------\rmonth#group |\r1#Control | 350.63 3.10 344.56 356.70\r1#Medication | 343.55 3.10 337.48 349.62\r2#Control | 346.90 2.69 341.64 352.16\r2#Medication | 367.88 2.69 362.62 373.14\r3#Control | 350.06 3.13 343.92 356.20\r3#Medication | 367.92 3.13 361.78 374.06\r---------------------------------------------------------------\rEstimated sleep by month and treatment group\rBefore testing our two main questions of interest, let’s assess the overall interaction of group by month using the contrast command below. The overall interaction is significant. contrast group#month Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------\r| df chi2 P\u003echi2\r-------------+----------------------------------\rsleep |\rgroup#month | 2 30.21 0.0000\r------------------------------------------------\rNow to test our questions of interest regarding the initial effect of medication and the sustained effect of medication, we can apply the ar. contrast operator to month and interact that with group, as shown below. contrast ar.month#group,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r-----------------------------------------------------------------------\r| Contrast Std. err. z P\u003e|z|\r-------------------------------+---------------------------------------\rsleep |\rmonth#group |\r(2 vs 1) (Medication vs base) | 28.06 5.30 5.30 0.000\r(3 vs 2) (Medication vs base) | -3.12 5.41 -0.58 0.564\r--------------------------------------------","date":"2024-01-14","objectID":"/17.chapter17time-as-a-categorical-predictor/:2:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter17 ：Time as a categorical predictor","uri":"/17.chapter17time-as-a-categorical-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3 Example 3: Time (categorical) by three groups This study includes 300 participants, 100 assigned to each of the three treatment groups. The main variables of interest in this study are treatment group (with three levels) and month (with three levels). use sleep_catcat33.dta list in 1/6,sepby(id) summarize mixed sleep i.group##i.month ||id:,noconstant residuals(un,t(month)) nolog Mixed-effects ML regression Number of obs = 900\rGroup variable: id Number of groups = 300\rObs per group:\rmin = 3\ravg = 3.0\rmax = 3\rWald chi2(8) = 284.84\rLog likelihood = -4318.4068 Prob \u003e chi2 = 0.0000\r-------------------------------------------------------------------------------\rsleep | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r--------------+----------------------------------------------------------------\rgroup |\rMedication | 2.60 4.25 0.61 0.540 -5.72 10.92\rEducation | 3.17 4.25 0.75 0.455 -5.15 11.49\r|\rmonth |\r2 | 8.06 3.72 2.17 0.030 0.77 15.35\r3 | 13.35 4.06 3.29 0.001 5.39 21.31\r|\rgroup#month |\rMedication#2 | 21.88 5.26 4.16 0.000 11.57 32.19\rMedication#3 | 18.22 5.75 3.17 0.002 6.96 29.48\rEducation#2 | 7.11 5.26 1.35 0.177 -3.20 17.42\rEducation#3 | 34.49 5.75 6.00 0.000 23.23 45.75\r|\r_cons | 344.70 3.00 114.80 0.000 338.82 350.58\r-------------------------------------------------------------------------------\r------------------------------------------------------------------------------\rRandom-effects parameters | Estimate Std. err. [95% conf. interval]\r-----------------------------+------------------------------------------------\rid: (empty) |\r-----------------------------+------------------------------------------------\rResidual: Unstructured |\rvar(e1) | 901.52 73.61 768.20 1057.98\rvar(e2) | 836.25 68.28 712.59 981.38\rvar(e3) | 918.42 74.99 782.60 1077.81\rcov(e1,e2) | 177.08 51.16 76.80 277.35\rcov(e1,e3) | 84.56 52.76 -18.85 187.97\rcov(e2,e3) | 160.48 51.44 59.66 261.30\r------------------------------------------------------------------------------\rLR test vs. linear model: chi2(5) = 24.71 Prob \u003e chi2 = 0.0002\rNote: The reported degrees of freedom assumes the null hypothesis is not on the boundary of the parameter\rspace. If this is not true, then the reported test is conservative.\rThe margins and marginsplot commands are used below to estimate the predicted mean of sleep by group and month and to graph the results. margins month#group,nopvalues marginsplot,noci Adjusted predictions Number of obs = 900\rExpression: Linear prediction, fixed portion, predict()\r---------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r--------------+------------------------------------------------\rmonth#group |\r1#Control | 344.70 3.00 338.82 350.58\r1#Medication | 347.30 3.00 341.42 353.18\r1#Education | 347.87 3.00 341.99 353.75\r2#Control | 352.76 2.89 347.09 358.43\r2#Medication | 377.24 2.89 371.57 382.91\r2#Education | 363.04 2.89 357.37 368.71\r3#Control | 358.05 3.03 352.11 363.99\r3#Medication | 378.87 3.03 372.93 384.81\r3#Education | 395.71 3.03 389.77 401.65\r---------------------------------------------------------------\rSleep by month and treatment group\rLet’s now use the contrast command to test the group by month interaction. This overall interaction is significant. contrast group#month Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------\r| df chi2 P\u003echi2\r-------------+----------------------------------\rsleep |\rgroup#month | 4 62.05 0.0000\r------------------------------------------------\rLet’s form an interaction contrast in which we apply reference group contrasts to treatment group (r.group) and reverse adjacent group contrasts to month (ar.month). contrast ar.month#r.group,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r--------------------------------------------------------------------------\r| Contrast Std. err. z P\u003e|z|\r----------------------------------+---------------------------------------\rslee","date":"2024-01-14","objectID":"/17.chapter17time-as-a-categorical-predictor/:3:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter17 ：Time as a categorical predictor","uri":"/17.chapter17time-as-a-categorical-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4 Comparing models with different residual covariance structures The selection of the covariance structure impacts the estimates of the standard errors of the coefficients, but not the point estimates of the coefficients. When you have only three time points, an unstructured covariance can be a good choice. However, as the number of time points increases, the number of variances and covariances estimated by an unstructured covariance matrix increases dramatically. For example, if you have five time points, an unstructured covariance estimates five variances and 10 covariances (a total of 15 parameters). In such cases, you might consider more parsimonious covariance structures, such as the exchangeable, ar (autoregressive), or banded residual types. This leads to the question of how to choose among models using different covariance structures. My first recommendation would be to select a residual covariance structure that is grounded in theory or suggested by previous research. However, such information may be scarce or nonexistent. In such cases, you can fit different covariance structures seeking the residual covariance structure that combines the fewest parameters with the best measure of fit—using, for example, Akaike information criterion (AIC) or Bayesian information criterion (BIC). Stata makes this process easy, as illustrated below use sleep_cat3.dta mixed sleep i.month || id:,noconstant residuals(unstructured,t(month)) estimate store m_un mixed sleep i.month || id:,noconstant residuals(exchangeable,t(month)) estimate store m_ex mixed sleep i.month || id:,noconstant residuals(ar 1,t(month)) estimate store m_ar1 Using the dataset from example 1, models are fit using three different covariance structures: unstructured, exchangeable, and ar 1. After fitting each model, the estimates store command is used to store the estimates from the respective model. estimate stats m_un m_ex m_ar1 Akaike's information criterion and Bayesian information criterion\r-----------------------------------------------------------------------------\rModel | N ll(null) ll(model) df AIC BIC\r-------------+---------------------------------------------------------------\rm_un | 900 . -4367.631 9 8753.262 8796.484\rm_ex | 900 . -4372.205 5 8754.409 8778.421\rm_ar1 | 900 . -4370.91 5 8751.82 8775.832\r-----------------------------------------------------------------------------\rNote: BIC uses N = number of observations. See [R] IC note.\rRemember that when it comes to AIC and BIC, smaller is better. The ar 1 and exchangeable models have smaller AIC values than the unstructured model. Likewise, the ar 1 and exchangeable models also have smaller BIC values than the unstructured model. The ar 1 and exchangeable models also have the added benefit of including four fewer residual covariance parameters (5 versus 9). The ar 1 and exchangeable covariance structure appear to provide a fairly similar quality of fit, and both fit better than the unstructured covariance structure. Warning! Likelihood-ratio test It is tempting to ask whether the difference in covariance structures is significantly different and to want to use a command like lrtest to test whether one covariance structure fits significantly better than another. A key assumption of a likelihood-ratio test is that one model is nested within another model, where one model can be created from the other by omitting one or more parameters. In many (or perhaps most) cases, the models formed by comparing two different residual covariance structures are not nested within each other and the likelihood-ratio test is not valid. However, the AIC and BIC indices can be used even when models are not nested within each other. See [ME] mixed for a list of all available covariance structures you can choose within the residuals() option. Furthermore, chapter 7 of Singer and Willett (2003) provides additional descriptions of these residual covariance structures, including information to help you choose among the different st","date":"2024-01-14","objectID":"/17.chapter17time-as-a-categorical-predictor/:4:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter17 ：Time as a categorical predictor","uri":"/17.chapter17time-as-a-categorical-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"5 Analyses with small samples The mixed command uses large-sample methods. Applying such methods with smallsample sizes may lead to overly liberal statistical tests (that is, greater type I error rates). The mixed command allows you to specify the dfmethod() option to select testing methods that are more appropriate for small-sample sizes. Specifying the dfmethod() option is easy—knowing the best method to select is not so easy. For an introduction to the different small-sample methods you can choose from, I encourage you to see help mixed, especially the section in the PDF documentation titled Small-sample inference for fixed effects. That section describes five different small-sample adjustment methods: residual, repeated, anova, satterthwaite, and kroger. This leaves two remaining methods to consider: dfmethod(kroger) and dfmethod(satterthwaite). The dfmethod(kroger) option (described in Kenward and Roger (1997)) and dfmethod(satterthwaite) option (described in Satterthwaite (1946)) offer methods that are more applicable when making inferences with longitudinal models with small-sample sizes. There will likely be continued research in this area, and the potential for new techniques and options to arise. For now, it seems that the Kenward–Roger method is probably the most generally useful method available, although sensitivity analyses considering the Satterthwaite method would seem prudent. use sleep_catcat23small.dta count tab group month sort id month list in 1/20,sepby(id) summarize use large-sample statistical methods.(note how the significance of each parameter is tested using z-test) mixed sleep i.group##i.month || id:,noconstant residuals(un,t(month)) Performing gradient-based optimization: Iteration 0: Log likelihood = -364.19065 Iteration 1: Log likelihood = -363.52715 Iteration 2: Log likelihood = -363.12776 Iteration 3: Log likelihood = -363.1244 Iteration 4: Log likelihood = -363.1244 Computing standard errors ...\rMixed-effects ML regression Number of obs = 80\rGroup variable: id Number of groups = 30\rObs per group:\rmin = 1\ravg = 2.7\rmax = 3\rWald chi2(5) = 27.47\rLog likelihood = -363.1244 Prob \u003e chi2 = 0.0000\r-------------------------------------------------------------------------------\rsleep | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r--------------+----------------------------------------------------------------\rgroup |\rMedication | -19.76 9.34 -2.12 0.034 -38.06 -1.47\r|\rmonth |\r2 | -6.20 8.58 -0.72 0.470 -23.01 10.62\r3 | -20.99 10.16 -2.07 0.039 -40.91 -1.07\r|\rgroup#month |\rMedication#2 | 41.98 11.93 3.52 0.000 18.58 65.37\rMedication#3 | 48.40 14.15 3.42 0.001 20.68 76.13\r|\r_cons | 361.38 6.60 54.75 0.000 348.45 374.32\r-------------------------------------------------------------------------------\r------------------------------------------------------------------------------\rRandom-effects parameters | Estimate Std. err. [95% conf. interval]\r-----------------------------+------------------------------------------------\rid: (empty) |\r-----------------------------+------------------------------------------------\rResidual: Unstructured |\rvar(e1) | 612.04 163.59 362.46 1033.48\rvar(e2) | 459.72 122.77 272.38 775.92\rvar(e3) | 505.46 145.96 287.00 890.21\rcov(e1,e2) | 40.15 105.05 -165.75 246.05\rcov(e1,e3) | -111.18 115.76 -338.07 115.71\rcov(e2,e3) | 60.67 110.88 -156.66 277.99\r------------------------------------------------------------------------------\rLR test vs. linear model: chi2(5) = 2.13 Prob \u003e chi2 = 0.8305\rNote: The reported degrees of freedom assumes the null hypothesis is not on the boundary of the parameter\rspace. If this is not true, then the reported test is conservative.\rThe contrast command is used below to compare the change in sleep across months between the medication group versus the control group. contrast ar.month#group,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r-----------------------------------------------------------------------\r| Contrast Std. er","date":"2024-01-14","objectID":"/17.chapter17time-as-a-categorical-predictor/:5:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter17 ：Time as a categorical predictor","uri":"/17.chapter17time-as-a-categorical-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter considers models that involve the interaction of two categorical predictors with a linear continuous predictor.","date":"2024-01-13","objectID":"/14.chapter14continuous-by-categorical-by-categorical-interactions/","tags":["Interaction","stata"],"title":"Chapter14 ：Continuous by continuous by categorical interactions","uri":"/14.chapter14continuous-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter considers models that involve the interaction of two categorical predictors with a linear continuous predictor. ","date":"2024-01-13","objectID":"/14.chapter14continuous-by-categorical-by-categorical-interactions/:0:0","tags":["Interaction","stata"],"title":"Chapter14 ：Continuous by continuous by categorical interactions","uri":"/14.chapter14continuous-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 Chapter overview This chapter blends these two modeling techniques by exploring how the slope of the continuous variable varies as a function of the interaction of the two categorical variables. Let’s consider a hypothetical example of a model with income as the outcome variable. The predictors include gender (a two-level categorical variable), education (treated as a three-level categorical variable), and age (a continuous variable). Income can be modeled as a function of each of the predictors, as well as the interactions of all the predictors. A three-way interaction of age by gender by education would imply that the effect of age interacts with gender by education. One way to visualize such an interaction would be to graph age on the $x$ axis, with separate lines for the levels of education and separate graphs for gender. Fitted values of income as a function of age, education, and gender\rThe age slope by level of education and gender\rBut consider the differences in the age slopes between females and males at each level of education. This difference is $-250 (150-400)$ for non–high school graduates, whereas this difference is $-350 (250-600)$ for high school graduates, and the difference is $-700 (600-1300)$ for college graduates. The difference in the age slopes between females and males seems to be much larger for college graduates than for high school graduates and non–high school graduates. Let’s explore this in more detail with an example using the GSS dataset. To focus on the linear effect of age, we will keep those who are 22 to 55 years old. use gss_ivrm.dta keep if (age\u003e=22 \u0026 age\u003c=55) reg realrinc i.gender##i.educ3##c.age i.race,vce(robust) noci Linear regression Number of obs = 25,718\rF(13, 25704) = 411.30\rProb \u003e F = 0.0000\rR-squared = 0.1839\rRoot MSE = 23556\r-----------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t|\r-------------------+---------------------------------------\rgender |\rFemale | 1337.13 1693.69 0.79 0.430\r|\reduc3 |\rHS | 550.48 1782.19 0.31 0.757\rColl | -11156.10 2618.98 -4.26 0.000\r|\rgender#educ3 |\rFemale#HS | 783.10 2021.65 0.39 0.698\rFemale#Coll | 7657.91 3164.30 2.42 0.016\r|\rage | 413.87 45.62 9.07 0.000\r|\rgender#c.age |\rFemale | -264.98 50.66 -5.23 0.000\r|\reduc3#c.age |\rHS | 175.85 54.75 3.21 0.001\rColl | 897.33 77.47 11.58 0.000\r|\rgender#educ3#c.age |\rFemale#HS | -80.31 60.95 -1.32 0.188\rFemale#Coll | -414.66 93.27 -4.45 0.000\r|\rrace |\rblack | -2935.14 273.33 -10.74 0.000\rother | 185.40 956.34 0.19 0.846\r|\r_cons | 2691.23 1495.78 1.80 0.072\r-----------------------------------------------------------\rLet’s test the interaction of gender, education, and age using the contrast command below. The three-way interaction is significant. contrast i.gender#i.educ3#c.age Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------------\r| df F P\u003eF\r-------------------+----------------------------------\rgender#educ3#c.age | 2 10.17 0.0000\r|\rDenominator | 25704\r------------------------------------------------------\rTo begin the process of interpreting the three-way interaction, let’s create a graph of the adjusted means as a function of age, education, and gender. compute the adjusted means by gender and education for age 22 and 55 margins gender#educ3,at(age=(22 55)) Predictive margins Number of obs = 25,718\rModel VCE: Robust\rExpression: Linear prediction, predict()\r1._at: age = 22\r2._at: age = 55\r----------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r-----------------+----------------------------------------------------------------\r_at#gender#educ3 |\r1#Male#not hs | 11404.29 542.03 21.04 0.000 10341.88 12466.70\r1#Male#HS | 15823.46 349.34 45.29 0.000 15138.72 16508.20\r1#Male#Coll | 19989.51 916.25 21.82 0.000 18193.61 21785.41\r1#Female#not hs | 6911.76 353.90 19.53 0.000 6218.10 7605.42\r1#Female#HS | 10347.31 219.7","date":"2024-01-13","objectID":"/14.chapter14continuous-by-categorical-by-categorical-interactions/:1:0","tags":["Interaction","stata"],"title":"Chapter14 ：Continuous by continuous by categorical interactions","uri":"/14.chapter14continuous-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2 Simple effects of gender on the age slope We can use the contrast command to test the simple effect of gender on the age slope. This is illustrated below. contrast gender#c.age@educ3,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r-----------------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r-------------------------+---------------------------------------\rgender@educ3#c.age |\r(Female vs base) not hs | -264.98 50.66 -5.23 0.000\r(Female vs base) HS | -345.29 33.99 -10.16 0.000\r(Female vs base) Coll | -679.64 78.45 -8.66 0.000\r-----------------------------------------------------------------\rThe first test compares the age slope for females versus males among non–high school graduates. Referring to table 14.2, this test compares $\\beta\\tiny 1F$with$\\beta\\tiny 1M$ . The difference in these age slopes is $-264.98(148.89 - 413.87)$, and this difference is significant. The age slope for females who did not graduate high school is 264.98 units smaller than the age slope for males who did not graduate high school. The second test is similar to the first, except the comparison is made among high school graduates, comparing $\\beta\\tiny 2F$ with $\\beta\\tiny 2M$ from table 14.2. This test is also significant. The third test compares the age slope between females and males among college graduates (that is, comparing $\\beta\\tiny 3F$ with $\\beta\\tiny 3M$ ). This test is also significant. ","date":"2024-01-13","objectID":"/14.chapter14continuous-by-categorical-by-categorical-interactions/:2:0","tags":["Interaction","stata"],"title":"Chapter14 ：Continuous by continuous by categorical interactions","uri":"/14.chapter14continuous-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3 Simple effects of education on the age slope We can also look at the simple effects of education on the age slope at each level of gender. contrast educ3#c.age@gender Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------------\r| df F P\u003eF\r-------------------+----------------------------------\reduc3@gender#c.age |\rMale | 2 70.96 0.0000\rFemale | 2 43.37 0.0000\rJoint | 4 57.21 0.0000\r|\rDenominator | 25704\r------------------------------------------------------\rThe first test compares the age slope among the three levels of education for males. $H\\tiny 0$:$\\beta\\tiny 1M$ = $\\beta\\tiny 2M$ = $\\beta\\tiny 3M$ This test is significant. The age slope significantly differs as a function of education among males. The second test is like the first test, except that the comparisons are made for females. $H\\tiny 0$:$\\beta\\tiny 1F$ = $\\beta\\tiny 2F$ = $\\beta\\tiny 3F$ This test is also significant. Among females, the age slope significantly differs among the three levels of education. ","date":"2024-01-13","objectID":"/14.chapter14continuous-by-categorical-by-categorical-interactions/:3:0","tags":["Interaction","stata"],"title":"Chapter14 ：Continuous by continuous by categorical interactions","uri":"/14.chapter14continuous-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4 Simple contrasts on education for the age slope We can further dissect the simple effects tested above by applying contrast coefficients to the education factor.For example, say that we used the ar. contrast operator to form reverse adjacent group comparisons. contrast ar.educ3#c.age@gender,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r---------------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r-----------------------+---------------------------------------\reduc3@gender#c.age |\r(HS vs not hs) Male | 175.85 54.75 3.21 0.001\r(HS vs not hs) Female | 95.54 26.84 3.56 0.000\r(Coll vs HS) Male | 721.48 69.75 10.34 0.000\r(Coll vs HS) Female | 387.13 49.39 7.84 0.000\r---------------------------------------------------------------\r","date":"2024-01-13","objectID":"/14.chapter14continuous-by-categorical-by-categorical-interactions/:4:0","tags":["Interaction","stata"],"title":"Chapter14 ：Continuous by continuous by categorical interactions","uri":"/14.chapter14continuous-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"5 Partial interaction on education for the age slope The three-way interaction can be dissected by forming contrasts on the three-level categorical variable. Say that we use reverse adjacent group comparisons on education, which compares high school graduates with non–high school graduates and college graduates with high school graduates. contrast ar.educ3#r.gender#c.age,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r-------------------------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r---------------------------------+---------------------------------------\reduc3#gender#c.age |\r(HS vs not hs) (Female vs Male) | -80.31 60.95 -1.32 0.188\r(Coll vs HS) (Female vs Male) | -334.35 85.49 -3.91 0.000\r-------------------------------------------------------------------------\rThe first comparison tests the interaction of the contrast of high school graduates versus non–high school graduates by gender by age. The difference in the age slope between high school graduates and non–high school graduates for females is 244.43 minus 148.89 (95.54). For males, this difference is 589.72 minus 413.87 (175.85). The difference in these differences is $-80.31$, which is not significant (see the first comparison from the margins command).The difference in the age slope comparing high school graduates with non–high school graduates is not significantly different for males and females. The second test forms the same kind of comparison, but compares college graduates with high school graduates. The difference in the age slope comparing female college graduates with female high school graduates is 631.56 minus 244.43 (387.13). This difference for males is 1,311.20 minus 589.72 (721.48). The difference of these differences is and is statistically significant (see the second comparison from the margins output). The increase in the age slope comparing college graduates with high school graduates is greater for males than it is for females. ","date":"2024-01-13","objectID":"/14.chapter14continuous-by-categorical-by-categorical-interactions/:5:0","tags":["Interaction","stata"],"title":"Chapter14 ：Continuous by continuous by categorical interactions","uri":"/14.chapter14continuous-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"One of the unique features of multilevel models is the ability to study cross-level interactions—the interactions of a level-1 variable with a level-2 variable. Such interactions allow you to explore the extent to which the effect of a level-1 variable is moderated by a level-2 variable. ","date":"2024-01-13","objectID":"/15.chapter15multilevel-models/","tags":["Multilevel","Interaction","stata"],"title":"Chapter15 ：Multilevel models","uri":"/15.chapter15multilevel-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"One of the unique features of multilevel models is the ability to study cross-level interactions—the interactions of a level-1 variable with a level-2 variable. Such interactions allow you to explore the extent to which the effect of a level-1 variable is moderated by a level-2 variable. ","date":"2024-01-13","objectID":"/15.chapter15multilevel-models/:0:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter15 ：Multilevel models","uri":"/15.chapter15multilevel-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 Chapter overview This chapter contains four examples, all illustrating multilevel models where students are nested within schools. These four examples provide the opportunity to explore four kinds of crosslevel interactions: continuous by continuous (example 1) continuous by categorical (example 2) categorical by continuous (example 3) categorical by categorical (example 4) All of these examples are completely hypothetical and have been constructed to simplify the interpretation and visualization of the results. ","date":"2024-01-13","objectID":"/15.chapter15multilevel-models/:1:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter15 ：Multilevel models","uri":"/15.chapter15multilevel-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2 Example 1: Continuous by continuous interaction Consider a two-level multilevel model where students are nested within schools. One hundred schools were randomly sampled from a population of schools, and students were randomly sampled from each of the schools. Two student-level variables were measured: socioeconomic status (ses) and a standardized writing test score (write). Furthermore, a school-level variable was measured: the number students per computer within the school, stucomp use school_write.dta sum list in 1/5,abbreviate(30) Variable | Obs Mean Std. dev. Min Max\r-------------+---------------------------------------------------------\rschoolid | 3,026 50.71481 29.01184 1 100\rstuid | 3,026 17.08824 10.81501 1 52\rwrite | 3,026 542.1325 191.3663 0 1200\rses | 3,026 49.78352 10.18169 14.1897 85.06909\rstucomp | 3,026 5.857066 3.533295 1.149443 16.50701\r+------------------------------------------------+\r| schoolid stuid write ses stucomp |\r|------------------------------------------------|\r1. | 1 1 553 55.38129 3.350059 |\r2. | 1 2 530 61.13125 3.350059 |\r3. | 1 3 604 47.61407 3.350059 |\r4. | 1 4 433 48.26278 3.350059 |\r5. | 1 5 370 47.9762 3.350059 |\r+------------------------------------------------+\rThe aim of this hypothetical study is to determine if the greater availability of computers at a school reduces the strength of the relationship between socioeconomic status and writing test scores. In other words, the goal is to determine if there is a cross-level interaction of ses and stucomp in the prediction of write. The mixed command predicts write from c.ses, c.stucomp, and the interaction c.stucomp#c.ses. The random-effects portion of the model indicates that ses is a random effect across levels of schoolid. The covariance(un) (un is short for unstructured) option permits the random intercept and ses slope to be correlated. (In this and all subsequent examples in this chapter, the nolog and noheader options are used to save space.These options suppress the iteration log and the header information.) mixed write c.stucomp##c.ses || schoolid:ses,covariance(un)nolog noheader ---------------------------------------------------------------------------------\rwrite | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r----------------+----------------------------------------------------------------\rstucomp | -26.75 2.69 -9.95 0.000 -32.01 -21.48\rses | 0.36 0.68 0.53 0.597 -0.97 1.69\r|\rc.stucomp#c.ses | 0.24 0.10 2.28 0.023 0.03 0.44\r|\r_cons | 602.29 18.18 33.12 0.000 566.65 637.93\r---------------------------------------------------------------------------------\r------------------------------------------------------------------------------\rRandom-effects parameters | Estimate Std. err. [95% conf. interval]\r-----------------------------+------------------------------------------------\rschoolid: Unstructured |\rvar(ses) | 8.92 1.73 6.09 13.05\rvar(_cons) | 372.35 1229.73 0.58 241085.03\rcov(ses,_cons) | 19.20 35.82 -51.01 89.40\r-----------------------------+------------------------------------------------\rvar(Residual) | 9820.89 261.13 9322.20 10346.25\r------------------------------------------------------------------------------\rLR test vs. linear model: chi2(3) = 3253.82 Prob \u003e chi2 = 0.0000\rNote: LR test is conservative and provided only for reference.\rNote! Fixed and random effects The fixed effects are specified after the dependent variable and before the ||. The random effects are specified after the ||. As expected, the c.stucomp#c.ses interaction is significant. We can use themargins command below with the dydx(ses) option to compute the ses slope for schools that have between one and eight students per computer. margins,dydx(ses) at(stucomp=(1(1)8)) vsquish Average marginal effects Number of obs = 3,026\rExpression: Linear prediction, fixed portion, predict()\rdy/dx wrt: ses\r1._at: stucomp = 1\r2._at: stucomp = 2\r3._at: stucomp = 3\r4._at: stucomp = 4\r5._at: stucomp = 5\r6._at: stucomp = 6\r7._at: stucomp = 7\r8._at: stucomp = 8\r--","date":"2024-01-13","objectID":"/15.chapter15multilevel-models/:2:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter15 ：Multilevel models","uri":"/15.chapter15multilevel-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3 Example 2: Continuous by categorical interaction In this study, standardized reading scores were measured as well as the socioeconomic status of the student. Of the 100 schools, 35 were private (non-Catholic), 35 were public, and 30 were Catholic schools The goal of this hypothetical study is to examine the relationship between socioeconomic status (ses) and reading scores (read), and to determine if the strength of that relationship varies as a function of the type of school (private, public, or Catholic). This involves examining the cross-level interaction of ses and schtype. The mixed command for performing this analysis is shown below. The variable read is predicted from ses, schtype, and the interaction of these two variables. The variable ses is specified as a random coefficient that varies across schools. use school_read.dta sum list in 1/5,abbreviate(30) mixed read i.schtype##c.ses || schoolid:ses,covariance(un) nolog noheader --------------------------------------------------------------------------------\rread | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r---------------+----------------------------------------------------------------\rschtype |\rPublic | 41.97 23.27 1.80 0.071 -3.64 87.57\rCatholic | 152.48 23.79 6.41 0.000 105.86 199.10\r|\rses | 4.04 0.67 6.00 0.000 2.72 5.36\r|\rschtype#c.ses |\rPublic | -1.24 0.95 -1.30 0.192 -3.11 0.62\rCatholic | -3.37 0.99 -3.41 0.001 -5.30 -1.44\r|\r_cons | 519.28 16.57 31.33 0.000 486.80 551.77\r--------------------------------------------------------------------------------\r------------------------------------------------------------------------------\rRandom-effects parameters | Estimate Std. err. [95% conf. interval]\r-----------------------------+------------------------------------------------\rschoolid: Unstructured |\rvar(ses) | 12.13 2.12 8.61 17.10\rvar(_cons) | 4.90 82.77 0.00 1.14e+15\rcov(ses,_cons) | 6.75 31.24 -54.47 67.97\r-----------------------------+------------------------------------------------\rvar(Residual) | 10015.68 264.26 9510.90 10547.24\r------------------------------------------------------------------------------\rLR test vs. linear model: chi2(3) = 4052.81 Prob \u003e chi2 = 0.0000\rNote: LR test is conservative and provided only for reference.\rThe contrast command is used below to test the overall schtype#c.ses interaction. This tests the following null hypothesis: $H\\tiny 0$:$\\beta\\tiny 1$ = $\\beta\\tiny 2$ = $\\beta\\tiny 3$ $\\beta\\tiny 1$ is the average ses slope for private schools,$\\beta\\tiny 2$ is the average ses slope for public schools, and $\\beta\\tiny 3$ is the average ses slope for Catholic schools. This test is significant, indicating that the ses slopes differ by schtype. contrast schtype#c.ses Margins: asbalanced\r-------------------------------------------------\r| df chi2 P\u003echi2\r--------------+----------------------------------\rread |\rschtype#c.ses | 2 11.82 0.0027\r-------------------------------------------------\rLet’s create a graph that illustrates the ses slopes by schtype. We do this using the margins command to compute the adjusted means of reading scores as a function of ses and schtype, and then graphing these adjusted means using the marginsplot. margins schtype, at(ses=(20(5)80)) marginsplot,noci Reading score by socioeconomic status and school type\rLet’s now use the margins command combined with the dydx(ses) option to estimate the ses slope for each of the three different types of schools. margins,dydx(ses) over(schtype) vsquish Average marginal effects Number of obs = 2,973\rExpression: Linear prediction, fixed portion, predict()\rdy/dx wrt: ses\rOver: schtype\r------------------------------------------------------------------------------\r| Delta-method\r| dy/dx std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\rses |\rschtype |\rPrivate | 4.04 0.67 6.00 0.000 2.72 5.36\rPublic | 2.80 0.67 4.17 0.000 1.48 4.12\rCatholic | 0.67 0.72 0.93 0.350 -0.74 2.09\r-----------------------------","date":"2024-01-13","objectID":"/15.chapter15multilevel-models/:3:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter15 ：Multilevel models","uri":"/15.chapter15multilevel-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4 Example 3: Categorical by continuous interaction The aim of this study is to look at gender differences in math performance and to determine if smaller class sizes are associated with smaller gender differences in math scores. In other words, the aim is to determine if there is a cross-level interaction between gender (a level-1 predictor) and class size (a level-2 predictor). use school_math.dta summarize list in 1/5 The mixed command is used to predict math from gender, clsize, and the interaction of these variables. The variable gender is specified as a random coefficient at the school level. mixed math i.gender##c.clsize ||schoolid:gender,covariance(un) nolog Mixed-effects ML regression Number of obs = 2,926\rGroup variable: schoolid Number of groups = 100\rObs per group:\rmin = 8\ravg = 29.3\rmax = 52\rWald chi2(3) = 59.38\rLog likelihood = -17691.732 Prob \u003e chi2 = 0.0000\r-----------------------------------------------------------------------------------\rmath | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r------------------+----------------------------------------------------------------\rgender |\rFemale | 13.83 12.41 1.11 0.265 -10.49 38.16\rclsize | -1.34 0.32 -4.15 0.000 -1.97 -0.71\r|\rgender#c.clsize |\rFemale | -1.10 0.48 -2.30 0.021 -2.04 -0.16\r|\r_cons | 451.94 8.34 54.18 0.000 435.59 468.29\r-----------------------------------------------------------------------------------\r------------------------------------------------------------------------------\rRandom-effects parameters | Estimate Std. err. [95% conf. interval]\r-----------------------------+------------------------------------------------\rschoolid: Unstructured |\rvar(gender) | 244.79 231.45 38.37 1561.80\rvar(_cons) | 35.46 96.33 0.17 7273.92\rcov(gender,_cons) | 78.98 115.00 -146.42 304.37\r-----------------------------+------------------------------------------------\rvar(Residual) | 10273.41 277.07 9744.46 10831.07\r------------------------------------------------------------------------------\rLR test vs. linear model: chi2(3) = 15.08 Prob \u003e chi2 = 0.0018\rNote: LR test is conservative and provided only for reference.\rTo help interpret this effect, we can graph the results using the margins and marginsplot commands margins gender, at(clsize=(15(5)40)) marginsplot,noci Math scores by gender and average class size\rLet’s assess the significance of the gender difference for class sizes ranging from 15 to 40 in five-student increments using the margins command below. margins r.gender,at(clsize=(15(5)40)) contrast(pveffects nowald) vsquish Contrasts of adjusted predictions Number of obs = 2,926\rExpression: Linear prediction, fixed portion, predict()\r1._at: clsize = 15\r2._at: clsize = 20\r3._at: clsize = 25\r4._at: clsize = 30\r5._at: clsize = 35\r6._at: clsize = 40\r------------------------------------------------------------\r| Delta-method\r| Contrast std. err. z P\u003e|z|\r--------------------+---------------------------------------\rgender@_at |\r(Female vs Male) 1 | -2.72 6.10 -0.45 0.656\r(Female vs Male) 2 | -8.24 4.61 -1.79 0.074\r(Female vs Male) 3 | -13.76 4.10 -3.35 0.001\r(Female vs Male) 4 | -19.27 4.88 -3.95 0.000\r(Female vs Male) 5 | -24.79 6.51 -3.81 0.000\r(Female vs Male) 6 | -30.31 8.51 -3.56 0.000\r------------------------------------------------------------\rThe margins command is repeated below for 15 to 40 students per class in onestudent increments (the output is omitted to save space). The marginsplot command is then used to visualize the gender differences (with a confidence interval) across the entire spectrum of class sizes margins r.gender,at(clsize=(15(1)40)) contrast(effects) marginsplot,recastci(rarea) ciopts(fcolor(%20)) yline(0) Gender difference in reading score by average class size\r","date":"2024-01-13","objectID":"/15.chapter15multilevel-models/:4:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter15 ：Multilevel models","uri":"/15.chapter15multilevel-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"5 Example 4: Categorical by continuous interaction This study focuses on gender differences in standardized science scores, and whether such differences vary by school size. In this study, each school is classified into one of three sizes: small, medium, or large. Thus, the focus of this study is on the cross-level interaction of gender by school size, where both gender and school size are categorical variables. use school_science.dta summarize list in 1/5 mixed science i.gender##i.schsize || schoolid: gender,covariance(un) nolog noheader -----------------------------------------------------------------------------------\rscience | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r------------------+----------------------------------------------------------------\rgender |\rFemale | -19.65 6.65 -2.96 0.003 -32.68 -6.62\r|\rschsize |\rMedium | 18.56 7.36 2.52 0.012 4.13 32.99\rLarge | 40.16 7.20 5.58 0.000 26.06 54.27\r|\rgender#schsize |\rFemale#Medium | 1.47 9.54 0.15 0.878 -17.23 20.17\rFemale#Large | 22.24 9.29 2.39 0.017 4.03 40.45\r|\r_cons | 394.97 5.14 76.89 0.000 384.90 405.04\r-----------------------------------------------------------------------------------\r------------------------------------------------------------------------------\rRandom-effects parameters | Estimate Std. err. [95% conf. interval]\r-----------------------------+------------------------------------------------\rschoolid: Unstructured |\rvar(gender) | 38.52 205.24 0.00 1.32e+06\rvar(_cons) | 155.78 115.88 36.25 669.43\rcov(gender,_cons) | 17.52 128.51 -234.35 269.38\r-----------------------------+------------------------------------------------\rvar(Residual) | 9772.30 271.33 9254.71 10318.83\r------------------------------------------------------------------------------\rLR test vs. linear model: chi2(3) = 12.15 Prob \u003e chi2 = 0.0069\rNote: LR test is conservative and provided only for reference.\rThe contrast command is used to test the overall gender#schsize interaction. This test is significant. contrast gender#schsize Contrasts of marginal linear predictions\rMargins: asbalanced\r--------------------------------------------------\r| df chi2 P\u003echi2\r---------------+----------------------------------\rscience |\rgender#schsize | 2 7.17 0.0278\r--------------------------------------------------\rcontrast gender#schsize To help interpret this interaction, we can use the margins command to display the adjusted mean of science scores as a function of gender and schsize margins schsize#gender,nopvalues vsquish marginsplot,noci Adjusted predictions Number of obs = 2,764\rExpression: Linear prediction, fixed portion, predict()\r----------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r---------------+------------------------------------------------\rschsize#gender |\rSmall#Male | 394.97 5.14 384.90 405.04\rSmall#Female | 375.32 5.46 364.62 386.03\rMedium#Male | 413.53 5.27 403.20 423.86\rMedium#Female | 395.36 5.56 384.45 406.26\rLarge#Male | 435.14 5.04 425.26 445.01\rLarge#Female | 437.73 5.36 427.23 448.23\r----------------------------------------------------------------\rScience scores by gender and school size\rOne way to further understand this interaction is by testing the simple effect of gender for each school size. This is performed using the contrast command, as shown below. contrast gender@schsize,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r-----------------------------------------------------------------\r| Contrast Std. err. z P\u003e|z|\r-------------------------+---------------------------------------\rscience |\rgender@schsize |\r(Female vs base) Small | -19.65 6.65 -2.96 0.003\r(Female vs base) Medium | -18.18 6.84 -2.66 0.008\r(Female vs base) Large | 2.59 6.49 0.40 0.690\r-----------------------------------------------------------------\rWe can also further probe the interaction by using partial interaction tests. Let’s apply the ar. contrast operator to school size (comparing each group with th","date":"2024-01-13","objectID":"/15.chapter15multilevel-models/:5:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter15 ：Multilevel models","uri":"/15.chapter15multilevel-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter considers models involving the analysis of longitudinal data.  ","date":"2024-01-13","objectID":"/16.chapter16time-as-a-continuous-predictor/","tags":["Multilevel","Interaction","stata"],"title":"Chapter16 ：Time as a continuous predictor","uri":"/16.chapter16time-as-a-continuous-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter considers models involving the analysis of longitudinal data. Such designs involve participants that are observed at more than one time point and time is generally treated as one of the important predictors in the model. Like any predictor, we need to ask ourselves how we want to model the relationship between time and the outcome. A key distinction is whether time will be treated as a continuous variable or as a categorical variable. There are several approaches that can be used for modeling longitudinal data, including repeated-measures analysis of variance (ANOVA), generalized estimating equations (GEE), and multilevel modeling. This chapter will focus on using multilevel modeling for analyzing longitudinal models where time is treated as level 1 and the person will be treated as level 2. In such a model, characteristics that change as a function of time are level-1 predictors and characteristics that are a property of the person that do not change over time are level-2 predictors. ","date":"2024-01-13","objectID":"/16.chapter16time-as-a-continuous-predictor/:0:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter16 ：Time as a continuous predictor","uri":"/16.chapter16time-as-a-continuous-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 Example 1: Linear effect of time Let’s begin by considering a simple model in which we look at the linear effect of time on the outcome variable. For example, let’s consider a study in which we are looking at the number of minutes people sleep at night over a seven-week period. For the sake of this example, assume that the people were selected for the study because they have recently experienced a stressful event in their life, and the purpose of the study is to understand the natural course of sleep change over the seven weeks after a stressful event. use sleep_conlin.dta list in 1/5 summarize +---------------------+\r| id obsday sleep |\r|---------------------|\r1. | 1 1 382 |\r2. | 1 6 382 |\r3. | 1 13 390 |\r4. | 1 21 378 |\r5. | 1 27 401 |\r+---------------------+\rThe dataset for this study is organized in a long format, with one observation per person per night of observation. Variable | Obs Mean Std. dev. Min Max\r-------------+---------------------------------------------------------\rid | 600 38 21.66677 1 75\robsday | 600 23.565 14.82854 1 52\rsleep | 600 360.785 48.13086 175 528\rInstead, we can fit a random-intercept model that accounts for the nonindependence of the residuals within each person. Such a model is fit below first using the xtset command to specify that id is the panel variable and obsday is the time variable. We can then use the xtreg command to fit a random-intercept model predicting sleep from obsday xtset id obsday Panel variable: id (weakly balanced)\rTime variable: obsday, 1 to 52, but with gaps\rDelta: 1 unit\rxtreg sleep obsday Random-effects GLS regression Number of obs = 600\rGroup variable: id Number of groups = 75\rR-squared: Obs per group:\rWithin = 0.1652 min = 8\rBetween = 0.0062 avg = 8.0\rOverall = 0.0218 max = 8\rWald chi2(1) = 103.56\rcorr(u_i, X) = 0 (assumed) Prob \u003e chi2 = 0.0000\r------------------------------------------------------------------------------\rsleep | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\robsday | 0.51 0.05 10.18 0.000 0.41 0.61\r_cons | 348.80 5.31 65.66 0.000 338.39 359.21\r-------------+----------------------------------------------------------------\rsigma_u | 44.412437\rsigma_e | 18.051717\rrho | .85821678 (fraction of variance due to u_i)\r------------------------------------------------------------------------------\rThe interpretation of the obsday coefficient is straightforward. For each additional day in the study, nightly minutes of sleep increased by, on average, by 0.51 minutes. In this model, the coefficient for obsday is treated as a fixed effect. The model recognizes that people randomly vary in terms of their average sleep time at the start of the study (represented by the random intercept). But perhaps people also vary individually in their trajectory of sleep times across the weeks of the study. By adding obsday as a random coefficient (that is, a random slope), the model can account for both individual differences in the average length of sleep at the start of the study (that is, a random intercept) and individual differences in the trajectory of sleep times across the weeks of the study (that is, a random slope for obsday). mixed sleep obsday || id:obsday, covariance(un) nolog Mixed-effects ML regression Number of obs = 600\rGroup variable: id Number of groups = 75\rObs per group:\rmin = 8\ravg = 8.0\rmax = 8\rWald chi2(1) = 20.54\rLog likelihood = -2488.5378 Prob \u003e chi2 = 0.0000\r-----------------------------------------------------------------------------------\rsleep | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r------------------+----------------------------------------------------------------\robsday | 0.51 0.11 4.53 0.000 0.29 0.73\r_cons | 348.82 3.31 105.43 0.000 342.34 355.31\r-----------------------------------------------------------------------------------\r------------------------------------------------------------------------------\rRandom-effects parameters | Estimate Std. e","date":"2024-01-13","objectID":"/16.chapter16time-as-a-continuous-predictor/:1:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter16 ：Time as a continuous predictor","uri":"/16.chapter16time-as-a-continuous-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2 Example 2: Linear effect of time by a categorical predictor Let’s consider another sleep study in which participants diagnosed with insomnia were randomly assigned to one of three different treatments to increase the number of minutes of sleep at night. The three different treatments were 1) control group (no treatment), 2) medication group (where a sleep medication is given), or 3) education group (where the participants receive education about how to sleep better and longer). This model involves a combination of a continuous predictor (time) and a threelevel categorical predictor (treatment group). use sleep_cat3conlin.dta list in 1/5 summarize We can fit a model that predicts sleep from the observation day, the group assignment, and the interaction of these two variables using the mixed command shown below. Note that the random-effects portion of the model specifies that obsday is a random effect. [Thinking in terms of a multilevel model, group#obsday is a crosslevel interaction of a level-2 variable (group) with a level-1 variable (obsday).] mixed sleep i.group##c.obsday || id:obsday,covariance(un)nolog Mixed-effects ML regression Number of obs = 600\rGroup variable: id Number of groups = 75\rObs per group:\rmin = 8\ravg = 8.0\rmax = 8\rWald chi2(5) = 66.55\rLog likelihood = -2482.6017 Prob \u003e chi2 = 0.0000\r-----------------------------------------------------------------------------------\rsleep | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r------------------+----------------------------------------------------------------\rgroup |\rMedication | 35.48 9.50 3.74 0.000 16.86 54.10\rEducation | 5.55 9.50 0.58 0.559 -13.07 24.17\r|\robsday | -0.07 0.18 -0.37 0.715 -0.42 0.29\r|\rgroup#c.obsday |\rMedication | 0.18 0.25 0.71 0.475 -0.32 0.68\rEducation | 0.83 0.25 3.27 0.001 0.33 1.33\r|\r_cons | 339.45 6.72 50.53 0.000 326.28 352.61\r-----------------------------------------------------------------------------------\r------------------------------------------------------------------------------\rRandom-effects parameters | Estimate Std. err. [95% conf. interval]\r-----------------------------+------------------------------------------------\rid: Unstructured |\rvar(obsday) | 0.72 0.13 0.50 1.03\rvar(_cons) | 1079.51 184.18 772.68 1508.17\rcov(obsday,_cons) | 22.52 4.23 14.22 30.82\r-----------------------------+------------------------------------------------\rvar(Residual) | 108.37 7.22 95.10 123.48\r------------------------------------------------------------------------------\rLR test vs. linear model: chi2(3) = 1425.42 Prob \u003e chi2 = 0.0000\rNote: LR test is conservative and provided only for reference.\rWe first use the margins command to estimate the predicted means by group at selected values of obsday. We follow that with the marginsplot command to graph the predicted means computed by the margins command. margins group,at(obsday=(0 45)) marginsplot,noci Adjusted predictions Number of obs = 600\rExpression: Linear prediction, fixed portion, predict()\r1._at: obsday = 0\r2._at: obsday = 45\r-------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. z P\u003e|z| [95% conf. interval]\r--------------+----------------------------------------------------------------\r_at#group |\r1#Control | 339.45 6.72 50.53 0.000 326.28 352.61\r1#Medication | 374.93 6.72 55.83 0.000 361.76 388.09\r1#Education | 345.00 6.72 51.36 0.000 331.83 358.16\r2#Control | 336.49 13.61 24.71 0.000 309.80 363.17\r2#Medication | 380.14 13.60 27.94 0.000 353.47 406.80\r2#Education | 379.42 13.61 27.88 0.000 352.75 406.10\r-------------------------------------------------------------------------------\rMinutes of sleep at night by time and treatment group\rThe slope appears to be slightly negative for the control group. In other words, their sleep durations appear to mildly decrease as a linear function of the observation day. For the medication group, the slope appears to be slightly positive. By contrast, sleep durations increase as a linear f","date":"2024-01-13","objectID":"/16.chapter16time-as-a-continuous-predictor/:2:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter16 ：Time as a continuous predictor","uri":"/16.chapter16time-as-a-continuous-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3 Example 3:Piecewise modeling of time Time can be modeled in a piecewise fashion by breaking up the days of observation into the baseline phase and the treatment phase. In this example, we can assess the slope of the relationship between sleep duration and time (obsday) during the baseline and treatment phases. We can also test for a sudden jump in sleep duration on the 31st day, corresponding to the start of the treatment phase. use sleep_conpw.dta list in 1/5,sepby(id) summarize We need to create some variables to prepare for the piecewise analysis. First, we use the mkspline command to create the variables named obsday1m and obsday2m, placing the knot at 31 days (corresponding to the start of the treatment phase). mkspline obsday1m 31 obsday2m = obsday,marginal Next, we create the variable trtphase that is coded 0 for the baseline phase (where obsday was 1 to 30) and coded 1 during the treatment phase (where obsday is 31 or more). generate trtphase = 0 if obsday \u003c= 30 replace trtphase = 1 if obsday \u003e=31 \u0026 !missing(obsday) We are now ready to run a piecewise model that predicts sleep from obsday1m, obsday2m, and trtphase. In this example, obsday1m, obsday2m, and trtphase are also included as random effects. mixed sleep obsday1m obsday2m trtphase || id: obsday1m obsday2m trtphase,covariance(un)nolog Mixed-effects ML regression Number of obs = 600\rGroup variable: id Number of groups = 75\rObs per group:\rmin = 8\ravg = 8.0\rmax = 8\rWald chi2(3) = 70.48\rLog likelihood = -2601.8889 Prob \u003e chi2 = 0.0000\r----------------------------------------------------------------------------------------\rsleep | Coefficient Std. err. z P\u003e|z| [95% conf. interval]\r-----------------------+----------------------------------------------------------------\robsday1m | -0.01 0.12 -0.05 0.961 -0.24 0.23\robsday2m | 0.54 0.17 3.27 0.001 0.22 0.87\rtrtphase | 11.14 1.94 5.74 0.000 7.33 14.94\r_cons | 349.01 10.64 32.80 0.000 328.15 369.87\r----------------------------------------------------------------------------------------\r------------------------------------------------------------------------------\rRandom-effects parameters | Estimate Std. err. [95% conf. interval]\r-----------------------------+------------------------------------------------\rid: Unstructured |\rvar(obsday1m) | 0.83 0.18 0.55 1.25\rvar(obsday2m) | 0.69 0.33 0.27 1.78\rvar(trtphase) | 59.53 48.90 11.90 297.83\rvar(_cons) | 8425.42 1386.81 6102.18 11633.16\rcov(obsday1m,obsday2m) | -0.37 0.19 -0.73 -0.00\rcov(obsday1m,trtphase) | -1.54 2.14 -5.73 2.65\rcov(obsday1m,_cons) | 32.85 11.53 10.25 55.46\rcov(obsday2m,trtphase) | 2.84 2.78 -2.62 8.29\rcov(obsday2m,_cons) | -5.19 15.46 -35.50 25.12\rcov(trtphase,_cons) | -133.63 180.15 -486.71 219.45\r-----------------------------+------------------------------------------------\rvar(Residual) | 101.86 8.07 87.20 118.97\r------------------------------------------------------------------------------\rLR test vs. linear model: chi2(10) = 2057.12 Prob \u003e chi2 = 0.0000\rNote: LR test is conservative and provided only for reference.\rWe can then use the margins command to compute the predicted means for these key days. When obsday equals 31, we estimate the predicted mean assuming trtphase is 0 and 1, to estimate the jump in the fitted values due to the start of the treatment phase. (The noatlegend option is included to save space.) margins,at(obsday1m = 1 obsday2m = 0 trtphase=0) /// at(obsday1m = 31 obsday2m = 0 trtphase=0) /// at(obsday1m = 31 obsday2m = 0 trtphase=1) /// at(obsday1m = 49 obsday2m = 18 trtphase=1) nopvalues noatlegend Adjusted predictions Number of obs = 600\rExpression: Linear prediction, fixed portion, predict()\r--------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r-------------+------------------------------------------------\r_at |\r1 | 349.01 10.68 328.07 369.94\r2 | 348.83 12.30 324.72 372.94\r3 | 359.97 12.16 336.14 383.79\r4 | 369.65 13.11 343.95 395.34\r---------------------------------","date":"2024-01-13","objectID":"/16.chapter16time-as-a-continuous-predictor/:3:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter16 ：Time as a continuous predictor","uri":"/16.chapter16time-as-a-continuous-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4 Example 4: Piecewise effects of time by a categorical predictor Let’s consider an extension of the previous example that includes a baseline and treatment phase, but where participants are divided into different groups and receive different kinds of treatments during the treatment phase. The first 30 days of the study are a baseline period during which the sleep is observed, but no treatment is administered to any of the groups. Starting on the 31st day, the medication group receives sleep medication, the sleep education group receives education about how to lengthen their sleep, and the control group receives nothing. The first phase of the study (days 1 to 30) is called the baseline phase, and the second phase (day 31 until the end of the study) is called the treatment phase. We can study the slope of the relationship between sleep duration and time during each of these phases, as well as the change (jump or drop) in sleep that occurs at the transition from baseline to the treatment phase. Furthermore, we can investigate the impact of the treatment group assignment (control, medication, and education) on the slope in each phase, as well as the jump or drop in sleep due to the start of the treatment phase. use sleep_cat3pw.dta list in 1/5,sepby(id) summarize Imagine the educational periods (before high school graduation and after high school graduation) being replaced with the baseline and treatment phases. Also, imagine gender (male and female) being replaced with treatment group assignment (control, medication, and education). First, the mkspline command is used to create the variables obsday1m and obsday2m, specifying 31 as the knot. By including the marginal option, obsday1m will represent the slope for the baseline period and obsday2m will represent the change in the slope for the treatment period compared with the baseline period. mkspline obsday1m 31 obsday2m = obsday,marginal To account for the jump in sleep at the start of the treatment phase, we use the generate and replace commands to create the variable trtphase that is coded: 0 = baseline and 1 = treatment. generate trtphase = 0 if obsday \u003c 31 replace trtphase = 1 if obsday \u003e= 31 \u0026 !missing(obsday) label define trtlab 0 \"baseline\" 1 \"treatment\" label values trtphase trtlab The mixed command for fitting this model is shown below. Note the variables trtphase, obsday1m, and obsday2m are specified as random effects. The nolog, noheader, and noretable options are used to suppress the iteration log, header, and random-effects table to save space. mixed sleep i.group##(i.trtphase c.obsday1m c.obsday2m), || id:trtphase obsday1m obsday2m,covariance(un) nolog noheader noretable noci ---------------------------------------------------------------\rsleep | Coefficient Std. err. z P\u003e|z|\r-----------------------+---------------------------------------\rgroup |\rMedication | -2.07 3.41 -0.61 0.543\rEducation | 16.31 3.41 4.78 0.000\r|\rtrtphase |\rtreatment | -4.20 2.79 -1.51 0.132\robsday1m | -0.04 0.30 -0.14 0.890\robsday2m | -0.10 0.51 -0.19 0.849\r|\rgroup#trtphase |\rMedication#treatment | 34.28 4.00 8.58 0.000\rEducation#treatment | 1.65 3.97 0.41 0.678\r|\rgroup#c.obsday1m |\rMedication | 0.40 0.42 0.95 0.343\rEducation | -0.14 0.42 -0.34 0.735\r|\rgroup#c.obsday2m |\rMedication | -0.29 0.72 -0.40 0.686\rEducation | 2.48 0.72 3.45 0.001\r|\r_cons | 351.36 2.41 145.54 0.000\r---------------------------------------------------------------\rWe can now use the margins command to compute the predicted mean of sleep for each group at four key points—for the beginning of the study, the first day of the treatment phase (with and without treatment), and the 49th day of the study. (The noatlegend option is specified to save space.) margins group,at(obsday1m = 1 obsday2m = 0 trtphase=0) /// at(obsday1m = 31 obsday2m = 0 trtphase=0) /// at(obsday1m = 31 obsday2m = 0 trtphase=1) /// at(obsday1m = 49 obsday2m = 18 trtphase=1)nopvalues noatlegend Adjusted predictions Number of obs = 600\rExpression: Linear pre","date":"2024-01-13","objectID":"/16.chapter16time-as-a-continuous-predictor/:4:0","tags":["Multilevel","Interaction","stata"],"title":"Chapter16 ：Time as a continuous predictor","uri":"/16.chapter16time-as-a-continuous-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4.1 Baseline slope [means the prior to the treatment phase] We can test the equality of all the baseline slopes using the contrast command below. This test is not significant. contrast group#c.obsday1m Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------\r| df chi2 P\u003echi2\r-----------------+----------------------------------\rsleep |\rgroup#c.obsday1m | 2 1.78 0.4108\r----------------------------------------------------\rYou can estimate the baseline slope for each group using the margins command below. margins, dydx(obsday1m) over(group) Average marginal effects Number of obs = 600\rExpression: Linear prediction, fixed portion, predict()\rdy/dx wrt: obsday1m\rOver: group\r------------------------------------------------------------------------------\r| Delta-method\r| dy/dx std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\robsday1m |\rgroup |\rControl | -0.04 0.30 -0.14 0.890 -0.63 0.55\rMedication | 0.36 0.30 1.20 0.229 -0.23 0.95\rEducation | -0.19 0.30 -0.62 0.537 -0.77 0.40\r------------------------------------------------------------------------------\r","date":"2024-01-13","objectID":"/16.chapter16time-as-a-continuous-predictor/:4:1","tags":["Multilevel","Interaction","stata"],"title":"Chapter16 ：Time as a continuous predictor","uri":"/16.chapter16time-as-a-continuous-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4.2 Change in slopes: Treatment versus baseline You can test the equality of the changes in slope coefficients for all three groups using the contrast command below. This test is significant. contrast group#c.obsday2m Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------\r| df chi2 P\u003echi2\r-----------------+----------------------------------\rsleep |\rgroup#c.obsday2m | 2 17.95 0.0001\r----------------------------------------------------\rYou can estimate the change in slope for each group using the margins command below. The change in slope for the control group and medication groups is not significant. For the education group (group 3), the slope during the treatment phase is significantly greater than the slope during the baseline phase. margins,dydx(obsday2m) over(group) Average marginal effects Number of obs = 600\rExpression: Linear prediction, fixed portion, predict()\rdy/dx wrt: obsday2m\rOver: group\r------------------------------------------------------------------------------\r| Delta-method\r| dy/dx std. err. z P\u003e|z| [95% conf. interval]\r-------------+----------------------------------------------------------------\robsday2m |\rgroup |\rControl | -0.10 0.51 -0.19 0.849 -1.09 0.90\rMedication | -0.39 0.51 -0.76 0.447 -1.39 0.61\rEducation | 2.38 0.51 4.70 0.000 1.39 3.38\r------------------------------------------------------------------------------\r","date":"2024-01-13","objectID":"/16.chapter16time-as-a-continuous-predictor/:4:2","tags":["Multilevel","Interaction","stata"],"title":"Chapter16 ：Time as a continuous predictor","uri":"/16.chapter16time-as-a-continuous-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4.3 Jump at treatment We can test the equality of the jump or drop for all three groups using the contrast command below. This test is significant. contrast group#trtphase Contrasts of marginal linear predictions\rMargins: asbalanced\r--------------------------------------------------\r| df chi2 P\u003echi2\r---------------+----------------------------------\rsleep |\rgroup#trtphase | 2 92.58 0.0000\r--------------------------------------------------\rWe can estimate the size of the jump or drop in sleep durations at the start of the treatment phase for each group using the contrast command below. contrast trtphase@group,pveffects nowald Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------------------------------\r| Contrast Std. err. z P\u003e|z|\r--------------------------------+---------------------------------------\rsleep |\rtrtphase@group |\r(treatment vs base) Control | -4.20 2.79 -1.51 0.132\r(treatment vs base) Medication | 30.08 2.86 10.52 0.000\r(treatment vs base) Education | -2.56 2.83 -0.90 0.366\r------------------------------------------------------------------------\r","date":"2024-01-13","objectID":"/16.chapter16time-as-a-continuous-predictor/:4:3","tags":["Multilevel","Interaction","stata"],"title":"Chapter16 ：Time as a continuous predictor","uri":"/16.chapter16time-as-a-continuous-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4.4 Comparisons among groups Let’s now use the margins command to compare each group with the control group at each of these days. (The noatlegend option is specified to save space.) margins r.group,at(obsday1m=30 obsday2m=0 trtphase=0) /// at(obsday1m=35 obsday2m=4 trtphase=1) /// at(obsday1m=40 obsday2m=9 trtphase=1) /// at(obsday1m=45 obsday2m=14 trtphase=1) /// contrast(nowald pveffects)noatlegend Contrasts of adjusted predictions Number of obs = 600\rExpression: Linear prediction, fixed portion, predict()\r-------------------------------------------------------------------\r| Delta-method\r| Contrast std. err. z P\u003e|z|\r---------------------------+---------------------------------------\rgroup@_at |\r(Medication vs Control) 1 | 9.98 12.37 0.81 0.420\r(Medication vs Control) 2 | 45.10 12.87 3.50 0.000\r(Medication vs Control) 3 | 45.65 13.50 3.38 0.001\r(Medication vs Control) 4 | 46.21 14.68 3.15 0.002\r(Education vs Control) 1 | 12.00 12.38 0.97 0.332\r(Education vs Control) 2 | 22.84 12.87 1.77 0.076\r(Education vs Control) 3 | 34.51 13.50 2.56 0.011\r(Education vs Control) 4 | 46.18 14.67 3.15 0.002\r-------------------------------------------------------------------\rFocusing on the comparison of the medication group with the control group, the difference is not significant when observation day is 30 ($p=0.420$); prior to the treatment phase. However, the difference is significant at each of the time points tested during the treatment phase, when observation day is 35, 40, and 45 ( = 0.000, 0.001, and 0.002, respectively). Shifting our attention to the comparison of the education group with the control group, the comparison is not significant when observation day is 30 ($p=0.332$); prior to the start of the treatment phase. The difference remains nonsignificant early in the treatment phase when observation day is 35 ($p=0.076$) but is significant later inthe treatment phase when observation day is 40 ($p=0.011$) and 45 ($p=0.002$). ","date":"2024-01-13","objectID":"/16.chapter16time-as-a-continuous-predictor/:4:4","tags":["Multilevel","Interaction","stata"],"title":"Chapter16 ：Time as a continuous predictor","uri":"/16.chapter16time-as-a-continuous-predictor/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter illustrates how to fit models in which a continuous variable, fit in a piecewise manner, is interacted with a categorical variable.  ","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter illustrates how to fit models in which a continuous variable, fit in a piecewise manner, is interacted with a categorical variable. ","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:0:0","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 One knot and one jump For example, education was modeled in a piecewise manner including one knot at 12 years of education. This knot signifies both a change of slope and change in intercept (jump). The model also includes gender as a categorical variable and estimates separate slopes and intercepts for each level of gender Piecewise regression with one knot and one jump, labeled with estimated slopes\rFigure shows the adjusted means of income as a function of education with labels for each of the slopes. For men, the slope for non–high school graduates is labeled $\\beta\\tiny M1$ and the slope for high school graduates is labeled $\\beta\\tiny M2$ . For women, the slope for non–high school graduates is labeled $\\beta\\tiny F1$ and the slope for high school graduates is labeled $\\beta\\tiny F2$. The income jump at 12 years of education for men is labeled as $\\alpha\\tiny M1$. The corresponding jump at 12 years of education for women is labeled as $\\alpha\\tiny F1$. Note how an arrow head points to a sudden jump in income at 12 years of education To fit this model, we will use separate slope and separate intercept coding with respect to the gender groups. This means that we will estimate separate intercept terms for men and women (this includes separate jumps,$\\alpha\\tiny M1$ and $\\alpha\\tiny F1$ ). It will also estimate separate slope terms for men (that is, $\\beta\\tiny M1$ and $\\beta\\tiny M2$ ) and separate slope terms for women (that is,$\\beta\\tiny F1$ and $\\beta\\tiny F2$). We first use the mkspline command to create the variables ed1 and ed2 with a knot at 12 years of education. the variables ed1 and ed2 will reflect the slopes of the individual line segments before and after the knot. mkspline ed1 12 ed2 = educ We are now ready to run the piecewise regression model, shown in the regress command below. The regress command includes ibn.gender used in combination with the noconstant option to yield separate estimates of the intercept for each gender. reg realrinc ibn.gender ibn.gender#i.hsgrad /// ibn.gender#c.ed1 ibn.gender#c.ed2 i.race, vce(robust) noconstant noci Linear regression Number of obs = 3,126\rF(6, 3120) = 614.01\rProb \u003e F = 0.0000\rR-squared = 0.5132\rRoot MSE = 20860\r--------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t|\r----------------+---------------------------------------\rgender |\rFemale | 10502.44 1991.84 5.27 0.000\r|\rgender#hsgrad |\rFemale#HS Grad | 2891.28 893.57 3.24 0.001\r|\rgender#c.ed1 |\rFemale | 95.54 211.64 0.45 0.652\r|\rgender#c.ed2 |\rFemale | 3260.42 247.08 13.20 0.000\r|\rrace |\rblack | -1174.41 902.16 -1.30 0.193\rother | -4732.20 1169.14 -4.05 0.000\r--------------------------------------------------------\rNote! Model shortcut Stata expands the expression ibn.gender#(i.hsgrad c.ed1 c.ed2) to become ibn.gender#i.hsgrad ibn.gender#c.ed1 ibn.gender#c.ed2, yielding the same model we saw before. The first two columns of the table repeat the name and estimate of the coefficient from the regress output. The third column shows the symbol used to represent the coefficient in figure above, providing a cross reference between the output of the regression model and figure. The last column shows the symbolic name of the regression coefficient that we can use with the lincom command for making comparisons among the coefficients. Summary of piecewise regression results with one knot\rLet’s begin by interpreting the change in intercept (jump) terms. The jump in the adjusted mean of income for men at 12 years of education is 3,382.01, and the corresponding jump for women is 1,748.10. Each of these jumps is statistically significant. Let’s now interpret the slopes. For men, the educ slope is 86.71 for non–high school graduates and is 4,057.64 for high school graduates. For women, the slope is 241.61 for non–high school graduates and is 2,529.55 for high school graduates. Aside from the educ slope for male non–high school graduates (86.71), all of these slopes are significantl","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:1:0","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.1 Comparing slopes across gender Let’s begin by testing the equality of the educ slopes of women and men who have not graduated high school. In other words, is $\\beta\\tiny F1$= $\\beta\\tiny M1$ ? contrast gender#c.ed1,pveffects nowald Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r------------------+---------------------------------------\rgender#c.ed1 |\r(Female vs base) | 154.90 179.32 0.86 0.388\r----------------------------------------------------------\rLet’s compare the educ slopes of men and women for those who graduated high school by testing whether $\\beta\\tiny F2$ = $\\beta\\tiny M2$ . contrast gender#c.ed2,pveffects nowald Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r------------------+---------------------------------------\rgender#c.ed2 |\r(Female vs base) | -1528.09 187.09 -8.17 0.000\r----------------------------------------------------------\rThe difference in the slopes (women versus men) is $-1,528.09$. For every year of education beyond the 12th year, the income for men increases by $1,528.09$ more than for women. ","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:1:1","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.2 Comparing slopes across education let’s examine the change in slope between high school graduates and non–high school graduates for men. is $\\beta\\tiny M2$ = $\\beta\\tiny M1$ ? As shown in table , the symbolic names for these coefficients are 1.gender#ed2 and 1.gender#ed1. We can compare these coefficients using the lincom command, as shown below. lincom 1.gender#c.ed2 - 1.gender#c.ed1 ( 1) - 1bn.gender#c.ed1 + 1bn.gender#c.ed2 = 0\r------------------------------------------------------------------------------\rrealrinc | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\r(1) | 3970.93 212.70 18.67 0.000 3554.04 4387.83\r------------------------------------------------------------------------------\rMen show a significantly higher slope after graduating high school than men who have not graduated high school. The difference (comparing high school graduates with non–high school graduates) is 3,970.93. We can formulate the same kind of test for women, comparing the educ slope for high school graduates with that for non–high school graduates. lincom 2.gender#c.ed2 - 2.gender#c.ed1 Is $\\beta\\tiny F2$ = $\\beta\\tiny F1$ ? ( 1) - 2.gender#c.ed1 + 2.gender#c.ed2 = 0\r------------------------------------------------------------------------------\rrealrinc | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\r(1) | 2287.94 147.26 15.54 0.000 1999.30 2576.58\r------------------------------------------------------------------------------\r","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:1:2","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.3 Difference in differences of slopes（gain in slope） The previous section showed that, for men, the educ slope after graduating high school minus the slope before graduating high school equals 3,970.93. Let’s call this the gain in slope due to graduating high school. For women, the gain in slope due to graduating high school is 2,287.94. We might ask if the gain in slope due to graduating high school differs by gender. The lincom command below tests the gain in slope for men compared with the gain in slope for women. lincom (1.gender#c.ed2 - 1.gender#c.ed1) - (2.gender#c.ed2 - 2.gender#c.ed1) ( 1) - 1bn.gender#c.ed1 + 2.gender#c.ed1 + 1bn.gender#c.ed2 - 2.gender#c.ed2 = 0\r------------------------------------------------------------------------------\rrealrinc | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\r(1) | 1682.99 259.29 6.49 0.000 1174.78 2191.20\r------------------------------------------------------------------------------\r","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:1:3","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.4 Comparing changes in intercepts Let’s now ask whether the jump in income at 12 years of education is equal for men and women. the income for men jumps by $3,382.01$ at 12 years of education, whereas the corresponding jump for women is $1,748.10$. Are these jumps equal? Is $\\alpha\\tiny F1$ = $\\alpha\\tiny M1$ ? contrast gender#hsgrad,pveffects nowald Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r------------------------------------+---------------------------------------\rgender#hsgrad |\r(Female vs base) (HS Grad vs base) | -1633.91 766.98 -2.13 0.033\r----------------------------------------------------------------------------\rSome people might find that this test is more intuitive if performed using the following lincom command below. lincom 2.gender#1.hsgrad - 1.gender#1.hsgrad ( 1) - 1bn.gender#1.hsgrad + 2.gender#1.hsgrad = 0\r------------------------------------------------------------------------------\rrealrinc | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\r(1) | -1633.91 766.98 -2.13 0.033 -3137.23 -130.59\r------------------------------------------------------------------------------\r","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:1:4","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.5 Computing and comparing adjusted means Let’s now turn our attention to how we can compute adjusted means for this regression model. Before we can compute adjusted means with respect to education (that is, educ), we need to know how to express the level of education in terms of hsgrad, ed1, and ed2. showcoding educ hsgrad ed1 ed2 //the command is not fit for stata18 Let’s now estimate the adjusted mean for a female (that is, gender=2) with 15 years of education, as shown below. To indicate 15 years of education, we specify that hsgrad equals 1, ed1 equals 12, and ed2 equals 3. margins,nopvalues at(gender=2 hsgrad=1 ed1=12 ed2=3) Predictive margins Number of obs = 32,183\rModel VCE: Robust\rExpression: Linear prediction, predict()\rAt: gender = 2\rhsgrad = 1\red1 = 12\red2 = 3\r--------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r-------------+------------------------------------------------\r_cons | 18993.22 234.17 18534.24 19452.20\r--------------------------------------------------------------\rWe can estimate the adjusted mean of income for men and women with 15 years of education using the margins command below. margins gender,nopvalues at(hsgrad=1 ed1=12 ed2=3) Predictive margins Number of obs = 32,183\rModel VCE: Robust\rExpression: Linear prediction, predict()\rAt: hsgrad = 1\red1 = 12\red2 = 3\r--------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r-------------+------------------------------------------------\rgender |\rMale | 33171.77 351.09 32483.62 33859.93\rFemale | 18993.22 234.17 18534.24 19452.20\r--------------------------------------------------------------\rSpecifying the r. contrast operator indicates we want to use reference group comparisons, comparing the adjusted mean of income for women versus men. margins r.gender,at(hsgrad=1 ed1=12 ed2=3) contrast(nowald pveffects) Contrasts of predictive margins Number of obs = 32,183\rModel VCE: Robust\rExpression: Linear prediction, predict()\rAt: hsgrad = 1\red1 = 12\red2 = 3\r----------------------------------------------------------\r| Delta-method\r| Contrast std. err. t P\u003e|t|\r------------------+---------------------------------------\rgender |\r(Female vs Male) | -14178.55 422.06 -33.59 0.000\r----------------------------------------------------------\r","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:1:5","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.6 Graphing adjusted means Let’s graph the adjusted means as a function of education and gender. To make this graph, we need to compute the adjusted means separately for men and women with 0 years of education, 12 years of education (assuming the absence and presence of a high school diploma), and 20 years of education. margins gender,at(hsgrad=0 ed1=0 ed2=0) /// at(hsgrad=0 ed1=12 ed2=0) /// at(hsgrad=1 ed1=12 ed2=0) /// at(hsgrad=1 ed1=12 ed2=8) nopvalues Predictive margins Number of obs = 32,183\rModel VCE: Robust\rExpression: Linear prediction, predict()\r1._at: hsgrad = 0\red1 = 0\red2 = 0\r2._at: hsgrad = 0\red1 = 12\red2 = 0\r3._at: hsgrad = 1\red1 = 12\red2 = 0\r4._at: hsgrad = 1\red1 = 12\red2 = 8\r--------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r-------------+------------------------------------------------\r_at#gender |\r1#Male | 16576.36 1384.62 13862.46 19290.26\r1#Female | 6757.19 910.63 4972.32 8542.05\r2#Male | 17616.85 590.78 16458.91 18774.80\r2#Female | 9656.48 345.83 8978.65 10334.32\r3#Male | 20998.86 297.50 20415.75 21581.96\r3#Female | 11404.58 179.77 11052.22 11756.94\r4#Male | 53459.97 1042.02 51417.56 55502.37\r4#Female | 31640.96 761.69 30148.02 33133.90\r--------------------------------------------------------------\rpreserve clear input educ yhatm yhatf 0 16576.36 6757.187 12 17616.85 9656.485 12 20998.86 11404.58 20 53459.97 31640.96 end graph twoway line yhatm yhatf educ,xline(12)legend(label(1 \"Men\")label(2 \"Women\")) /// xtitle(Education)ytitle(Adjusted mean) Fitted values from piecewise model with one knot and one jump at educ = 12\rWe can automate the creation of this graph by extending the strategy First, we use the matrix command to create a matrix named yhat that contains the adjusted means computed by the margins command. **First,rerun the -margins- command from above quietly margins gender, at(hsgrad=0 ed1=0 ed2=0) /// at(hsgrad=0 ed1=12 ed2=0) /// at(hsgrad=1 ed1=12 ed2=0) /// at(hsgrad=1 ed1=12 ed2=8) ** store the adjusted means in a matrix named -yhat- matrix yhat = r(b)' //NOTE: We must use transpose here Now, we encounter a twist because the adjusted means are computed as a function two variables (education and gender). Looking at the output of the margins command, let’s focus on the order in which the adjusted means are displayed with respect to education and gender. Eight adjusted means are shown, corresponding to the following levels of educ, hsgrad, and gender: educ=0, hsgrad=0, gender=1 educ=0, hsgrad=0, gender=2 educ=12, hsgrad=0, gender=1 educ=12, hsgrad=0, gender=2 educ=12, hsgrad=1, gender=1 educ=12, hsgrad=1, gender=2 educ=20, hsgrad=1, gender=1 educ=20, hsgrad=1, gender=2 The matrix command is used to create a matrix named educ that reflects the levels of education shown in the bulleted list above. The matrix command is used again, this time to create a matrix called gender that contains the levels of gender shown in the bulleted list above. * store levels of education in a matrix named -educ- matrix educ = (0\\0\\12\\12\\12\\12\\20\\20) * store levels of gender in a matrix named -gender- matrix gender = (1\\2\\1\\2\\1\\2\\1\\2) The svmat command is then used three times, to save the matrices named yhat, educ, and gender to the current dataset. The list command is then used to show the variables yhat1, educ1, and gender1 for the first 10 observations of the dataset. svmat yhat //save the matrix -yhat- to the current dataset svmat educ //save the matrix -educ- to the current dataset svmat gender //save the matrix -gender- to the current dataset list yhat1 educ1 gender1 in 1/10,sep(2) +----------------------------+\r| yhat1 educ1 gender1 |\r|----------------------------|\r1. | 16576.36 0 1 |\r2. | 6757.188 0 2 |\r|----------------------------|\r3. | 17616.85 12 1 |\r4. | 9656.484 12 2 |\r|----------------------------|\r5. | 20998.86 12 1 |\r6. | 11404.58 12 2 |\r|----------------------------|\r7. | 53459.96 20 1 |\r8. | 31640.96 20 2 |\r|----------------------","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:1:6","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2 Two knots and two jumps This section covers piecewise regression models with two knots and two jumps interacted with a categorical variable. For example,income was predicted from education modeled in a piecewise fashion with two knots and two jumps at 12 and 16 years of education.Furthermore, these piecewise terms were interacted with gender. Piecewise regression with two knots and two jumps, labeled with estimated slopes\rThe jump at 12 years of education for men is labeled as $\\alpha\\tiny M1$. The jump at 16 years of education for men is labeled as $\\alpha\\tiny M2$. These corresponding jumps for women are labeled $\\alpha\\tiny F1$and $\\alpha\\tiny F2$. Piecewise regression with two knots and two jumps, labeled with estimated intercepts\rLet’s now illustrate how to perform this analysis. First, the mkspline command is used to create the variables ed1, ed2, and ed3 based on the knots that are specified at 12 and 16 years of education use gss_ivrm.dta mkspline ed1 12 ed2 16 ed3 = educ The variables are now ready to run the piecewise regression model. The regress command, shown below, includes ibn.gender used in combination with the noconstant option. This models separate intercepts for men and women. The model also includes the interaction of ibn.gender with i.hsgrad and i.cograd. This models the jump in income due to graduating high school and graduating college separately for men and women. Finally, the model includes the interaction of ibn.gender with c.ed1, c.ed2, and c.ed3. This models the educ slope for non–high school graduates, high school graduates, and college graduates, estimating these slopes separately for men and women. The variable i.race is included as a covariate. reg realrinc ibn.gender ibn.gender#(hsgrad cograd c.ed1 c.ed2 c.ed3) i.race,vce(robust) noconstant noci Linear regression Number of obs = 32,183\rF(14, 32169) = 2602.08\rProb \u003e F = 0.0000\rR-squared = 0.4885\rRoot MSE = 24897\r--------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t|\r----------------+---------------------------------------\rgender |\rMale | 17084.37 1389.70 12.29 0.000\rFemale | 7262.50 921.90 7.88 0.000\r|\rgender#hsgrad |\rMale#HS Grad | 4927.53 655.33 7.52 0.000\rFemale#HS Grad | 2400.89 372.21 6.45 0.000\r|\rgender#cograd |\rMale#CO Grad | 9230.72 1224.50 7.54 0.000\rFemale#CO Grad | 1615.19 789.62 2.05 0.041\r|\rgender#c.ed1 |\rMale | 89.40 151.40 0.59 0.555\rFemale | 243.64 98.84 2.46 0.014\r|\rgender#c.ed2 |\rMale | 1592.64 255.81 6.23 0.000\rFemale | 1724.74 182.70 9.44 0.000\r|\rgender#c.ed3 |\rMale | 4279.36 521.83 8.20 0.000\rFemale | 3608.45 496.99 7.26 0.000\r|\rrace |\rblack | -3361.24 246.84 -13.62 0.000\rother | -1813.49 840.47 -2.16 0.031\r--------------------------------------------------------\rLet’s now see how to form comparisons among these coefficients. Specifically, let’s learn how to compare slopes between men and women compare slopes across the levels of education, comparing college graduates, high school graduates, and non–high school graduates compare changes in slope between men and women due to graduating high school and due to graduating college compare changes in intercept (the jumps in income due to graduating high school and college) by gender compare changes in intercept (the jumps in income due to graduating high school and college) across levels of education ","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:2:0","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.1 Comparing slopes across gender (Cross Section) Let’s begin by testing the equality of the slopes between women and men who have not graduated high school—testing whether $\\beta\\tiny F1$= $\\beta\\tiny M1$. contrast gender#c.ed1,pveffects nowald //slope between Men and Women (not-grad) Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r------------------+---------------------------------------\rgender#c.ed1 |\r(Female vs base) | 154.25 179.30 0.86 0.390\r----------------------------------------------------------\rthe difference in the educ slope between women and men before graduating high school is 154.25. However, this difference is not statistically significant. In other words, prior to graduating high school, the educ slope is not significantly different for men and women. contrast gender#c.ed2,pveffects nowald //slope between Men and Women (grad) Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r------------------+---------------------------------------\rgender#c.ed2 |\r(Female vs base) | 132.11 314.04 0.42 0.674\r----------------------------------------------------------\rcontrast gender#c.ed3,pveffects nowald //slope between Men and Women (grad-col) Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r------------------+---------------------------------------\rgender#c.ed3 |\r(Female vs base) | -670.91 721.07 -0.93 0.352\r----------------------------------------------------------\r","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:2:1","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.2 Comparing slopes across education Let’s begin by comparing the slopes for high school graduates with non–high school graduates, starting with men. testing whether $\\beta\\tiny M2$ = $\\beta\\tiny M1$ . comparing the slopes for high school graduates with non-high school graduates (Men). lincom 1.gender#c.ed2 - 1.gender#c.ed1 ( 1) - 1bn.gender#c.ed1 + 1bn.gender#c.ed2 = 0\r------------------------------------------------------------------------------\rrealrinc | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\r(1) | 1503.24 297.13 5.06 0.000 920.85 2085.63\r------------------------------------------------------------------------------\rcomparing the slopes for high school graduates with non-high school graduates (Women). lincom 2.gender#c.ed2 - 2.gender#c.ed1 ( 1) - 2.gender#c.ed1 + 2.gender#c.ed2 = 0\r------------------------------------------------------------------------------\rrealrinc | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\r(1) | 1481.10 208.81 7.09 0.000 1071.82 1890.38\r------------------------------------------------------------------------------\rMen : cograd vs. hsgrad lincom 1.gender#c.ed3 - 1.gender#c.ed2 ( 1) - 1bn.gender#c.ed2 + 1bn.gender#c.ed3 = 0\r------------------------------------------------------------------------------\rrealrinc | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\r(1) | 2686.72 581.27 4.62 0.000 1547.41 3826.03\r------------------------------------------------------------------------------\rWomen : cograd vs. hsgrad lincom 2.gender#c.ed3 - 2.gender#c.ed2 ( 1) - 2.gender#c.ed2 + 2.gender#c.ed3 = 0\r------------------------------------------------------------------------------\rrealrinc | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\r(1) | 1883.71 530.20 3.55 0.000 844.49 2922.93\r------------------------------------------------------------------------------\r","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:2:2","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.3 Difference in differences of slopes Let’s now test the gain in slope due to graduating high school for men compared with the gain in slope due to graduating high school for women, which is ($\\beta\\tiny M2$ - $\\beta\\tiny M1$) -($\\beta\\tiny F2$ - $\\beta\\tiny F1$) lincom (1.gender#c.ed2 - 1.gender#c.ed1) - (2.gender#c.ed2 - 2.gender#c.ed1) ( 1) - 1bn.gender#c.ed1 + 2.gender#c.ed1 + 1bn.gender#c.ed2 - 2.gender#c.ed2 = 0\r------------------------------------------------------------------------------\rrealrinc | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\r(1) | 22.14 361.97 0.06 0.951 -687.33 731.61\r------------------------------------------------------------------------------\rlincom (1.gender#c.ed3 - 1.gender#c.ed2) - (2.gender#c.ed3 - 2.gender#c.ed2) ( 1) - 1bn.gender#c.ed2 + 2.gender#c.ed2 + 1bn.gender#c.ed3 - 2.gender#c.ed3 = 0\r------------------------------------------------------------------------------\rrealrinc | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\r(1) | 803.01 786.64 1.02 0.307 -738.84 2344.87\r------------------------------------------------------------------------------\r","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:2:3","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.4 Comparing changes in intercepts by gender Let’s now ask whether the jump in income at 12 years of education is the same for men and women. Let’s test whether these jumps are equal (whether $\\alpha\\tiny F1$= $\\alpha\\tiny M1$ ). contrast gender#hsgrad,pveffects nowald Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r------------------------------------+---------------------------------------\rgender#hsgrad |\r(Female vs base) (HS Grad vs base) | -2526.64 752.53 -3.36 0.001\r----------------------------------------------------------------------------\rtest whether these jump are equal(αF2=αM2？) contrast gender#cograd,pveffects nowald Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r------------------------------------+---------------------------------------\rgender#cograd |\r(Female vs base) (CO Grad vs base) | -7615.52 1455.05 -5.23 0.000\r----------------------------------------------------------------------------\r","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:2:4","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.5 Comparing changes in intercepts by education We might ask whether the jump due to graduating college ($9,230.72$) is equal to the jump due to graduating high school ($4,927.53$). whether the jump due to graduating college is equal to the jump due to graduating high school(Men) lincom 1.gender#1.cograd - 1.gender#1.hsgrad ( 1) - 1bn.gender#1.hsgrad + 1bn.gender#1.cograd = 0\r------------------------------------------------------------------------------\rrealrinc | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\r(1) | 4303.19 1327.65 3.24 0.001 1700.93 6905.44\r------------------------------------------------------------------------------\rlincom 2.gender#1.cograd - 2.gender#1.hsgrad ( 1) - 2.gender#1.hsgrad + 2.gender#1.cograd = 0\r------------------------------------------------------------------------------\rrealrinc | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\r(1) | -785.70 831.22 -0.95 0.345 -2414.92 843.52\r------------------------------------------------------------------------------\r","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:2:5","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.6 Computing and comparing adjusted means Let’s now see how to compute adjusted means in the context of this model. To compute adjusted means with respect to education, we need to express education in terms of hsgrad, cograd, ed1, ed2, and ed3. showcoding educ hsgrad cograd ed1 ed2 ed3 We can estimate the adjusted mean for men and women (separately) in one margins command. hoding education constant at 15-years of education margins gender,nopvalues at(hsgrad=1 cograd=0 ed1=12 ed2=3 ed3=0) Predictive margins Number of obs = 32,183\rModel VCE: Robust\rExpression: Linear prediction, predict()\rAt: hsgrad = 1\rcograd = 0\red1 = 12\red2 = 3\red3 = 0\r--------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r-------------+------------------------------------------------\rgender |\rMale | 27322.43 652.72 26043.07 28601.78\rFemale | 17221.21 488.53 16263.68 18178.75\r--------------------------------------------------------------\rLet’s now use the margins command to compare the adjusted mean of income for women versus men among those with 15 years of education. Specifying the r. contrast operator indicates we want to use reference group comparisons. margins r.gender,at(hsgrad=1 cograd=0 ed1=12 ed2=3 ed3=0) Contrasts of predictive margins Number of obs = 32,183\rModel VCE: Robust\rExpression: Linear prediction, predict()\rAt: hsgrad = 1\rcograd = 0\red1 = 12\red2 = 3\red3 = 0\r------------------------------------------------\r| df F P\u003eF\r-------------+----------------------------------\rgender | 1 154.10 0.0000\r|\rDenominator | 32169\r------------------------------------------------\r-------------------------------------------------------------------\r| Delta-method\r| Contrast std. err. [95% conf. interval]\r------------------+------------------------------------------------\rgender |\r(Female vs Male) | -10101.21 813.72 -11696.14 -8506.28\r-------------------------------------------------------------------\rat 15 years of education, the adjusted mean of income for men is $10,101.21$ higher than for women. ","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:2:6","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.7 Graphing adjusted means margins gender,at(hsgrad=0 cograd=0 ed1=0 ed2=0 ed3=0) /// at(hsgrad=0 cograd=0 ed1=12 ed2=0 ed3=0) /// at(hsgrad=1 cograd=0 ed1=12 ed2=0 ed3=0) /// at(hsgrad=1 cograd=0 ed1=12 ed2=4 ed3=0) /// at(hsgrad=1 cograd=1 ed1=12 ed2=4 ed3=0) /// at(hsgrad=1 cograd=1 ed1=12 ed2=4 ed3=4) Predictive margins Number of obs = 32,183\rModel VCE: Robust\rExpression: Linear prediction, predict()\r1._at: hsgrad = 0\rcograd = 0\red1 = 0\red2 = 0\red3 = 0\r2._at: hsgrad = 0\rcograd = 0\red1 = 12\red2 = 0\red3 = 0\r3._at: hsgrad = 1\rcograd = 0\red1 = 12\red2 = 0\red3 = 0\r4._at: hsgrad = 1\rcograd = 0\red1 = 12\red2 = 4\red3 = 0\r5._at: hsgrad = 1\rcograd = 1\red1 = 12\red2 = 4\red3 = 0\r6._at: hsgrad = 1\rcograd = 1\red1 = 12\red2 = 4\red3 = 4\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_at#gender |\r1#Male | 16544.24 1384.77 11.95 0.000 13830.04 19258.45\r1#Female | 6722.38 909.92 7.39 0.000 4938.90 8505.85\r2#Male | 17616.99 590.89 29.81 0.000 16458.82 18775.15\r2#Female | 9646.09 345.63 27.91 0.000 8968.64 10323.54\r3#Male | 22544.52 282.66 79.76 0.000 21990.49 23098.54\r3#Female | 12046.99 142.20 84.72 0.000 11768.27 12325.71\r4#Male | 28915.06 896.20 32.26 0.000 27158.47 30671.66\r4#Female | 18945.96 667.08 28.40 0.000 17638.46 20253.45\r5#Male | 38145.78 830.91 45.91 0.000 36517.16 39774.40\r5#Female | 20561.15 422.35 48.68 0.000 19733.33 21388.97\r6#Male | 55263.21 1736.33 31.83 0.000 51859.94 58666.48\r6#Female | 34994.95 1760.66 19.88 0.000 31543.99 38445.92\r------------------------------------------------------------------------------\rpreserve clear input educ yhatm yhatf 0 16544.24 6722.377 12 17616.99 9646.094 12 22544.52 12046.99 16 28915.06 18945.96 16 38145.78 20561.15 20 55263.21 34994.95 end graph twoway line yhatm yhatf educ,xlabel(0(4)20) xline(12 16) restore Fitted values from piecewise model with two knots and two jumps\rAutomate the creation of the graph *First,rerun the -margins- command from above quietly margins gender,at(hsgrad=0 cograd=0 ed1=0 ed2=0 ed3=0) /// at(hsgrad=0 cograd=0 ed1=12 ed2=0 ed3=0) /// at(hsgrad=1 cograd=0 ed1=12 ed2=0 ed3=0) /// at(hsgrad=1 cograd=0 ed1=12 ed2=4 ed3=0) /// at(hsgrad=1 cograd=1 ed1=12 ed2=4 ed3=0) /// at(hsgrad=1 cograd=1 ed1=12 ed2=4 ed3=4) /// *Store the adjusted means(from the -margins- command) in a matrix named -yhat- matrix yhat = r(b)' //note: we must transpose r(b) here *Store levels of education in a matrix named -educ- matrix educ = (0\\0\\12\\12\\12\\12\\16\\16\\16\\16\\20\\20) *Store levels of gender in a matrix named -gender- matrix gender = (1\\2\\1\\2\\1\\2\\1\\2\\1\\2\\1\\2) svmat yhat //save the matrix -yhat- to the current dataset svmat educ //save the matrix -educ- to the current dataset svmat gender //save the matrix -gender- to the current dataset graph twoway (line yhat1 educ1 if gender1==1) /// (line yhat1 educ1 if gender1==2), /// xline(12 16) legend(label(1 \"Men\") label(2 \"Women\")) /// xtitle(Education) ytitle(Adjusted mean) ","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:2:7","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3 Comparing coding schemes This chapter has focused on one coding scheme for fitting models that interact a categorical variable and a continuous variable fit in a piecewise manner.Depending on your research question, another coding scheme might be more useful. ","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:3:0","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.1 Coding scheme #1 This coding scheme will be called coding scheme #1. The noheader option is used in this and subsequent examples to save space. use gss_ivrm.dta mkspline ed1 12 ed2 = educ reg realrinc ibn.gender ibn.gender#(i.hsgrad c.ed1 c.ed2) i.race,vce(robust) noconstant noheader noci --------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t|\r----------------+---------------------------------------\rgender |\rMale | 17144.66 1389.51 12.34 0.000\rFemale | 7325.48 922.52 7.94 0.000\r|\rgender#hsgrad |\rMale#HS Grad | 3382.00 661.55 5.11 0.000\rFemale#HS Grad | 1748.10 389.36 4.49 0.000\r|\rgender#c.ed1 |\rMale | 86.71 151.38 0.57 0.567\rFemale | 241.61 98.92 2.44 0.015\r|\rgender#c.ed2 |\rMale | 4057.64 150.38 26.98 0.000\rFemale | 2529.55 110.43 22.91 0.000\r|\rrace |\rblack | -3521.34 246.66 -14.28 0.000\rother | -1946.16 839.73 -2.32 0.020\r--------------------------------------------------------\rIntercept and slope coefficients from piecewise regression fit using coding scheme #1\r","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:3:1","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.2 Coding scheme #2 Let’s now fit a model using what I call coding scheme #2. This coding scheme is the same as coding scheme #1, except that the marginal option is used on the mkspline command. use gss_ivrm.dta mkspline ed1m 12 ed2m = educ,marginal reg realrinc ibn.gender ibn.gender#(i.hsgrad c.ed1m c.ed2m) i.race,vce(robust) noconstant noheader noci --------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t|\r----------------+---------------------------------------\rgender |\rMale | 17144.66 1389.51 12.34 0.000\rFemale | 7325.48 922.52 7.94 0.000\r|\rgender#hsgrad |\rMale#HS Grad | 3382.00 661.55 5.11 0.000\rFemale#HS Grad | 1748.10 389.36 4.49 0.000\r|\rgender#c.ed1m |\rMale | 86.71 151.38 0.57 0.567\rFemale | 241.61 98.92 2.44 0.015\r|\rgender#c.ed2m |\rMale | 3970.93 212.70 18.67 0.000\rFemale | 2287.94 147.26 15.54 0.000\r|\rrace |\rblack | -3521.34 246.66 -14.28 0.000\rother | -1946.16 839.73 -2.32 0.020\r--------------------------------------------------------\rSummary of regression results and meaning of coefficients for coding schemes #1 and #2\rYou can see that the only difference is in the final row of the table. Coding scheme #1 estimates $\\beta\\tiny M2$ and $\\beta\\tiny F2$, whereas coding scheme #2 estimates ($\\beta\\tiny M2$-$\\beta\\tiny M1$)and ($\\beta\\tiny F2$-$\\beta\\tiny F1$). ","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:3:2","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.3 Coding scheme #3 This coding scheme is like coding scheme #1, in that the marginal option is omitted from the mkspline command. Unlike coding scheme #1, coding scheme #3 specifies i.gender (instead of ibn.gender) and omits the noconstant option use gss_ivrm.dta mkspline ed1 12 ed2 = educ reg realrinc i.gender##(i.hsgrad c.ed1 c.ed2) i.race,vce(robust) noheader noci --------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t|\r----------------+---------------------------------------\rgender |\rFemale | -9819.17 1638.56 -5.99 0.000\r|\rhsgrad |\rHS Grad | 3382.00 661.55 5.11 0.000\red1 | 86.71 151.38 0.57 0.567\red2 | 4057.64 150.38 26.98 0.000\r|\rgender#hsgrad |\rFemale#HS Grad | -1633.91 766.98 -2.13 0.033\r|\rgender#c.ed1 |\rFemale | 154.90 179.32 0.86 0.388\r|\rgender#c.ed2 |\rFemale | -1528.09 187.09 -8.17 0.000\r|\rrace |\rblack | -3521.34 246.66 -14.28 0.000\rother | -1946.16 839.73 -2.32 0.020\r|\r_cons | 17144.66 1389.51 12.34 0.000\r--------------------------------------------------------\r","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:3:3","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.4 Coding scheme #4 Finally, let’s consider a fourth coding scheme. This coding scheme is like coding scheme #3, except that the marginal option is included on the mkspline command. use gss_ivrm.dta mkspline ed1m 12 ed2m = educ,marginal reg realrinc i.gender##(i.hsgrad c.ed1m c.ed2m) i.race,vce(robust) noheader noci --------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t|\r----------------+---------------------------------------\rgender |\rFemale | -9819.17 1638.56 -5.99 0.000\r|\rhsgrad |\rHS Grad | 3382.00 661.55 5.11 0.000\red1m | 86.71 151.38 0.57 0.567\red2m | 3970.93 212.70 18.67 0.000\r|\rgender#hsgrad |\rFemale#HS Grad | -1633.91 766.98 -2.13 0.033\r|\rgender#c.ed1m |\rFemale | 154.90 179.32 0.86 0.388\r|\rgender#c.ed2m |\rFemale | -1682.99 259.29 -6.49 0.000\r|\rrace |\rblack | -3521.34 246.66 -14.28 0.000\rother | -1946.16 839.73 -2.32 0.020\r|\r_cons | 17144.66 1389.51 12.34 0.000\r--------------------------------------------------------\rSummary of regression results and meaning of coefficients for coding schemes #3 and #4\rIn comparing coding schemes #3 and #4, the only difference is in the final row of the table. Coding scheme #3 estimates $\\beta\\tiny MM2$ and ($\\beta\\tiny F2$-$\\beta\\tiny M2$). By comparison, coding scheme #4 estimates ($\\beta\\tiny M2$-$\\beta\\tiny M1$) and($\\beta\\tiny F2$-$\\beta\\tiny F1$)-($\\beta\\tiny M2$-$\\beta\\tiny M1$) . ","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:3:4","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.5 Choosing coding schemes We can deliberately choose the coding scheme that might make the most sense given our research question. Say that the emphasis of your research study was to test gender differences in the educ slope among high school graduates (that is,$\\beta\\tiny F2$-$\\beta\\tiny M2$). In that case, coding scheme #3 might be the most useful, because the coefficient associated with gender#ed2 directly estimates this difference ($\\beta\\tiny F2$-$\\beta\\tiny M2$). Had you chosen coding scheme #1, you could still estimate this difference, but would need to also use the contrast gender#c.ed1 command to compute this difference. Instead, imagine that your research question focused on gender differences in the educ slope for high school graduates versus non–high school graduates. In that case, coding system #4 might be the most useful because the coefficient associated with gender#ed2m directly estimates this difference ($\\beta\\tiny F2$-$\\beta\\tiny F1$)-($\\beta\\tiny M2$-$\\beta\\tiny M1$). ","date":"2024-01-11","objectID":"/12.chapter12piecewise-by-categorical-interactions/:3:5","tags":["Interaction","stata"],"title":"Chapter12 ：Piecewise by categorical interactions","uri":"/12.chapter12piecewise-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter explores models that include a categorical variable interacted with two continuous variables. ","date":"2024-01-11","objectID":"/13.chapter13continuous-by-continuous-by-categorical-interactions/","tags":["Interaction","stata"],"title":"Chapter13 ：Continuous by continuous by categorical interactions","uri":"/13.chapter13continuous-by-continuous-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter explores models that include a categorical variable interacted with two continuous variables. ","date":"2024-01-11","objectID":"/13.chapter13continuous-by-continuous-by-categorical-interactions/:0:0","tags":["Interaction","stata"],"title":"Chapter13 ：Continuous by continuous by categorical interactions","uri":"/13.chapter13continuous-by-continuous-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 Linear by linear by categorical interactions In this section, we will explore whether the size of the c.age##c.educ interaction depends on gender. ","date":"2024-01-11","objectID":"/13.chapter13continuous-by-continuous-by-categorical-interactions/:1:0","tags":["Interaction","stata"],"title":"Chapter13 ：Continuous by continuous by categorical interactions","uri":"/13.chapter13continuous-by-continuous-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.1 Fitting separate models for males and females To get a general sense of the size of the c.age#c.educ interaction separately for males and females, let’s fit separate models for males and females. Doing so separately for each level of gender by including the by gender, sort: prefix. (The vsquish and noheader options are used to save space.) The first set of results is restricted to analyzing only males, and the second set is restricted to analyzing only females. use gss_ivrm.dta keep if (age\u003e=22 \u0026 age\u003c=55) \u0026 (educ\u003e=12) by gender,sort:regress realrinc c.age##c.educ,vce(robust)vsquish noheader --------------------------------------------------------------------------------------------------------------\r-\u003e gender = Male\r------------------------------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\rage | -1092.29 195.71 -5.58 0.000 -1475.92 -708.65\reduc | -1831.07 526.01 -3.48 0.001 -2862.14 -799.99\rc.age#c.educ | 134.46 14.35 9.37 0.000 106.33 162.60\r_cons | 25275.91 7128.77 3.55 0.000 11302.24 39249.58\r------------------------------------------------------------------------------\r--------------------------------------------------------------------------------------------------------------\r-\u003e gender = Female\r------------------------------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\rage | -876.56 158.24 -5.54 0.000 -1186.74 -566.39\reduc | -966.11 426.78 -2.26 0.024 -1802.69 -129.54\rc.age#c.educ | 87.95 11.97 7.34 0.000 64.47 111.42\r_cons | 17020.70 5661.95 3.01 0.003 5922.30 28119.10\r------------------------------------------------------------------------------\rWe can visualize this in two different ways, by focusing on the slope in the direction of age or by focusing on the slope in the direction of educ. Let’s begin by making a graph that focuses on the slope in the direction of age by placing age on the $x$ axis, with separate lines for educ. Fitted values for age by education interaction for males (left) and females (right)\rAmong males, the age slope increases by 134.46 units for every oneyear increase in educ. Among females, the age slope increases by 87.95 units for every one-year increase in educ. We can visualize the c.age#c.educ interaction another way, focusing on the educ slope by placing educ on the $x$ axis, with separate lines for age. Fitted values for age by education interaction for males (left) and females (right) with education on the $x$ axis\rFor males, the educ slope increases by 134.46 units for every one-year increase in age. For females, the educ slope increases by 87.95 units for every oneyear increase in age. To summarize, the coefficient for c.age#c.educ is 134.46 for males and is 87.95 for females. This suggests that the size of the c.age#c.educ coefficient may be significantly greater for males than for females. Let’s test this by analyzing males and females together in a single model. ","date":"2024-01-11","objectID":"/13.chapter13continuous-by-continuous-by-categorical-interactions/:1:1","tags":["Interaction","stata"],"title":"Chapter13 ：Continuous by continuous by categorical interactions","uri":"/13.chapter13continuous-by-continuous-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.2 Fitting a combined model for males and females We now create a regression model that includes both males and females and predicts realrinc from age, educ, and the interaction of these two continuous variables. By specifying ibn.gender in conjunction with the noconstant option, the model fits separate intercepts by gender. By specifying the interaction of ibn.gender with c.age, with c.educ, and with c.age#c.educ, the model fits separate estimates of age, educ, and age#educ by gender. reg realrinc ibn.gender ibn.gender#(c.age c.educ c.age#c.educ)i.race,vce(robust) noconstant noci Linear regression Number of obs = 22,367\rF(10, 22357) = 3126.19\rProb \u003e F = 0.0000\rR-squared = 0.5339\rRoot MSE = 24543\r------------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t|\r--------------------+---------------------------------------\rgender |\rMale | 27044.09 7125.42 3.80 0.000\rFemale | 19020.31 5695.44 3.34 0.001\r|\rgender#c.age |\rMale | -1116.24 195.43 -5.71 0.000\rFemale | -903.50 158.76 -5.69 0.000\r|\rgender#c.educ |\rMale | -1929.04 526.08 -3.67 0.000\rFemale | -1063.55 428.25 -2.48 0.013\r|\rgender#c.age#c.educ |\rMale | 136.00 14.34 9.48 0.000\rFemale | 89.52 12.01 7.46 0.000\r|\rrace |\rblack | -3067.11 305.07 -10.05 0.000\rother | 493.01 1192.16 0.41 0.679\r------------------------------------------------------------\rWe can express these results as two though there are two regression equations, one for males and one for females. \\begin{align} Males:\\widehat{realrinc} = 27044.09 + - 1116.24age + - 1929.04educ \\notag\\ \\end{align} \\begin{align} \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\ +136.00age*educ + -3067.11black + 493.01other \\notag \\ \\end{align} \\begin{align} Females:\\widehat{realrinc} = 19020.31 + - 903.50age + - 1063.55educ \\notag\\ \\end{align} \\begin{align} \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\ +89.52age*educ + -3067.11black + 493.01other \\notag \\ \\end{align} Let’s now test the difference of the c.age#c.educ interaction for females versus males using the contrast command below. contrast gender#c.age#c.educ,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r--------------------+---------------------------------------\rgender#c.age#c.educ |\r(Female vs base) | -46.48 18.71 -2.48 0.013\r------------------------------------------------------------\rThis test is significant,This means that the c.age#c.educ interaction is significantly lower for females than for males. Let’s explore how to interpret this interaction. Note! What about the lower order effects? We have been focusing on the gender#c.age#c.educ interaction, but you might wonder about the lower order effects, such as gender#c.age or gender#c.educ. It is important to include these effects in the model to preserve the interpretation of the gender#c.age#c.educ interaction. However, there is little to gain by trying to interpret these effects. ","date":"2024-01-11","objectID":"/13.chapter13continuous-by-continuous-by-categorical-interactions/:1:2","tags":["Interaction","stata"],"title":"Chapter13 ：Continuous by continuous by categorical interactions","uri":"/13.chapter13continuous-by-continuous-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.3 Interpreting the interaction focusing in the age slope To help interpret this interaction, let’s visualize it by making a graph that focuses on the age slope. margins gender, at(age=(22 55) educ=(12(2)20)) marginsplot,bydimension(gender) plotdimension(educ,allsimple)legend(subtitle(Education) rows(2)) noci Fitted values by age ( axis), education (separate lines), and gender (separate panels)\rFor males, the age slope increases by 136.00 units for every one-unit increase in education. For females, the age slope increases by 89.52 units for every one-unit increase in education. The test of the gender#c.age#c.educ effect represents the difference in these interaction terms and this graph shows one way to visualize this. The margins command can be used to show the size of the age slope by specifying the dydx(age) option. Let’s use the margins command to estimate the age slope for each of the levels of education expressed as a separate line in figure margins gender,at(educ=(12(2)20)) dydx(age) Average marginal effects Number of obs = 22,367\rModel VCE: Robust\rExpression: Linear prediction, predict()\rdy/dx wrt: age\r1._at: educ = 12\r2._at: educ = 14\r3._at: educ = 16\r4._at: educ = 18\r5._at: educ = 20\r------------------------------------------------------------------------------\r| Delta-method\r| dy/dx std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\rage |\r_at#gender |\r1#Male | 515.76 34.61 14.90 0.000 447.93 583.60\r1#Female | 170.70 19.56 8.73 0.000 132.36 209.03\r2#Male | 787.76 28.15 27.98 0.000 732.58 842.94\r2#Female | 349.73 16.77 20.85 0.000 316.85 382.61\r3#Male | 1059.76 45.09 23.50 0.000 971.38 1148.14\r3#Female | 528.76 36.51 14.48 0.000 457.19 600.33\r4#Male | 1331.76 70.14 18.99 0.000 1194.28 1469.24\r4#Female | 707.80 59.48 11.90 0.000 591.20 824.39\r5#Male | 1603.76 97.22 16.50 0.000 1413.20 1794.32\r5#Female | 886.83 83.05 10.68 0.000 724.06 1049.60\r------------------------------------------------------------------------------\r","date":"2024-01-11","objectID":"/13.chapter13continuous-by-continuous-by-categorical-interactions/:1:3","tags":["Interaction","stata"],"title":"Chapter13 ：Continuous by continuous by categorical interactions","uri":"/13.chapter13continuous-by-continuous-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.4 Interpreting the interaction focusing on the educ slope Now, let’s visualize this interaction by making a graph that focuses on the educ slope. margins gender, at(age=(25(10)55) educ=(12 20)) marginsplot, bydimension(gender) xdimension(educ) noci legend(rows(2)subtitle(Age)) Fitted values by education ( axis), age (separate lines), and gender (separate panels)\rThis graph illustrates the gender#c.age#c.educ interaction by showing how the educ slope increases more rapidly as a function of age for males than for females. The dydx(educ) option can be used with the margins command to compute the educ slope for each of the lines displayed in figure margins gender,at(age=(25(10)55)) dydx(educ) Average marginal effects Number of obs = 22,367\rModel VCE: Robust\rExpression: Linear prediction, predict()\rdy/dx wrt: educ\r1._at: age = 25\r2._at: age = 35\r3._at: age = 45\r4._at: age = 55\r------------------------------------------------------------------------------\r| Delta-method\r| dy/dx std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\reduc |\r_at#gender |\r1#Male | 1470.96 206.19 7.13 0.000 1066.82 1875.10\r1#Female | 1174.36 155.32 7.56 0.000 869.91 1478.81\r2#Male | 2830.96 144.29 19.62 0.000 2548.14 3113.78\r2#Female | 2069.52 104.23 19.85 0.000 1865.22 2273.83\r3#Male | 4190.97 200.68 20.88 0.000 3797.61 4584.32\r3#Female | 2964.69 162.58 18.24 0.000 2646.02 3283.35\r4#Male | 5550.97 317.61 17.48 0.000 4928.43 6173.50\r4#Female | 3859.85 266.13 14.50 0.000 3338.21 4381.49\r------------------------------------------------------------------------------\r","date":"2024-01-11","objectID":"/13.chapter13continuous-by-continuous-by-categorical-interactions/:1:4","tags":["Interaction","stata"],"title":"Chapter13 ：Continuous by continuous by categorical interactions","uri":"/13.chapter13continuous-by-continuous-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.5 Estimating and comparing adjusted means by gender The difference in the adjusted means by gender varies as a function of age and educ due to the interaction of gender with c.age#c.educ. Thus, any comparisons by gender should be performed by specifying a particular value of age and educ. The size of the difference (as well as the significance of the difference) between males and females depends on both educ and age. You could repeat the margins commands above to obtain comparisons between males and females for a variety ofvalues of age and educ. Or you can specify multiple values at once within the margins command. or example, the margins command below estimates the adjusted mean for males and females for the combinations of 12, 15, and 20 years of education and 30, 40, and 50 years of age margins gender,at(educ=(12 16 20)age=(30 40 50)) Predictive margins Number of obs = 22,367\rModel VCE: Robust\rExpression: Linear prediction, predict()\r1._at: age = 30\reduc = 12\r2._at: age = 30\reduc = 16\r3._at: age = 30\reduc = 20\r4._at: age = 40\reduc = 12\r5._at: age = 40\reduc = 16\r6._at: age = 40\reduc = 20\r7._at: age = 50\reduc = 12\r8._at: age = 50\reduc = 16\r9._at: age = 50\reduc = 20\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_at#gender |\r1#Male | 18995.85 290.94 65.29 0.000 18425.60 19566.10\r1#Female | 11006.03 210.47 52.29 0.000 10593.50 11418.57\r2#Male | 27599.71 480.25 57.47 0.000 26658.37 28541.04\r2#Female | 17493.80 332.35 52.64 0.000 16842.37 18145.22\r3#Male | 36203.56 1106.97 32.71 0.000 34033.83 38373.29\r3#Female | 23981.56 788.13 30.43 0.000 22436.77 25526.36\r4#Male | 24153.48 377.11 64.05 0.000 23414.31 24892.64\r4#Female | 12713.02 213.38 59.58 0.000 12294.77 13131.26\r5#Male | 38197.33 507.34 75.29 0.000 37202.92 39191.75\r5#Female | 22781.44 373.68 60.96 0.000 22048.99 23513.88\r6#Male | 52241.19 1088.80 47.98 0.000 50107.08 54375.31\r6#Female | 32849.85 845.68 38.84 0.000 31192.26 34507.45\r7#Male | 29311.10 662.83 44.22 0.000 28011.91 30610.30\r7#Female | 14420.00 351.09 41.07 0.000 13731.84 15108.17\r8#Male | 48794.96 831.12 58.71 0.000 47165.91 50424.02\r8#Female | 28069.07 659.91 42.53 0.000 26775.61 29362.54\r9#Male | 68278.83 1742.40 39.19 0.000 64863.60 71694.06\r9#Female | 41718.15 1479.36 28.20 0.000 38818.50 44617.79\r------------------------------------------------------------------------------\rmargins r.gender,at(educ=(12 16 20) age=(30 40 50)) Contrasts of predictive margins Number of obs = 22,367\rModel VCE: Robust\rExpression: Linear prediction, predict()\r1._at: age = 30\reduc = 12\r2._at: age = 30\reduc = 16\r3._at: age = 30\reduc = 20\r4._at: age = 40\reduc = 12\r5._at: age = 40\reduc = 16\r6._at: age = 40\reduc = 20\r7._at: age = 50\reduc = 12\r8._at: age = 50\reduc = 16\r9._at: age = 50\reduc = 20\r-------------------------------------------------------\r| df F P\u003eF\r--------------------+----------------------------------\rgender@_at |\r(Female vs Male) 1 | 1 499.54 0.0000\r(Female vs Male) 2 | 1 296.57 0.0000\r(Female vs Male) 3 | 1 80.27 0.0000\r(Female vs Male) 4 | 1 701.62 0.0000\r(Female vs Male) 5 | 1 595.57 0.0000\r(Female vs Male) 6 | 1 196.14 0.0000\r(Female vs Male) 7 | 1 395.11 0.0000\r(Female vs Male) 8 | 1 381.32 0.0000\r(Female vs Male) 9 | 1 134.57 0.0000\rJoint | 4 483.36 0.0000\r|\rDenominator | 22357\r-------------------------------------------------------\r---------------------------------------------------------------------\r| Delta-method\r| Contrast std. err. [95% conf. interval]\r--------------------+------------------------------------------------\rgender@_at |\r(Female vs Male) 1 | -7989.82 357.48 -8690.50 -7289.14\r(Female vs Male) 2 | -10105.91 586.83 -11256.13 -8955.68\r(Female vs Male) 3 | -12222.00 1364.16 -14895.84 -9548.16\r(Female vs Male) 4 | -11440.46 431.91 -12287.03 -10593.89\r(Female vs Male) 5 | -15415.90 631.69 -16654.05 -14177.75\r(Female vs Male) 6 | ","date":"2024-01-11","objectID":"/13.chapter13continuous-by-continuous-by-categorical-interactions/:1:5","tags":["Interaction","stata"],"title":"Chapter13 ：Continuous by continuous by categorical interactions","uri":"/13.chapter13continuous-by-continuous-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2 Linear by quadratic by categorical interactions In this section, we will explore whether the size of the c.educ#c.age#c.age interaction depends on gender. ","date":"2024-01-11","objectID":"/13.chapter13continuous-by-continuous-by-categorical-interactions/:2:0","tags":["Interaction","stata"],"title":"Chapter13 ：Continuous by continuous by categorical interactions","uri":"/13.chapter13continuous-by-continuous-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.1 Fitting separate models for males and females Let’s begin by fitting a model that estimates the c.educ#c.age#c.age interaction in two separate models: one fit for males and another fit for females. This is performed by using the by gender, sort: prefix before the regress command that predicts realrinc from c.educ#c.age#c.age. use gss_ivrm.dta keep if (age\u003e=22 \u0026 age\u003c=80) \u0026 (educ\u003e=12) by gender,sort:reg realrinc c.educ##c.age##c.age,vce(robust)vsquish noheader ------------------------------------------------------------------------------------\r-\u003e gender = Male\r------------------------------------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\r-------------------+----------------------------------------------------------------\reduc | -12420.54 1305.49 -9.51 0.000 -14979.50 -9861.58\rage | -6160.36 876.56 -7.03 0.000 -7878.55 -4442.17\rc.educ#c.age | 661.99 65.00 10.18 0.000 534.57 789.41\rc.age#c.age | 56.96 9.96 5.72 0.000 37.43 76.48\rc.educ#c.age#c.age | -6.21 0.74 -8.43 0.000 -7.66 -4.77\r_cons | 131388.59 17570.69 7.48 0.000 96947.42 165829.75\r------------------------------------------------------------------------------------\r------------------------------------------------------------------------------------\r-\u003e gender = Female\r------------------------------------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\r-------------------+----------------------------------------------------------------\reduc | -6103.40 867.43 -7.04 0.000 -7803.68 -4403.12\rage | -3703.10 599.49 -6.18 0.000 -4878.18 -2528.02\rc.educ#c.age | 366.29 45.81 8.00 0.000 276.49 456.08\rc.age#c.age | 35.83 7.23 4.95 0.000 21.65 50.00\rc.educ#c.age#c.age | -3.57 0.56 -6.43 0.000 -4.66 -2.48\r_cons | 69959.52 11406.57 6.13 0.000 47600.98 92318.06\r------------------------------------------------------------------------------------\rThis suggests that the size of this interaction might be more negative for males than for females. Before pursuing whether this difference is significant, let’s first visualize the c.educ#c.age#c.age interaction by gender to gain a further understanding of what this interaction means. Fitted values for education by age-squared interaction for males (left) and females (right)\rthe male exhibit a greater increase in the curvature in the relationship between age and income as a function of education than do females. Let’s test whether the c.educ#c.age#c.age interaction is significantly different for males versus females. To do this, we fit a combined model that includes both males and females to permit a statistical test of c.educ#c.age#c.age by gender. ","date":"2024-01-11","objectID":"/13.chapter13continuous-by-continuous-by-categorical-interactions/:2:1","tags":["Interaction","stata"],"title":"Chapter13 ：Continuous by continuous by categorical interactions","uri":"/13.chapter13continuous-by-continuous-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.2 Fitting a common model for males and females Let’s fit one model for males and females together using a separate intercept and separate slopes coding system that provides separate intercept and slope estimates for males and females. Separate intercepts by gender are obtained by specifying ibn.gender in conjunction with the noconstant option. Then, the model uses the shortcut notation to interact ibn.gender with each term created by c.educ#c.age#c.age. reg realrinc ibn.gender ibn.gender#(c.educ##c.age##c.age) i.race,vce(robust)noconstant noci Linear regression Number of obs = 25,964\rF(14, 25950) = 2544.34\rProb \u003e F = 0.0000\rR-squared = 0.5263\rRoot MSE = 25748\r------------------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t|\r--------------------------+---------------------------------------\rgender |\rMale | 133604.98 17548.76 7.61 0.000\rFemale | 73711.44 11412.10 6.46 0.000\r|\rgender#c.educ |\rMale | -12550.64 1305.36 -9.61 0.000\rFemale | -6329.01 867.97 -7.29 0.000\r|\rgender#c.age |\rMale | -6202.10 875.06 -7.09 0.000\rFemale | -3809.82 598.99 -6.36 0.000\r|\rgender#c.educ#c.age |\rMale | 665.21 64.93 10.24 0.000\rFemale | 374.24 45.78 8.17 0.000\r|\rgender#c.age#c.age |\rMale | 57.18 9.95 5.75 0.000\rFemale | 36.76 7.22 5.09 0.000\r|\rgender#c.educ#c.age#c.age |\rMale | -6.23 0.74 -8.47 0.000\rFemale | -3.65 0.55 -6.58 0.000\r|\rrace |\rblack | -3474.03 291.35 -11.92 0.000\rother | -299.12 1110.20 -0.27 0.788\r------------------------------------------------------------------\rWe can express results as separate regression equations for males and females, as shown below \\begin{align} Males:\\widehat{realrinc} = 133605 + -12550.64educ + -6202.10age \\notag\\ \\end{align} \\begin{align} \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\ +665.21educ × age + 57.18age^2+-6.23educ × age^2 \\notag \\ \\end{align} \\begin{align} \\quad \\quad \\quad \\quad \\ +-3474.03black + -299.12other \\notag \\ \\end{align} \\begin{align} Females:\\widehat{realrinc} = 73711.44 + -6329.01educ + -3809.82age \\notag\\ \\end{align} \\begin{align} \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\ +665.21educ × age + 57.18age^2+-6.23educ × age^2 \\notag \\ \\end{align} \\begin{align} \\quad \\quad \\quad \\quad \\ +-3474.03black + -299.12other \\notag \\ \\end{align} The coefficient for the c.educ#c.age#c.age interaction is $-6.23$ for males and is $-3.65$ for females. We can compare these coefficients using the contrast command, as shown below. ","date":"2024-01-11","objectID":"/13.chapter13continuous-by-continuous-by-categorical-interactions/:2:2","tags":["Interaction","stata"],"title":"Chapter13 ：Continuous by continuous by categorical interactions","uri":"/13.chapter13continuous-by-continuous-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.3 Interpreting the interaction To help us understand this interaction, let’s visualize it by making a graph showing the c.educ#c.age#c.age interaction separately for males and females. compute adjusted means for age 22 to 90(in one-year increments) and for 12 to 20 years of education (in two-year increments),separately for males and females. margins gender,at(age=(22(1)80) educ=(12(2)20)) marginsplot,bydimension(gender) xdimension(age)plotdimension(educ,allsimp) noci recast(line) scheme(s1mono)legend(subtitle(Education)rows(1)) Fitted values by age ( axis), education (separate lines), and gender (separate panels)\rWe see a similar pattern for both males and females; as educ increases, the inverted U-shape for the relationship between income and age becomes more pronounced. However, this effect appears stronger for males than for females. This is confirmed by the significant gender#c.educ#c.age#c.age interaction. In other words, this interaction shows that the degree to which the quadratic effect of age changes as a function of educ is stronger for males than it is for females. ","date":"2024-01-11","objectID":"/13.chapter13continuous-by-continuous-by-categorical-interactions/:2:3","tags":["Interaction","stata"],"title":"Chapter13 ：Continuous by continuous by categorical interactions","uri":"/13.chapter13continuous-by-continuous-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.4 Estimating and comparing adjusted means by gender The difference in income for males and females depends on both age and gender. The margins command can be used to compute estimates of, and differences in, the adjusted means by gender. margins gender,at(educ=16 age=30) Predictive margins Number of obs = 25,964\rModel VCE: Robust\rExpression: Linear prediction, predict()\rAt: educ = 16\rage = 30\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\rgender |\rMale | 27288.24 430.84 63.34 0.000 26443.78 28132.71\rFemale | 17867.20 303.52 58.87 0.000 17272.28 18462.12\r------------------------------------------------------------------------------\rBy adding the r. contrast operator to gender, we estimate the difference in these adjusted means, comparing females with males. This difference is significant. margins r.gender,at(educ=16 age=30) contrast(nowald pveffects) Contrasts of predictive margins Number of obs = 25,964\rModel VCE: Robust\rExpression: Linear prediction, predict()\rAt: educ = 16\rage = 30\r----------------------------------------------------------\r| Delta-method\r| Contrast std. err. t P\u003e|t|\r------------------+---------------------------------------\rgender |\r(Female vs Male) | -9421.04 529.92 -17.78 0.000\r----------------------------------------------------------\rWe can specify multiple values in the at() option to estimate the mean for males and females for various combinations of age and educ. The margins command below estimates the mean for males and females for the combinations of 12, 16, and 20 years of education and 30, 40, and 50 years of age. margins gender,at(educ=(12 16 20) age=(30 40 50)) Predictive margins Number of obs = 25,964\rModel VCE: Robust\rExpression: Linear prediction, predict()\r1._at: educ = 12\rage = 30\r2._at: educ = 12\rage = 40\r3._at: educ = 12\rage = 50\r4._at: educ = 16\rage = 30\r5._at: educ = 16\rage = 40\r6._at: educ = 16\rage = 50\r7._at: educ = 20\rage = 30\r8._at: educ = 20\rage = 40\r9._at: educ = 20\rage = 50\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\r_at#gender |\r1#Male | 20106.88 284.18 70.75 0.000 19549.88 20663.89\r1#Female | 11414.41 198.42 57.53 0.000 11025.50 11803.33\r2#Male | 25574.42 439.87 58.14 0.000 24712.25 26436.58\r2#Female | 13294.97 249.62 53.26 0.000 12805.70 13784.24\r3#Male | 27517.24 502.91 54.72 0.000 26531.51 28502.98\r3#Female | 13767.08 273.95 50.25 0.000 13230.11 14304.04\r4#Male | 27288.24 430.84 63.34 0.000 26443.78 28132.71\r4#Female | 17867.20 303.52 58.87 0.000 17272.28 18462.12\r5#Male | 41909.91 621.06 67.48 0.000 40692.60 43127.22\r5#Female | 24497.60 435.14 56.30 0.000 23644.71 25350.50\r6#Male | 48019.96 712.27 67.42 0.000 46623.87 49416.06\r6#Female | 26799.67 508.67 52.69 0.000 25802.64 27796.70\r7#Male | 34469.61 988.60 34.87 0.000 32531.90 36407.31\r7#Female | 24319.99 710.54 34.23 0.000 22927.30 25712.68\r8#Male | 58245.41 1334.69 43.64 0.000 55629.34 60861.49\r8#Female | 35700.24 981.01 36.39 0.000 33777.40 37623.07\r9#Male | 68522.68 1525.11 44.93 0.000 65533.38 71511.98\r9#Female | 39832.27 1144.65 34.80 0.000 37588.69 42075.85\r------------------------------------------------------------------------------\rmargins r.gender,at(educ=(12 16 20)age=(30 40 50)) Contrasts of predictive margins Number of obs = 25,964\rModel VCE: Robust\rExpression: Linear prediction, predict()\r1._at: educ = 12\rage = 30\r2._at: educ = 12\rage = 40\r3._at: educ = 12\rage = 50\r4._at: educ = 16\rage = 30\r5._at: educ = 16\rage = 40\r6._at: educ = 16\rage = 50\r7._at: educ = 20\rage = 30\r8._at: educ = 20\rage = 40\r9._at: educ = 20\rage = 50\r-------------------------------------------------------\r| df F P\u003eF\r--------------------+----------------------------------\rgender@_at |\r(Female vs","date":"2024-01-11","objectID":"/13.chapter13continuous-by-continuous-by-categorical-interactions/:2:4","tags":["Interaction","stata"],"title":"Chapter13 ：Continuous by continuous by categorical interactions","uri":"/13.chapter13continuous-by-continuous-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter illustrates models that involve a continuous variable modeled using a polynomial term interacted with a categorical variable. ","date":"2024-01-10","objectID":"/11.chapter11polynomial-by-categorical-interactions/","tags":["Interaction","stata"],"title":"Chapter11 ：Polynomial by categorical interactions","uri":"/11.chapter11polynomial-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter illustrates models that involve a continuous variable modeled using a polynomial term interacted with a categorical variable. This chapter covers two types of polynomial terms: quadratic and cubic. ","date":"2024-01-10","objectID":"/11.chapter11polynomial-by-categorical-interactions/:0:0","tags":["Interaction","stata"],"title":"Chapter11 ：Polynomial by categorical interactions","uri":"/11.chapter11polynomial-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 Quadratic by categorical interactions ","date":"2024-01-10","objectID":"/11.chapter11polynomial-by-categorical-interactions/:1:0","tags":["Interaction","stata"],"title":"Chapter11 ：Polynomial by categorical interactions","uri":"/11.chapter11polynomial-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.1 Quadratic by two-level categorical Let’s now use the GSS dataset to fit a model predicting income from age (modeled using a quadratic term), whether one is a college graduate, as well as the interaction of these variables. use gss_ivrm.dta keep if age\u003e=22 \u0026 age\u003c=80 lowess realrinc age,generate(yhat_lowess) nograph graph twoway line yhat_lowess age,sort Let’s begin by looking at the relationship between age and income using a lowess smoother. The lowess command is used to create the variable yhat_lowess that contains the lowess smoothed values of realrinc. The graph command creates a graph showing the lowess smoothed values across age. Lowess smoothed values of income by age\rLet’s create this same kind of graph but separating people based on the variable cograd, which is coded: 0 = noncollege graduate and 1 = college graduate. The lowess command is issued twice, each with an if specification. lowess realrinc age if cograd == 0,generate(yhat_lowess0)nograph lowess realrinc age if cograd == 1,generate(yhat_lowess1)nograph graph twoway line yhat_lowess0 yhat_lowess1 age,sort Lowess smoothed values of income predicted from age by college graduation status\rBased on this visual inspection of the data, a regression model predicting realrinc from age and cograd would not only need to account for the quadratic trend in age, but also the difference in the quadratic trend in age for college graduates versus noncollege graduates. Let’s fit a model that includes an intercept, age, and age squared for noncollege graduates, and a separate intercept, age, and age squared for college graduates. Specifying ibn.cograd with the noconstant option yields separate intercept estimates by college graduation status. Specifying ibn.cograd#c.age yields separate age estimates by college graduation status, and ibn.cograd#c.age#c.age yields separate age#age estimates by college graduation status. reg realrinc ibn.cograd ibn.cograd#c.age ibn.cograd#c.age#c.age female,noconstant vce(robust) Linear regression Number of obs = 30,576\rF(7, 30569) = 5127.56\rProb \u003e F = 0.0000\rR-squared = 0.5088\rRoot MSE = 24865\r------------------------------------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\r-------------------+----------------------------------------------------------------\rcograd |\rNot CO Grad | -11096.99 1107.78 -10.02 0.000 -13268.29 -8925.69\rCO Grad | -52730.93 3688.05 -14.30 0.000 -59959.66 -45502.21\r|\rcograd#c.age |\rNot CO Grad | 1594.87 57.26 27.85 0.000 1482.64 1707.10\rCO Grad | 3941.46 194.37 20.28 0.000 3560.49 4322.43\r|\rcograd#c.age#c.age |\rNot CO Grad | -16.20 0.66 -24.48 0.000 -17.50 -14.90\rCO Grad | -37.81 2.30 -16.46 0.000 -42.31 -33.30\r|\rfemale | -12457.11 278.57 -44.72 0.000 -13003.12 -11911.11\r------------------------------------------------------------------------------------\rNote! Model shortcut Stata expands the expression ibn.cograd#(c.age c.age#c.age) to become ibn.cograd#c.age ibn.cograd#c.age#c.age, yielding the same model shown previously. $$Non-college-grad: \\widehat{realrinc}=-11096.99 + 1594.87age + - 16.20age^2 + - 12457.11female $$ $$College-grad: \\widehat{realrinc}=-52730.93 + 3941.46age + -37.81age^2 + -12457.11female $$ Let’s visualize the impact of the differences in these quadratic coefficients by graphing the adjusted means as a function of age and college graduation status (that is, cograd), adjusting for gender. margins cograd, nopvalues at(age=(22(1)80)) marginsplot,noci recast(line) scheme(simono) Fitted values from quadratic by two-level categorical model\rWe can ask whether the degree of curvature between college graduates and non-college graduates is significantly different. The contrast command below tests whether the quadratic term for non-college graduates is equal to the quadratic term for college graduates. contrast cograd#c.age#c.age Contrasts of marginal linear predictions\rMargins: asbalanced\r-----------------------------------------------","date":"2024-01-10","objectID":"/11.chapter11polynomial-by-categorical-interactions/:1:1","tags":["Interaction","stata"],"title":"Chapter11 ：Polynomial by categorical interactions","uri":"/11.chapter11polynomial-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.2 Quadratic by three-level categorical Let’s begin our exploration by using the lowess command to generate variables that contain the smoothed relationship between realrinc and age, separately for each of the three levels of educ3. use gss_ivrm.dta keep if age\u003e=22 \u0026 age\u003c=80 lowess realrinc age if educ3 == 1,generate(yhat_lowess1) nograph lowess realrinc age if educ3 == 2,generate(yhat_lowess2) nograph lowess realrinc age if educ3 == 3,generate(yhat_lowess3) nograph graph twoway line yhat_lowess1 yhat_lowess2 yhat_lowess3 age,sort legend(order(1 \"Non-HS grad\" 2 \"HS grad\" 3 \"CO grad\")) Lowess smoothed values of income by age, separated by three levels of education\rLet’s now fit a model that includes a quadratic term for age as well as an interaction of educ3 and the quadratic term for age. reg realrinc ibn.educ3 ibn.educ3#(c.age c.age#c.age) female,noconstant vce(robust) Linear regression Number of obs = 30,576\rF(10, 30566) = 3691.50\rProb \u003e F = 0.0000\rR-squared = 0.5129\rRoot MSE = 24761\r-----------------------------------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\r------------------+----------------------------------------------------------------\reduc3 |\rnot hs | -8609.97 1907.08 -4.51 0.000 -12347.92 -4872.01\rHS | -11115.20 1375.93 -8.08 0.000 -13812.07 -8418.32\rColl | -52556.01 3687.82 -14.25 0.000 -59784.28 -45327.74\r|\reduc3#c.age |\rnot hs | 1245.11 93.00 13.39 0.000 1062.82 1427.40\rHS | 1625.58 72.45 22.44 0.000 1483.59 1767.58\rColl | 3940.66 194.33 20.28 0.000 3559.76 4321.57\r|\reduc3#c.age#c.age |\rnot hs | -12.46 1.00 -12.40 0.000 -14.43 -10.49\rHS | -16.04 0.87 -18.54 0.000 -17.74 -14.34\rColl | -37.81 2.30 -16.46 0.000 -42.31 -33.31\r|\rfemale | -12750.67 278.86 -45.72 0.000 -13297.24 -12204.10\r-----------------------------------------------------------------------------------\rThe regression equation is written in this fashion as shown below. $$NonHis-grad:\\widehat{realrinc}=-8609.67 + 1245.11age + - 12.46age^2 + - 12750.67female$$ $$His-grad:\\widehat{realrinc}=-11115.2 + 1625.59age + -16.04age^2 + -12750.67female$$ $$College-grad:\\widehat{realrinc}=-52556.01 + 3940.66age + -37.81age^2 + -12750.67female$$ To help interpret the regression coefficients, let’s use the margins and marginsplot commands to visualize the adjusted means as a function of age and educ3. margins educ3,at(age=(22(1)80)) marginsplot,noci recast(line) scheme(simono) Fitted values from age (quadratic) by education level\rThe contrast command can be used to compare the quadratic coefficients among the educational groups. The contrast command below tests the equality of the quadratic coefficient for all three levels of educ3. contrast educ3#c.age#c.age Contrasts of marginal linear predictions\rMargins: asbalanced\r-----------------------------------------------------\r| df F P\u003eF\r------------------+----------------------------------\reduc3#c.age#c.age | 2 51.24 0.0000\r|\rDenominator | 30566\r-----------------------------------------------------\rThis test is significant, indicating that the quadratic term for age is not equal across all three groups. the a. contrast operator is used below to compare the quadratic terms for adjacent education groups.(Specific compare) contrast a.educ3#c.age#c.age,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r------------------+---------------------------------------\reduc3#c.age#c.age |\r(not hs vs HS) | 3.58 1.33 2.70 0.007\r(HS vs Coll) | 21.77 2.45 8.87 0.000\r----------------------------------------------------------\rLet’s now see how to use the margins command to compute adjusted means. The margins command below computes the adjusted mean holding age constant at 30, separately for each education group. margins educ3,nopvalues at(age=30) Predictive margins Number of obs = 30,576\rModel VCE: Robust\rExpression: Linear prediction, predict()\rAt: ","date":"2024-01-10","objectID":"/11.chapter11polynomial-by-categorical-interactions/:1:2","tags":["Interaction","stata"],"title":"Chapter11 ：Polynomial by categorical interactions","uri":"/11.chapter11polynomial-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2 Cubic by categorical interactions This section describes models that involve interactions of a categorical variable with a continuous variable where the continuous variable is fit using a cubic polynomial term. A cubic by categorical interaction allows the groups formed by the categorical variable to differ in cubic trend. In that example, we saw a cubic relationship between year of birth and number of children. Suppose that we divided women into two groups: those who graduated college and those who did not graduate college. Those who graduated college might show a different kind of cubic trend across years of birth compared with non-college graduates. use gss_ivrm.dta keep if (age\u003e=45 \u0026 age \u003c=55) \u0026 (yrborn\u003e=1920 \u0026 yrborn\u003c=1960) \u0026 female==1 Let’s begin by visualizing the nature of the relationship between year of birth and number of children separately for college graduates and non-college graduates. We can do this using a lowess smoothed regression relating year of birth to number of children. lowess children yrborn if cograd==0,generate(yhatlowess0) nograph lowess children yrborn if cograd==1,generate(yhatlowess1) nograph graph twoway line yhatlowess0 yhatlowess1 yrborn,sort Lowess smoothed values of number of children by year of birth, separated by college graduation status\rThe graph in figure suggests that the relationship between year of birth and number of children may show a cubic trend. Furthermore, the cubic trend may differ based on whether the woman graduated college, suggesting an interaction of the cubic trend for year of birth and whether the woman graduated college. Let’s fit a model using a separate slope and separate intercept strategy that allows us to compare the cubic trend in year of birth for college graduates with non-college graduates. Such a model is fit using the regress command below. The first term in the model is ibn.cograd. When specified in combination with noconstant option, the model fits separate intercepts by college graduation status. The model also includes ibn.cograd interacted with the linear term for year of birth, the quadratic term for year of birth, and the cubic term for year of birth. This yields separate estimates of these terms by college graduation status. reg children ibn.cograd ibn.cograd#(c.yrborn40 c.yrborn40#c.yrborn40 c.yrborn40#c.yrborn40#c.yrborn40) i.race,noconstant noci Note：the centered variable, yrborn40, is used to represent year of birth to reduce collinearity. Source | SS df MS Number of obs = 5,037\r-------------+---------------------------------- F(10, 5027) = 1223.60\rModel | 34370.3789 10 3437.03789 Prob \u003e F = 0.0000\rResidual | 14120.6211 5,027 2.80895585 R-squared = 0.7088\r-------------+---------------------------------- Adj R-squared = 0.7082\rTotal | 48491 5,037 9.62696049 Root MSE = 1.676\r--------------------------------------------------------------------------------\rchildren | Coefficient Std. err. t P\u003e|t|\r----------------------------------------+---------------------------------------\rcograd |\rNot CO Grad | 2.80 0.04 65.64 0.000\rCO Grad | 1.96 0.09 22.43 0.000\r|\rcograd#c.yrborn40 |\rNot CO Grad | -0.09 0.01 -15.21 0.000\rCO Grad | -0.06 0.01 -5.35 0.000\r|\rcograd#c.yrborn40#c.yrborn40 |\rNot CO Grad | -0.00 0.00 -4.07 0.000\rCO Grad | 0.00 0.00 0.41 0.685\r|\rcograd#c.yrborn40#c.yrborn40#c.yrborn40 |\rNot CO Grad | 0.00 0.00 9.42 0.000\rCO Grad | 0.00 0.00 1.88 0.060\r|\rrace |\rblack | 0.57 0.07 8.49 0.000\rother | 0.67 0.13 5.26 0.000\r--------------------------------------------------------------------------------\rThe regression equation are devided to two part as shown below: \\begin{align} Non-college-grad:\\widehat{children} = -2.8 - 0.088yrborn40 + -0.00093yrborn40^2 \\notag\\ \\end{align} \\begin{align} \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\ +0.00021yrborn40^3 + 0.57black + 0.67other \\notag \\ \\end{align} \\begin{align} College-grad:\\widehat{children} = 1.96 + -0.063yrborn + 0.00024yrborn40^2 \\notag\\ \\end{align} \\beg","date":"2024-01-10","objectID":"/11.chapter11polynomial-by-categorical-interactions/:2:0","tags":["Interaction","stata"],"title":"Chapter11 ：Polynomial by categorical interactions","uri":"/11.chapter11polynomial-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter illustrates models that involve interactions of categorical and continuous variables.","date":"2024-01-09","objectID":"/10.chapter10linear-by-categorical-interactions/","tags":["Interaction","stata"],"title":"Chapter10 ：Linear by categorical interactions","uri":"/10.chapter10linear-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter illustrates models that involve interactions of categorical and continuous variables. ","date":"2024-01-09","objectID":"/10.chapter10linear-by-categorical-interactions/:0:0","tags":["Interaction","stata"],"title":"Chapter10 ：Linear by categorical interactions","uri":"/10.chapter10linear-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 Linear and two-level categorical: No interaction ","date":"2024-01-09","objectID":"/10.chapter10linear-by-categorical-interactions/:1:0","tags":["Interaction","stata"],"title":"Chapter10 ：Linear by categorical interactions","uri":"/10.chapter10linear-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.1 Overview This section introduces the concepts involving models that combine continuous and categorical predictors with no interaction. Let’s begin by considering a hypothetical simple regression model predicting income from age, focusing on people who are aged 22 to 55. $$ \\widehat{realrinc}=9000 + 400age $$ Simple linear regression predicting income from age\rLet’s now expand upon this model and introduce a categorical variable with two levels reflecting whether the respondent graduated college (named cograd in the dataset gss_ivrm.dta). It is coded 0 if the respondent did not graduate college and 1 if the respondent did graduate college. One continuous and one categorical predictor with labels for slopes and intercepts\rthe intercept for those who did not graduate college is 4,000 and for those who did graduate college is 21,000. The difference in theseintercepts is 17,000. One equation is given for noncollege graduates and another for college graduates, as shown below. $$ Noncollege - graduate:\\widehat{realrinc}=4000 + 400age $$ $$ College - graduate:\\widehat{realrinc}=21000 + 400age $$ This regression model can also be expressed as one equation, as shown below. $$ \\widehat{realrinc}=4000 + 17000cograd + 400age $$ The predicted values at 30, 40, and 50 years of age have been computed both for those who did and for those who did not graduate college and are graphed in figure below： One continuous and one categorical predictor with labels for predicted values\r","date":"2024-01-09","objectID":"/10.chapter10linear-by-categorical-interactions/:1:1","tags":["Interaction","stata"],"title":"Chapter10 ：Linear by categorical interactions","uri":"/10.chapter10linear-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.2 Examples using the GSS This section applies the model predicting realrinc from age and cograd using the GSS dataset. use gss_ivrm.dta keep if age\u003e=22 \u0026 age\u003c=55 reg realrinc age i.cograd female,vce(robust) Linear regression Number of obs = 25,718\rF(3, 25714) = 906.12\rProb \u003e F = 0.0000\rR-squared = 0.1569\rRoot MSE = 23938\r------------------------------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\rage | 539.90 15.27 35.36 0.000 509.98 569.83\r|\rcograd |\rCO Grad | 14176.56 425.16 33.34 0.000 13343.22 15009.90\rfemale | -12119.58 295.27 -41.05 0.000 -12698.33 -11540.83\r_cons | 4171.57 518.76 8.04 0.000 3154.76 5188.37\r------------------------------------------------------------------------------\rBefore interpreting the coefficients for this model, let’s create a graph showing the adjusted means as a function of age and cograd.2 First, we use the margins command to compute the adjusted means at ages 22 and 55 for each level of cograd. margins cograd,nopvalues at(age=(22 55)) vsquish marginsplot,noci Predictive margins Number of obs = 25,718\rModel VCE: Robust\rExpression: Linear prediction, predict()\r1._at: age = 22\r2._at: age = 55\r----------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r---------------+------------------------------------------------\r_at#cograd |\r1#Not CO Grad | 10048.09 216.20 9624.33 10471.85\r1#CO Grad | 24224.65 398.82 23442.95 25006.35\r2#Not CO Grad | 27864.91 344.23 27190.20 28539.62\r2#CO Grad | 42041.46 553.37 40956.82 43126.11\r----------------------------------------------------------------\rFitted values of continuous and categorical model without interaction\rThese parallel lines have the same slope, as represented by the coefficient for age. Regardless of whether you graduated college, the age slope is 539.90. We can use the margins command to compute adjusted means based on this model. For example, the adjusted mean for someone who graduated college and was 40 years old, adjusting for gender, is 33,942.91. margins,nopvalues at(cograd=1 age=40) Predictive margins Number of obs = 25,718\rModel VCE: Robust\rExpression: Linear prediction, predict()\rAt: age = 40\rcograd = 1\r--------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r-------------+------------------------------------------------\r_cons | 33942.91 419.99 33119.71 34766.11\r--------------------------------------------------------------\rLet’s repeat this command, but this time obtain the adjusted means separately for those who did and for those who did not graduate college. margins cograd,nopvalues at(age=40) Predictive margins Number of obs = 25,718\rModel VCE: Robust\rExpression: Linear prediction, predict()\rAt: age = 40\r--------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r-------------+------------------------------------------------\rcograd |\rNot CO Grad | 19766.35 151.46 19469.48 20063.23\rCO Grad | 33942.91 419.99 33119.71 34766.11\r--------------------------------------------------------------\rNote the difference between these two values corresponds to the main effect of cograd. Here $33942.91 - 19766.35 = 14176.56$ . You could repeat the above command for any given level of age and the difference in the adjusted means would remain the same. The adjusted means are computed by setting all the covariates (that is, age and female) to the average value of the entire sample. In this example, adding the at((mean) age female) option specifies that age and female should be held constant at their mean. margins cograd,nopvalues at((mean) age female) Adjusted predictions Number of obs = 25,718\rModel VCE: Robust\rExpression: Linear prediction, predict()\rAt: age = 37.28214 (mean)\rfemale = .4951785 (mean)\r-----------------------------","date":"2024-01-09","objectID":"/10.chapter10linear-by-categorical-interactions/:1:2","tags":["Interaction","stata"],"title":"Chapter10 ：Linear by categorical interactions","uri":"/10.chapter10linear-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2 Linear by two-level categorical interactions ","date":"2024-01-09","objectID":"/10.chapter10linear-by-categorical-interactions/:2:0","tags":["Interaction","stata"],"title":"Chapter10 ：Linear by categorical interactions","uri":"/10.chapter10linear-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.1 Overview Let’s build upon the model that was illustrated by including age and cograd as predictors, as well as the interaction between age and cograd. Linear by two-level categorical predictor with labels for intercepts and slopes\rThis figure includes labels showing the intercept and slope for those who did not graduate college (labeled “Int0” and “Slope0”) and the intercept and slope for those who did graduate college (labeled “Int1” and “Slope1”). The regression equation for this hypothetical example can be written as shown below. $$ \\widehat{realrinc}=9300 + - 1300cograd + 250age + 450cograd*age $$ The intercept in the regression equation corresponds to the intercept for those who did not graduate college (that is, 9,300). The coefficient for cograd is the difference in the intercepts (that is, ). Note how this represents the difference between a college graduate versus a noncollege graduate when age is held constant at zero (which is implausible and completely absurd). This is often called the main effect of cograd, but that is misleading because in the presence of the interaction, this term represents the effect of cograd when age is held constant at zero. The interaction term (cograd*age) is the difference in the slopes comparing those who graduated college with those who did not graduate college (that is,$700 - 250 = 450$ ). This interaction term compares the slope of college graduates with the slope of noncollege graduates. It can be easier to understand this model when it is written as two equations $$ Noncollege - graduate:\\widehat{realrinc}=9300 + 250age $$ $$ College - graduate:\\widehat{realrinc}=8000 + 700age $$ ","date":"2024-01-09","objectID":"/10.chapter10linear-by-categorical-interactions/:2:1","tags":["Interaction","stata"],"title":"Chapter10 ：Linear by categorical interactions","uri":"/10.chapter10linear-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.2 Examples using the GSS use gss_ivrm.dta keep if age\u003e=22 \u0026 age\u003c=55 reg realrinc i.cograd age i.cograd#c.age female,vce(robust) Linear regression Number of obs = 25,718\rF(4, 25713) = 710.76\rProb \u003e F = 0.0000\rR-squared = 0.1661\rRoot MSE = 23807\r------------------------------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\rcograd |\rCO Grad | -8648.27 1426.59 -6.06 0.000 -11444.48 -5852.07\rage | 375.15 14.68 25.56 0.000 346.39 403.92\r|\rcograd#c.age |\rCO Grad | 607.52 42.24 14.38 0.000 524.73 690.31\r|\rfemale | -12004.56 293.46 -40.91 0.000 -12579.76 -11429.37\r_cons | 10224.97 480.32 21.29 0.000 9283.52 11166.41\r------------------------------------------------------------------------------\rAs a shorthand, we can specify i.cograd##c.age. The ## operator includes both the main effects and interactions of the variables specified. reg realrinc i.cograd##c.age female,vce(robust) Linear regression Number of obs = 25,718\rF(4, 25713) = 710.76\rProb \u003e F = 0.0000\rR-squared = 0.1661\rRoot MSE = 23807\r------------------------------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\rcograd |\rCO Grad | -8648.27 1426.59 -6.06 0.000 -11444.48 -5852.07\rage | 375.15 14.68 25.56 0.000 346.39 403.92\r|\rcograd#c.age |\rCO Grad | 607.52 42.24 14.38 0.000 524.73 690.31\r|\rfemale | -12004.56 293.46 -40.91 0.000 -12579.76 -11429.37\r_cons | 10224.97 480.32 21.29 0.000 9283.52 11166.41\r------------------------------------------------------------------------------\r$$ \\widehat{realrinc}=10224.97 + - 8648.27cograd + 375.15age + 607.5224cograd*age + - 12004.56female $$ Let’s create a graph to aid in the process of interpreting the results. compute the adjusted means for ages22 and 55 separately for each level of cograd margins cograd,nopvalues at(age=(22 55)) marginsplot,noci Predictive margins Number of obs = 25,718\rModel VCE: Robust\rExpression: Linear prediction, predict()\r1._at: age = 22\r2._at: age = 55\r----------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r---------------+------------------------------------------------\r_at#cograd |\r1#Not CO Grad | 12533.97 185.57 12170.25 12897.69\r1#CO Grad | 17251.19 549.53 16174.07 18328.31\r2#Not CO Grad | 24914.08 349.75 24228.55 25599.61\r2#CO Grad | 49679.54 950.38 47816.74 51542.34\r----------------------------------------------------------------\rFitted values for linear by two-level categorical predictor model\rThe figure indicate the slope between cograd \u0026 notcograd are significant different. 2.2.1 Estimates of slopes Let’s use the margins command to compute the age slope for college graduates and noncollege graduates. The dydx(age) option is used with the over(cograd) option to compute the age slope at each level of cograd. margins,dydx(age) over(cograd) Average marginal effects Number of obs = 25,718\rModel VCE: Robust\rExpression: Linear prediction, predict()\rdy/dx wrt: age\rOver: cograd\r------------------------------------------------------------------------------\r| Delta-method\r| dy/dx std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\rage |\rcograd |\rNot CO Grad | 375.15 14.68 25.56 0.000 346.39 403.92\rCO Grad | 982.68 39.55 24.84 0.000 905.15 1060.20\r------------------------------------------------------------------------------\r2.2.2 Estimates and contrasts on means Let’s begin the investigation of the effect of cograd by computing the adjusted mean of income for each level of cograd, holding age constant at 30. margins cograd, nopvalues at(age=30) Predictive margins Number of obs = 25,718\rModel VCE: Robust\rExpression: Linear prediction, predict()\rAt: age = 30\r--------------------------------------","date":"2024-01-09","objectID":"/10.chapter10linear-by-categorical-interactions/:2:2","tags":["Interaction","stata"],"title":"Chapter10 ：Linear by categorical interactions","uri":"/10.chapter10linear-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3 Linear by three-level categorical interactions Let’s now explore a model in which a three-level categorical variable is interacted with a continuous variable. Let’s extend the previous example by considering three educational groups: 1) non–high school graduates, 2) high school graduates, and 3) college graduates. Although not shown in the graph, the intercept is 3,000 for group 1, 3,700 for group 2, and for group 3. Linear by three-level categorical predictor with labels for slopes\r$$ \\widehat{realrinc}=3000 + 700hsgrad + -800cograd + 300age + 100hsgradage + 700cogradage $$ The value of 3,000 is the intercept for those who did not graduate college. The coefficient for hsgrad is the difference in the intercepts between high school graduates and non–high school graduates (that is,$3700 - 3000 = 700$ ). The coefficient for cograd is the difference in the intercepts between college graduates and non–high school graduates (that is,$- 5000 - 3000 = -8000$). $$ Non-hsgrad:\\widehat{realrinc}=3000 + 300age $$ $$ Hsgrad:\\widehat{realrinc}=3700 + 400age $$ $$ College-grad:\\widehat{realrinc}=-5000 + 1000age $$ If we do not reject this null hypothesis, then the education by interaction terms may no longer be needed and could be omitted from the model. If we do reject this null hypothesis, we might be further interested in forming specific contrasts among the slopes. We might be further interested in forming specific contrasts among the slopes, for example, We might be interested in comparing the different educational groups given different levels of age. As an illustration, the predicted mean of income for those aged 30, 40, and 50 for eacheducational group have been computed and are plotted in figure below. ","date":"2024-01-09","objectID":"/10.chapter10linear-by-categorical-interactions/:3:0","tags":["Interaction","stata"],"title":"Chapter10 ：Linear by categorical interactions","uri":"/10.chapter10linear-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.1 Examples using the GSS Let’s continue to use age as the continuous predictor and realrinc as the outcome, but now we will use a three-category education variable, educ3. use gss_ivrm.dta keep if age\u003e=22 \u0026 age\u003c=55 reg realrinc i.educ3##c.age female,vce(robust) Linear regression Number of obs = 25,718\rF(6, 25711) = 563.58\rProb \u003e F = 0.0000\rR-squared = 0.1728\rRoot MSE = 23712\r------------------------------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\reduc3 |\rHS | 1906.98 1098.01 1.74 0.082 -245.18 4059.14\rColl | -6287.54 1643.87 -3.82 0.000 -9509.63 -3065.46\r|\rage | 299.15 28.35 10.55 0.000 243.58 354.72\r|\reduc3#c.age |\rHS | 120.00 32.96 3.64 0.000 55.40 184.60\rColl | 682.88 48.71 14.02 0.000 587.42 778.35\r|\rfemale | -12258.80 293.44 -41.78 0.000 -12833.96 -11683.63\r_cons | 8012.50 944.86 8.48 0.000 6160.51 9864.49\r------------------------------------------------------------------------------\rBefore interpreting these results, let’s make a graph showing the adjusted means by age and educ3. compute the adjusted means at ages22 and 55 for each level of educ3 margins educ3,nopvalues at(age=(22 55)) marginsplot,noci Predictive margins Number of obs = 25,718\rModel VCE: Robust\rExpression: Linear prediction, predict()\r1._at: age = 22\r2._at: age = 55\r--------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r-------------+------------------------------------------------\r_at#educ3 |\r1#not hs | 8523.52 361.18 7815.59 9231.45\r1#HS | 13070.41 208.67 12661.40 13479.43\r1#Coll | 17259.39 549.76 16181.83 18336.94\r2#not hs | 18395.49 655.31 17111.04 19679.94\r2#HS | 26902.25 405.94 26106.58 27697.93\r2#Coll | 49666.47 950.09 47804.25 51528.69\r--------------------------------------------------------------\rFitted values for linear by three-level categorical predictor model\rThe dydx(age) is combined with the over(educ3) option to compute the age slope separately for each level of educ3. 3.1.1 Estimates and contrasts on slopes margins,dydx(age) over(educ3) Average marginal effects Number of obs = 25,718\rModel VCE: Robust\rExpression: Linear prediction, predict()\rdy/dx wrt: age\rOver: educ3\r------------------------------------------------------------------------------\r| Delta-method\r| dy/dx std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\rage |\reduc3 |\rnot hs | 299.15 28.35 10.55 0.000 243.58 354.72\rHS | 419.15 16.85 24.87 0.000 386.12 452.18\rColl | 982.03 39.55 24.83 0.000 904.51 1059.55\r------------------------------------------------------------------------------\rThe output not only shows the age slope at each level of educ3 but also includes the standard error, confidence interval, and a test of whether the slope is significantly different from 0. Let’s test whether these slopes are equal to each other. In other words,let’s test the null hypothesis： $$ H_{0} = \\beta{1} = \\beta{2} = \\beta{3} $$ contrast educ3#c.age Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------\r| df F P\u003eF\r-------------+----------------------------------\reduc3#c.age | 2 105.89 0.0000\r|\rDenominator | 25711\r------------------------------------------------\rThis interaction is significant, so we can reject the null hypothesis that these slopes are all equal. $$ H_{0} = \\beta{1} = \\beta{3} $$ $$ H_{0} = \\beta{2} = \\beta{3} $$ The following contrast command tests these two null hypotheses.Specifying rb3.educ3 uses reference group comparisons with group 3 as the baseline (comparison) group. We can reject both null hypotheses. contrast rb3.educ3#c.age,nowald pveffect Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r------------------+---------------------","date":"2024-01-09","objectID":"/10.chapter10linear-by-categorical-interactions/:3:1","tags":["Interaction","stata"],"title":"Chapter10 ：Linear by categorical interactions","uri":"/10.chapter10linear-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter illustrates models involving interactions of three categorical variables, with an emphasis on how to interpret the interaction of the three categorical variables.","date":"2024-01-07","objectID":"/9.chapter9categorical-by-categorical-by-categorical-interactions/","tags":["Categorical","Interaction","stata"],"title":"Chapter9 ：Categorical by categorical by categorical interactions","uri":"/9.chapter9categorical-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter illustrates models involving interactions of three categorical variables, with an emphasis on how to interpret the interaction of the three categorical variables. This chapter focuses on three types of interactions: two by two by two model two by two by three model three by three by four model ","date":"2024-01-07","objectID":"/9.chapter9categorical-by-categorical-by-categorical-interactions/:0:0","tags":["Categorical","Interaction","stata"],"title":"Chapter9 ：Categorical by categorical by categorical interactions","uri":"/9.chapter9categorical-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 Two by two by two models a new study using a two by two by two model in which treatment has two levels (control group versus happiness therapy), depression status has two levels (nondepressed versus mildly depressed), and season has two levels (winter and summer). opt-2by2by2.dta anova opt depstat##treat##season Number of obs = 240 R-squared = 0.5677\rRoot MSE = 8.01794 Adj R-squared = 0.5546\rSource | Partial SS df MS F Prob\u003eF\r---------------------+----------------------------------------------------\rModel | 19584.517 7 2797.7881 43.52 0.0000\r|\rdepstat | 1601.6667 1 1601.6667 24.91 0.0000\rtreat | 13470.017 1 13470.017 209.53 0.0000\rdepstat#treat | 35.266667 1 35.266667 0.55 0.4596\rseason | 3713.0667 1 3713.0667 57.76 0.0000\rdepstat#season | 220.41667 1 220.41667 3.43 0.0653\rtreat#season | 112.06667 1 112.06667 1.74 0.1880\rdepstat#treat#season | 432.01667 1 432.01667 6.72 0.0101\r|\rResidual | 14914.667 232 64.287356 ---------------------+----------------------------------------------------\rTotal | 34499.183 239 144.34805 Note! Three-way interaction shortcut Specifying depstat##treat##season is a shortcut for specifying all main effects, two-way interactions, and the three-way interaction of depstat, treat, and season. This both saves time and helps ensure that you include all lower order effects. Even if not significant, these lower order effects should be included in the model. The depstat#treat#season interaction is significant ($F = 6.72$,$p = 0.0101$). Let’s use the margins command to show the mean of optimism broken down by these three factors. margins depstat#treat#season,nopvalues Adjusted predictions Number of obs = 240\rExpression: Linear prediction, predict()\r----------------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r---------------------+------------------------------------------------\rdepstat#treat#season |\rNon#Con#Winter | 44.40 1.46 41.52 47.28\rNon#Con#Summer | 51.67 1.46 48.78 54.55\rNon#HT#Winter | 59.93 1.46 57.05 62.82\rNon#HT#Summer | 64.57 1.46 61.68 67.45\rMild#Con#Winter | 39.23 1.46 36.35 42.12\rMild#Con#Summer | 44.97 1.46 42.08 47.85\rMild#HT#Winter | 50.93 1.46 48.05 53.82\rMild#HT#Summer | 64.77 1.46 61.88 67.65\r----------------------------------------------------------------------\rLet’s then use the marginsplot command to make a graph of the means, showing treat on the $x$ axis and the different seasons in separate panels. marginsplot,xdimension(treat) bydimension(season)noci Optimism by treatment, depression status, and season\r","date":"2024-01-07","objectID":"/9.chapter9categorical-by-categorical-by-categorical-interactions/:1:0","tags":["Categorical","Interaction","stata"],"title":"Chapter9 ：Categorical by categorical by categorical interactions","uri":"/9.chapter9categorical-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.1 Simple interactions by season One way that we can dissect the three-way interaction is by looking at the simple interactions of treatment by depression status at each season. Looking at figure，it appears that the interaction is not significant during the winter and is significant during the summer. We test this using the contrast command, which tests the treat#depstat interaction at each level of season. contrast treat#depstat@season Contrasts of marginal linear predictions\rMargins: asbalanced\r--------------------------------------------------------\r| df F P\u003eF\r---------------------+----------------------------------\rtreat#depstat@season |\rWinter | 1 1.71 0.1917\rSummer | 1 5.55 0.0193\rJoint | 2 3.63 0.0279\r|\rDenominator | 232\r--------------------------------------------------------\r","date":"2024-01-07","objectID":"/9.chapter9categorical-by-categorical-by-categorical-interactions/:1:1","tags":["Categorical","Interaction","stata"],"title":"Chapter9 ：Categorical by categorical by categorical interactions","uri":"/9.chapter9categorical-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.2 Simple interactions by depression status Another way to dissect this three-way interaction is by looking at the simple interaction of treatment by season at each level of depression status. To visualize this, let’s rerun the margins command and then use the marginsplot command to graph the means showing treat on the $x$ axis and separate panels for those who are nondepressed and mildly depressed. margins depstat#treat#season marginsplot,xdimension(treat)bydimension(depstat) noci Adjusted predictions Number of obs = 240\rExpression: Linear prediction, predict()\r--------------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r---------------------+----------------------------------------------------------------\rdepstat#treat#season |\rNon#Con#Winter | 44.40 1.46 30.33 0.000 41.52 47.28\rNon#Con#Summer | 51.67 1.46 35.29 0.000 48.78 54.55\rNon#HT#Winter | 59.93 1.46 40.94 0.000 57.05 62.82\rNon#HT#Summer | 64.57 1.46 44.11 0.000 61.68 67.45\rMild#Con#Winter | 39.23 1.46 26.80 0.000 36.35 42.12\rMild#Con#Summer | 44.97 1.46 30.72 0.000 42.08 47.85\rMild#HT#Winter | 50.93 1.46 34.79 0.000 48.05 53.82\rMild#HT#Summer | 64.77 1.46 44.24 0.000 61.88 67.65\r--------------------------------------------------------------------------------------\rOptimism by treatment, season, and depression status\rThere is an interaction of treatment by season for those who are mildly depressed, but no such interaction for those who are not depressed. We can test the interaction of treatment by season at each level of depression status using the following contrast command: contrast treat#season@depstat Contrasts of marginal linear predictions\rMargins: asbalanced\r--------------------------------------------------------\r| df F P\u003eF\r---------------------+----------------------------------\rtreat#season@depstat |\rNon | 1 0.81 0.3693\rMild | 1 7.65 0.0061\rJoint | 2 4.23 0.0157\r|\rDenominator | 232\r--------------------------------------------------------\r","date":"2024-01-07","objectID":"/9.chapter9categorical-by-categorical-by-categorical-interactions/:1:2","tags":["Categorical","Interaction","stata"],"title":"Chapter9 ：Categorical by categorical by categorical interactions","uri":"/9.chapter9categorical-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.3 Simple effects We might want to know whether the effect of happiness therapy is significant for each combination of season and depression status. contrast treat@season#depstat,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r--------------------------+---------------------------------------\rtreat@season#depstat |\r(HT vs base) Winter#Non | 15.53 2.07 7.50 0.000\r(HT vs base) Winter#Mild | 11.70 2.07 5.65 0.000\r(HT vs base) Summer#Non | 12.90 2.07 6.23 0.000\r(HT vs base) Summer#Mild | 19.80 2.07 9.56 0.000\r------------------------------------------------------------------\rThe mean optimism for those in the happiness therapy group is always greater than the control group at each level of season and at each level of depression status. ","date":"2024-01-07","objectID":"/9.chapter9categorical-by-categorical-by-categorical-interactions/:1:3","tags":["Categorical","Interaction","stata"],"title":"Chapter9 ：Categorical by categorical by categorical interactions","uri":"/9.chapter9categorical-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2 Two by two by three models Let’s now consider an example with three factors, two of which have two levels and one of which has three levels. In this example, the treatment variable now has three levels: 1.control group, 2. traditional therapy, and 3. happiness therapy. use opt-3by2by2.dta Let’s now use the anova command to predict opt from depstat, treat, season, all two-way interactions of these variables, and the three-way interaction. anova opt depstat##treat##season Number of obs = 360 R-squared = 0.4912\rRoot MSE = 7.9879 Adj R-squared = 0.4751\rSource | Partial SS df MS F Prob\u003eF\r---------------------+----------------------------------------------------\rModel | 21435.233 11 1948.6576 30.54 0.0000\r|\rdepstat | 2423.2111 1 2423.2111 37.98 0.0000\rtreat | 13811.217 2 6905.6083 108.23 0.0000\rdepstat#treat | 3.9055556 2 1.9527778 0.03 0.9699\rseason | 3960.1 1 3960.1 62.06 0.0000\rdepstat#season | 253.34444 1 253.34444 3.97 0.0471\rtreat#season | 469.11667 2 234.55833 3.68 0.0263\rdepstat#treat#season | 514.33889 2 257.16944 4.03 0.0186\r|\rResidual | 22204.667 348 63.806513 ---------------------+----------------------------------------------------\rTotal | 43639.9 359 121.55961 The three-way interaction is significant ( $F = 4.03$ , $p = 0.0186$ ). Let’s use the margins and marginsplot commands to display and graph the means by each of these categorical variables. margins depstat#treat#season,nopvalues marginsplot,bydimension(depstat) noci Adjusted predictions Number of obs = 360\rExpression: Linear prediction, predict()\r-----------------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r----------------------+------------------------------------------------\rdepstat#treat#season |\rNon#Con#Winter (S1) | 44.70 1.46 41.83 47.57\rNon#Con#Summer (S2) | 49.70 1.46 46.83 52.57\rNon#TT#Winter (S1) | 54.33 1.46 51.46 57.20\rNon#TT#Summer (S2) | 59.40 1.46 56.53 62.27\rNon#HT#Winter (S1) | 59.77 1.46 56.90 62.64\rNon#HT#Summer (S2) | 64.57 1.46 61.70 67.44\rMild#Con#Winter (S1) | 39.63 1.46 36.76 42.50\rMild#Con#Summer (S2) | 44.20 1.46 41.33 47.07\rMild#TT#Winter (S1) | 49.23 1.46 46.36 52.10\rMild#TT#Summer (S2) | 54.70 1.46 51.83 57.57\rMild#HT#Winter (S1) | 49.33 1.46 46.46 52.20\rMild#HT#Summer (S2) | 64.23 1.46 61.36 67.10\r-----------------------------------------------------------------------\rOptimism by treatment, season, and depression status\r","date":"2024-01-07","objectID":"/9.chapter9categorical-by-categorical-by-categorical-interactions/:2:0","tags":["Categorical","Interaction","stata"],"title":"Chapter9 ：Categorical by categorical by categorical interactions","uri":"/9.chapter9categorical-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.1 Simple interactions by depression status It appears that the treat#season interaction might not be significant for those who are not depressed (see the left panel of figure) but might be significant for those who are mildly depressed (see the right panel of figure ). We can explore this by assessing the treat#season interaction at each level of depstat using the contrastcommand below. contrast treat#season@depstat Contrasts of marginal linear predictions\rMargins: asbalanced\r--------------------------------------------------------\r| df F P\u003eF\r---------------------+----------------------------------\rtreat#season@depstat |\rNon | 2 0.00 0.9955\rMild | 2 7.70 0.0005\rJoint | 4 3.85 0.0044\r|\rDenominator | 348\r--------------------------------------------------------\rLet’s further dissect this simple interaction by applying contrasts to the treatment factor through the use of simple partial interactions. ","date":"2024-01-07","objectID":"/9.chapter9categorical-by-categorical-by-categorical-interactions/:2:1","tags":["Categorical","Interaction","stata"],"title":"Chapter9 ：Categorical by categorical by categorical interactions","uri":"/9.chapter9categorical-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.2 Simple partial interaction by depression status Say that we want to form two comparisons with respect to treat, comparing group 2 versus 1 (traditional therapy versus control group) and comparing group 3 versus 2 (happiness therapy versus traditional therapy). The interaction of treatment (traditional therapy versus control group) by season for those who are mildly depressed is visualized in the left panel of figure below, and the interaction of treatment (happiness therapy versus traditional therapy) by season for those who are mildly depressed is visualized in the right panel of figure below. Simple partial interactions\rTo understand this, let’s break it into two parts. The first part, ar.treat#season, creates the interactions of treatment (traditional therapy versus control group) by season, and treatment (happiness therapy versus traditional therapy) by season. The second part, @2.depstat indicates the contrasts will be performed only for level 2 of depstat (the mildly depressed group) contrast ar.treat#season@2.depstat Contrasts of marginal linear predictions\rMargins: asbalanced\r-------------------------------------------------------------\r| df F P\u003eF\r--------------------------+----------------------------------\rtreat#season@depstat |\r(TT vs Con) (joint) Mild | 1 0.10 0.7578\r(HT vs TT) (joint) Mild | 1 10.46 0.0013\rJoint | 2 7.70 0.0005\r|\rDenominator | 348\r-------------------------------------------------------------\rLet’s now further understand this simple partial interaction through the use of simple contrasts. ","date":"2024-01-07","objectID":"/9.chapter9categorical-by-categorical-by-categorical-interactions/:2:2","tags":["Categorical","Interaction","stata"],"title":"Chapter9 ：Categorical by categorical by categorical interactions","uri":"/9.chapter9categorical-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.3 Simple contrasts In particular, let’s ask whether there is a difference between happiness therapy and traditional therapy separately for each season focusing only on those who are mildly depressed. We can perform this test by specifying ar3.treat@season#2.depstat on the contrast command. Let’s break this into two parts. The first part, ar3.treat, requests the comparison of treatment group 3 versus 2 (happiness therapy versus traditional therapy). The second part, @season#2.depstat, requests that the contrasts be performed at each level of season and at level 2 of depression status (mildly depressed) contrast ar3.treat@season#2.depstat,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r---------------------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r-----------------------------+---------------------------------------\rtreat@season#depstat |\r(HT vs TT) Winter (S1)#Mild | 0.10 2.06 0.05 0.961\r(HT vs TT) Summer (S2)#Mild | 9.53 2.06 4.62 0.000\r---------------------------------------------------------------------\r","date":"2024-01-07","objectID":"/9.chapter9categorical-by-categorical-by-categorical-interactions/:2:3","tags":["Categorical","Interaction","stata"],"title":"Chapter9 ：Categorical by categorical by categorical interactions","uri":"/9.chapter9categorical-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.4 Partial interactions Say that we wanted to further understand this interaction by applying adjacent group contrasts to the treatment factor. These contrasts compare group 2 versus 1 (traditional therapy versus control group) and group 3 versus 2 (happiness therapy versus traditional therapy). We could interact these contrasts with season and depression status. Simple partial interaction: T2 vs. T1 by season by depstat\rSimple partial interaction: T3 vs. T2 by season by depstat\rWe can test each of these partial interactions using the contrast command below. contrast ar.treat#season#depstat Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------------\r| df F P\u003eF\r-----------------------------+----------------------------------\rtreat#season#depstat |\r(TT vs Con) (joint) (joint) | 1 0.04 0.8400\r(HT vs TT) (joint) (joint) | 1 5.53 0.0193\rJoint | 2 4.03 0.0186\r|\rDenominator | 348\r----------------------------------------------------------------\r","date":"2024-01-07","objectID":"/9.chapter9categorical-by-categorical-by-categorical-interactions/:2:4","tags":["Categorical","Interaction","stata"],"title":"Chapter9 ：Categorical by categorical by categorical interactions","uri":"/9.chapter9categorical-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3 Three by three by three models and beyond Let’s consider an extension of the example involving the factors treat, depstat, and season, except that these factors have three, three and four levels, respectively. The three levels of treat are 1) control group, 2) traditional therapy, and 3) happiness therapy. The three levels of depstat are 1) nondepressed, 2) mildly depressed, and 3) severely depressed. The four levels of season are 1) winter, 2) spring, 3) summer, and 4)fall. use opt-3by3by4.dta anova opt depstat##treat##season Number of obs = 1,080 R-squared = 0.6172\rRoot MSE = 6.00663 Adj R-squared = 0.6043\rSource | Partial SS df MS F Prob\u003eF\r---------------------+----------------------------------------------------\rModel | 60718.9 35 1734.8257 48.08 0.0000\r|\rdepstat | 18782.039 2 9391.0194 260.29 0.0000\rtreat | 34636.039 2 17318.019 480.00 0.0000\rdepstat#treat | 1702.0056 4 425.50139 11.79 0.0000\rseason | 3522.8333 3 1174.2778 32.55 0.0000\rdepstat#season | 1004.2722 6 167.3787 4.64 0.0001\rtreat#season | 185.11667 6 30.852778 0.86 0.5275\rdepstat#treat#season | 886.59444 12 73.88287 2.05 0.0179\r|\rResidual | 37667.067 1,044 36.079566 ---------------------+----------------------------------------------------\rTotal | 98385.967 1,079 91.182546 The three-way interaction of depstat#treat#season is significant ( $F = 2.05$ , $p=0.0179$ ). To begin to understand the nature of this interaction, let’s first graph the interaction using the margins and marginsplot commands. margins depstat#treat#season marginsplot,xdimension(treat)bydimension(season) noci legend(rows(1)) Adjusted predictions Number of obs = 1,080\rExpression: Linear prediction, predict()\r--------------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r---------------------+----------------------------------------------------------------\rdepstat#treat#season |\rNon#Con#Winter | 44.63 1.10 40.70 0.000 42.48 46.79\rNon#Con#Spring | 47.53 1.10 43.34 0.000 45.38 49.69\rNon#Con#Summer | 49.70 1.10 45.32 0.000 47.55 51.85\rNon#Con#Fall | 47.83 1.10 43.62 0.000 45.68 49.99\rNon#TT#Winter | 54.30 1.10 49.51 0.000 52.15 56.45\rNon#TT#Spring | 57.33 1.10 52.28 0.000 55.18 59.49\rNon#TT#Summer | 60.37 1.10 55.05 0.000 58.21 62.52\rNon#TT#Fall | 57.03 1.10 52.01 0.000 54.88 59.19\rNon#HT#Winter | 61.30 1.10 55.90 0.000 59.15 63.45\rNon#HT#Spring | 62.70 1.10 57.17 0.000 60.55 64.85\rNon#HT#Summer | 64.70 1.10 59.00 0.000 62.55 66.85\rNon#HT#Fall | 62.87 1.10 57.33 0.000 60.71 65.02\rMild#Con#Winter | 39.60 1.10 36.11 0.000 37.45 41.75\rMild#Con#Spring | 42.73 1.10 38.97 0.000 40.58 44.89\rMild#Con#Summer | 44.50 1.10 40.58 0.000 42.35 46.65\rMild#Con#Fall | 42.73 1.10 38.97 0.000 40.58 44.89\rMild#TT#Winter | 49.17 1.10 44.83 0.000 47.01 51.32\rMild#TT#Spring | 52.20 1.10 47.60 0.000 50.05 54.35\rMild#TT#Summer | 54.10 1.10 49.33 0.000 51.95 56.25\rMild#TT#Fall | 52.03 1.10 47.45 0.000 49.88 54.19\rMild#HT#Winter | 51.20 1.10 46.69 0.000 49.05 53.35\rMild#HT#Spring | 56.37 1.10 51.40 0.000 54.21 58.52\rMild#HT#Summer | 65.33 1.10 59.58 0.000 63.18 67.49\rMild#HT#Fall | 56.17 1.10 51.22 0.000 54.01 58.32\rSevere#Con#Winter | 36.67 1.10 33.44 0.000 34.51 38.82\rSevere#Con#Spring | 39.67 1.10 36.17 0.000 37.51 41.82\rSevere#Con#Summer | 39.60 1.10 36.11 0.000 37.45 41.75\rSevere#Con#Fall | 39.97 1.10 36.44 0.000 37.81 42.12\rSevere#TT#Winter | 47.20 1.10 43.04 0.000 45.05 49.35\rSevere#TT#Spring | 50.23 1.10 45.81 0.000 48.08 52.39\rSevere#TT#Summer | 49.23 1.10 44.89 0.000 47.08 51.39\rSevere#TT#Fall | 50.00 1.10 45.59 0.000 47.85 52.15\rSevere#HT#Winter | 46.67 1.10 42.55 0.000 44.51 48.82\rSevere#HT#Spring | 49.77 1.10 45.38 0.000 47.61 51.92\rSevere#HT#Summer | 48.57 1.10 44.29 0.000 46.41 50.72\rSevere#HT#Fall | 50.20 1.10 45.78 0.000 48.05 52.35\r--------------------------------------------------------------------------------------\rOptimism by treatment, season, and depression status\r","date":"2024-01-07","objectID":"/9.chapter9categorical-by-categorical-by-categorical-interactions/:3:0","tags":["Categorical","Interaction","stata"],"title":"Chapter9 ：Categorical by categorical by categorical interactions","uri":"/9.chapter9categorical-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.1 Partial interactions and interaction contrasts Let’s explore ways to dissect the three-way interaction of depstat#treat#season by applying contrasts to one or more of the factors. Let’s begin by testing whether the interaction of treat#depstat is the same for each season compared with winter (season 1). This is performed by applying the r. contrast operator to season and interacting that with treat and depstat. contrast r.season#treat#depstat Contrasts of marginal linear predictions\rMargins: asbalanced\r-----------------------------------------------------------------------\r| df F P\u003eF\r------------------------------------+----------------------------------\rseason#treat#depstat |\r(Spring vs Winter) (joint) (joint) | 4 0.46 0.7619\r(Summer vs Winter) (joint) (joint) | 4 5.36 0.0003\r(Fall vs Winter) (joint) (joint) | 4 0.40 0.8115\rJoint | 12 2.05 0.0179\r|\rDenominator | 1044\r-----------------------------------------------------------------------\rNote! Fall versus spring A custom contrast is applied to season to obtain the comparison of fall versus spring (season 4 versus 2).[ contrast {season 0 -1 0 1}#treat#depstat ] The contrast command below performs this comparison by applying the r3. contrast to season to compare summer with winter (season 3 versus 1) and the r2. contrast on depstat to compare those who are mildly depressed with those who are nondepressed (levels 2 versus 1). These terms are all interacted (that is, r3.season#r2.depstat#treat) yielding a test of summer versus winter (season 3 versus 1) by mildly depressed versus nondepressed (levels 2 versus 1) by treatment. *Season(winter vs. summer) by depstat(nondepressed vs. mildly depressed)by treat contrast r3.season#r2.depstat#treat Contrasts of marginal linear predictions\rMargins: asbalanced\r--------------------------------------------------------\r| df F P\u003eF\r---------------------+----------------------------------\rseason#depstat#treat | 2 9.03 0.0001\r|\rDenominator | 1044\r--------------------------------------------------------\rInteraction contrast of season (winter versus summer) by depression status (mildly depressed versus nondepressed) by treatment\rThis significant test indicates that the two-way interaction formed by interacting depression status (nondepressed versus mildly depressed) by treatment differs by season (winter versus summer) Say that we wanted to take this test and focus on the contrast of happiness therapy versus traditional therapy (group 3 versus 2).1 This yields an interaction of season (winter versus summer) by depression status (nondepressed versus mildly depressed) by treatment (happiness therapy versus traditional therapy). *season(winter vs. summer) by depstat (nondepressed vs mildly depressed) by treat(HT vs. TT) contrast r3.season#r2.depstat#r3b2.treat,noeffects Contrasts of marginal linear predictions\rMargins: asbalanced\r--------------------------------------------------------\r| df F P\u003eF\r---------------------+----------------------------------\rseason#depstat#treat | 1 14.64 0.0001\r|\rDenominator | 1044\r--------------------------------------------------------\rInteraction contrast of season (winter versus summer) by depression status (mildly depressed versus nondepressed) by treatment (HT versus TT)\r","date":"2024-01-07","objectID":"/9.chapter9categorical-by-categorical-by-categorical-interactions/:3:1","tags":["Categorical","Interaction","stata"],"title":"Chapter9 ：Categorical by categorical by categorical interactions","uri":"/9.chapter9categorical-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.2 Simple interactions Let’s now explore a different way to dissect the three-way interaction through the use of simple interaction tests. contrast depstat#treat@season Contrasts of marginal linear predictions\rMargins: asbalanced\r--------------------------------------------------------\r| df F P\u003eF\r---------------------+----------------------------------\rdepstat#treat@season |\rWinter | 4 3.75 0.0049\rSpring | 4 2.29 0.0575\rSummer | 4 9.88 0.0000\rFall | 4 2.01 0.0904\rJoint | 16 4.48 0.0000\r|\rDenominator | 1044\r--------------------------------------------------------\rThe treat#depstat interaction is significant for winter (season 1) and summer (season 3). The treat#depstat interaction is not significant in the spring (season 2) or fall (season 4) Let’s perform this same contrast command, but apply the r2. contrast to depstat that compares those who are nondepressed versus mildly depressed. This yields four partial interactions of depression status (nondepressed versus mildly depressed) by treatment at each level of season. contrast r2.depstat#treat@season Contrasts of marginal linear predictions\rMargins: asbalanced\r-----------------------------------------------------------------\r| df F P\u003eF\r------------------------------+----------------------------------\rdepstat#treat@season |\r(Mild vs Non) (joint) Winter | 2 3.49 0.0309\r(Mild vs Non) (joint) Spring | 2 0.27 0.7631\r(Mild vs Non) (joint) Summer | 2 5.74 0.0033\r(Mild vs Non) (joint) Fall | 2 0.38 0.6851\rJoint | 8 2.47 0.0119\r|\rDenominator | 1044\r-----------------------------------------------------------------\rOptimism by treatment and season focusing on mildly depressed versus nondepressed\rLet’s also apply the r3b2. contrast to treat, comparing group 3 with group 2 (happiness therapy to traditional therapy). This yields an interaction contrast of depression status (mildly depressed versus nondepressed) by treatment (happiness therapy versus traditional therapy) performed at each of the four seasons. contrast r2.depstat#r3b2.treat@season,noeffects //r2 means non-dep vs mild-dep Contrasts of marginal linear predictions\rMargins: asbalanced\r--------------------------------------------------------------------\r| df F P\u003eF\r---------------------------------+----------------------------------\rdepstat#treat@season |\r(Mild vs Non) (HT vs TT) Winter | 1 5.13 0.0238\r(Mild vs Non) (HT vs TT) Spring | 1 0.30 0.5844\r(Mild vs Non) (HT vs TT) Summer | 1 9.90 0.0017\r(Mild vs Non) (HT vs TT) Fall | 1 0.60 0.4385\rJoint | 4 3.98 0.0033\r|\rDenominator | 1044\r--------------------------------------------------------------------\rSimple interaction contrast of depression status by treatment at each season\r","date":"2024-01-07","objectID":"/9.chapter9categorical-by-categorical-by-categorical-interactions/:3:2","tags":["Categorical","Interaction","stata"],"title":"Chapter9 ：Categorical by categorical by categorical interactions","uri":"/9.chapter9categorical-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.3 Simple effects and simple comparisons We might be interested in focusing on the simple effects of treatment across the levels of season and depression status. contrast treat@season#depstat We could further refine this test by focusing on the comparison of happiness therapy versus traditional therapy (group 3 versus 2) by applying the r3b2. contrast operator to treat, as shown below. contrast r3b2.treat@season#depstat,nowald pveffects this contrast is specified as r3b2.treat,which indicates to compare group 3 with group2. This could have also been specified as a custom contrast,{treat 0 -1 1} Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r--------------------------+---------------------------------------\rtreat@season#depstat |\r(HT vs TT) Winter#Non | 7.00 1.55 4.51 0.000\r(HT vs TT) Winter#Mild | 2.03 1.55 1.31 0.190\r(HT vs TT) Winter#Severe | -0.53 1.55 -0.34 0.731\r(HT vs TT) Spring#Non | 5.37 1.55 3.46 0.001\r(HT vs TT) Spring#Mild | 4.17 1.55 2.69 0.007\r(HT vs TT) Spring#Severe | -0.47 1.55 -0.30 0.764\r(HT vs TT) Summer#Non | 4.33 1.55 2.79 0.005\r(HT vs TT) Summer#Mild | 11.23 1.55 7.24 0.000\r(HT vs TT) Summer#Severe | -0.67 1.55 -0.43 0.667\r(HT vs TT) Fall#Non | 5.83 1.55 3.76 0.000\r(HT vs TT) Fall#Mild | 4.13 1.55 2.67 0.008\r(HT vs TT) Fall#Severe | 0.20 1.55 0.13 0.897\r------------------------------------------------------------------\rThis shows that for some combinations of season and depstat, the difference between happiness and traditional therapy is significant. For example, in season 1 (winter) and depression status 1 (nondepressed), the difference between happiness and traditional therapy is significant, with happiness therapy yielding optimism scores that are 7 points greater than traditional therapy ","date":"2024-01-07","objectID":"/9.chapter9categorical-by-categorical-by-categorical-interactions/:3:3","tags":["Categorical","Interaction","stata"],"title":"Chapter9 ：Categorical by categorical by categorical interactions","uri":"/9.chapter9categorical-by-categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter illustrates models that involve the interaction of two categorical variables. ","date":"2024-01-06","objectID":"/8.chapter8categorical-by-categorical-interactions/","tags":["Categorical","Interaction","stata"],"title":"Chapter8 ：Categorical by categorical interactions","uri":"/8.chapter8categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter illustrates models that involve the interaction of two categorical variables.The emphasis of this chapter is not only how to test for interactions between factor variables but also how to understand and dissect those interactions. This chapter focuses on three types of interactions: two by two interactions. two by three interactions. three by three interactions. ","date":"2024-01-06","objectID":"/8.chapter8categorical-by-categorical-interactions/:0:0","tags":["Categorical","Interaction","stata"],"title":"Chapter8 ：Categorical by categorical interactions","uri":"/8.chapter8categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 Two by two models: Example 1 In this first hypothetical study, she seeks to determine the effectiveness of happiness therapy by comparing the optimism of people who have completed happiness therapy treatment with the optimism of people in a control group who received no treatment. The researcher is interested in not only assessing the effectiveness of happiness therapy but also assessing whether its effectiveness depends on whether the person has been diagnosed as clinically depressed. This yields a two by two research design, crossing treatment group assignment (control group versus happiness therapy) with depression status (nondepressed versus depressed). The variable treat indicates the treatment assignment, coded: 1 = control group (Con) and 2 = happiness therapy (HT). The variable depstat reflects the person’s depression status at the beginning of the study and is coded: 1 = nondepressed and 2 = depressed. The variable opt is the optimism score at the end of the study. In this dataset, opt has a mean of 44.5, a minimum of 16, and a maximum of 80. Let’s now run an analysis that predicts opt based on treat, depstat, and the interaction of these two variables. This analysis uses the anova command (instead of the regress command) because the anova command directly shows the significance tests for each of the main effects as well as the interaction. use opt-2by2.dta anova opt depstat##treat Number of obs = 120 R-squared = 0.4889\rRoot MSE = 10.0136 Adj R-squared = 0.4757\rSource | Partial SS df MS F Prob\u003eF\r--------------+----------------------------------------------------\rModel | 11126 3 3708.6667 36.99 0.0000\r|\rdepstat | 7426.1333 1 7426.1333 74.06 0.0000\rtreat | 2803.3333 1 2803.3333 27.96 0.0000\rdepstat#treat | 896.53333 1 896.53333 8.94 0.0034\r|\rResidual | 11631.467 116 100.27126 --------------+----------------------------------------------------\rTotal | 22757.467 119 191.23922 Note! The anova and regress commands if you want use regress command in stead of the anova command to fit the previous model, there are two caveats: the regress command will require an extra step using the contrast command to test the overall interaction (for example, contrast depstat#treat). the tests of the main effects differ when using the regress command compared with the anova command. The significant interaction indicates that the effect of happiness therapy for those who are nondepressed is significantly different from the effect of happiness therapy for those who are depressed. margins treat#depstat,nopvalues marginsplot Adjusted predictions Number of obs = 120\rExpression: Linear prediction, predict()\r---------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r--------------+------------------------------------------------\rtreat#depstat |\rCon#Non | 44.87 1.83 41.25 48.49\rCon#Dep | 34.60 1.83 30.98 38.22\rHT#Non | 60.00 1.83 56.38 63.62\rHT#Dep | 38.80 1.83 35.18 42.42\r---------------------------------------------------------------\rGraph of means\r","date":"2024-01-06","objectID":"/8.chapter8categorical-by-categorical-interactions/:1:0","tags":["Categorical","Interaction","stata"],"title":"Chapter8 ：Categorical by categorical interactions","uri":"/8.chapter8categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.1 Simple effects The significant interaction indicates that the effect of happiness therapy is different for those who are depressed versus nondepressed. Each of these effects is called a simple effect, because they reflect the effect of one variable while holding another variable constant. We can estimate and test these simple effects using the contrast command, as shown below. Note the use of the @ symbol. This requests the simple effect of treat at each level of depstat. contrast treat@depstat contrast treat@depstat,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r------------------+---------------------------------------\rtreat@depstat |\r(HT vs base) Non | 15.13 2.59 5.85 0.000\r(HT vs base) Dep | 4.20 2.59 1.62 0.107\r----------------------------------------------------------\rBy adding the nowald and pveffects options, the contrast command displays a table with the estimate of the simple effect, the standard error, and a significance test of the simple effect. This shows the effect of treatment among those who are nondepressed equals 15.1, and this effect is significant ( $t = 5.85,p=0.000$ ). Among those who are depressed, the treatment effect is 4.2, and this difference is not significant ( $t = 1.62,p=0.107$ ). Note! Contrast options Combining the nowald and pveffects options provides a concise output that includes an estimate of the size of the contrast and a test of its significance. ","date":"2024-01-06","objectID":"/8.chapter8categorical-by-categorical-interactions/:1:1","tags":["Categorical","Interaction","stata"],"title":"Chapter8 ：Categorical by categorical interactions","uri":"/8.chapter8categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.2 Estimating the size of the interaction As we saw in the analysis of the simple effects, the simple effect of treatment is 4.2 for those who are depressed and is 15.1 for those who are nondepressed. Taking the difference in these simple effects ($4.2 - 15.1$) gives us an estimate of the size of the interaction, which is $-10.9$. This is the same value that we obtain if we estimate the interaction using the contrast command below. Contrasts of marginal linear predictions\rMargins: asbalanced\r--------------------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r----------------------------+---------------------------------------\rtreat#depstat |\r(HT vs base) (Dep vs base) | -10.93 3.66 -2.99 0.003\r--------------------------------------------------------------------\rthe $p$-value for this test matches the $p$-value of the treat#depstat interaction from the original anova command. ","date":"2024-01-06","objectID":"/8.chapter8categorical-by-categorical-interactions/:1:2","tags":["Categorical","Interaction","stata"],"title":"Chapter8 ：Categorical by categorical interactions","uri":"/8.chapter8categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.3 More about interaction let’s further explore what we mean by an interaction by considering the hypothetical pattern of results shown in below. Two by two with no interaction\rThis is an example pattern in which there is no interaction between treatment and depression status. One way we can see the absence of an interaction is by seeing that the line for D1 is parallel to the line for D2. ","date":"2024-01-06","objectID":"/8.chapter8categorical-by-categorical-interactions/:1:3","tags":["Categorical","Interaction","stata"],"title":"Chapter8 ：Categorical by categorical interactions","uri":"/8.chapter8categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2 Two by three models his section considers models where one of the categorical variables has two levels and the other categorical variable has three levels. ","date":"2024-01-06","objectID":"/8.chapter8categorical-by-categorical-interactions/:2:0","tags":["Categorical","Interaction","stata"],"title":"Chapter8 ：Categorical by categorical interactions","uri":"/8.chapter8categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.1 Example 2 Referring to the example from the previous section, depression status had two levels, nondepressed and depressed. Suppose that we instead use three categories for depression status: nondepressed, mildly depressed, and severely depressed. Let’s now perform an analysis predicting optimism from treatment group, depression status, and the interaction of these two variables. use opt-2by3-ex1.dta anova opt depstat##treat Number of obs = 180 R-squared = 0.5402\rRoot MSE = 10.0154 Adj R-squared = 0.5270\rSource | Partial SS df MS F Prob\u003eF\r--------------+----------------------------------------------------\rModel | 20505.828 5 4101.1656 40.89 0.0000\r|\rdepstat | 15432.844 2 7716.4222 76.93 0.0000\rtreat | 3183.6056 1 3183.6056 31.74 0.0000\rdepstat#treat | 1889.3778 2 944.68889 9.42 0.0001\r|\rResidual | 17453.567 174 100.30785 --------------+----------------------------------------------------\rTotal | 37959.394 179 212.06366 As expected, the depstat#treat interaction is significant. We can compute the mean optimism as a function of depression status and treatment group by using the margins command below. Had there been additional predictors in the model, the margins command would have produced adjusted means, adjusting for the other predictors in the model. margins treat#depstat,nopvalues marginsplot Adjusted predictions Number of obs = 180\rExpression: Linear prediction, predict()\r---------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r--------------+------------------------------------------------\rtreat#depstat |\rCon#Non | 44.23 1.83 40.62 47.84\rCon#Mild | 39.63 1.83 36.02 43.24\rCon#Sev | 29.80 1.83 26.19 33.41\rHT#Non | 59.60 1.83 55.99 63.21\rHT#Mild | 49.73 1.83 46.12 53.34\rHT#Sev | 29.57 1.83 25.96 33.18\r---------------------------------------------------------------\rGraph of means\r2.1.1 Simple effects We can ask whether the effect of happiness therapy is significant at each level of depression status. contrast treat@depstat,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r-----------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r-------------------+---------------------------------------\rtreat@depstat |\r(HT vs base) Non | 15.37 2.59 5.94 0.000\r(HT vs base) Mild | 10.10 2.59 3.91 0.000\r(HT vs base) Sev | -0.23 2.59 -0.09 0.928\r-----------------------------------------------------------\rThis test of simple effects tells us that happiness therapy is significantly better than the control group for those who are nondepressed and for those who are mildly depressed. For those who are severely depressed, happiness therapy is not significantly different from being in the control group 2.1.2 Partial interactions Another way to dissect a two by three interaction is through the use of partial interactions. In this example, a partial interaction is constructed by applying a contrast operator to depstat and interacting that with treat. For example, applying the a.contrast operator to depstat yields two contrasts: group 1 versus 2 (Non vs Mild depression); and group 2 versus 3 (Mild vs Sev depression). Interacting a.depstat with treat forms two partial interactions. contrast a.depstat#treat Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------\r| df F P\u003eF\r-----------------------+----------------------------------\rdepstat#treat |\r(Non vs Mild) (joint) | 1 2.07 0.1516\r(Mild vs Sev) (joint) | 1 7.98 0.0053\rJoint | 2 9.42 0.0001\r|\rDenominator | 174\r----------------------------------------------------------\rThe first partial interaction is not significant ( $F = 2.07 , p = 0.1516$ ).The effect of happiness therapy (compared with the control group) is not significantly different for those who are nondepressed versus mildly depressed. The second partial interaction is significant ( $F = 7.98 , p = 0.0053$ ). The effect of happiness therapy (compared with the con","date":"2024-01-06","objectID":"/8.chapter8categorical-by-categorical-interactions/:2:1","tags":["Categorical","Interaction","stata"],"title":"Chapter8 ：Categorical by categorical interactions","uri":"/8.chapter8categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3 Three by three models: Example 4 Let’s now consider an example that illustrates a three by three design. there are three levels of treatment (control group, traditional therapy, and happiness therapy), and three depression groups (nondepressed, mildly depressed, and severely depressed) use opt-3by3.dta.dta anova opt depstat##treat Number of obs = 270 R-squared = 0.4898\rRoot MSE = 10.023 Adj R-squared = 0.4742\rSource | Partial SS df MS F Prob\u003eF\r--------------+----------------------------------------------------\rModel | 25175.519 8 3146.9398 31.32 0.0000\r|\rdepstat | 17664.096 2 8832.0481 87.92 0.0000\rtreat | 5250.363 2 2625.1815 26.13 0.0000\rdepstat#treat | 2261.0593 4 565.26481 5.63 0.0002\r|\rResidual | 26220.367 261 100.46117 --------------+----------------------------------------------------\rTotal | 51395.885 269 191.06277 margins treat#depstat,nopvalues marginsplot Adjusted predictions Number of obs = 270\rExpression: Linear prediction, predict()\r---------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r--------------+------------------------------------------------\rtreat#depstat |\rCon#Non | 44.20 1.83 40.60 47.80\rCon#Mild | 39.70 1.83 36.10 43.30\rCon#Sev | 29.90 1.83 26.30 33.50\rTT#Non | 54.53 1.83 50.93 58.14\rTT#Mild | 49.53 1.83 45.93 53.14\rTT#Sev | 39.80 1.83 36.20 43.40\rHT#Non | 59.33 1.83 55.73 62.94\rHT#Mild | 49.87 1.83 46.26 53.47\rHT#Sev | 30.10 1.83 26.50 33.70\r---------------------------------------------------------------\rMean optimism by treatment group and depression status\rWe can statistically dissect this interaction in four different ways: using simple effects , simple contrasts , partial interactions , or interaction contrasts . Each of these techniques is illustrated below ","date":"2024-01-06","objectID":"/8.chapter8categorical-by-categorical-interactions/:3:0","tags":["Categorical","Interaction","stata"],"title":"Chapter8 ：Categorical by categorical interactions","uri":"/8.chapter8categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.1 Simple effects One way to dissect the interaction is by looking at the effect of treatment at each level of depression status. contrast treat@depstat Contrasts of marginal linear predictions\rMargins: asbalanced\r-------------------------------------------------\r| df F P\u003eF\r--------------+----------------------------------\rtreat@depstat |\rNon | 2 17.86 0.0000\rMild | 2 9.96 0.0001\rSev | 2 9.56 0.0001\rJoint | 6 12.46 0.0000\r|\rDenominator | 261\r-------------------------------------------------\rThese results show that the effect of treat is significant at each level of depstat. ","date":"2024-01-06","objectID":"/8.chapter8categorical-by-categorical-interactions/:3:1","tags":["Categorical","Interaction","stata"],"title":"Chapter8 ：Categorical by categorical interactions","uri":"/8.chapter8categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.2 Simple contrast contrast r.treat@1.depstat,nowald pveffects This yields a comparison of each treatment group with the reference group (that is, group 1, the control group) at each level of depression status. Contrasts of marginal linear predictions\rMargins: asbalanced\r---------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r-----------------+---------------------------------------\rtreat@depstat |\r(TT vs Con) Non | 10.33 2.59 3.99 0.000\r(HT vs Con) Non | 15.13 2.59 5.85 0.000\r---------------------------------------------------------\rLet’s now perform these simple contrasts for those who are mildly depressed. contrast r.treat@2.depstat,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r------------------+---------------------------------------\rtreat@depstat |\r(TT vs Con) Mild | 9.83 2.59 3.80 0.000\r(HT vs Con) Mild | 10.17 2.59 3.93 0.000\r----------------------------------------------------------\rFinally, let’s perform these simple contrasts for those who are severely depressed. contrast r.treat@3.depstat,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r---------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r-----------------+---------------------------------------\rtreat@depstat |\r(TT vs Con) Sev | 9.90 2.59 3.83 0.000\r(HT vs Con) Sev | 0.20 2.59 0.08 0.938\r---------------------------------------------------------\rIf you prefer, you can obtain all six of these simple contrasts at once using the contrast command below contrast r.treat@depstat,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r------------------+---------------------------------------\rtreat@depstat |\r(TT vs Con) Non | 10.33 2.59 3.99 0.000\r(TT vs Con) Mild | 9.83 2.59 3.80 0.000\r(TT vs Con) Sev | 9.90 2.59 3.83 0.000\r(HT vs Con) Non | 15.13 2.59 5.85 0.000\r(HT vs Con) Mild | 10.17 2.59 3.93 0.000\r(HT vs Con) Sev | 0.20 2.59 0.08 0.938\r----------------------------------------------------------\r","date":"2024-01-06","objectID":"/8.chapter8categorical-by-categorical-interactions/:3:2","tags":["Categorical","Interaction","stata"],"title":"Chapter8 ：Categorical by categorical interactions","uri":"/8.chapter8categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.3 Partial interaction I used the means from the margins command to create a visual depiction of these two partial interactions, shown in figure below. The left panel of figure illustrates the comparison of treatment group 2 versus 1 (traditional therapy versus control) interacted with depression status. The right panel illustrates the comparison of treatment group 3 versus 1 (happiness therapy versus control) interacted with depression status Partial interactions\rcontrast r.treat#depstat Contrasts of marginal linear predictions\rMargins: asbalanced\r--------------------------------------------------------\r| df F P\u003eF\r---------------------+----------------------------------\rtreat#depstat |\r(TT vs Con) (joint) | 2 0.01 0.9891\r(HT vs Con) (joint) | 2 8.64 0.0002\rJoint | 4 5.63 0.0002\r|\rDenominator | 261\r--------------------------------------------------------\rThe first partial interaction is not significant.The difference in optimism between traditional therapy and the control group does not differ among the levels of depression status. This result represent the left panel:The effect of traditional therapy (versus the control group) is similar for all three lines (representing the three levels of depression). The first partial interaction is not significant.The difference in optimism between happiness therapy and the control group depends on the level of depression. This result represent the right panel:the effect of happiness therapy (compared with the control group) may be similar for those who are nondepressed and mildly depressed but different for those who are severely depressed. ","date":"2024-01-06","objectID":"/8.chapter8categorical-by-categorical-interactions/:3:3","tags":["Categorical","Interaction","stata"],"title":"Chapter8 ：Categorical by categorical interactions","uri":"/8.chapter8categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.4 Interaction contrasts Suppose we applied the r. contrast operator to treatment group and the a. contrast operator to depression status. This would create contrasts of each treatment group against the control group (that is, group 2 versus 1 and group 3 versus 1) interacted with contrasts of adjacent levels of depression groups (that is, group 1 versus 2 and group 2 versus 3). contrast a.depstat#r.treat,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r-------------------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r---------------------------+---------------------------------------\rdepstat#treat |\r(Non vs Mild) (TT vs Con) | 0.50 3.66 0.14 0.891\r(Non vs Mild) (HT vs Con) | 4.97 3.66 1.36 0.176\r(Mild vs Sev) (TT vs Con) | -0.07 3.66 -0.02 0.985\r(Mild vs Sev) (HT vs Con) | 9.97 3.66 2.72 0.007\r-------------------------------------------------------------------\rInteraction contrasts\rLet’s begin by interpreting the fourth interaction contrast, the only one that was significant. The significance of this interaction contrast indicates that the effect of happiness therapy (compared with the control group) is different for those who are mildly depressed compared with those who are severely depressed. ","date":"2024-01-06","objectID":"/8.chapter8categorical-by-categorical-interactions/:3:4","tags":["Categorical","Interaction","stata"],"title":"Chapter8 ：Categorical by categorical interactions","uri":"/8.chapter8categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4 Unbalanced designs Even in the context of a randomized experiment, it is unusual to have the same number of observations in each cell. This section presents an example of an unbalanced design. This will allow us to consider two different strategies that can be used for estimating adjusted means, the asobserved strategy and the as-balanced strategy. As we will see, the asbalanced option can be used with the margins command to estimate margins as though the design were balanced, even if the actual design is not balanced. use gss_ivrm.dta tab married cograd if !missing(happy7),row +----------------+\r| Key |\r|----------------|\r| frequency |\r| row percentage |\r+----------------+\rmarital: |\rmarried=1, |\runmarried= | College graduate\r0 | (1=yes, 0=no)\r(recoded) | Not CO Gr CO Grad | Total\r-----------+----------------------+----------\rUnmarried | 454 147 | 601 | 75.54 24.46 | 100.00 -----------+----------------------+----------\rMarried | 403 153 | 556 | 72.48 27.52 | 100.00 -----------+----------------------+----------\rTotal | 857 300 | 1,157 | 74.07 25.93 | 100.00 Before performing the analysis, let’s compute the mean of happy7 by married and cograd using the tabulate command below tab married cograd,sum(happy7) marital: |\rmarried=1, |\runmarried= | College graduate\r0 | (1=yes, 0=no)\r(recoded) | Not CO Gr CO Grad | Total\r-----------+----------------------+----------\rUnmarried | 5.3039648 5.5170068 | 5.3560732\r| 1.0590229 .99556373 | 1.0470596\r| 454 147 | 601\r-----------+----------------------+----------\rMarried | 5.7121588 5.6862745 | 5.705036\r| .8986037 .79032462 | .86953016\r| 403 153 | 556\r-----------+----------------------+----------\rTotal | 5.495916 5.6033333 | 5.5237684\r| 1.0071217 .89926887 | .9810472\r| 857 300 | 1157\rLet’s now perform an analysis that predicts happy7 from married, cograd, and the interaction of these two variables. anova happy7 married##cograd Number of obs = 1,157 R-squared = 0.0362\rRoot MSE = .964375 Adj R-squared = 0.0337\rSource | Partial SS df MS F Prob\u003eF\r---------------+----------------------------------------------------\rModel | 40.284425 3 13.428142 14.44 0.0000\r|\rmarried | 18.502336 1 18.502336 19.89 0.0000\rcograd | 1.94355 1 1.94355 2.09 0.1486\rmarried#cograd | 3.1674385 1 3.1674385 3.41 0.0652\r|\rResidual | 1072.3119 1,153 .93001903 ---------------+----------------------------------------------------\rTotal | 1112.5964 1,156 .96245361 We can see that the married#cograd interaction is not significant ($p = 0.0652$). Let’s assume that we want to retain this interaction. (We may want to retain it based on theoretical considerations or because its -value is close to 0.05.) Let’s now turn our attention to married, which is significant ($p\u003c0.001$). To understand this significant result, let’s use the margins command to compute the adjusted means by the levels of married. margins married,nopvalues Predictive margins Number of obs = 1,157\rExpression: Linear prediction, predict()\r--------------------------------------------------------------\r| Delta-method\r| Margin std. err. [95% conf. interval]\r-------------+------------------------------------------------\rmarried |\rUnmarried | 5.36 0.04 5.28 5.44\rMarried | 5.71 0.04 5.63 5.79\r--------------------------------------------------------------\rWe can think of this adjusted mean as being computed by taking each cell mean of happy7 among those who are not married and weighing it by the corresponding proportion of those are college graduates or noncollege graduates, as illustrated below. $ 454 num ÷ 601 = 0.74$ $ 147 num ÷ 601 = 0.26$ $ 5.3039648 * 0.7407 + 5.5170068 * 0.2593 = 5.3592066 $ We can likewise compute the adjusted mean for those who are married using the same strategy, as shown below. $ 5.7121588 * 0.7407 + 5.6862745 * 0.2593 = 5.05447$ The key point is that the adjusted means are computed by creating a weighted average of cell means that is weighted by the observed proportions of observations in the data (in this case, the observed proportions of cograd)","date":"2024-01-06","objectID":"/8.chapter8categorical-by-categorical-interactions/:4:0","tags":["Categorical","Interaction","stata"],"title":"Chapter8 ：Categorical by categorical interactions","uri":"/8.chapter8categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"5 Main effects with interactions: anova versus regress This section considers the meaning of main effects in the presence of an interaction when using the regress command compared with the anova command. In the presence of interactions, this can lead to conflicting estimates of so-called main effects for the regress command versus the anova command. Let’s use the dataset for this example and show the mean optimism by treatment group and depression status. use opt-2by2.dta anova opt treat##depstat Number of obs = 120 R-squared = 0.4889\rRoot MSE = 10.0136 Adj R-squared = 0.4757\rSource | Partial SS df MS F Prob\u003eF\r--------------+----------------------------------------------------\rModel | 11126 3 3708.6667 36.99 0.0000\r|\rtreat | 2803.3333 1 2803.3333 27.96 0.0000\rdepstat | 7426.1333 1 7426.1333 74.06 0.0000\rtreat#depstat | 896.53333 1 896.53333 8.94 0.0034\r|\rResidual | 11631.467 116 100.27126 --------------+----------------------------------------------------\rTotal | 22757.467 119 191.23922 Let’s now perform this analysis but instead use the regress command. reg opt treat##depstat,vsquish Source | SS df MS Number of obs = 120\r-------------+---------------------------------- F(3, 116) = 36.99\rModel | 11126 3 3708.66667 Prob \u003e F = 0.0000\rResidual | 11631.4667 116 100.271264 R-squared = 0.4889\r-------------+---------------------------------- Adj R-squared = 0.4757\rTotal | 22757.4667 119 191.239216 Root MSE = 10.014\r-------------------------------------------------------------------------------\ropt | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r--------------+----------------------------------------------------------------\rtreat |\rHT | 15.13 2.59 5.85 0.000 10.01 20.25\rdepstat |\rDep | -10.27 2.59 -3.97 0.000 -15.39 -5.15\rtreat#depstat |\rHT#Dep | -10.93 3.66 -2.99 0.003 -18.18 -3.69\r_cons | 44.87 1.83 24.54 0.000 41.25 48.49\r-------------------------------------------------------------------------------\rLet’s now compare the results of the anova command with the results of the regress command, focusing on the significance tests. These comparisons are a bit tricky, because the anova command reports $F$ statistics, whereas the regress command reports $t$ statistics. But we can square the value from the regress command to convert it into an equivalent of an statistic. If we square $t$ the value of $-2.99$ from the regress command, we obtain the 8.94, the same value as $F$ the statistic from the anova command. Using the contrast treat#depstat command following the regress command also yields the same results as the anova command. The value from the contrast command is the same as the $F$ value from the anova command, 8.94. contrast treat#depstat Contrasts of marginal linear predictions\rMargins: asbalanced\r-------------------------------------------------\r| df F P\u003eF\r--------------+----------------------------------\rtreat#depstat | 1 8.94 0.0034\r|\rDenominator | 116\r-------------------------------------------------\rLet’s now compare the test of treat from the anova command with the regress command. We square of the $t$ value for the treat effect from the regress command(5.85) and obtain 34.22. This is different from the $F$ value for the treat effect from the anova command, 27.96. contrast treat Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------\r| df F P\u003eF\r-------------+----------------------------------\rtreat | 1 27.96 0.0000\r|\rDenominator | 116\r------------------------------------------------\rThis might seem perplexing, but there is a perfectly logical explanation for this. The reason for these discrepancies is because of differences in the coding used by the anova and regress commands. The regress command uses dummy (0/1) coding, whereas the anova command and the contrast command use effect ($-1/1$) coding.The interpretation of the interactions is the same whether you use effect coding or dummy coding, but the meaning of the main effects differ. The interpretation of the inte","date":"2024-01-06","objectID":"/8.chapter8categorical-by-categorical-interactions/:5:0","tags":["Categorical","Interaction","stata"],"title":"Chapter8 ：Categorical by categorical interactions","uri":"/8.chapter8categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"6 Interpreting confidence intervals use gss_ivrm.dta anova happy7 i.marital3##i.gender c.health Number of obs = 783 R-squared = 0.1246\rRoot MSE = .941504 Adj R-squared = 0.1178\rSource | Partial SS df MS F Prob\u003eF\r----------------+----------------------------------------------------\rModel | 97.864391 6 16.310732 18.40 0.0000\r|\rmarital3 | 26.17214 2 13.08607 14.76 0.0000\rgender | 3.8126465 1 3.8126465 4.30 0.0384\rmarital3#gender | 2.4708203 2 1.2354101 1.39 0.2488\rhealth | 51.985684 1 51.985684 58.65 0.0000\r|\rResidual | 687.86996 776 .88643037 ----------------+----------------------------------------------------\rTotal | 785.73436 782 1.0047754 Let’s compute the adjusted means of happiness by marital3 by gender using the margins command and graph them using the marginsplot command margins marital3#gender marginsplot Predictive margins Number of obs = 783\rExpression: Linear prediction, predict()\r---------------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r----------------------+----------------------------------------------------------------\rmarital3#gender |\rMarried#Male | 5.68 0.09 59.93 0.000 5.50 5.87\rMarried#Female | 5.69 0.06 98.87 0.000 5.57 5.80\rPrevmarried#Male | 5.13 0.11 44.95 0.000 4.90 5.35\rPrevmarried#Female | 5.29 0.08 64.16 0.000 5.13 5.46\rNever married#Male | 5.27 0.09 57.86 0.000 5.09 5.45\rNever married#Female | 5.55 0.09 60.82 0.000 5.37 5.73\r---------------------------------------------------------------------------------------\rAdjusted means of happiness by marital status and gender\rwe would need to test the effect of gender at each level of marital status using the margins command, as shown below. margins gender@marital3,contrast(nowald pveffects) Contrasts of predictive margins Number of obs = 783\rExpression: Linear prediction, predict()\r------------------------------------------------------------------------\r| Delta-method\r| Contrast std. err. t P\u003e|t|\r--------------------------------+---------------------------------------\rgender@marital3 |\r(Female vs base) Married | 0.01 0.11 0.05 0.961\r(Female vs base) Prevmarried | 0.17 0.14 1.20 0.231\r(Female vs base) Never married | 0.29 0.13 2.21 0.027\r------------------------------------------------------------------------\rIn summary, the marginsplot command provides a graphical display of the results calculated by the margins command. Sometimes, the appearance of the confidence intervals of individual groups might tempt you to inappropriately make statistical inferences about the comparisons between the groups. To avoid this trap, you can directly form comparisons among the groups of interest to ascertain the significance of the group differences. ","date":"2024-01-06","objectID":"/8.chapter8categorical-by-categorical-interactions/:6:0","tags":["Categorical","Interaction","stata"],"title":"Chapter8 ：Categorical by categorical interactions","uri":"/8.chapter8categorical-by-categorical-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter covers models that involve categorical predictors.","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter covers models that involve categorical predictors. The emphasis is on how to make contrasts among levels of the categorical predictor to answer interesting questions regarding the differences among the categories. ","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:0:0","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 Comparing two groups using a t test The simplest kind of categorical predictor has two levels. Examples of such twolevel predictors include gender (male versus female), treatment assignment (treatment group versus control group), or whether one is married (married versus not married). The variable happy7 indicates the happiness rating of the respondent on a 1 to 7 scale, where 7 is completely happy and 1 is completely unhappy. To compare the average happiness between those who are married and unmarried, we can perform an independent groups $t$test, as shown below. use gss_ivrm.dta ttest happy7, by(married) The variable happy7 indicates the happiness rating of the respondent on a 1 to 7 scale, where 7 is completely happy and 1 is completely unhappy. To compare the average happiness between those who are married and unmarried, we can perform an independent groups $t$ test, Two-sample t test with equal variances\r------------------------------------------------------------------------------\rGroup | Obs Mean Std. err. Std. dev. [95% conf. interval]\r---------+--------------------------------------------------------------------\rUnmarrie | 604 5.35596 .0425197 1.044982 5.272456 5.439465\rMarried | 556 5.705036 .0368763 .8695302 5.632602 5.77747\r---------+--------------------------------------------------------------------\rCombined | 1,160 5.523276 .0287773 .9801179 5.466815 5.579737\r---------+--------------------------------------------------------------------\rdiff | -.3490757 .0567084 -.4603384 -.237813\r------------------------------------------------------------------------------\rdiff = mean(Unmarrie) - mean(Married) t = -6.1556\rH0: diff = 0 Degrees of freedom = 1158\rHa: diff \u003c 0 Ha: diff != 0 Ha: diff \u003e 0\rPr(T \u003c t) = 0.0000 Pr(|T| \u003e |t|) = 0.0000 Pr(T \u003e t) = 1.0000\rThe t-test command shows that the average happiness is 5.356 for those who are unmarried and 5.705 for those who are married. The difference between these means is $-0.349$, and that difference is significantly different from 0 (with a two-tailed value of 0.0000). The difference between these means is negative. We can interpret this result to say that those who are unmarried are significantly less happy than those who are married. We could also say that those who are married are significantly happier than those who are unmarried ","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:1:0","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2 More groups and more predictors We are seldom interested in simply comparing two groups in the absence of any additional predictors (covariates). Let’s extend the previous example in two ways. First, let’s use a five-level measure of marital status, which is coded: 1 = married, 2 = widowed, 3 = divorced, 4 = separated, and 5 = never married. Second, let’s include additional predictors (covariates): gender,2 race, and age. We begin by testing the overall null hypothesis that the average happiness is equal among the five marital status groups: $$ H_{0} = \\mu_{2} = \\mu_{3} = \\mu_{4} = \\mu_{5}$$ I included the i. prefix before each of the categorical variables and the c. prefix in front of age to specify that it is a continuous variable. use gss_ivrm.dta anova happy7 i.marital i.gender i.race c.age Number of obs = 1,156 R-squared = 0.0488\rRoot MSE = .957826 Adj R-squared = 0.0422\rSource | Partial SS df MS F Prob\u003eF\r-----------+----------------------------------------------------\rModel | 53.979104 8 6.747388 7.35 0.0000\r|\rmarital | 47.156057 4 11.789014 12.85 0.0000\rgender | 3.8954776 1 3.8954776 4.25 0.0396\rrace | .92449374 2 .46224687 0.50 0.6043\rage | 5.3200497 1 5.3200497 5.80 0.0162\r|\rResidual | 1052.2934 1,147 .91743103 -----------+----------------------------------------------------\rTotal | 1106.2725 1,155 .95781168 Note! The anova and regress commands In this chapter (as well as chapters 8 and 9), my focus will be on starting the analysis with an assessment of the main effects (and interactions, if any) using the omnibus $F$-tests provided by the anova command. I would emphasize, however, that these same results could be obtained via the regress command. Note! Omnibus $F$-tests Omnibus F-tests are statistical tests used to assess the overall significance or goodness-of-fit of a regression model. These tests evaluate the joint significance of multiple coefficients in the model or the overall explanatory power of the regression equation. In regression analysis, the Omnibus F-test is typically used to determine whether there is a significant relationship between the predictor variables and the dependent variable. It examines the overall fit of a model by comparing the variance explained by the model to the unexplained variance (residuals) around the regression line. The steps involved in performing an Omnibus F-test include: Fit the regression model using the given predictor variables. Calculate the overall F-statistic using the explained and unexplained variance from the model. Obtain the degrees of freedom for the F-distribution based on the number of predictors and the sample size. Compare the calculated F-statistic with the critical value from the F-distribution at a specified significance level (commonly 0.05 or 0.01). If the calculated F-statistic exceeds the critical value, the null hypothesis is rejected, indicating that the overall model is statistically significant. The overall test of marital is significant ( , ). After adjusting for gender, race, and age, we can reject the null hypothesis that the average happiness is equal among the five marital status groups. Let’s probe this finding in more detail. Suppose that our research hypothesis (prior to even seeing the data) was that those who are married will be happier than each of the four other marital status groups. We can frame this as four separate null hypotheses, shown below $$H_{0}: \\mu_{2} = \\mu_{1} $$ $$H_{0}: \\mu_{3} = \\mu_{1} $$ $$H_{0}: \\mu_{4} = \\mu_{1} $$ $$H_{0}: \\mu_{5} = \\mu_{1} $$ Let’s begin the exploration of these tests by using the margins command to compute the adjusted mean of happiness by marital status. margins marital marginsplot Predictive margins Number of obs = 1,156\rExpression: Linear prediction, predict()\r--------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r---------------+----------------------------------------------------------------\rmarital |","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:2:0","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3 Overview of contrast operators the contrast operators table provides a brief description of each contrast operator and shows the section of this chapter in which each contrast operator is covered. Summary of contrast operators\r","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:3:0","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4 Compare each group against a reference group This section provides further examples illustrating the r. contrast operator for making reference group contrasts. Let’s continue with the example that predicting happiness from marital status, adjusting for gender, race, and age. use gss_ivrm.dta anova happy7 i.marital i.gender i.race c.age Number of obs = 1,156 R-squared = 0.0488\rRoot MSE = .957826 Adj R-squared = 0.0422\rSource | Partial SS df MS F Prob\u003eF\r-----------+----------------------------------------------------\rModel | 53.979104 8 6.747388 7.35 0.0000\r|\rmarital | 47.156057 4 11.789014 12.85 0.0000\rgender | 3.8954776 1 3.8954776 4.25 0.0396\rrace | .92449374 2 .46224687 0.50 0.6043\rage | 5.3200497 1 5.3200497 5.80 0.0162\r|\rResidual | 1052.2934 1,147 .91743103 -----------+----------------------------------------------------\rTotal | 1106.2725 1,155 .95781168 ","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:4:0","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4.1 Selecting a specific contrast Suppose you wanted to focus on the contrast of group 3 to group 1 (divorced versus married) and group 5 to group 1 (never married versus married). You can perform those two contrasts by specifying the r(3 5). contrast operator. This compares each of the groups within the parentheses with the reference group (group 1). contrast r(3 5).marital,pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r---------------------------------------------------------------\r| df F P\u003eF\r----------------------------+----------------------------------\rmarital |\r(divorced vs married) | 1 39.38 0.0000\r(never married vs married) | 1 4.18 0.0411\rJoint | 2 19.78 0.0000\r|\rDenominator | 1147\r---------------------------------------------------------------\r--------------------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r----------------------------+---------------------------------------\rmarital |\r(divorced vs married) | -0.50 0.08 -6.28 0.000\r(never married vs married) | -0.16 0.08 -2.04 0.041\r--------------------------------------------------------------------\rNote! Options on the contrast command The previous contrast command included two options: nowald and pveffects. This yields concise output that fits well on the pages of this book. When you run such analyses yourself, I recommend specifying the options nowald and effects, which will display both significance tests and confidence intervals associated with each comparison. ","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:4:1","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4.2 Selecting a different reference group Suppose that instead we wanted to compare each group with a different reference group. We can specify the rb5. contrast operator, which requests reference group contrasts using group 5 (never married) as the baseline (reference) group. contrast rb5.marital,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r------------------------------+---------------------------------------\rmarital |\r(married vs never married) | 0.16 0.08 2.04 0.041\r(widowed vs never married) | -0.30 0.14 -2.06 0.040\r(divorced vs never married) | -0.34 0.09 -3.62 0.000\r(separated vs never married) | -0.39 0.18 -2.24 0.026\r----------------------------------------------------------------------\r","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:4:2","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4.3 Selecting a contrast and reference group You can both specify the reference group and specify the contrasts to be made at one time. contrast r(1 3)b5.marital,nowald pveffects //(1 versus 5)(3 versus 5) Contrasts of marginal linear predictions\rMargins: asbalanced\r---------------------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r-----------------------------+---------------------------------------\rmarital |\r(married vs never married) | 0.16 0.08 2.04 0.041\r(divorced vs never married) | -0.34 0.09 -3.62 0.000\r---------------------------------------------------------------------\r","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:4:3","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"5 Compare each group against the grand mean This section illustrates the g. contrast operator that compares each group with the grand mean of all groups. a researcher might be interested in comparing the mean happiness of each marital status group versus the grand mean of all groups. use gss_ivrm.dta anova happy7 i.marital i.gender i.race c.age margins g.marital,contrast(nowald pveffects) the prefix “g” means the mean happiness of each marital status group with the grand mean Contrasts of predictive margins Number of obs = 1,156\rExpression: Linear prediction, predict()\r-----------------------------------------------------------------\r| Delta-method\r| Contrast std. err. t P\u003e|t|\r-------------------------+---------------------------------------\rmarital |\r(married vs mean) | 0.33 0.05 6.12 0.000\r(widowed vs mean) | -0.12 0.10 -1.18 0.240\r(divorced vs mean) | -0.17 0.07 -2.44 0.015\r(separated vs mean) | -0.22 0.14 -1.61 0.107\r(never married vs mean) | 0.18 0.07 2.48 0.013\r-----------------------------------------------------------------\rmarginsplot,yline(0) When the confidence interval for a contrast excludes zero, the difference is significant at the 5% level. Contrasts comparing each group with the grand mean\r","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:5:0","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"6 Compare adjacent means This section illustrates contrasts that compare the means of adjacent groups, for example, group 1 versus 2, group 2 versus 3, group 3 versus 4. These kinds of contrasts are especially useful for studies where you expect a nonlinear relationship between an ordinal or interval predictor and outcome. For example, consider a hypothetical study about the dosage of a new pain medication where the researchers expect that at a certain dosage level the effects of the medication will kick in and lead to a statistically significant reduction in pain. The medication dosages range from 0 mg to 250 mg incrementing by 50 mg, yielding six dosage groups. use pain codebook dosegrp Type: Numeric (float)\rLabel: dosegrp\rRange: [1,6] Units: 1\rUnique values: 6 Missing .: 0/180\rTabulation: Freq. Numeric Label\r30 1 0mg\r30 2 50mg\r30 3 100mg\r30 4 150mg\r30 5 200mg\r30 6 250mg\rLet’s begin the analysis relating pain to medication dosage by testing the most general null hypothesis that could be tested—that the average pain is equal across all six dosage groups: $$ H_{0} = \\mu_{2} = \\mu_{3} = \\mu_{4} = \\mu_{5}$$ This null hypothesis is tested using the anova command shown below. anova pain i.dosegrp Number of obs = 180 R-squared = 0.4602\rRoot MSE = 10.4724 Adj R-squared = 0.4447\rSource | Partial SS df MS F Prob\u003eF\r-----------+----------------------------------------------------\rModel | 16271.694 5 3254.3389 29.67 0.0000\r|\rdosegrp | 16271.694 5 3254.3389 29.67 0.0000\r|\rResidual | 19082.633 174 109.67031 -----------+----------------------------------------------------\rTotal | 35354.328 179 197.51021 he $F$ value is 29.67 and is significant. We can reject the overall null hypothesis. Let’s use the margins command and the marginsplot command to display and graph the predicted mean of pain by dosegrp. margins dosegrp //the predicted mean of pain by dosegrp marginsplot Adjusted predictions Number of obs = 180\rExpression: Linear prediction, predict()\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\rdosegrp |\r0mg | 71.83 1.91 37.57 0.000 68.06 75.61\r50mg | 70.60 1.91 36.93 0.000 66.83 74.37\r100mg | 72.13 1.91 37.73 0.000 68.36 75.91\r150mg | 70.40 1.91 36.82 0.000 66.63 74.17\r200mg | 54.70 1.91 28.61 0.000 50.93 58.47\r250mg | 48.30 1.91 25.26 0.000 44.53 52.07\r------------------------------------------------------------------------------\rMean pain rating by dosage group\rFor this study, the research question of interest focuses on the test of each dosage against the previous dosage to determine the dosage that leads to a statistically significant decrease in pain. This leads us to five specific null hypotheses. $$H_{0}: \\mu_{1} = \\mu_{2} $$ $$H_{0}: \\mu_{2} = \\mu_{3} $$ $$H_{0}: \\mu_{3} = \\mu_{4} $$ $$H_{0}: \\mu_{4} = \\mu_{5} $$ $$H_{0}: \\mu_{5} = \\mu_{6} $$ Let’s now test each of the hypotheses using the contrast command with the a. contrast operator to compare each dosage with the adjacent (subsequent) dosage. margins a.dosegrp,contrast(nowald pveffects) margins a.dosegrp,contrast(nowald cieffects) //specify cieffects in lieu of pveffects marginsplot,yline(0) Contrasts of adjusted predictions Number of obs = 180\rExpression: Linear prediction, predict()\r----------------------------------------------------------\r| Delta-method\r| Contrast std. err. t P\u003e|t|\r------------------+---------------------------------------\rdosegrp |\r(0mg vs 50mg) | 1.23 2.70 0.46 0.649\r(50mg vs 100mg) | -1.53 2.70 -0.57 0.571\r(100mg vs 150mg) | 1.73 2.70 0.64 0.522\r(150mg vs 200mg) | 15.70 2.70 5.81 0.000\r(200mg vs 250mg) | 6.40 2.70 2.37 0.019\r----------------------------------------------------------\rContrasts of each dosage to the previous dosage\r","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:6:0","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"6.1 Reverse adjacent contrasts The ar. contrast operator, shown below, provides adjacent group contrasts in reverse order. contrast ar.dosegrp,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r------------------+---------------------------------------\rdosegrp |\r(50mg vs 0mg) | -1.23 2.70 -0.46 0.649\r(100mg vs 50mg) | 1.53 2.70 0.57 0.571\r(150mg vs 100mg) | -1.73 2.70 -0.64 0.522\r(200mg vs 150mg) | -15.70 2.70 -5.81 0.000\r(250mg vs 200mg) | -6.40 2.70 -2.37 0.019\r----------------------------------------------------------\r","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:6:1","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"6.2 Selecting a specific contrast When making adjacent group contrasts, you can select a specific contrast. contrast a1.dosegrp,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r-------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r---------------+---------------------------------------\rdosegrp |\r(0mg vs 50mg) | 1.23 2.70 0.46 0.649\r-------------------------------------------------------\rWe can also select contrasts using the ar. contrast operator. The ar. contrast operator forms contrasts with the previous group, so specifying ar3. (as shown below) contrasts group 3 (100 mg) with the previous group (group 2, 50 mg). contrast ar3.dosegrp,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r---------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r-----------------+---------------------------------------\rdosegrp |\r(100mg vs 50mg) | 1.53 2.70 0.57 0.571\r---------------------------------------------------------\rWe can combine selected contrasts as well. Suppose we wanted to test the equality of the mean pain ratings for the first four groups. contrast a(1 2 3).dosegrp Contrasts of marginal linear predictions\rMargins: asbalanced\r-----------------------------------------------------\r| df F P\u003eF\r------------------+----------------------------------\rdosegrp |\r(0mg vs 50mg) | 1 0.21 0.6489\r(50mg vs 100mg) | 1 0.32 0.5714\r(100mg vs 150mg) | 1 0.41 0.5223\rJoint | 3 0.21 0.8918\r|\rDenominator | 174\r-----------------------------------------------------\r-------------------------------------------------------------------\r| Contrast Std. err. [95% conf. interval]\r------------------+------------------------------------------------\rdosegrp |\r(0mg vs 50mg) | 1.23 2.70 -4.10 6.57\r(50mg vs 100mg) | -1.53 2.70 -6.87 3.80\r(100mg vs 150mg) | 1.73 2.70 -3.60 7.07\r-------------------------------------------------------------------\r","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:6:2","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"7 Comparing the mean of subsequent or previous levels This section describes contrasts that compare each group mean with the mean of the subsequent groups (also known as Helmert contrasts). use pain2 tab dosage Medication |\rdosage in |\rmg | Freq. Percent Cum.\r------------+-----------------------------------\r300 | 30 16.67 16.67\r400 | 30 16.67 33.33\r500 | 30 16.67 50.00\r600 | 30 16.67 66.67\r800 | 30 16.67 83.33\r1000 | 30 16.67 100.00\r------------+-----------------------------------\rTotal | 180 100.00\rLet’s begin by testing this overall null hypothesis using the anova command below. $$ H_{0} : \\mu_{300} = \\mu_{400} = \\mu_{500} = \\mu_{600}= \\mu_{800}= \\mu_{1000}$$ anova pain i.dosage Number of obs = 180 R-squared = 0.2052\rRoot MSE = 10.5056 Adj R-squared = 0.1824\rSource | Partial SS df MS F Prob\u003eF\r-----------+----------------------------------------------------\rModel | 4958.8667 5 991.77333 8.99 0.0000\r|\rdosage | 4958.8667 5 991.77333 8.99 0.0000\r|\rResidual | 19204.133 174 110.36858 -----------+----------------------------------------------------\rTotal | 24163 179 134.98883 Let’s use the margins and marginsplot commands to show and graph the means by the six levels of dosage. margins dosage marginsplot Adjusted predictions Number of obs = 180\rExpression: Linear prediction, predict()\r------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\rdosage |\r300 | 43.83 1.92 22.85 0.000 40.05 47.62\r400 | 37.60 1.92 19.60 0.000 33.81 41.39\r500 | 31.87 1.92 16.61 0.000 28.08 35.65\r600 | 29.63 1.92 15.45 0.000 25.85 33.42\r800 | 30.63 1.92 15.97 0.000 26.85 34.42\r1000 | 29.43 1.92 15.35 0.000 25.65 33.22\r------------------------------------------------------------------------------\rMean pain rating by dosage\r$$H_{0}: \\mu_{300} = \\mu \u003e _{\\text{300}}$$ $$H_{0}: \\mu_{400} = \\mu \u003e _{\\text{400}}$$ $$H_{0}: \\mu_{500} = \\mu \u003e _{\\text{500}}$$ $$H_{0}: \\mu_{600} = \\mu \u003e _{\\text{600}}$$ $$H_{0}: \\mu_{800} = \\mu \u003e _{\\text{1000}}$$ Let’s now test each of the null hypotheses below using the margins command combined with the h. contrast operator. margins h.dosage,contrast(nowald pveffects) Contrasts of adjusted predictions Number of obs = 180\rExpression: Linear prediction, predict()\r--------------------------------------------------------\r| Delta-method\r| Contrast std. err. t P\u003e|t|\r----------------+---------------------------------------\rdosage |\r(300 vs \u003e 300) | 12.00 2.10 5.71 0.000\r(400 vs \u003e 400) | 7.21 2.14 3.36 0.001\r(500 vs \u003e 500) | 1.97 2.21 0.89 0.376\r(600 vs \u003e 600) | -0.40 2.35 -0.17 0.865\r(800 vs 1000) | 1.20 2.71 0.44 0.659\r--------------------------------------------------------\rmarginsplot,yline(0) xlabel(, angle(45)) Mean pain rating by dosage\rWhen the confidence interval for the contrast excludes zero,the difference is significant at the 5% level. ","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:7:0","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"7.1 Comparing the mean of previous levels contrast j.dosage,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r---------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r-----------------+---------------------------------------\rdosage |\r(400 vs 300) | -6.23 2.71 -2.30 0.023\r(500 vs \u003c 500) | -8.85 2.35 -3.77 0.000\r(600 vs \u003c 600) | -8.13 2.21 -3.67 0.000\r(800 vs \u003c 800) | -5.10 2.14 -2.38 0.018\r(1000 vs \u003c1000) | -5.28 2.10 -2.51 0.013\r---------------------------------------------------------\r","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:7:1","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"7.2 Selecting a specific contrast contrast h400.dosage,nowald pveffects we wanted to focus only on the contrast of those whose value of dosage was 400 to those who have higher values of dosage. Contrasts of marginal linear predictions\rMargins: asbalanced\r-------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r---------------+---------------------------------------\rdosage |\r(400 vs \u003e400) | 7.21 2.14 3.36 0.001\r-------------------------------------------------------\rAlternatively, we might want to focus only on the contrasts of 400 mg versus the subsequent groups and 500 mg versus the subsequent groups. contrast h(400 500).dosage,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r-------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r---------------+---------------------------------------\rdosage |\r(400 vs \u003e400) | 7.21 2.14 3.36 0.001\r(500 vs \u003e500) | 1.97 2.21 0.89 0.376\r-------------------------------------------------------\rselect the contrast of 500mg versus the mean of the previous groups contrast j500.dosage,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r-------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r---------------+---------------------------------------\rdosage |\r(500 vs \u003c500) | -8.85 2.35 -3.77 0.000\r-------------------------------------------------------\r","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:7:2","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"Polynomial contrasts Let’s consider the use of polynomial contrasts for assessing nonlinear trends (for example, quadratic, cubic, or quartic). We can specify q.dosegrp on the contrast command to compute tests of polynomial trend with respect to dosegrp. (The noeffects option is used to save space and focus on the results of the Wald tests.) use pain contrast q.dosegrp,noeffects compute tests of polynomial trend with respect to dosegrp. Margins: asbalanced\r------------------------------------------------\r| df F P\u003eF\r-------------+----------------------------------\rdosegrp |\r(linear) | 1 109.12 0.0000\r(quadratic) | 1 29.25 0.0000\r(cubic) | 1 0.00 0.9824\r(quartic) | 1 8.39 0.0043\r(quintic) | 1 1.62 0.2048\rJoint | 5 29.67 0.0000\r|\rDenominator | 174\r------------------------------------------------\rSuppose you wanted to fit the relationship between a predictor and outcome using a linear term and wanted to assess whether there are significant nonlinear trends in the relationship between the predictor and outcome. You can use the contrast command to test only the nonlinear terms, as shown below. contrast q(2/6).dosegrp,noeffects Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------\r| df F P\u003eF\r-------------+----------------------------------\rdosegrp |\r(quadratic) | 1 29.25 0.0000\r(cubic) | 1 0.00 0.9824\r(quartic) | 1 8.39 0.0043\r(quintic) | 1 1.62 0.2048\rJoint | 4 9.81 0.0000\r|\rDenominator | 174\r------------------------------------------------\rThe joint test of all the nonlinear terms (powers 2 through 6) is significant.In such a case, it would be inadvisable to fit the relationship between the predictor and outcome using only a linear fit. The q.contrast operator assumes that the levels of dosegrp are equidistant from each other. In next example the level of dosage are not equidistant and we would have obtained different results by specifying q.dosage compared with specifying p.dosage. contrast p.dosage,noeffects the p.contrast operator to dosage.this tests the polynomial trends based on the actual dosage,accounting for the differing gaps among the levels of dosage. Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------\r| df F P\u003eF\r-------------+----------------------------------\rdosage |\r(linear) | 1 28.37 0.0000\r(quadratic) | 1 13.44 0.0003\r(cubic) | 1 2.60 0.1084\r(quartic) | 1 0.47 0.4956\r(quintic) | 1 0.05 0.8202\rJoint | 5 8.99 0.0000\r|\rDenominator | 174\r------------------------------------------------\rspecific on cubic,quartic,and quintic contrast p(3/6).dosage,noeffects ","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:8:0","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"9 Custom contrasts For those times when you want to make another kind of contrast, you can specify a custom contrast. Let’s begin by illustrating how to perform custom contrasts using simple examples that compare one group with another group. For the first example, let’s compare the mean of group 1 (married) with group 5 (not married). The custom contrast is enclosed within curly braces by specifying the variable name followed by the contrast coefficients. The contrast coefficients map to the levels (groups) of the variable. In this example, the contrast coefficient of 1 is applied to group 1, and is applied to group 5. (A contrast coefficient of 0 is applied to groups 2, 3, and 4.) The result is a contrast of group 1 minus group 5. contrast {marital 1 0 0 0 -1} the contrast coef of 1 is applied to group 1,and -1 is applied to group 5(a contrast coefficient of 0 is applied to group 2 3 and 4) Contrasts of marginal linear predictions\rMargins: asbalanced\r------------------------------------------------\r| df F P\u003eF\r-------------+----------------------------------\rmarital | 1 4.18 0.0411\r|\rDenominator | 1147\r------------------------------------------------\r--------------------------------------------------------------\r| Contrast Std. err. [95% conf. interval]\r-------------+------------------------------------------------\rmarital |\r(1) | 0.16 0.08 0.01 0.31\r--------------------------------------------------------------\rLet’s switch the above contrast. Let’s compare group 5 (not married) with group 1 (married), as shown below. contrast {marital -1 0 0 0 1},nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r-----------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r-------------+---------------------------------------\rmarital |\r(1) | -0.16 0.08 -2.04 0.041\r-----------------------------------------------------\rSay that we want to compare those who are married (group 1) with the average of those who are separated and divorced (groups 3 and 4). We can form that contrast as shown below. contrast{marital 1 0 -.5 -.5 0},nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r-----------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r-------------+---------------------------------------\rmarital |\r(1) | 0.52 0.10 5.34 0.000\r-----------------------------------------------------\rSuppose we want to compare those who are married (group 1) with the average of those who are widowed, divorced, and separated (groups 2, 3, and 4). contrast{marital 1 -.33333333 -.33333333 -.33333333 0},nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r-----------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r-------------+---------------------------------------\rmarital |\r(1) | 0.50 0.08 6.10 0.000\r-----------------------------------------------------\rNote! Contrasts must sum to zero The contrast coefficients that we specify in a custom contrast must sum to zero. In the previous example, the contrast coefficients for groups 2, 3, and 4 are expressed as -.33333333, using eight digits after the decimal point. Although the sum of the coefficients for that custom contrast is not exactly zero, it is close enough to zero to satisfy the margins command. You can use inline expansions to directly specify the fraction , as shown in the contrast command below. contrast {marital 1 ‘=-1/3’ ‘=-1/3’ ‘=-1/3’ 0}, nowald pveffects Let’s form a contrast of the average of those who are married (group 1) and separated (group 4) to the average of those who are widowed (group 2) and divorced (group 3). Note how the coefficients for groups 1 and 4 are specified as 0.5 and the coefficients for groups 2 and 3 are specified as . contrast{marital .5 -.5 -.5 .5 0},nowald pveffects Margins: asbalanced\r-----------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r-------------+---------------------------------------\rmarital |\r(1) | ","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:9:0","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"10 Weighted contrasts So far, the examples I have shown estimate the mean for groups 2 and 3 combined by obtaining the mean for group 2 and the mean for group 3, and then averaging those means. Stata calls this the as-balanced approach, because it gives equal weights to the groups even if their sample sizes are different. We could, instead, weight the means for groups 2 and 3 proportionate to their sample size. Stata calls this the as-observed approach, because the means are weighted in proportion to their observed sample size. anova happy7 i.marital3 margins h.marital3,contrast(nowald pveffects) Number of obs = 1,160 R-squared = 0.0369\rRoot MSE = .962698 Adj R-squared = 0.0352\rSource | Partial SS df MS F Prob\u003eF\r-----------+----------------------------------------------------\rModel | 41.07881 2 20.539405 22.16 0.0000\r|\rmarital3 | 41.07881 2 20.539405 22.16 0.0000\r|\rResidual | 1072.2927 1,157 .92678716 -----------+----------------------------------------------------\rTotal | 1113.3716 1,159 .96063119 Let’s use the h. contrast operator to compare each group with the mean of the subsequent groups, using the as-balanced approach. Let’s focus our attention on the first contrast, Married vs \u003eMarried, which compares those who are married (group 1) versus those who are previously married and never married (groups 2 and 3). Compare each group with the mean of the subsequent groups,using the as-balanced approach Contrasts of adjusted predictions Number of obs = 1,160\rExpression: Linear prediction, predict()\r-------------------------------------------------------------------------\r| Delta-method\r| Contrast std. err. t P\u003e|t|\r---------------------------------+---------------------------------------\rmarital3 |\r(Married vs \u003eMarried) | 0.34 0.06 6.07 0.000\r(Prevmarried vs Never married) | -0.20 0.08 -2.50 0.012\r-------------------------------------------------------------------------\rWe can manually compute this estimate: $5.705036 - (5.263323 + 5.459649)/2 = 0.34355$ Let’s compare this estimate with the as-observed approach, which weights the mean of groups 2 and 3 (previously married and never married) by their sample size. The hw. contrast operator is used to obtain the as-observed estimate. margins h.marital3,contrast(nowald pveffects) Contrasts of adjusted predictions Number of obs = 1,160\rExpression: Linear prediction, predict()\r-------------------------------------------------------------------------\r| Delta-method\r| Contrast std. err. t P\u003e|t|\r---------------------------------+---------------------------------------\rmarital3 |\r(Married vs \u003eMarried) | 0.34 0.06 6.07 0.000\r(Prevmarried vs Never married) | -0.20 0.08 -2.50 0.012\r-------------------------------------------------------------------------\rInstead of weighting groups 2 and 3 equally—by (1/2)—groups 2 and 3 are weighted by their individual sample size divided by the combined sample size. The $N$ for group 2 (previously married) is 319, and the $N$ for group 3 (never married) is 285, and the combined for the two groups is 604. We can manually compute this estimate: $5.705036 - (5.263323*(319/604)+ 5.459649*(285/604)) = 0.34907573$ Thus, if you want estimates that are weighted as a function of the proportion of observations in each group, then the as-observed estimates will provide you the kind of estimates that you desire. ","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:10:0","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"11 Pairwise comparisons Sometimes, you want to test all pairwise comparisons that can be formed for a factor variable. The pwcompare command can be used to form such comparisons. use gss_ivrm,clear anova happy7 i.marital i.gender i.race c.age We can use the pwcompare command to form all pairwise comparisons among the marital status groups. Because of the many comparisons, we might want to make adjustments to the values to account for the multiple comparisons. For example, let’s use Šidák’s method for adjusting for multiple comparisons by adding the mcompare(sidak) option. pwcompare marital,pveffects mcompare(sidak) Pairwise comparisons of marginal linear predictions\rMargins: asbalanced\r---------------------------\r| Number of\r| comparisons\r-------------+-------------\rmarital3 | 3\r---------------------------\r----------------------------------------------------------------------\r| Sidak\r| Contrast Std. err. t P\u003e|t|\r------------------------------+---------------------------------------\rmarital3 |\rPrevmarried vs Married | -0.44 0.07 -6.53 0.000\rNever married vs Married | -0.25 0.07 -3.50 0.001\rNever married vs Prevmarried | 0.20 0.08 2.50 0.037\r----------------------------------------------------------------------\rThe bonferroni or scheffe method could have alternatively been specified within the mcompare() option. When you have balanced data, the tukey, snk, duncan, or dunnett method can be specified within mcompare() (see [R] pwcompare for more details) ","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:11:0","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"12 Interpreting confidence intervals The marginsplot command displays margins and confidence intervals that were computed from the most recent margins command. Sometimes, these confidence intervals might tempt you into falsely believing that they tell us about differences among groups. use gss_ivrm,clear anova happy7 i.marital i.gender i.race c.age The margins command is used to estimate the adjusted means of happiness by marital. The output also includes the 95% confidence interval for each adjusted mean. margins marital marginsplot Predictive margins Number of obs = 1,156\rExpression: Linear prediction, predict()\r--------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r---------------+----------------------------------------------------------------\rmarital |\rmarried | 5.70 0.04 139.63 0.000 5.62 5.78\rwidowed | 5.25 0.12 44.51 0.000 5.01 5.48\rdivorced | 5.20 0.07 75.92 0.000 5.06 5.33\rseparated | 5.15 0.16 31.25 0.000 4.83 5.47\rnever married | 5.54 0.06 87.86 0.000 5.42 5.67\r--------------------------------------------------------------------------------\rAdjusted means of happiness by marital status\rOur eye might be tempted to use the overlap (or lack of overlap) of confidence intervals between groups to draw conclusions about the significance of the differences between groups. However, such conclusions would not be appropriate. For example, although the confidence intervals for those who are separated and never married overlap, the output from the contrast command below shows that the difference in these means is statistically significant ($P = 0.026$) contrast ar.marital,nowald pveffects Contrasts of marginal linear predictions\rMargins: asbalanced\r----------------------------------------------------------------------\r| Contrast Std. err. t P\u003e|t|\r------------------------------+---------------------------------------\rmarital |\r(widowed vs married) | -0.45 0.12 -3.67 0.000\r(divorced vs widowed) | -0.05 0.13 -0.34 0.732\r(separated vs divorced) | -0.05 0.18 -0.28 0.777\r(never married vs separated) | 0.39 0.18 2.24 0.026\r----------------------------------------------------------------------\r","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:12:0","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"13 Testing categorical variables using regression The analyses in this chapter have been conducted using the anova command because it produces concise printed output. However, this is not to imply that we cannot perform such tests using the regress command. use gss_ivrm,clear anova happy7 i.marital i.gender i.race c.age reg happy7 i.marital i.gender i.race c.age Source | SS df MS Number of obs = 1,156\r-------------+---------------------------------- F(8, 1147) = 7.35\rModel | 53.9791041 8 6.74738801 Prob \u003e F = 0.0000\rResidual | 1052.29339 1,147 .917431026 R-squared = 0.0488\r-------------+---------------------------------- Adj R-squared = 0.0422\rTotal | 1106.27249 1,155 .957811681 Root MSE = .95783\r--------------------------------------------------------------------------------\rhappy7 | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r---------------+----------------------------------------------------------------\rmarital |\rwidowed | -0.45 0.12 -3.67 0.000 -0.70 -0.21\rdivorced | -0.50 0.08 -6.28 0.000 -0.66 -0.34\rseparated | -0.55 0.17 -3.23 0.001 -0.88 -0.22\rnever married | -0.16 0.08 -2.04 0.041 -0.31 -0.01\r|\rgender |\rFemale | 0.12 0.06 2.06 0.040 0.01 0.23\r|\rrace |\rblack | -0.04 0.09 -0.42 0.676 -0.20 0.13\rother | -0.11 0.12 -0.95 0.341 -0.34 0.12\r|\rage | 0.01 0.00 2.41 0.016 0.00 0.01\r_cons | 5.42 0.11 47.90 0.000 5.19 5.64\r--------------------------------------------------------------------------------\rThe output of the regress command is lengthier because it uses dummy coding for each of the categorical variables and shows the effect for each of the dummy variables. To obtain the test of the overall effect of marital and the test of the overall effect of race, we can use the contrast command, as shown below. contrast marital race Contrasts of marginal linear predictions Margins: asbalanced ------------------------------------------------\r| df F P\u003eF\r-------------+----------------------------------\rmarital | 4 12.85 0.0000\r|\rrace | 2 0.50 0.6043\r|\rDenominator | 1147\r------------------------------------------------\r**The contrast, margins, marginsplot, and pwcompare commands work the same way after the regress command as they do after the anova command.**For example, wecan use the margins command to obtain the adjusted means broken down by marital. contrast r.marital,nowald pveffects Predictive margins Number of obs = 1,156\rModel VCE: OLS\rExpression: Linear prediction, predict()\r--------------------------------------------------------------------------------\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r---------------+----------------------------------------------------------------\rmarital |\rmarried | 5.70 0.04 139.63 0.000 5.62 5.78\rwidowed | 5.25 0.12 44.51 0.000 5.01 5.48\rdivorced | 5.20 0.07 75.92 0.000 5.06 5.33\rseparated | 5.15 0.16 31.25 0.000 4.83 5.47\rnever married | 5.54 0.06 87.86 0.000 5.42 5.67\r--------------------------------------------------------------------------------\r","date":"2024-01-05","objectID":"/7.chapter7categorical-predictors/:13:0","tags":["Categorical","stata"],"title":"Chapter7 ：Categorical predictors","uri":"/7.chapter7categorical-predictors/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter explores models that involve the interaction of three continuous linear predictors.","date":"2024-01-04","objectID":"/6.continuous-by-continuous-by-continuous-interactions/","tags":["Continuous predictors","Interaction","stata"],"title":"Chapter6 ：Continuous by continuous by continuous interactions","uri":"/6.continuous-by-continuous-by-continuous-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter explores models that involve the interaction of three continuous linear predictors. ","date":"2024-01-04","objectID":"/6.continuous-by-continuous-by-continuous-interactions/:0:0","tags":["Continuous predictors","Interaction","stata"],"title":"Chapter6 ：Continuous by continuous by continuous interactions","uri":"/6.continuous-by-continuous-by-continuous-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 Overview Let’s first consider a hypothetical example looking at income (adjusted for inflation) as a function of age, education, and year born. To illustrate the meaning of an interaction of these three continuous predictor, let’s first consider a model that does not include a three-way interaction. $$ \\widehat{realrin}=3365394 + — 55747age + -66053educ + -1712yrborn + 109age * educ + 28age * yrborn + 33.2educ*yrborn$$ The variable age is plotted on the $x$(horizontal) axis, educ is on the $z$axis (representing depth), and the predicted value of realrinc is on the $y$(vertical) axis. Separate graphs are used to represent yrborn.The lines with respect to age are drawn thicker to help accentuate changes in the slope of the relationship between age and the outcome as a function of educ and yrborn. Three-dimensional graph of predicted values from model without the three-way interaction\rlet’s now consider a model that includes a three-way interaction.This regression model predicts realrinc from age, educ, yrborn, the two-way interactions of these predictors (ageeduc, ageyrborn, and educyrborn), and the three-way interaction (ageeduc*yrborn). \\begin{align} \\widehat{realrin} \u0026= -2004225 + 66407age + 313720educ + 1042yrborn +- 8524age\\ \\cdot educ \\notag\\ \\end{align} \\begin{align} +- 34.7age \\cdot yrborn +- 161.62educ \\cdot yrborn + 4.43age \\ \\cdot educ \\cdot yrborn \\notag \\end{align} Three-dimensional graph of predicted values from model with the three-way interaction\rage on the $x$axis, educ on the $z$axis, the predicted value of realrinc on the $y$axis. Separate graphs represent yrborn for the years of birth 1930, 1940, 1950, and 1960 shown from left to right and then top to bottom For those born in 1930 (the top left panel), the age slope is mildly negative for those with 12 years of education and remains mildly negative across the different levels of education. Skipping forward to those born in 1960 (the bottom right panel), the age slope is mildly positive for those with 12 years of education and is sharply positive for those with 20 years of education. Looking across the panels from those born in 1930 (top left) to those born in 1960 (bottom right), we can see how the increase in the age slope due to higher levels of education grows as the year of birthincreases from 1930 to 1960. For those born in 1930, the age slope remains largely the same for all education levels. By contrast, for those born in 1960, the age slope increases considerably with increasing levels of educ. In other words, the size of the ageeduc interaction changes as a function of yrborn. This is a result of, and a way to describe, the interaction of ageeduc*yrborn. Let’s see how these models can be visualized using two-dimensional graphs. Two-dimensional graph of adjusted means for model with the threeway interaction\rIn 1960, the age slope increases considerably with increasing education. We can compare how the age slope changes as a function of education for those born in 1930 with those born in 1960. For those born in 1930, the age slope remains mildly negative for all levels of education. By contrast, for those born in 1960, the age slope grows increasingly positive with increasing levels of education. As we saw in the three-dimensional graph, this two-dimensional graph illustrates how the age×educ interaction changes as a function of yrborn, which is another way of saying that there is an age × educ × yrborn interaction. ","date":"2024-01-04","objectID":"/6.continuous-by-continuous-by-continuous-interactions/:1:0","tags":["Continuous predictors","Interaction","stata"],"title":"Chapter6 ：Continuous by continuous by continuous interactions","uri":"/6.continuous-by-continuous-by-continuous-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.1 Example using GSS data Let’s now illustrate a model with an interaction of three continuous variables using the GSS dataset. This example predicts realrinc from age, educ, and yrborn. use gss_ivrm.dta keep if (age\u003e=30 \u0026 age\u003c=55) \u0026 (educ\u003e=12) \u0026 (yrborn\u003e=1930 \u0026 yrborn\u003c=1960) ","date":"2024-01-04","objectID":"/6.continuous-by-continuous-by-continuous-interactions/:2:0","tags":["Continuous predictors","Interaction","stata"],"title":"Chapter6 ：Continuous by continuous by continuous interactions","uri":"/6.continuous-by-continuous-by-continuous-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.1.1 A model without a three-way interaction Linear regression Number of obs = 11,765\rF(8, 11756) = 138.87\rProb \u003e F = 0.0000\rR-squared = 0.1142\rRoot MSE = 24595\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t|\rage | -48960.89 8316.82 -5.89 0.000\reduc | -61659.03 34481.49 -1.79 0.074\ryrborn | -1529.75 290.82 -5.26 0.000\r|\rc.age#c.educ | 109.44 18.74 5.84 0.000\r|\rc.age#c.yrborn | 24.53 4.28 5.73 0.000\r|\rc.educ#c.yrborn | 30.92 17.50 1.77 0.077\r|\rrace |\rblack | -5274.18 464.22 -11.36 0.000\rother | -4616.74 1145.59 -4.03 0.000\r|\r_cons | 3.01e+06 570438.47 5.28 0.000\rWe can visualize these results using the margins and the marginsplot commands. The margins command is used to obtain the fitted value for ages 30 and 55, for educations of 12 to 20 years (in two-year increments), and for the years of birth 1930, 1940, 1950, and 1960. The marginsplot command is then used to graph these values, placing age on the $x$axis, using separate panels for yrborn, and using separate lines for educ margins, at(age=(30 55) educ=(12(2)20) yrborn=(1930(10)1960)) marginsplot,xdimension(age) bydimension(yrborn) /// plotdimension(educ,allsimple) legend(row(2)subtitle(Education)) /// recast(line) scheme(s1mono) noci ylabel(,angle(0)) Adjusted means from model without the three-way interaction, showing age on the $x$ axis, separate panels for year of birth, and separate lines for education\rAlthough the people born in different years might show a different age slope at 12 years of education, the increase in the age slope as a function of education is the same across the years of birth. In fact, the coefficient for c.age#c.educ describes the exact degree of this increase. For every unit increase in educ, the age slope increases by 109.44. This is true for all years of birth. Let’s see this for ourselves using the margins command. The margins command below estimates the age slope for those with 14 and 15 years of education who were born in 1930. result compute: -88.59 + 109.44 = 20.85 margins,dydx(age) at(educ=(14 15) yrborn=1930) vsquish Average marginal effects Number of obs = 11,765\rModel VCE: Robust\rExpression: Linear prediction, predict()\rdy/dx wrt: age\r1._at: educ = 14\ryrborn = 1930\r2._at: educ = 15\ryrborn = 1930\r------------------------------------------------------------------------------\r| Delta-method\r| dy/dx std. err. t P\u003e|t| [95% conf. interval]\rage |\r_at |\r1 | -88.59 81.21 -1.09 0.275 -247.78 70.59\r2 | 20.85 86.72 0.24 0.810 -149.14 190.84\r------------------------------------------------------------------------------\rLet’s make the same comparison, except for those born in 1940. margins,dydx(age)at(educ=(14 15) yrborn=1940) vsquish result compute: 266.1371 - 156.6922 = 109.44 Average marginal effects Number of obs = 11,765\rModel VCE: Robust\rExpression: Linear prediction, predict()\rdy/dx wrt: age\r1._at: educ = 14\ryrborn = 1940\r2._at: educ = 15\ryrborn = 1940\r------------------------------------------------------------------------------\r| Delta-method\r| dy/dx std. err. t P\u003e|t| [95% conf. interval]\r-------------+----------------------------------------------------------------\rage |\r_at |\r1 | 156.69 46.04 3.40 0.001 66.45 246.93\r2 | 266.14 53.97 4.93 0.000 160.35 371.92\r------------------------------------------------------------------------------\r","date":"2024-01-04","objectID":"/6.continuous-by-continuous-by-continuous-interactions/:2:1","tags":["Continuous predictors","Interaction","stata"],"title":"Chapter6 ：Continuous by continuous by continuous interactions","uri":"/6.continuous-by-continuous-by-continuous-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.1.2 A three-way interaction model Let’s now consider a model that includes a three-way interaction of age, educ, and yrborn when predicting realrinc. Such models can have a high degree of multicollinearity that can lead to numerical instability of the estimates and inflated standard errors. To avoid these problems, Aiken and West (1991) recommend centering the predictors prior to the analysis. Let’s adopt this advice and center age, educ, and yrborn before attempting an analysis that includes the interaction of these three variables. The following generate commands create centered versions of these variables, centering them around values that I chose to ease the interpretation of the centered values. generate yrborn30 = yrborn - 1930 generate age40 = age - 40 generate educ16 = educ - 16 The variable yrborn is centered by subtracting 1930, creating a new variable called yrborn30. This centered variable will range from 0 to 30, corresponding to the years of birth ranging from 1930 to 1960. The centered version of age is called age40 and contains the value of age minus 40. An age of 30 would be represented as using the centered version of age (age40). Finally, the centered version of educ is called educ16, which contains educ minus 16. We could refer to 20 years of education via the centered variable by specifying that educ16 equals 4. The rest of this section will use these centered variables for the analysis. Let’s now form a model that predicts realrinc from age40, educ16, and yrborn30, the two-way interactions among these variables, and the three-way interaction of these variables. reg realrinc c.age40##c.educ16##c.yrborn30 i.race,vce(robust) noci Linear regression Number of obs = 11,765\rF(9, 11755) = 126.24\rProb \u003e F = 0.0000\rR-squared = 0.1145\rRoot MSE = 24591\r--------------------------------------------------------------------\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t|\r----------------------------+---------------------------------------\rage40 | -27.89 135.31 -0.21 0.837\reduc16 | 2800.94 366.46 7.64 0.000\r|\rc.age40#c.educ16 | 26.63 42.54 0.63 0.531\r|\ryrborn30 | -88.92 55.93 -1.59 0.112\r|\rc.age40#c.yrborn30 | 32.56 6.69 4.87 0.000\r|\rc.educ16#c.yrborn30 | 12.89 17.90 0.72 0.471\r|\rc.age40#c.educ16#c.yrborn30 | 4.32 2.19 1.97 0.049\r|\rrace |\rblack | -5279.07 463.47 -11.39 0.000\rother | -4648.00 1142.81 -4.07 0.000\r|\r_cons | 33441.15 1161.80 28.78 0.000\r--------------------------------------------------------------------\rWe can visualize the c.age40#c.educ16#c.yrborn30 interaction in a variety of ways. We can focus on the age40 slope, the educ16 slope, or the yrborn30 slope. As we have done throughout this chapter, let’s focus our attention on the age slope (via the centered variable age40). Note! Retaining main effects and two-way interactions Looking at the estimates from the regress command, you might notice that some of the main effects and two-way interaction terms are not significant. Even though they are not significant, it is important to retain these effects to preserve the interpretation of the three-way interaction term. 1.1.2.1 Visualizing the three-way interaction The margins command is used to compute the adjusted means at different values of the centered variable using the at() option. In terms of the uncentered variables, the at() option specifies ages 30 and 55, educations from 12 to 20 in 2-unit increments, and years of birth from 1930 to 1960 in 10-unit increments. Then, the marginsplot command is used to graph the fitted values, placing age40 on the $x$ axis, with separate lines for educ16 and with separate panels for yrborn30. margins, at(age40=(-10 15) educ16=(-4(2)4) yrborn30=(0(10)30)) marginsplot,xdimension(age40) bydimension(yrborn30) /// plotdimension(educ16,allsimple) legend(row(2) subtitle(Education)) /// recast(line) scheme(s1mono) noci ylabel(, angle(0)) Adjusted means from model with the three-way interaction as a function of age ( $x$ axis), year of birth (separate panels), and education (","date":"2024-01-04","objectID":"/6.continuous-by-continuous-by-continuous-interactions/:2:2","tags":["Continuous predictors","Interaction","stata"],"title":"Chapter6 ：Continuous by continuous by continuous interactions","uri":"/6.continuous-by-continuous-by-continuous-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter illustrates how to interpret interactions of two continuous predictors. ","date":"2024-01-01","objectID":"/5.continuous-by-continuous-interactions/","tags":["Continuous predictors","Interaction","stata"],"title":"Chapter5 ：Continuous by continuous interactions","uri":"/5.continuous-by-continuous-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter illustrates how to interpret interactions of two continuous predictors. ","date":"2024-01-01","objectID":"/5.continuous-by-continuous-interactions/:0:0","tags":["Continuous predictors","Interaction","stata"],"title":"Chapter5 ：Continuous by continuous interactions","uri":"/5.continuous-by-continuous-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 Linear by linear interactions A linear by linear interaction implies that the slope of the relationship between one of the predictors and the outcome changes as a linear function of the other predictor. Let’s begin with a hypothetical example in which we predict income (realrinc) from age (age) and education (educ) without an interaction. $$ \\widehat{realrin}=-41300 + 600age + 3000educ $$ Three-dimensional graph of fitted values from model without aninteraction\rLet’s now consider a second hypothetical regression model that contains an interaction of age (age) and education (educ). $$ \\widehat{realrin}=2400 + -1100age - 1600educ + 120age*educ $$ Three-dimensional graph of fitted values from model with an interaction\rLet’s transition to the use of two-dimensional graphs for illustrating the precise role of linear by linear interactions. Two-dimensional graph of fitted values from model without an interaction (left panel) and with an interaction (right panel)\rThe left panel shows a two-dimensional representation of the model without an interaction, and the right panel shows a two-dimensional representation of the model with an interaction. ","date":"2024-01-01","objectID":"/5.continuous-by-continuous-interactions/:1:0","tags":["Continuous predictors","Interaction","stata"],"title":"Chapter5 ：Continuous by continuous interactions","uri":"/5.continuous-by-continuous-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.1 Example using GSS data use gss_ivrm.dta keep if (age\u003e=22 \u0026 age \u003c=55) \u0026 (educ\u003e=12) Let’s fit a model where we predict realrinc from educ, age, and the interaction of these two variables. reg realrinc c.educ##c.age female, vce(robust) Linear regression Number of obs = 22,367\rF(4, 22362) = 664.04\rProb \u003e F = 0.0000\rR-squared = 0.1712\rRoot MSE = 24677\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\reduc | -1487.58 339.60 -4.38 0.000 -2153.21 -821.94\rage | -1036.79 126.23 -8.21 0.000 -1284.21 -789.38\r|\rc.educ#c.age | 114.91 9.38 12.25 0.000 96.52 133.30\r|\rfemale | -12553.79 328.28 -38.24 0.000 -13197.25 -11910.33\r_cons | 28738.43 4550.48 6.32 0.000 19819.18 37657.69\r","date":"2024-01-01","objectID":"/5.continuous-by-continuous-interactions/:1:1","tags":["Continuous predictors","Interaction","stata"],"title":"Chapter5 ：Continuous by continuous interactions","uri":"/5.continuous-by-continuous-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.2 Interpreting the interaction in terms of age We can use the margins and marginsplot commands to visualize this interaction. Let’s visualize this interaction by graphing the adjusted means with age on the X axis and with separate lines for each level of educ. The margins command is used with the at() option to compute the adjusted means for ages 22 and 55 and educations ranging from 12 to 20 in two-year increments. margins, at(age=(22 55) educ=(12(2)20)) Then, the marginsplot command is used to graph the adjusted means with age on the axis and with separate lines for each level of educ.The legend() option is included to customize the display of the graph legend. marginsplot,noci legend(rows(3) title(Education)) Adjusted means for a linear by linear interaction with age on the X axis\rwe can use the margins command combined with the dydx(age) option to estimate the slope for each of the lines displayed in figure margins, at(educ=(12(2)20)) dydx(age) vsquish Average marginal effects Number of obs = 22,367\rModel VCE: Robust\rExpression: Linear prediction, predict()\rdy/dx wrt: age\r1._at: educ = 12\r2._at: educ = 14\r3._at: educ = 16\r4._at: educ = 18\r5._at: educ = 20\r| Delta-method\r| dy/dx std. err. t P\u003e|t| [95% conf. interval]\rage |\r_at |\r1 | 342.16 19.78 17.30 0.000 303.39 380.93\r2 | 571.99 16.30 35.09 0.000 540.03 603.94\r3 | 801.81 29.06 27.59 0.000 744.85 858.77\r4 | 1031.64 46.12 22.37 0.000 941.23 1122.04\r5 | 1261.46 64.14 19.67 0.000 1135.73 1387.19\rThe estimates of the age slope increase as a function of educ. For example, at 12 years of education, the age slope is 342.16, and at 14 years of education, the age slope is 571.99. For a two-unit increase in education, the age slope increases by 229.83 ($571.99-342.16$). We can relate this to the coefficient for the c.educ#c.age interaction, which is the amount by which the age slope changes for every one-year increase in educ. For every one-year increase in educ, the age slope increases by 114.91($229.83÷2=114.91$). ","date":"2024-01-01","objectID":"/5.continuous-by-continuous-interactions/:1:2","tags":["Continuous predictors","Interaction","stata"],"title":"Chapter5 ：Continuous by continuous interactions","uri":"/5.continuous-by-continuous-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.3 Interpreting the interaction in terms of education Now, let’s explore the meaning of the interaction by focusing on the educ slope. Let’s visualize this by creating a graph of the adjusted means showing educ on the $x$ axis and separate lines for age. First, the margins command is used to create adjusted means for 12 and 20 years of education and ages ranging from 25 to 55 in 10-year increments. margins, at(educ=(12(2)20)) dydx(age) vsquish Predictive margins Number of obs = 22,367\rModel VCE: Robust\rExpression: Linear prediction, predict()\r1._at: educ = 12\rage = 25\r2._at: educ = 12\rage = 35\r3._at: educ = 12\rage = 45\r4._at: educ = 12\rage = 55\r5._at: educ = 20\rage = 25\r6._at: educ = 20\rage = 35\r7._at: educ = 20\rage = 45\r8._at: educ = 20\rage = 55\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r_at |\r1 | 13091.92 236.14 55.44 0.000 12629.07 13554.76\r2 | 16513.52 172.78 95.57 0.000 16174.85 16852.19\r3 | 19935.12 286.72 69.53 0.000 19373.12 20497.11\r4 | 23356.72 461.33 50.63 0.000 22452.47 24260.96\r5 | 24173.82 876.85 27.57 0.000 22455.13 25892.50\r6 | 36788.43 619.20 59.41 0.000 35574.75 38002.11\r7 | 49403.04 906.00 54.53 0.000 47627.21 51178.86\r8 | 62017.65 1442.62 42.99 0.000 59190.01 64845.28\rmarginsplot,noci legend(title(Age)) Adjusted means for a linear by linear interaction with education on the axis\rwe can estimate the slope of each line (that is, the educ slope) using the margins command, as shown below. margins,at(age=(25(10)55)) dydx(educ)vsquish Average marginal effects Number of obs = 22,367\rModel VCE: Robust\rExpression: Linear prediction, predict()\rdy/dx wrt: educ\r1._at: age = 25\r2._at: age = 35\r3._at: age = 45\r4._at: age = 55\r| Delta-method\r| dy/dx std. err. t P\u003e|t| [95% conf. interval]\reduc |\r_at |\r1 | 1385.24 129.78 10.67 0.000 1130.87 1639.61\r2 | 2534.36 90.87 27.89 0.000 2356.24 2712.48\r3 | 3683.49 131.45 28.02 0.000 3425.83 3941.15\r4 | 4832.62 209.54 23.06 0.000 4421.90 5243.33\rAs age increases, so does the educ slope. For every one-unitincrease in age, the educ slope increases by 114.91, the estimate of the age#educ interaction. For a 10-unit increase in age, the educ slope would be expected to increase by 1,149.12. ","date":"2024-01-01","objectID":"/5.continuous-by-continuous-interactions/:1:3","tags":["Continuous predictors","Interaction","stata"],"title":"Chapter5 ：Continuous by continuous interactions","uri":"/5.continuous-by-continuous-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.4 Interpreting the interaction in terms of age slope We can visualize the age by educ interaction by illustrating the way that the age slope changes as a function of education. The margins command below includes the dydx(age) option to estimate the age slope at each level of educ. margins,dydx(age) at(educ=(12(1)20))vsquish Average marginal effects Number of obs = 22,367\rModel VCE: Robust\rExpression: Linear prediction, predict()\rdy/dx wrt: age\r1._at: educ = 12\r2._at: educ = 13\r3._at: educ = 14\r4._at: educ = 15\r5._at: educ = 16\r6._at: educ = 17\r7._at: educ = 18\r8._at: educ = 19\r9._at: educ = 20\r| Delta-method\r| dy/dx std. err. t P\u003e|t| [95% conf. interval]\rage |\r_at |\r1 | 342.16 19.78 17.30 0.000 303.39 380.93\r2 | 457.07 15.51 29.47 0.000 426.68 487.47\r3 | 571.99 16.30 35.09 0.000 540.03 603.94\r4 | 686.90 21.61 31.78 0.000 644.54 729.26\r5 | 801.81 29.06 27.59 0.000 744.85 858.77\r6 | 916.72 37.39 24.52 0.000 843.44 990.01\r7 | 1031.64 46.12 22.37 0.000 941.23 1122.04\r8 | 1146.55 55.07 20.82 0.000 1038.61 1254.49\r9 | 1261.46 64.14 19.67 0.000 1135.73 1387.19\rThis shows that the age slope increases as a function of education. In fact, the age slope increases by 114.91 units for every one-unit increase in educ. We can visualize these age slopes as a function of education using the marginsplot command (shown below). marginsplot ","date":"2024-01-01","objectID":"/5.continuous-by-continuous-interactions/:1:4","tags":["Continuous predictors","Interaction","stata"],"title":"Chapter5 ：Continuous by continuous interactions","uri":"/5.continuous-by-continuous-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.5 Interpreting the interaction in terms of the educ slope We can visualize the age by educ interaction by focusing on the way that the educ slope changes as a function of age. The margins command below estimates the educ slope for ages ranging from 25 to 55 in five-year increments. margins,dydx(educ)at(age=(25(5)55))vsquish Average marginal effects Number of obs = 22,367\rModel VCE: Robust\rExpression: Linear prediction, predict()\rdy/dx wrt: educ\r1._at: age = 25\r2._at: age = 30\r3._at: age = 35\r4._at: age = 40\r5._at: age = 45\r6._at: age = 50\r7._at: age = 55\r| Delta-method\r| dy/dx std. err. t P\u003e|t| [95% conf. interval]\reduc |\r_at |\r1 | 1385.24 129.78 10.67 0.000 1130.87 1639.61\r2 | 1959.80 101.73 19.26 0.000 1760.40 2159.20\r3 | 2534.36 90.87 27.89 0.000 2356.24 2712.48\r4 | 3108.93 102.80 30.24 0.000 2907.43 3310.43\r5 | 3683.49 131.45 28.02 0.000 3425.83 3941.15\r6 | 4258.05 168.50 25.27 0.000 3927.78 4588.33\r7 | 4832.62 209.54 23.06 0.000 4421.90 5243.33\rThis shows that the educ slope increases as a function of age. For each five-year increase in age, the educ slope increases by 574.56 (that is, the age#educ coefficient multiplied by five, $114.91 × 5$). marginsplot,noci ","date":"2024-01-01","objectID":"/5.continuous-by-continuous-interactions/:1:5","tags":["Continuous predictors","Interaction","stata"],"title":"Chapter5 ：Continuous by continuous interactions","uri":"/5.continuous-by-continuous-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2 Linear by quadratic interactions This section considers models with an interaction of two continuous variables, where one of the variables is fit linearly and the other is fit using a quadratic term. A quadratic term introduces curvature in the fitted relationship between the predictor and outcome. Let’s first illustrate a model that contains a quadratic predictor and a linear predictor but no interaction between these predictors. This hypothetical example uses realrinc as the outcome variable, age as the quadratic predictor, and educ as the linear predictor. $$ \\widehat{realrin}=-7000 + 2100age + - age^2 + 3000educ $$ Three-dimensional graph of fitted values for linear and quadratic models without an interaction\rLet’s now consider a model that includes an interaction between age (as a quadratic term) and educ. This regression equation is shown below $$ \\widehat{realrin}=165000 + -8600age + 95age^2 +-14000educ + 780age*educ + - 8age^2 * educ $$ The key term is the age squared by education interaction. This governs the degree of the curvature in age as a function of education. Three-dimensional graph of fitted values for linear and quadratic models with an interaction\rAs education increases from 12 to 20years, the degree of the inverted U-shape grows as a function of education. This is due to the interaction of education with age squared. When quadratic terms become more negative, the inverted U-shape becomes more pronounced. Thus for every one-unit increase in education, the relationship between income and age shows more of an inverted U-shape. We can clearly see how the degree of the curvature in relationship between income and age is the same across the levels of education. The right panel of figure shows a two-dimensional representation of figure above, where there was an interaction of education with the quadratic term for age. Two-dimensional graph of fitted values for linear and quadratic models without an interaction (left panel) and with linear by quadratic interaction (right panel)\r","date":"2024-01-01","objectID":"/5.continuous-by-continuous-interactions/:2:0","tags":["Continuous predictors","Interaction","stata"],"title":"Chapter5 ：Continuous by continuous interactions","uri":"/5.continuous-by-continuous-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.1 Example using GSS data Let’s use the GSS dataset to fit a model that predicts realrinc from educ (treated as a continuous linear variable) interacted with age (treated as a continuous variable fit using a quadratic term). use gss_ivrm.dta keep if (age\u003e=22 \u0026 age\u003c=80) \u0026 (educ\u003e=12) reg realrinc c.educ c.age##c.age female,vce(robust) Let’s begin by fitting a model that predicts realrinc using educ as a linear term and age as a quadratic term but no interaction between education and age. Linear regression Number of obs = 25,964\rF(4, 25959) = 785.49\rProb \u003e F = 0.0000\rR-squared = 0.1658\rRoot MSE = 26078\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\reduc | 3071.27 93.46 32.86 0.000 2888.08 3254.45\rage | 2245.62 79.05 28.41 0.000 2090.67 2400.57\r|\rc.age#c.age | -21.77 0.94 -23.09 0.000 -23.62 -19.92\r|\rfemale | -13344.78 319.66 -41.75 0.000 -13971.33 -12718.23\r_cons | -64839.08 2060.09 -31.47 0.000 -68876.96 -60801.20\rThe quadratic term is significant and is negative. This negative coefficient indicates that the relationship between age and income has an inverted U-shape. Let’s use the margins and marginsplot commands to visualize the adjusted means of income as a function of age while holding education constant at 12 to 20 years of education (in two-year increments). We first compute the adjusted means as a function of age and education using the margins command. Then, the marginsplot command graphs the adjusted means on the $y$ axis and age on the $x$ axis, with separate lines for each level of education. margins,at(age=(22(1)80) educ=(12(2)20)) marginsplot,noci legend(rows(2))recast(line) scheme(s1mono) Adjusted means at 12, 14, 16, 18, and 20 years of education\rLet’s now fit a model that includes an interaction of educ with the quadratic term for age. reg realrinc c.educ##c.age##c.age female,vce(robust) noci Linear regression Number of obs = 25,964\rF(6, 25957) = 558.27\rProb \u003e F = 0.0000\rR-squared = 0.1757\rRoot MSE = 25924\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t|\reduc | -9317.78 817.82 -11.39 0.000\rage | -5009.26 551.97 -9.08 0.000\r|\rc.educ#c.age | 517.95 41.56 12.46 0.000\r|\rc.age#c.age | 46.88 6.39 7.34 0.000\r|\rc.educ#c.age#c.age | -4.91 0.48 -10.18 0.000\r|\rfemale | -13160.33 317.92 -41.40 0.000\r_cons | 108559.85 10897.04 9.96 0.000\rLet’s interpret this as a function of age by graphing the adjusted means on the axis and age on the axis, with separate lines to indicate the levels of education. margins,at(age=(22 25(5)80) educ=(12(2)20)) marginsplot,plotdimension(,allsimple)legend(subtitle(Education)rows(2))noci recast(line) scheme(s1mono) Adjusted means from linear by quadratic model\rthe c.educ#c.age#c.age coefficient describes the degree to which the inverted U-shape changes as a function of education. This coefficient, which is $-4.9$, represents the change in the quadratic term for age for a one-unit increase in educ. As education increases by one unit, the quadratic term for age decreases by 4.9, creating a more inverted U-shape. For those with lower educations, the relationship between age and income tends to be more linear, and for those with higher levels of education, the relationship between age and income is more curved, showing a greater rise and fall across ages. ","date":"2024-01-01","objectID":"/5.continuous-by-continuous-interactions/:2:1","tags":["Continuous predictors","Interaction","stata"],"title":"Chapter5 ：Continuous by continuous interactions","uri":"/5.continuous-by-continuous-interactions/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter illustrates the use of piecewise regression. This involves fitting separate line segments, demarcated by knots, that account for the nonlinearity between the predictor and outcome.  ","date":"2023-12-29","objectID":"/3.continuous-predictors_piecewise-models/","tags":["Continuous predictors","stata"],"title":"Chapter4 ：Continuous predictors: Piecewise models","uri":"/3.continuous-predictors_piecewise-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter illustrates the use of piecewise regression. This involves fitting separate line segments, demarcated by knots, that account for the nonlinearity between the predictor and outcome. ","date":"2023-12-29","objectID":"/3.continuous-predictors_piecewise-models/:0:0","tags":["Continuous predictors","stata"],"title":"Chapter4 ：Continuous predictors: Piecewise models","uri":"/3.continuous-predictors_piecewise-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 Introduction to piecewise regression models A piecewise regression goes by several names, including spline regression,broken line regression, broken stick regression, and even hockey stick models. Consider the example, predicting annual income from education, shown in the figure below A knot can signify a change of slope and a change of intercept, yielding an increase (or decrease) in the outcome upon attaining a particular milestone. Note! Instantaneous jumps? As a thought experiment, imagine someone being one day short of graduating high school and the wages they would obtain as they seek a job. Compare this person with an identical job seeker who has one more day of education (that is, they graduated high school). These two people are identical except that one crossed the threshold of getting a diploma. It is indeed plausible that the second person would be offered an annual income $2,200 more than the first person$. Suppose you have a predictor that shows a nonlinear relationship with the outcome, and you believe that a piecewise model with one knot signifying a change in slope would fit your data well. However, unlike the previous examples, you do not have a theoretical or practical basis for selecting the placement of the knot. You could haphazardly try a variety of placements for the knot, trying to find a placement that results in the best fitting model. Alternatively, you can let Stata do the work for you by using a least-squares procedure for selecting a location for the knot that produces the lowest residual sum of squares. ","date":"2023-12-29","objectID":"/3.continuous-predictors_piecewise-models/:1:0","tags":["Continuous predictors","stata"],"title":"Chapter4 ：Continuous predictors: Piecewise models","uri":"/3.continuous-predictors_piecewise-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2 Piecewise with one known knot This section illustrates piecewise regression with one knot. ","date":"2023-12-29","objectID":"/3.continuous-predictors_piecewise-models/:2:0","tags":["Continuous predictors","stata"],"title":"Chapter4 ：Continuous predictors: Piecewise models","uri":"/3.continuous-predictors_piecewise-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.1 Examples using the GSS Let’s use the GSS dataset to illustrate a piecewise model with one knot, focusing on the relationship between income and education. use gss_ivrm.dta reg realrinc i.educ,vce(robust) margins educ marginsplot,noci It looks like the relationship between education and income could be fit well with a piecewise model with a knot at 12 (corresponding to graduating high school). The following subsections illustrate two different ways to fit this kind of piecewise model: using individual slope coding and using change in slope coding. 2.1.1 Individual slope coding The individual slope coding scheme estimates the slope of each line segment of the piecewise model. The first step is to create two new variables that are coded to represent the educ slope before and after graduating high school. The mkspline command creates the variables ed1 and ed2 based on the original variable educ. The value of 12 is inserted between ed1 and ed2, indicating that this is the knot. mkspline ed1 12 ed2 = educ showcoding educ ed1 ed2 The next step is to use the regress command to predict realrinc from ed1 and ed2. This will yield a piecewise model with 12 years of education as the knot. reg realrinc ed1 ed2 female,vce(robust) Linear regression Number of obs = 32,183\rF(3, 32179) = 1030.24\rProb \u003e F = 0.0000\rR-squared = 0.1420\rRoot MSE = 25045\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\red1 | 832.27 72.38 11.50 0.000 690.40 974.14\red2 | 3441.33 93.42 36.84 0.000 3258.21 3624.44\rfemale | -12372.38 276.35 -44.77 0.000 -12914.05 -11830.72\r_cons | 12052.18 793.42 15.19 0.000 10497.04 13607.31\rAmong non–high school graduates(ed1), each additional year of education is predicted to increase income by $832.27$.The coefficient for ed2 is the slope for those with 12 or more years of education (high school graduates). Income increases by $3,441.33$ for each additional year of education beyond 12 years of education(ed2). Each of these slopes is significantly different from 0. Say that we want to compute the adjusted mean given that a person has eight years of education. We can compute this adjusted mean using the margins command; To graph the adjusted means as a function of education, we need to compute the adjusted means when education is 0, 12, and 20. The margins command below computes these three adjusted means by using the at() option three times. margins,at(ed1=0 ed2=0) at(ed1=12 ed2=0) at(ed1=12 ed2=8) vsquish Predictive margins Number of obs = 32,183\rModel VCE: Robust\rExpression: Linear prediction, predict()\r1._at: ed1 = 0\red2 = 0\r2._at: ed1 = 12\red2 = 0\r3._at: ed1 = 12\red2 = 8\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r_at |\r1 | 5964.98 794.24 7.51 0.000 4408.23 7521.72\r2 | 15952.22 161.46 98.80 0.000 15635.75 16268.69\r3 | 43482.83 657.66 66.12 0.000 42193.79 44771.86\rpreserve clear input educ yhat 0 5964.977 12 15952.22 20 43482.83 end graph twoway line yhat educ,xlabel(0(4)20) xline(12) restore 2.1.2 Change in slope coding This coding scheme estimates the slope for the line segment before the first knot (for example, for non–high school graduates) and then estimates the difference in the slopes of adjacent line segments (for example, for high school graduates versus non–high school graduates). This strategy emphasizes the change in slope that occurs at each knot. The key difference is that we add the marginal option to the mkspline command, as shown below. I name the variables ed1m and ed2m, adding the m to emphasize that these variables were created using the marginal option. mkspline ed1m 12 ed2m = educ,marginal The next step is to enter ed1m and ed2m as predictors of income, as shown below. reg realrinc ed1m ed2m female,vce(robust) Linear regression Number of obs = 32,183\rF(3, 32179) = 1030.24\rProb \u003e F = 0.0000\rR-squared = 0.1420\rRoot MSE = 25045\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\red1m | 832.27 72.38 11.50 0.000 690.40 974.14\red2m | 2609.06 134.40 19.41","date":"2023-12-29","objectID":"/3.continuous-predictors_piecewise-models/:2:1","tags":["Continuous predictors","stata"],"title":"Chapter4 ：Continuous predictors: Piecewise models","uri":"/3.continuous-predictors_piecewise-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3 Piecewise with two known knots ","date":"2023-12-29","objectID":"/3.continuous-predictors_piecewise-models/:3:0","tags":["Continuous predictors","stata"],"title":"Chapter4 ：Continuous predictors: Piecewise models","uri":"/3.continuous-predictors_piecewise-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.1 Examples using the GSS use gss_ivrm.dta mkspline ed1m 12 ed2m 16 ed3m = educ,marginal showcoding educ ed1m ed2m ed3m The coefficient for ed2m will represent the change in slope for high school graduates versus non–high school graduates. The coefficient for ed3m will represent the change in slope comparing college graduates with high school graduates. reg realrinc ed1m ed2m ed3m female,vce(robust) Linear regression Number of obs = 32,183\rF(4, 32178) = 784.42\rProb \u003e F = 0.0000\rR-squared = 0.1432\rRoot MSE = 25029\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\red1m | 939.67 70.55 13.32 0.000 801.40 1077.95\red2m | 2022.42 144.24 14.02 0.000 1739.71 2305.13\red3m | 1657.45 412.15 4.02 0.000 849.62 2465.28\rfemale | -12336.74 277.09 -44.52 0.000 -12879.84 -11793.64\r_cons | 11215.32 787.53 14.24 0.000 9671.73 12758.92\rWe can use the margins command to compute adjusted means for any given value of education by expressing education in terms of the variables ed1m, ed2m, and ed3m. To graph the entire range of education values, we need to compute the adjusted means for the minimum of education (0), for each of the knots (12 and 16), and for the maximum of education (20). The margins command below computes these adjusted means using the at() option once for each of these four levels of education. margins,at(ed1m=0 ed2m=0 ed3m=0) /// at(ed1m=12 ed2m=0 ed3m=0) /// at(ed1m=16 ed2m=4 ed3m=0) /// at(ed1m=20 ed2m=8 ed3m=4) vsquish Predictive margins Number of obs = 32,183\rModel VCE: Robust\rExpression: Linear prediction, predict()\r1._at: ed1m = 0\red2m = 0\red3m = 0\r2._at: ed1m = 12\red2m = 0\red3m = 0\r3._at: ed1m = 16\red2m = 4\red3m = 0\r4._at: ed1m = 20\red2m = 8\red3m = 4\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r_at |\r1 | 5145.66 785.16 6.55 0.000 3606.72 6684.60\r2 | 16421.73 145.60 112.79 0.000 16136.36 16707.11\r3 | 28270.11 383.13 73.79 0.000 27519.16 29021.06\r4 | 46748.30 1242.79 37.62 0.000 44312.38 49184.22\rWe can then manually input the adjusted means from the margins command into a dataset as shown below. The graph command is then used to graph the relationship between education and income. preserve clear input educ yhat 0 5154.66 12 16421.73 16 28270.11 20 46748.3 end graph twoway line yhat educ,xlabel(0(4)20) xline(12 16) xtitle(Education) ytitle(Adjusted mean) restore ","date":"2023-12-29","objectID":"/3.continuous-predictors_piecewise-models/:3:1","tags":["Continuous predictors","stata"],"title":"Chapter4 ：Continuous predictors: Piecewise models","uri":"/3.continuous-predictors_piecewise-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4 Piecewise with one knot and one jump ","date":"2023-12-29","objectID":"/3.continuous-predictors_piecewise-models/:4:0","tags":["Continuous predictors","stata"],"title":"Chapter4 ：Continuous predictors: Piecewise models","uri":"/3.continuous-predictors_piecewise-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4.1 Examples using the GSS use gss_ivrm.dta mkspline ed1m 12 ed2m = educ,marginal reg realrinc ed1m ed2m hsgrad female,vce(robust) The variable hsgrad is coded 1 if someone has 12 or more years of education, and 0 otherwise. Linear regression Number of obs = 32,183\rF(4, 32178) = 803.67\rProb \u003e F = 0.0000\rR-squared = 0.1425\rRoot MSE = 25039\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\red1m | 273.10 105.88 2.58 0.010 65.56 480.64\red2m | 3102.95 141.83 21.88 0.000 2824.97 3380.94\rhsgrad | 2721.54 412.82 6.59 0.000 1912.40 3530.67\rfemale | -12391.31 276.30 -44.85 0.000 -12932.87 -11849.74\r_cons | 16345.24 988.38 16.54 0.000 14407.98 18282.49\rAt 12 years of education, the adjusted mean is computed twice, once assuming the absence of a high school degree and once assuming a high school degree, illustrating the jump in income due to graduating high school. How tow interpret the regression coefficient? For each additional year of education (up to 12), income increases by $273.10.$figure shows the adjusted means given zero and one year of education. This difference in these adjusted means equals 273.10 $(10521.83-10248.73)$ The coefficient of ed2m is 3,102.95, which is the change (increase) in slope for high school graduates compared with non–high school graduates. Finally, the hsgrad coefficient (2,721.54) represents the predicted jump (increase in income) due to graduating high school. showcoding educ hsgrad ed1m ed2m margins,at(ed1m=0 ed2m=0 hsgrad=0) /// at(ed1m=12 ed2m=0 hsgrad=0) /// at(ed1m=12 ed2m=0 hsgrad=1) /// at(ed1m=20 ed2m=8 hsgrad=1) vsquish Predictive margins Number of obs = 32,183\rModel VCE: Robust\rExpression: Linear prediction, predict()\r1._at: ed1m = 0\red2m = 0\rhsgrad = 0\r2._at: ed1m = 12\red2m = 0\rhsgrad = 0\r3._at: ed1m = 12\red2m = 0\rhsgrad = 1\r4._at: ed1m = 20\red2m = 8\rhsgrad = 1\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r1 | 10248.73 987.42 10.38 0.000 8313.35 12184.10\r2 | 13525.93 374.97 36.07 0.000 12790.97 14260.89\r3 | 16247.46 174.79 92.95 0.000 15904.86 16590.07\r4 | 43255.90 664.00 65.14 0.000 41954.43 44557.37\rpreserve clear input educ yhat 0 10248.73 12 13525.93 12 16247.46 20 43255.9 end graph twoway line yhat educ,xlabel(0(4)20) xline(12) restore Note! Individual slope coding This model was fit using the change in slope coding method (that is, with the marginal option on the mkspline command). If you wished, you could fit this model using individual slope coding by omitting the marginal option on the mkspline command. ","date":"2023-12-29","objectID":"/3.continuous-predictors_piecewise-models/:4:1","tags":["Continuous predictors","stata"],"title":"Chapter4 ：Continuous predictors: Piecewise models","uri":"/3.continuous-predictors_piecewise-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"5 Piecewise with two knots and two jumps This section will illustrate a model with two knots signifying a change of slope and intercept. ","date":"2023-12-29","objectID":"/3.continuous-predictors_piecewise-models/:5:0","tags":["Continuous predictors","stata"],"title":"Chapter4 ：Continuous predictors: Piecewise models","uri":"/3.continuous-predictors_piecewise-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"5.1 Examples using the GSS use gss_ivrm.dta mkspline ed1m 12 ed2m 16 ed3m = educ,marginal reg realrinc ed1m ed2m ed3m hsgrad cograd female,vce(robust) Linear regression Number of obs = 32,183\rF(6, 32176) = 544.02\rProb \u003e F = 0.0000\rR-squared = 0.1458\rRoot MSE = 24991\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\red1m | 272.27 105.87 2.57 0.010 64.76 479.79\red2m | 1329.81 189.31 7.02 0.000 958.75 1700.88\red3m | 2540.17 398.71 6.37 0.000 1758.69 3321.65\rhsgrad | 3925.58 405.81 9.67 0.000 3130.18 4720.98\rcograd | 5740.76 733.17 7.83 0.000 4303.73 7177.80\rfemale | -12358.72 276.69 -44.67 0.000 -12901.04 -11816.40\r_cons | 16339.10 988.28 16.53 0.000 14402.04 18276.16\rshowcoding educ ed1m ed2m ed3m hsgrad cograd margins,at (ed1m=0 ed2m=0 ed3m=0 hsgrad=0 cograd=0) /// at(ed1m=12 ed2m=0 ed3m=0 hsgrad=0 cograd=0) /// at(ed1m=12 ed2m=0 ed3m=0 hsgrad=1 cograd=0) /// at(ed1m=16 ed2m=4 ed3m=0 hsgrad=1 cograd=0) /// at(ed1m=16 ed2m=4 ed3m=0 hsgrad=1 cograd=1) /// at(ed1m=20 ed2m=8 ed3m=4 hsgrad=1 cograd=1) vsquish noatlegend Predictive margins Number of obs = 32,183\rModel VCE: Robust\rExpression: Linear prediction, predict()\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r_at |\r1 | 10258.62 987.32 10.39 0.000 8323.44 12193.81\r2 | 13525.88 374.93 36.08 0.000 12791.01 14260.75\r3 | 17451.46 157.37 110.90 0.000 17143.02 17759.91\r4 | 23859.81 558.53 42.72 0.000 22765.07 24954.55\r5 | 29600.57 475.61 62.24 0.000 28668.35 30532.79\r6 | 46169.58 1255.73 36.77 0.000 43708.30 48630.87\rpreserve clear input educ yhat 0 10258.62 12 13525.88 12 17451.46 16 23858.81 16 29600.57 20 46169.58 end graph twoway line yhat educ,xlabel(0(4)20) xline(12 16) xtitle(Education) ytitle(Adjusted means) restore ","date":"2023-12-29","objectID":"/3.continuous-predictors_piecewise-models/:5:1","tags":["Continuous predictors","stata"],"title":"Chapter4 ：Continuous predictors: Piecewise models","uri":"/3.continuous-predictors_piecewise-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"6 Piecewise with an unknown knot This section illustrates a method for fitting a piecewise model with one knot where the location of the knot is uncertain. Consider the relationship between year of birth and level of education. In this dataset, the year of birth is recorded in the variable yrborn and education level is recorded in the variable educ. To visualize the relationship between year of birth and education, let’s make a graph showing the mean of educ at each level of yrborn. use gss_ivrm.dta keep if (yrborn\u003e=1905 \u0026 yrborn\u003c=1985) \u0026 !missing(educ) reg educ i.yrborn margins yrborn marginsplot,noci We could haphazardly select different years for the placement of the knot and select the knot that yields the best fitting model. Rather than manually doing this selection process, we can use the nl command to automate the process of selecting the optimal knot, by selecting a knot location that yields the lowest residual sum of squares. The code for fitting such a model is shown below. nl (educ = ({cons} + {b1}*yrborn)*(yrborn\u003c{knot}) + /// ({cons} + {b1}*{knot} + {b2}*(yrborn-{knot}))*(yrborn\u003e={knot})), /// initial(knot 1945 b1 .1 b2 -.0125 cons -181) Iteration 0: Residual SS = 464140\rIteration 1: Residual SS = 464092.3\rIteration 2: Residual SS = 464092.3\rSource | SS df MS\rNumber of obs = 52,873\rModel | 48574.186 3 16191.3952 R-squared = 0.0947\rResidual | 464092.28 52869 8.77815515 Adj R-squared = 0.0947\rRoot MSE = 2.962795\rTotal | 512666.47 52872 9.69636992 Res. dev. = 264897.3\reduc | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\r/cons | -152.07 3.24 -46.88 0.000 -158.43 -145.71\r/b1 | 0.09 0.00 50.59 0.000 0.08 0.09\r/knot | 1946.97 0.51 3800.38 0.000 1945.96 1947.97\r/b2 | -0.01 0.00 -2.98 0.003 -0.01 -0.00\rNote: Parameter cons is used as a constant term during estimation. The coefficient labeled cons is the constant for the model (the predicted value of educ when yrborn is 0). This value is generally uninteresting. The coefficient labeled knot is the location of the knot, selected as 1946.97 (which we can round to 1947). The coefficient labeled b1 is the slope of the relationship between education and year of birth for those born before the knot (before 1947). The coefficient labeled b2 is the slope for those born in or after 1947. The key is that 1947 was selected as the optimal location for the knot. How to compute the coef respectively and interpret the graph ? In place of educ, you would insert your outcome variable, and in place of yrborn, you would place your predictor variable. Then for the initial() option, you would insert plausible values for knot, b1, b2, and cons. These correspond to the placement of the knot, the slope before the knot, the slope after the knot, and the constant, respectively. In this example, I used 1945 as the knot. To estimate b1, I looked at the graph and saw that the mean education rose about 4 units from 1905 to 1945. Taking a change of 4 units divided by 40 gave me an estimate of 0.1 for b1. Likewise, I estimated that education declined by about 0.5 units from 1945 to 1985 and divided $-0.5$ units by 40 to yield $-0.0125$ as an estimate of b2. Finally, to estimate cons I estimated the average education to be 9.5 units at 1905 and then used the estimated slope of 0.1 to estimate the education at year 0 would be $9.5-1905×0.1=-181.$ ","date":"2023-12-29","objectID":"/3.continuous-predictors_piecewise-models/:6:0","tags":["Continuous predictors","stata"],"title":"Chapter4 ：Continuous predictors: Piecewise models","uri":"/3.continuous-predictors_piecewise-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"7 Piecewise model with multiple unknown knots This section illustrates the use of piecewise regression models where there could be multiple knots in the relationship between the predictor and outcome, and you have no basis for the placement of the knots. Consider the relationship between age and income. I would suspect that income would initially rise rapidly with increasing age, hit a peak, and then decline with increasing age. In such a case, we can intentionally fit a model with too many knots and then progressively remove the superfluous knots. use gss_ivrm.dta keep if age\u003c=80 * Model 0 reg realrinc i.age,vce(robust) The test of these indicators is significant, and they have an $R^2$ value of 0.0581. This $R^2$ value is the highest amount of variance we could hope to explain using age as a predictor. This is because this set of indicators accounts for every tiny bump and drop in the relationship between age and income. Any simpler model (for example, linear, quadratic, or piecewise) will not account for all the bumps and drops and thus will have a lower $R^2$ . However, a good model will account for the major bumps and drops and have an $R^2$ that is not too much smaller than the $R^2$ from the indicator model. This is our goal in fitting the piecewise model with multiple knots. Let’s visualize the relationship between age and income. We can do this by using the predict command to create predicted values based on the indicator model, in this case named yhatind. predict yhatind graph twoway line yhatind age,sort xlabel(18 20(5)80) graph twoway line yhatind age,xlabel(25 30 35 45 55 65) xline(25 30 35 45 55 65)sort Let’s first fit a model specified by the knots at ages 25, 30, 35, 45, 55, and 65, which we will call model 1. we will use the mkspline command to create knots at each of the ages, and this creates the variables age18to24m to age65to80m. model1 : full model* mkspline age18to24m 25 age25to29m 30 /// age30to34m 35 age35to44m 45 /// age45to54m 55 age55to64m 65 /// age65to80m=age,marginal reg realrinc age18to24m-age65to80m,vce(robust) Linear regression Number of obs = 32,100\rF(7, 32092) = 712.84\rProb \u003e F = 0.0000\rR-squared = 0.0559\rRoot MSE = 26153\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\rage18to24m | 1559.14 66.34 23.50 0.000 1429.10 1689.18\rage25to29m | -582.00 131.65 -4.42 0.000 -840.04 -323.96\rage30to34m | -45.67 188.34 -0.24 0.808 -414.82 323.48\rage35to44m | -494.46 184.93 -2.67 0.008 -856.93 -131.99\rage45to54m | -310.18 154.30 -2.01 0.044 -612.61 -7.76\rage55to64m | -827.83 190.35 -4.35 0.000 -1200.93 -454.73\rage65to80m | -4.33 214.92 -0.02 0.984 -425.59 416.93\r_cons | -25134.60 1502.49 -16.73 0.000 -28079.54 -22189.66\rWhen a coefficient regarding the change in the slope in the line segments is not significant, it indicates a knot that can be removed because the slopes before and after the knot are not significantly different. We can use this as a guide for removing superfluous knots. we can see that the slope for those who are 55 to 64 years old is similar to the slope for those who are 65 years old and older. Thus, the knot at age 65 is not really needed and we can assume one slope from ages 55 to 80. *model 2 :drop knot at age 65 drop age18to24m-age65to80m mkspline age18to24m 25 age25to29m 30 /// age30to34m 35 age35to44m 45 /// age45to54m 55 age55to80m=age,marginal reg realrinc age18to24m-age55to80m,vce(robust) Linear regression Number of obs = 32,100\rF(6, 32093) = 830.57\rProb \u003e F = 0.0000\rR-squared = 0.0559\rRoot MSE = 26152\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\rage18to24m | 1559.15 66.34 23.50 0.000 1429.11 1689.18\rage25to29m | -582.03 131.64 -4.42 0.000 -840.05 -324.01\rage30to34m | -45.57 188.23 -0.24 0.809 -414.50 323.36\rage35to44m | -494.68 184.31 -2.68 0.007 -855.94 -133.42\rage45to54m | -309.49 146.24 -2.12 0.034 -596.12 -22.87\rage55to80m | -829.99 133.15 -6.23 0.000 -1090.96 -569.01\r_cons | -25134.71 1502.46 -16.73 0.000 -28079.59 -22189.83\rI","date":"2023-12-29","objectID":"/3.continuous-predictors_piecewise-models/:7:0","tags":["Continuous predictors","stata"],"title":"Chapter4 ：Continuous predictors: Piecewise models","uri":"/3.continuous-predictors_piecewise-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"8 Automating graphs of piecewise models If you change your data (for example, fix incorrect values), the adjusted means will not be automatically updated to reflect the new data.This section illustrates a more efficient, but trickier, way of creating such graphs that does not require retyping the data. use gss_ivrm.dta mkspline ed1m 12 ed2m 16 ed3m = educ,marginal reg realrinc ed1m ed2m ed3m hsgrad cograd female,vce(robust) margins,at(ed1m=0 ed2m=0 ed3m=0 hsgrad=0 cograd=0) /// at(ed1m=1 ed2m=0 ed3m=0 hsgrad=0 cograd=0) /// at(ed1m=12 ed2m=0 ed3m=0 hsgrad=0 cograd=0) /// at(ed1m=12 ed2m=0 ed3m=0 hsgrad=1 cograd=0) /// at(ed1m=13 ed2m=1 ed3m=0 hsgrad=1 cograd=0) /// at(ed1m=16 ed2m=4 ed3m=0 hsgrad=1 cograd=0) /// at(ed1m=16 ed2m=4 ed3m=0 hsgrad=1 cograd=1) /// at(ed1m=17 ed2m=5 ed3m=1 hsgrad=1 cograd=1) /// at(ed1m=20 ed2m=8 ed3m=4 hsgrad=1 cograd=1) The following steps save the adjusted means from the margins command, as well as the corresponding values of education into the active dataset. The graph command is then used to graph the adjusted means by education, creating the graph shown below matrix yhat = r(b)' svmat yhat matrix educ = ( 0 \\ 1 \\ 12 \\ 12 \\ 13 \\ 16 \\ 16 \\ 17 \\ 20 ) svmat educ graph twoway line yhat1 educ1, xline(12 16) xtitle(Education) ytitle(Adjusted mean) Let’s walk through this process again, but do so more slowly. First, repeat the use gss_ivrm command, as well as the mkspline, regress, and margins commands from above. Now, the adjusted means computed by the margins command are stored in a matrix named r(b) with one row and nine columns, corresponding to the nine values specified with the at() option. use gss_ivrm.dta mkspline ed1m 12 ed2m 16 ed3m = educ,marginal reg realrinc ed1m ed2m ed3m hsgrad cograd female,vce(robust) margins,at(ed1m=0 ed2m=0 ed3m=0 hsgrad=0 cograd=0) /// at(ed1m=1 ed2m=0 ed3m=0 hsgrad=0 cograd=0) /// at(ed1m=12 ed2m=0 ed3m=0 hsgrad=0 cograd=0) /// at(ed1m=12 ed2m=0 ed3m=0 hsgrad=1 cograd=0) /// at(ed1m=13 ed2m=1 ed3m=0 hsgrad=1 cograd=0) /// at(ed1m=16 ed2m=4 ed3m=0 hsgrad=1 cograd=0) /// at(ed1m=16 ed2m=4 ed3m=0 hsgrad=1 cograd=1) /// at(ed1m=17 ed2m=5 ed3m=1 hsgrad=1 cograd=1) /// at(ed1m=20 ed2m=8 ed3m=4 hsgrad=1 cograd=1) matrix list r(b) r(b)[1,9]\r1. 2. 3. 4. 5. 6. 7. 8. 9.\r_at _at _at _at _at _at _at _at _at\rr1 10258.623 10530.894 13525.881 17451.464 19053.55 23859.81 29600.571 33742.824 46169.582\rLet’s store these adjusted means as a matrix called yhat and in the process transpose the matrix (converting the columns to rows). matrix yhat = r(b)' matrix list yhat yhat[9,1]\rr1\r1._at 10258.623\r2._at 10530.894\r3._at 13525.881\r4._at 17451.464\r5._at 19053.55\r6._at 23859.81\r7._at 29600.571\r8._at 33742.824\r9._at 46169.582\rWe can then save yhat into the current dataset with the svmat command. svmat yhat list yhat1 in 1/10 +----------+\r| yhat1 |\r|----------|\r1. | 10258.62 |\r2. | 10530.89 |\r3. | 13525.88 |\r4. | 17451.46 |\r5. | 19053.55 |\r6. | 23859.81 |\r7. | 29600.57 |\r8. | 33742.82 |\r9. | 46169.58 |\r10. | . |\r+----------+\rNow, let’s make a matrix containing the values of education. This is stored in the matrix named educ. We then save this matrix into the dataset, as shown below. matrix educ = (0 \\ 1 \\ 12 \\ 12 \\ 13 \\ 16 \\ 16 \\ 17 \\ 20) svmat educ list yhat1 educ1 in 1/10 +------------------+\r| yhat1 educ1 |\r|------------------|\r1. | 10258.62 0 |\r2. | 10530.89 1 |\r3. | 13525.88 12 |\r4. | 17451.46 12 |\r5. | 19053.55 13 |\r6. | 23859.81 16 |\r7. | 29600.57 16 |\r8. | 33742.82 17 |\r9. | 46169.58 20 |\r10. | . . |\r+------------------+\rNow, we can graph the adjusted means, called yhat1, by the levels of education, called educ1, as shown below. graph twoway line yhat1 educ1, xline(12 16)xtitle(Education) ytitle(Adjusted mean) Although the process of creating this graph is more complicated, the benefit is that it will automatically be updated if the dataset changes. This can be a little more work in the short run but saves us time in the long run. ","date":"2023-12-29","objectID":"/3.continuous-predictors_piecewise-models/:8:0","tags":["Continuous predictors","stata"],"title":"Chapter4 ：Continuous predictors: Piecewise models","uri":"/3.continuous-predictors_piecewise-models/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This chapter focuses on the use of polynomial terms to account for nonlinearity in the relationship between a continuous predictor and a continuous outcome.  ","date":"2023-12-28","objectID":"/2.continuous-predictors_polynomials/","tags":["Continuous predictors","stata"],"title":"Chapter3 ：Continuous predictors: Polynomials","uri":"/2.continuous-predictors_polynomials/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This Chapter focuses on how to interpret the coefficient of a continuous predictor in a linear regression model. ","date":"2023-12-28","objectID":"/2.continuous-predictors_polynomials/:0:0","tags":["Continuous predictors","stata"],"title":"Chapter3 ：Continuous predictors: Polynomials","uri":"/2.continuous-predictors_polynomials/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 Quadratic (squared) terms A quadratic (squared) term can be used to model curved relationships, accounting for one bend in the relationship between the predictor and outcome. Let’s relate the nature of the curve to the regression equation predicting income (realrinc) from age (age), shown below. $$ \\widehat{realrinc}=-30000 + 2500age + (-25age^{2}) $$ For an inverted U-shaped curve, we can compute the value of x that corresponds to the maximum value of y. If we call the linear coefficient of age and the quadratic coefficient of age b2 , then the value of age that yields the maximum income is given by -b1/(2×b2). Substituting 2,500 for b1 and -25 for b2 yields a value of 50;therefore,the maximum income occurs when someone is 50 years old. The degree of curvature (U-shape) would increase as the quadratic coefficient increases. The linear coefficient would still determine the slope when the predictor is at zero. Because the curve is U-shaped, the minimum of that curve would be represented by -b1/(2×b2), where is the linear coefficient and is the quadratic coefficient. ","date":"2023-12-28","objectID":"/2.continuous-predictors_polynomials/:1:0","tags":["Continuous predictors","stata"],"title":"Chapter3 ：Continuous predictors: Polynomials","uri":"/2.continuous-predictors_polynomials/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.1 Examples use gss_ivrm.dta keep if (age\u003c=80) Before fitting a quadratic model relating income to age, let’s assess the shape of the relationship between these variables using a locally weighted smoother. The lowess command is used to create the variable yhatlowess, which is the predicted value based on the locally weighted regression predicting income from age. Checking for nonlinearity lowess realrinc age,nograph gen(yhatlowess) line yhatlowess age,sort Let’s fit a model with a quadratic (squared) term to account for the bend in the relationship between age and income. We do this using the interaction operator ## (as shown below), which creates a term that multiplies age by age. Specifying c.age indicates to Stata that age should be treated as a continuous variable (instead of treating it as factor variable). 1.1.1 Interpreting the relationship between age and income reg realrinc c.age##c.age female,vce(robust) Note! Why not square age? You might be wondering why we do not instead use the generate command to create a new variable, say, age2, that contains the age squared. If we do this, the results of the regress command would be the same, but this would confuse the margins command. The margins command would think that age and age2 are two completely different variables. Linear regression Number of obs = 32,100\rF(3, 32096) = 1252.67\rProb \u003e F = 0.0000\rR-squared = 0.1089\rRoot MSE = 25407\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\rage | 2412.339 58.058 41.55 0.000 2298.544 2526.135\r|\rc.age#c.age | -24.202 0.696 -34.78 0.000 -25.566 -22.838\r|\rfemale | -1.24e+04 280.475 -44.28 0.000 -1.30e+04 -1.19e+04\r_cons | -2.57e+04 1038.412 -24.73 0.000 -2.77e+04 -2.36e+04\rWe can use the formula -b1/(2×b2) to compute the age at which income is at its maximum. This yields -2412.34/(2×-24.20) , which equals 49.84. The adjusted mean of income is highest for those who are 49.84 years old. Suppose we wanted to estimate the age slope for any given value of age. We can do so using the margins command combined with the dydx(age) option.Below,we obtain the age slope for ages ranging from 30 to 70 in 10-year increments. margins,at(age=(30(10)70))dydx(age)vsquish Average marginal effects Number of obs = 32,100\rModel VCE: Robust\rExpression: Linear prediction, predict()\rdy/dx wrt: age\r1._at: age = 30\r2._at: age = 40\r3._at: age = 50\r4._at: age = 60\r5._at: age = 70\r| Delta-method\r| dy/dx std. err. t P\u003e|t| [95% conf. interval]\rage |\r_at |\r1 | 960.222 18.274 52.55 0.000 924.404 996.039\r2 | 476.183 9.823 48.47 0.000 456.928 495.437\r3 | -7.856 15.700 -0.50 0.617 -38.628 22.915\r4 | -491.896 27.998 -17.57 0.000 -546.772 -437.019\r5 | -975.935 41.336 -23.61 0.000 -1056.955 -894.915\r1.1.2 Graphing adjusted means with confidence intervals The marginsplot command can be used to create a graph of the adjusted means with a shaded region showing the confidence interval for the adjusted means. We can then run the marginsplot command, adding the recast(line) and recastci(rarea) options to display the adjusted means as a line and the confidence interval as a shaded region. The resulting graph is shown in below. margins,at(age=(18(1)80)) marginsplot,recast(line) recastci(rarea) ","date":"2023-12-28","objectID":"/2.continuous-predictors_polynomials/:1:1","tags":["Continuous predictors","stata"],"title":"Chapter3 ：Continuous predictors: Polynomials","uri":"/2.continuous-predictors_polynomials/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2 Cubic (third power)terms Let’s examine the relationship between the year of birth and number of children a woman has using the gss_ivrm.dta dataset, focusing on women aged 45 to 55 born between 1920 and 1960. use gss_ivrm.dta keep if (age\u003e=45 \u0026 age\u003c=55) \u0026 (yrborn\u003e=1920 \u0026 yrborn\u003c=1960) \u0026 female==1 lowess children yrborn,gen(yhatlowess) nograph graph twoway line yhatlowess yrborn,sort Below,we fit a model predicting children from yrborn fit using a cubic term.The model specifies c.yrborn##c.yrborn##c.yrborn, which includes the cubic term for yrborn, the quadratic term for yrborn, and the linear term for yrborn. reg children c.yrborn##c.yrborn##c.yrborn,noci note: c.yrborn#c.yrborn#c.yrborn omitted because of collinearity. Source | SS df MS Number of obs = 5,049\rF(2, 5046) = 193.19\rModel | 1163.13728 2 581.56864 Prob \u003e F = 0.0000\rResidual | 15189.893 5,046 3.01028399 R-squared = 0.0711\rAdj R-squared = 0.0708\rTotal | 16353.0303 5,048 3.2395068 Root MSE = 1.735\rchildren | Coefficient Std. err. t P\u003e|t|\ryrborn | 1.431 0.816 1.75 0.079\r|\rc.yrborn#c.yrborn | -0.000 0.000 -1.81 0.071\r|\rc.yrborn#c.yrborn#c.yrborn | 0.000 (omitted)\r|\r_cons | -1344.881 791.474 -1.70 0.089\rThere was a problem running this model. There is a note saying that the cubic term was omitted because of collinearity. This is a common problem when entering cubic terms, which can be solved by centering yrborn. The dataset includes a variable called yrborn40, which is the variable yrborn centered around the year 1940 (that is, 1940 is subtracted from each value of yrborn). Let’s try fitting the above model again but instead using the variable yrborn40. reg children c.yrborn40##c.yrborn40##c.yrborn40,noci . reg children c.yrborn40##c.yrborn40##c.yrborn40,noci Source | SS df MS Number of obs = 5,049\rF(3, 5045) = 166.84\rModel | 1475.96126 3 491.987087 Prob \u003e F = 0.0000\rResidual | 14877.069 5,045 2.94887394 R-squared = 0.0903\rAdj R-squared = 0.0897\rTotal | 16353.0303 5,048 3.2395068 Root MSE = 1.7172\rchildren | Coefficient Std. err. t P\u003e|t|\ryrborn40 | -0.091 0.005 -17.34 0.000\r|\rc.yrborn40#c.yrborn40 | -0.001 0.000 -3.66 0.000\r|\rc.yrborn40#c.yrborn40#c.yrborn40 | 0.000 0.000 10.30 0.000\r|\r_cons | 2.741 0.037 73.67 0.000\rNote! Including linear, quadratic, and cubic terms When fitting this kind of a cubic model, you might find that the linear or quadratic coefficients are not significant. For the sake of parsimony, you might be tempted to omit those variables because they are not significant. However, it is essential that these terms be included in the model (even if not significant) to preserve the interpretation of the cubic term. Specifying at(yrborn40=(-20(1)20)) yields predicted means for years of birth ranging from 1920 to 1960 in one-year increments. reg children c.yrborn40##c.yrborn40##c.yrborn40,noci The marginsplot command is then used to create the graph shown in figure. The recast(line) and recastci(rarea) options display the predicted means as a solid line and the confidence interval as a shaded area. You can further explore how the yrborn40 slope varies as a function of year of birth by specifying multiple values within the at() option, as shown below. margins,at(yrborn40=(-20(5)20)) dydx(yrborn40) vsquish Conditional marginal effects Number of obs = 5,049\rModel VCE: OLS\rExpression: Linear prediction, predict()\rdy/dx wrt: yrborn40\r1._at: yrborn40 = -20\r2._at: yrborn40 = -15\r3._at: yrborn40 = -10\r4._at: yrborn40 = -5\r5._at: yrborn40 = 0\r6._at: yrborn40 = 5\r7._at: yrborn40 = 10\r8._at: yrborn40 = 15\r9._at: yrborn40 = 20\r| Delta-method\r| dy/dx std. err. t P\u003e|t| [95% conf. interval]\ryrborn40 |\r_at |\r1 | 0.191 0.023 8.34 0.000 0.146 0.236\r2 | 0.074 0.012 6.02 0.000 0.050 0.097\r3 | -0.013 0.005 -2.35 0.019 -0.023 -0.002\r4 | -0.067 0.004 -15.70 0.000 -0.076 -0.059\r5 | -0.091 0.005 -17.34 0.000 -0.101 -0.081\r6 | -0.083 0.005 -18.12 0.000 -0.092 -0.074\r7 | -0.044 0.004 -9.82 0.000 -0.052 -0.035\r8 | 0.027 0.010 2.70 0.007 0.007 0.047\r9 | 0.129 0.020 6.50 0.000 0.090 0.16","date":"2023-12-28","objectID":"/2.continuous-predictors_polynomials/:2:0","tags":["Continuous predictors","stata"],"title":"Chapter3 ：Continuous predictors: Polynomials","uri":"/2.continuous-predictors_polynomials/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.1 Fractional polynomial regression Fractional polynomial regression is more flexible by considering other kinds of power terms as well, including negative powers and fractional powers. The fp prefix automates the process of selecting the best fitting fractional polynomial model.It fits a variety of polynomial terms (and combinations of polynomial terms) and shows you the best fitting model. The default set of power terms the fp prefix will try includes -2,-1,-0.5, 0, 0.5, 1, 2, and 3 (where 0 indicates that the natural log of the predictor is used). Let’s look at this canvas of shapes, beginning with the negative powers (-2,-1,and -0.5). These models take the form of y=b*Xpower, where could be positive or negative, and could be -2,-1, or -0.5. Figure shows some possible shapes of these models, showing the powers -2,-1,-0.5 and in columns 1, 2, and 3. The graphs in the first row (with the positive coefficients) are all typified by a steep descent and then reaching a floor. The more strongly negative the power term, the stronger the descent. The second row is a vertical mirror image of the first. When the coefficient is negative, there is a sharp ascent and then a ceiling is reached. The more negative power terms are associated with a sharper ascent. The shapes of the relationship between and for the powers 1, 2, and 3 are shown in figure The first column shows a linear relationship between and . The second and third columns show the second and third power, with the top row showing a U-shaped bend and the bottom row showing an inverted U-shaped bend. The higher power terms are associated with a more rapid change in the outcome for a unit change in the predictor. the fp prefix (by default) will not only fit each of these eight powers alone, but also includes all two-way combinations of these powers. The right panel shows the formula combining these two curves, Note how this curve combines the rapid rise of the left panel with the gradual inverted U-shape of the middle panel. As you can imagine, being able to combine two different fractional polynomials can yield a flexible set of possible curve shapes for modeling your data. 2.1.1 Example using fractional polynomial regression First, we will run a regression predicting educ fromage, treating age as a categorical variable use gss_ivrm.dta keep if (age\u003c=80) reg educ i.age predict yhatmean graph twoway line yhatmean age,sort xlabel(20(5)80) Note how there is a curvilinear aspect to the relationship between age and education, suggesting that we might try including a quadratic term for educ. reg educ c.age##c.age Source | SS df MS Number of obs = 53,070\rF(2, 53067) = 1616.60\rModel | 29981.1031 2 14990.5515 Prob \u003e F = 0.0000\rResidual | 492083.501 53,067 9.27287205 R-squared = 0.0574\rAdj R-squared = 0.0574\rTotal | 522064.604 53,069 9.83746828 Root MSE = 3.0451\reduc | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\rage | 0.138 0.005 28.22 0.000 0.129 0.148\r|\rc.age#c.age | -0.002 0.000 -36.02 0.000 -0.002 -0.002\r|\r_cons | 10.763 0.107 100.14 0.000 10.553 10.974\rLet’s use the predict command to compute the fitted values from the quadratic model, naming the variable yhatq. Then, let’s graph the fitted values from the quadratic model and the mean of education by age, as shown below. predict yhatq graph twoway line yhatmean yhatq age,sort xlabel(20(5)80) Note how the quadratic fit line yields predicted values that are too high in the younger years and too low in the older years. Another way of putting this is that the quadratic line does not account for the rapid rise in education in the late teens and early 20s, nor does it account for the slow decline of education in later years. A fractional polynomial model, with its increased flexibility, could provide a more appropriate fit. We fit such a model by adding the fp prefix to the regress command, as shown below. fp \u003cage\u003e:reg educ \u003cage\u003e Fractional polynomial comparisons:\r| Test Residual Deviance\rage | df Deviance std. dev. diff. P Powers","date":"2023-12-28","objectID":"/2.continuous-predictors_polynomials/:2:1","tags":["Continuous predictors","stata"],"title":"Chapter3 ：Continuous predictors: Polynomials","uri":"/2.continuous-predictors_polynomials/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.2 Main effects with polynomial terms The meaning of main effects changes when polynomial terms are included in the model. In fact, the inclusion of polynomial terms can substantially change the coefficient for the main effect when compared with the main effect–only model. use gss_ivrm.dta keep if (age\u003c=80) reg realrinc age female,vce(robust) Linear regression Number of obs = 32,100\rF(2, 32097) = 1170.51\rProb \u003e F = 0.0000\rR-squared = 0.0787\rRoot MSE = 25833\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\rage | 320.07 10.70 29.93 0.000 299.10 341.03\rfemale | -12373.25 285.03 -43.41 0.000 -12931.91 -11814.59\r_cons | 15106.17 403.04 37.48 0.000 14316.19 15896.16\rHowever, there is a problem. As we saw in section 1.1, the relationship between income and age is not linear. As we did in that section, let’s add a quadratic term for age, as shown below. reg realrinc c.age##c.age female,vce(robust) Linear regression Number of obs = 32,100\rF(3, 32096) = 1252.67\rProb \u003e F = 0.0000\rR-squared = 0.1089\rRoot MSE = 25407\r| Robust\rrealrinc | Coefficient std. err. t P\u003e|t| [95% conf. interval]\rage | 2412.34 58.06 41.55 0.000 2298.54 2526.13\r|\rc.age#c.age | -24.20 0.70 -34.78 0.000 -25.57 -22.84\r|\rfemale | -12419.24 280.47 -44.28 0.000 -12968.98 -11869.49\r_cons | -25679.77 1038.41 -24.73 0.000 -27715.10 -23644.44\rThe quadratic term is significant, but we might suddenly become concerned that the main effect of age has skyrocketed from 320.07 in the linear model to 2,412.34 in the quadratic model. Why did the main effect change so much? Did we do something wrong? The key is that the term “main effect” is really a misnomer, because we expect this term to describe the general trend of the relationship between income and age. This value is meaningless for two reasons. First, nobody has income when they are zero years old. Second, this term no longer reflects the general trend. As we saw in section 1.1, the age slope changes for every level of age, so there is no such thing as a measure of general trend in this kind of model. ","date":"2023-12-28","objectID":"/2.continuous-predictors_polynomials/:2:2","tags":["Continuous predictors","stata"],"title":"Chapter3 ：Continuous predictors: Polynomials","uri":"/2.continuous-predictors_polynomials/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This Chapter focuses on how interpret the coefficient of a continuous predictor in a linear regression models. ","date":"2023-12-09","objectID":"/1.continuous-predictors_linear/","tags":["Continuous predictors","stata"],"title":"Chapter2 ：Continuous predictors:Linear","uri":"/1.continuous-predictors_linear/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"This Chapter focuses on how to interpret the coefficient of a continuous predictor in a linear regression model. ","date":"2023-12-09","objectID":"/1.continuous-predictors_linear/:0:0","tags":["Continuous predictors","stata"],"title":"Chapter2 ：Continuous predictors:Linear","uri":"/1.continuous-predictors_linear/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1 Simple linear regression This section illustrates the use of a continuous predictor for predicting a continuous outcome using ordinary least-squares regression. Terminology: Continuous and categorical variables When I use the termcontinuous variable, I am referring to a variable that is measured on an interval or ratio scale. By contrast, when I speak of a categorical variable,I am referring to either a nominal variable or an ordinal/interval/ratio variable that we wish to treat as though it were a nominal variable. Let’s run a simple regression model in which we predict the education of the respondent from the education of the respondent’s father. regress educ paeduc The result as shown below Source | SS df MS Number of obs = 696\rF(1, 694) = 228.14\rModel | 1649.70181 1 1649.70181 Prob \u003e F = 0.0000\rResidual | 5018.43038 694 7.23116769 R-squared = 0.2474\rAdj R-squared = 0.2463\rTotal | 6668.13218 695 9.5944348 Root MSE = 2.6891\reduc | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\rpaeduc | 0.359 0.024 15.10 0.000 0.313 0.406\r_cons | 9.740 0.286 34.08 0.000 9.179 10.301\rThe regression equation can be written as shown below $$ \\widehat{educ}=9.74 + 0.36paeduc $$ The intercept is the predicted mean of the respondent’s education when the father’s education is 0. For every one-year increase in the education of the father, we would predict that the education of the respondent increases by 0.36 years. ","date":"2023-12-09","objectID":"/1.continuous-predictors_linear/:1:0","tags":["Continuous predictors","stata"],"title":"Chapter2 ：Continuous predictors:Linear","uri":"/1.continuous-predictors_linear/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.1 Computing predicted means using the margins command Suppose we wanted to compute the predicted mean of education for the respondent, assuming separately that the father had 8, 12, or 16 years of education. margins,at (paeduc=(8 12 16)) vsquish Note: The vsquish option The vsquish option vertically squishes the output by omitting extra blank lines. Adjusted predictions Number of obs = 696\rModel VCE: OLS\rExpression: Linear prediction, predict()\r1._at: paeduc = 8\r2._at: paeduc = 12\r3._at: paeduc = 16\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r_at |\r1 | 12.616 0.128 98.93 0.000 12.365 12.866\r2 | 14.053 0.104 135.64 0.000 13.850 14.257\r3 | 15.491 0.153 101.42 0.000 15.191 15.791\rSometimes, we might want to compute the predicted means given a range of values for a predictor. For example, we might want to compute the predicted means when father’s education is 0, 4, 8, 12, 16, and 20. margins,at(paeduc=(0(4)20)) vsquish Rather than typing all of these values, we can specify 0(4)20, which tells Stata that we mean 0 to 20 in 4-unit increments. Adjusted predictions Number of obs = 696\rModel VCE: OLS\rExpression: Linear prediction, predict()\r1._at: paeduc = 0\r2._at: paeduc = 4\r3._at: paeduc = 8\r4._at: paeduc = 12\r5._at: paeduc = 16\r6._at: paeduc = 20\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r_at |\r1 | 9.740 0.286 34.08 0.000 9.179 10.301\r2 | 11.178 0.200 55.95 0.000 10.786 11.570\r3 | 12.616 0.128 98.93 0.000 12.365 12.866\r4 | 14.053 0.104 135.64 0.000 13.850 14.257\r5 | 15.491 0.153 101.42 0.000 15.191 15.791\r6 | 16.929 0.232 72.82 0.000 16.472 17.385\r","date":"2023-12-09","objectID":"/1.continuous-predictors_linear/:1:1","tags":["Continuous predictors","stata"],"title":"Chapter2 ：Continuous predictors:Linear","uri":"/1.continuous-predictors_linear/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"1.2 Graphing predicted means using the marginsplot command We can use the marginsplot command to create a graph showing the predicted means and confidence intervals based on the most recent margins command. marginsplot Note：The margins and marginsplot commands work together as a team. Let’s now create a graph that shows the fitted line with a shaded confidence interval. margins,at(paeduc=(0(1)20)) marginsplot,recast(line) recastci(rarea) The recast() option specifies that the fitted line should be displayed as a line graph (suppressing the markers). The recastci() option specifies that the confidence interval should be displayed as an rarea graph, displaying a shaded area for the confidence region. ","date":"2023-12-09","objectID":"/1.continuous-predictors_linear/:1:2","tags":["Continuous predictors","stata"],"title":"Chapter2 ：Continuous predictors:Linear","uri":"/1.continuous-predictors_linear/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2 Multiple regression Let’s now turn to a multiple regression model that predicts the respondent’s education from the father’s education (paeduc), the mother’s education (maeduc), and the age of the respondent (age). reg educ paeduc maeduc age Source | SS df MS Number of obs = 650\rF(3, 646) = 92.93\rModel | 1822.26082 3 607.420272 Prob \u003e F = 0.0000\rResidual | 4222.37918 646 6.53619069 R-squared = 0.3015\rAdj R-squared = 0.2982\rTotal | 6044.64 649 9.31377504 Root MSE = 2.5566\reduc | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\rpaeduc | 0.258 0.033 7.82 0.000 0.193 0.323\rmaeduc | 0.208 0.038 5.48 0.000 0.133 0.282\rage | 0.034 0.007 5.28 0.000 0.022 0.047\r_cons | 6.962 0.511 13.63 0.000 5.959 7.965\rThe equation as shown below $$ \\widehat{educ}=6.86 + 0.26paeduc + 0.21maeduc + 0.03age $$ The coefficients from this multiple regression model reflect the association between each predictor and the outcome after adjusting for all the other predictors. ","date":"2023-12-09","objectID":"/1.continuous-predictors_linear/:2:0","tags":["Continuous predictors","stata"],"title":"Chapter2 ：Continuous predictors:Linear","uri":"/1.continuous-predictors_linear/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"2.1 Computing adjusted means using the margins command The margins command allows us to hold more than one variable constant at a time. In the example below, we compute the adjusted means when the father’s education equals 8, 12, and 16, while holding the mother’s education constant at 14. reg educ paeduc maeduc age Predictive margins Number of obs = 650\rModel VCE: OLS\rExpression: Linear prediction, predict()\r1._at: paeduc = 8\rmaeduc = 14\r2._at: paeduc = 12\rmaeduc = 14\r3._at: paeduc = 16\rmaeduc = 14\r| Delta-method\r| Margin std. err. t P\u003e|t| [95% conf. interval]\r_at |\r1 | 13.544 0.211 64.18 0.000 13.130 13.958\r2 | 14.576 0.128 113.76 0.000 14.325 14.828\r3 | 15.609 0.152 102.52 0.000 15.310 15.908\rTerminology: Adjusted means For example, we can say that the predicted mean, given the father has 8 years of education, is 13.544 after adjusting for all other predictors. We could also call this an adjusted mean. The term adjusted mean implies after adjusting for all other predictors in the model. When using nonlinear models (such as a logistic regression model), we will use a more general term, such as predictive margin. ","date":"2023-12-09","objectID":"/1.continuous-predictors_linear/:2:1","tags":["Continuous predictors","stata"],"title":"Chapter2 ：Continuous predictors:Linear","uri":"/1.continuous-predictors_linear/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3 Checking for nonlinearity graphically This section illustrates graphical approaches for checking for nonlinearity in the relationship between a predictor and outcome variable. These approaches include examining scatterplots of the predictor and outcome. examining residual-versusfitted plots. creating plots based on locally weighted smoothers. plotting the mean of the outcome for each level of the predictor. ","date":"2023-12-09","objectID":"/1.continuous-predictors_linear/:3:0","tags":["Continuous predictors","stata"],"title":"Chapter2 ：Continuous predictors:Linear","uri":"/1.continuous-predictors_linear/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.1 Using scatterplots to check for nonlinearity Let’s look at a scatterplot of the size of the engine (displacement) by length of the car (length) with a line showing the linear fit, as shown in figure use autosubset graph twoway (scatter displacement length) (lfit displacement length),ytitle(\"Engine displacement(cu in.) \") legend (off) The relationship between these two variables looks fairly linear, but the addition of the linear fit line helps us to see the nonlinearity. Note how for short cars (whenlength is below 160) the fit line underpredicts and for longer cars (when length is above 210) the fit line also underpredicts. Using a scatterplot like this can be a simple means of looking at the linearity of the simple relationship between a predictor and outcome variable. However, this does not account for other predictors that you might want to include in a model. To this end, let’s next look at how we can use the residuals for checking linearity ","date":"2023-12-09","objectID":"/1.continuous-predictors_linear/:3:1","tags":["Continuous predictors","stata"],"title":"Chapter2 ：Continuous predictors:Linear","uri":"/1.continuous-predictors_linear/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.2 Checking for nonlinearity using residuals For example, let’s run a regression predicting displacement from length, trunk, and weight, as shown below reg displacement length trunk weight We can then look at the residuals versus the fitted values, as shown in figure rvfplot Note the U-shaped pattern of the residuals. This pattern suggests that the relationship between the predictors and outcome is not linear. There are many excellent resources that illustrate Stata’s regression diagnostic tools, including the manual entry for [R] regress postestimation. You can also see the help entries for avplot, rvfplot, and rvpplot. ","date":"2023-12-09","objectID":"/1.continuous-predictors_linear/:3:2","tags":["Continuous predictors","stata"],"title":"Chapter2 ：Continuous predictors:Linear","uri":"/1.continuous-predictors_linear/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.3 Checking for nonlinearity using locally weighted smoother With larger datasets, it can be harder to visualize nonlinearity using scatterplots or residual-versus-fitted plots. Suppose we want to determine the nature of the relationship between the year that the respondent was born (yrborn) and education level (educ). use gss_ivrm.dta scatter educ yrborn,msymbol(oh) It is hard to discern the nature of the relationship between year of birth and education using this scatterplot. With so many observations, the scatterplot is saturated with data points creating one big blotch that tells us little about the shape of the relationship between the predictor and outcome The lowess command below creates a graph showing the locally weighted regression of education on year of birth, as shown in figure lowess educ yrborn,msymbol(p) The lowess graph suggests that there is nonlinearity in the relationship between year of birth and education. Education increases with year of birth until the 1950s, at which point the smoothed education values level out and then start to decline. The graph produced by the lowess command is much more informative than the scatterplot alone. ","date":"2023-12-09","objectID":"/1.continuous-predictors_linear/:3:3","tags":["Continuous predictors","stata"],"title":"Chapter2 ：Continuous predictors:Linear","uri":"/1.continuous-predictors_linear/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"3.4 Graphing outcome mean at each level of predictor Another way to visualize the relationship between the predictor and outcome is to create a graph showing the mean of the outcome at each level of the predictor. Using the example predicting education from year of birth means creating a graph of the average of education at each level of year of birth. Although the variable yrborn can assume many values (from 1883 to 1990), the variable is composed of discrete integers with reasonably many observations (usually more than 100) for each value. In such a case, we can explore the nature of the relationship between the predictor and outcome by creating a graph of the mean of the outcome variable (education level) for each level of the predictor (year of birth). This kind of graph imposes no structure on the shape of the relationship between year of birth and education and allows us to observe the nature of the relationship between the predictor and the outcome. One simple way to create such a graph is to fit a regression model predicting the outcome treating the predictor variable as a factor variable. Following that, the margins command is used to obtain the predicted mean of the outcome for each level of the predictor. In the regress command below, specifying i.yrborn indicates that the variable yrborn should be treated as a factor variable. The following margins command computes the predicted mean of the outcome (education) at each year of birth reg educ i.yrborn margins yrborn marginsplot The predicted means vary erratically for the years before 1900 because of the few observations per year during those years. For these years, the confidence intervals are much wider compared with later years, reflecting greater uncertainty of the estimates because of fewer observations. If all the years had such few observations, then the entire graph might be dominated by wild swings in the means and show little about the nature of the relationship between the predictor and outcome. ","date":"2023-12-09","objectID":"/1.continuous-predictors_linear/:3:4","tags":["Continuous predictors","stata"],"title":"Chapter2 ：Continuous predictors:Linear","uri":"/1.continuous-predictors_linear/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4 Checking for nonlinearity analytically This section shows how to check for nonlinearity using analytic approaches, including adding power terms and using factor variables. ","date":"2023-12-09","objectID":"/1.continuous-predictors_linear/:4:0","tags":["Continuous predictors","stata"],"title":"Chapter2 ：Continuous predictors:Linear","uri":"/1.continuous-predictors_linear/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4.1 Adding power terms Another way to check for nonlinearity of a continuous variable is to add power terms (for example, quadratic or cubic). Using the example with education as a function of year of birth. Let’s introduce a quadratic term (in addition to the linear term) by adding c.yrborn#c.yrborn to the model. This introduces a quadratic effect that would account for one bend in the line relating year of birth to education. We would expect the quadratic term to be significant based on the graphs we saw in figures use gss_ivrm.dta Source | SS df MS Number of obs = 54,745\rF(2, 54742) = 3567.49\rModel | 63778.2779 2 31889.1389 Prob \u003e F = 0.0000\rResidual | 489328.985 54,742 8.93882184 R-squared = 0.1153\rAdj R-squared = 0.1153\rTotal | 553107.263 54,744 10.103523 Root MSE = 2.9898\reduc | Coefficient Std. err. t P\u003e|t|\ryrborn | 4.218 0.102 41.46 0.000\r|\rc.yrborn#c.yrborn | -0.001 0.000 -41.00 0.000\r|\r_cons | -4129.000 98.816 -41.78 0.000\rNote! Using the noci option for clearer output When models include interactions,those labels can get rather wide, and Stata is forced to display those labels across multiple lines, which can make the output confusing and hard to read. This omits the display of the confidence intervals, which makes enough room to display the label for every term in the model in a single line. but!,the confidence interval are also important for our research. The quadratic term is significant in this model.Furthermore, the R2 for this model increased to 0.1153 compared with 0.0881 for the linear model. This provides analytic support for including a quadratic term for year of birth when predicting education. A cubic term would imply that the line fitting year of birth and education has a tendency to have two bends in it.c.yrborn##c.yrborn##c.yrborn as a predictor is a shorthand for including the linear, quadratic, and cubic terms for year of birth (yrborn). reg educ c.yrborn##c.yrborn##c.yrborn,noci note: c.yrborn#c.yrborn#c.yrborn omitted because of collinearity. Source | SS df MS Number of obs = 926\rF(2, 923) = 11.47\rModel | 219.843988 2 109.921994 Prob \u003e F = 0.0000\rResidual | 8846.14737 923 9.584125 R-squared = 0.0242\rAdj R-squared = 0.0221\rTotal | 9065.99136 925 9.80107174 Root MSE = 3.0958\reduc | Coefficient Std. err. t P\u003e|t|\ryrborn | 6.159 1.287 4.79 0.000\r|\rc.yrborn#c.yrborn | -0.002 0.000 -4.79 0.000\r|\rc.yrborn#c.yrborn#c.yrborn | 0.000 (omitted)\r|\r_cons | -6015.618 1259.800 -4.78 0.000\rThere was a problem running this model. There is a note saying that the cubic term was omitted because of collinearity. This is a common problem when entering cubic terms, which can be solved by centering yrborn. The dataset includes a variable called yrborn40, which is the variable yrborn centered around the year 1940 (that is, 1940 is subtracted from each value of yrborn). reg educ c.yrborn40##c.yrborn40##c.yrborn40,noci Source | SS df MS Number of obs = 926\rF(3, 922) = 7.72\rModel | 222.175819 3 74.0586064 Prob \u003e F = 0.0000\rResidual | 8843.81554 922 9.59199083 R-squared = 0.0245\rAdj R-squared = 0.0213\rTotal | 9065.99136 925 9.80107174 Root MSE = 3.0971\reduc | Coefficient Std. err. t P\u003e|t|\ryrborn40 | 0.058 0.014 4.20 0.000\r|\rc.yrborn40#c.yrborn40 | -0.002 0.001 -2.14 0.033\r|\rc.yrborn40#c.yrborn40#c.yrborn40 | 0.000 0.000 0.49 0.622\r|\r_cons | 13.427 0.201 66.87 0.000\rThe results show, as expected, that the cubic term is significant. However, this dataset has many observations, so the model has the statistical power to detect very small effects. Note how the is 0.1162 for the cubic model compared with 0.1153 for the quadratic model. This is a trivial increase, suggesting that the cubic trend is not really an important term to include in this model. ","date":"2023-12-09","objectID":"/1.continuous-predictors_linear/:4:1","tags":["Continuous predictors","stata"],"title":"Chapter2 ：Continuous predictors:Linear","uri":"/1.continuous-predictors_linear/"},{"categories":["Interpreting and Visualizing Regression Models Using Stata"],"content":"4.2 Using factor variables The strategy of using factor variables to check for nonlinearity makes sense only when you have a relatively limited number of levels of the predictor that are coded as whole numbers. let’s perform an analysis to detect any kind of nonlinearity in the relationship between decade of age and health status. use gss_ivrm.dta reg health c.agedec i.agedec note: 8.agedec omitted because of collinearity. Source | SS df MS Number of obs = 40,984\rF(7, 40976) = 451.59\rModel | 2111.10121 7 301.585887 Prob \u003e F = 0.0000\rResidual | 27364.9308 40,976 .667828261 R-squared = 0.0716\rAdj R-squared = 0.0715\rTotal | 29476.032 40,983 .719225826 Root MSE = .81721\rhealth | Coefficient Std. err. t P\u003e|t| [95% conf. interval]\ragedec | -0.098 0.005 -18.46 0.000 -0.108 -0.087\r|\ragedec |\r20s | 0.110 0.028 3.97 0.000 0.055 0.164\r30s | 0.165 0.024 6.85 0.000 0.118 0.212\r40s | 0.135 0.022 6.24 0.000 0.093 0.178\r50s | 0.065 0.021 3.15 0.002 0.025 0.106\r60s | -0.006 0.021 -0.28 0.783 -0.047 0.036\r70s | -0.038 0.024 -1.60 0.110 -0.084 0.009\r80s | 0.000 (omitted)\r|\r_cons | 3.320 0.035 95.81 0.000 3.253 3.388\rThis unconventional-looking model divides the relationship between the age decade and the outcome into two pieces: the linear relationship, which is accounted for by c.agedec any remaining nonlinear components, which are explained by the indicator variables specified by i.agedec. Let’s now use the testparm command to perform a test of the indicator variables, giving us an overall test of the nonlinearity in the relationship between age decade and health status. testparm i.agedec (1) 2.agedec = 0\r(2) 3.agedec = 0\r(3) 4.agedec = 0\r(4) 5.agedec = 0\r(5) 6.agedec = 0\r(6) 7.agedec = 0\rF( 6, 40976) = 20.61\rProb \u003e F = 0.0000\rThis general strategy tells us that there is nonlinearity between the age decade and health status but does not pinpoint the exact nature of the nonlinearity. Let’s try another strategy that will pinpoint the nature of the nonlinearity. And use the contrast command with the p. contrast operator to obtain a detailed breakdown of possible nonlinear trends in the relationship between the age decade and health status reg health i.agedec contrast p.agedec Contrasts of marginal linear predictions Margins: asbalanced | df F P\u003eF\ragedec |\r(linear) | 1 1187.58 0.0000\r(quadratic) | 1 26.65 0.0000\r(cubic) | 1 52.99 0.0000\r(quartic) | 1 1.18 0.2778\r(quintic) | 1 1.00 0.3179\r(sextic) | 1 0.15 0.6959\r(septic) | 1 0.02 0.8748\rJoint | 7 451.59 0.0000\r|\rDenominator | 40976\r| Contrast Std. err. [95% conf. interval]\ragedec |\r(linear) | -0.260 0.008 -0.275 -0.245\r(quadratic) | -0.038 0.007 -0.053 -0.024\r(cubic) | 0.047 0.006 0.034 0.059\r(quartic) | 0.006 0.005 -0.005 0.016\r(quintic) | -0.004 0.004 -0.013 0.004\r(sextic) | 0.001 0.004 -0.006 0.009\r(septic) | -0.001 0.004 -0.008 0.006\r","date":"2023-12-09","objectID":"/1.continuous-predictors_linear/:4:2","tags":["Continuous predictors","stata"],"title":"Chapter2 ：Continuous predictors:Linear","uri":"/1.continuous-predictors_linear/"},{"categories":["documentation"],"content":"探索 Hugo - LoveIt 主题的全部内容和背后的核心概念.","date":"2020-03-06","objectID":"/theme-documentation-basics/","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"探索 Hugo - LoveIt 主题的全部内容和背后的核心概念. ","date":"2020-03-06","objectID":"/theme-documentation-basics/:0:0","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"1 准备 由于 Hugo 提供的便利性, Hugo 本身是这个主题唯一的依赖. 直接安装满足你操作系统 (Windows, Linux, macOS) 的最新版本  Hugo (\u003e 0.62.0). 为什么不支持早期版本的 Hugo?\r由于 Markdown 渲染钩子函数 在 Hugo 圣诞节版本 中被引入, 本主题只支持高于 0.62.0 的 Hugo 版本.\r推荐使用 Hugo extended 版本\r由于这个主题的一些特性需要将  SCSS 转换为  CSS, 推荐使用 Hugo extended 版本来获得更好的使用体验.\r","date":"2020-03-06","objectID":"/theme-documentation-basics/:1:0","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"2 安装 以下步骤可帮助你初始化新网站. 如果你根本不了解 Hugo, 我们强烈建议你按照此 快速入门文档 进一步了解它. ","date":"2020-03-06","objectID":"/theme-documentation-basics/:2:0","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"2.1 创建你的项目 Hugo 提供了一个 new 命令来创建一个新的网站: hugo new site my_website cd my_website ","date":"2020-03-06","objectID":"/theme-documentation-basics/:2:1","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"2.2 安装主题 LoveIt 主题的仓库是: https://github.com/dillonzq/LoveIt. 你可以下载主题的 最新版本  .zip 文件 并且解压放到 themes 目录. 另外, 也可以直接把这个主题克隆到 themes 目录: git clone https://github.com/dillonzq/LoveIt.git themes/LoveIt 或者, 初始化你的项目目录为 git 仓库, 并且把主题仓库作为你的网站目录的子模块: git init git submodule add https://github.com/dillonzq/LoveIt.git themes/LoveIt ","date":"2020-03-06","objectID":"/theme-documentation-basics/:2:2","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"2.3 基础配置 以下是 LoveIt 主题的基本配置: baseURL = \"http://example.org/\" # 更改使用 Hugo 构建网站时使用的默认主题 theme = \"LoveIt\" # 网站标题 title = \"我的全新 Hugo 网站\" # 网站语言, 仅在这里 CN 大写 [\"en\", \"zh-CN\", \"fr\", \"pl\", ...] languageCode = \"zh-CN\" # 语言名称 [\"English\", \"简体中文\", \"Français\", \"Polski\", ...] languageName = \"简体中文\" # 是否包括中日韩文字 hasCJKLanguage = true # 作者配置 [author] name = \"xxxx\" email = \"\" link = \"\" # 菜单配置 [menu] [[menu.main]] weight = 1 identifier = \"posts\" # 你可以在名称 (允许 HTML 格式) 之前添加其他信息, 例如图标 pre = \"\" # 你可以在名称 (允许 HTML 格式) 之后添加其他信息, 例如图标 post = \"\" name = \"文章\" url = \"/posts/\" # 当你将鼠标悬停在此菜单链接上时, 将显示的标题 title = \"\" [[menu.main]] weight = 2 identifier = \"tags\" pre = \"\" post = \"\" name = \"标签\" url = \"/tags/\" title = \"\" [[menu.main]] weight = 3 identifier = \"categories\" pre = \"\" post = \"\" name = \"分类\" url = \"/categories/\" title = \"\" # Hugo 解析文档的配置 [markup] # 语法高亮设置 (https://gohugo.io/content-management/syntax-highlighting) [markup.highlight] # false 是必要的设置 (https://github.com/dillonzq/LoveIt/issues/158) noClasses = false 注意\r在构建网站时, 你可以使用 --theme 选项设置主题. 但是, 我建议你修改配置文件 (config.toml) 将本主题设置为默认主题.\r","date":"2020-03-06","objectID":"/theme-documentation-basics/:2:3","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"2.4 创建你的第一篇文章 以下是创建第一篇文章的方法: hugo new posts/first_post.md 通过添加一些示例内容并替换文件开头的标题, 你可以随意编辑文章. 注意\r默认情况下, 所有文章和页面均作为草稿创建. 如果想要渲染这些页面, 请从元数据中删除属性 draft: true, 设置属性 draft: false 或者为 hugo 命令添加 -D/--buildDrafts 参数.\r","date":"2020-03-06","objectID":"/theme-documentation-basics/:2:4","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"2.5 在本地启动网站 使用以下命令启动网站: hugo serve 去查看 http://localhost:1313. 基本配置下的预览\r技巧\r当你运行 hugo serve 时, 当文件内容更改时, 页面会随着更改自动刷新.\r注意\r由于本主题使用了 Hugo 中的 .Scratch 来实现一些特性, 非常建议你为 hugo server 命令添加 --disableFastRender 参数来实时预览你正在编辑的文章页面. hugo serve --disableFastRender ","date":"2020-03-06","objectID":"/theme-documentation-basics/:2:5","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"2.6 构建网站 当你准备好部署你的网站时, 运行以下命令: hugo 会生成一个 public 目录, 其中包含你网站的所有静态内容和资源. 现在可以将其部署在任何 Web 服务器上. 技巧\r网站内容可以通过 Netlify 自动发布和托管 (了解有关通过 Netlify 进行 HUGO 自动化部署 的更多信息). 或者, 您可以使用 AWS Amplify, Github pages, Render 以及更多…\r","date":"2020-03-06","objectID":"/theme-documentation-basics/:2:6","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"3 配置 ","date":"2020-03-06","objectID":"/theme-documentation-basics/:3:0","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"3.1 网站配置 除了 Hugo 全局配置 和 菜单配置 之外, LoveIt 主题还允许您在网站配置中定义以下参数 (这是一个示例 config.toml, 其内容为默认值). 请打开下面的代码块查看完整的示例配置 : baseURL = \"http://example.org/\" # 更改使用 Hugo 构建网站时使用的默认主题 theme = \"LoveIt\" # 网站标题 title = \"我的全新 Hugo 网站\" # 网站语言, 仅在这里 CN 大写 [\"en\", \"zh-CN\", \"fr\", \"pl\", ...] languageCode = \"zh-CN\" # 语言名称 [\"English\", \"简体中文\", \"Français\", \"Polski\", ...] languageName = \"简体中文\" # 是否包括中日韩文字 hasCJKLanguage = true # 默认每页列表显示的文章数目 paginate = 12 # 谷歌分析代号 [UA-XXXXXXXX-X] googleAnalytics = \"\" # 版权描述，仅仅用于 SEO copyright = \"\" # 是否使用 robots.txt enableRobotsTXT = true # 是否使用 git 信息 enableGitInfo = true # 是否使用 emoji 代码 enableEmoji = true # 忽略一些构建错误 ignoreErrors = [\"error-remote-getjson\", \"error-missing-instagram-accesstoken\"] # 作者配置 [author] name = \"xxxx\" email = \"\" link = \"\" # 菜单配置 [menu] [[menu.main]] weight = 1 identifier = \"posts\" # 你可以在名称 (允许 HTML 格式) 之前添加其他信息, 例如图标 pre = \"\" # 你可以在名称 (允许 HTML 格式) 之后添加其他信息, 例如图标 post = \"\" name = \"文章\" url = \"/posts/\" # 当你将鼠标悬停在此菜单链接上时, 将显示的标题 title = \"\" [[menu.main]] weight = 2 identifier = \"tags\" pre = \"\" post = \"\" name = \"标签\" url = \"/tags/\" title = \"\" [[menu.main]] weight = 3 identifier = \"categories\" pre = \"\" post = \"\" name = \"分类\" url = \"/categories/\" title = \"\" [params] # 网站默认主题样式 [\"auto\", \"light\", \"dark\"] defaultTheme = \"auto\" # 公共 git 仓库路径，仅在 enableGitInfo 设为 true 时有效 gitRepo = \"\" # 哪种哈希函数用来 SRI, 为空时表示不使用 SRI # [\"sha256\", \"sha384\", \"sha512\", \"md5\"] fingerprint = \"\" # 日期格式 dateFormat = \"2006-01-02\" # 网站标题, 用于 Open Graph 和 Twitter Cards title = \"我的网站\" # 网站描述, 用于 RSS, SEO, Open Graph 和 Twitter Cards description = \"这是我的全新 Hugo 网站\" # 网站图片, 用于 Open Graph 和 Twitter Cards images = [\"/logo.png\"] # 页面头部导航栏配置 [params.header] # 桌面端导航栏模式 [\"fixed\", \"normal\", \"auto\"] desktopMode = \"fixed\" # 移动端导航栏模式 [\"fixed\", \"normal\", \"auto\"] mobileMode = \"auto\" # 页面头部导航栏标题配置 [params.header.title] # LOGO 的 URL logo = \"\" # 标题名称 name = \"\" # 你可以在名称 (允许 HTML 格式) 之前添加其他信息, 例如图标 pre = \"\" # 你可以在名称 (允许 HTML 格式) 之后添加其他信息, 例如图标 post = \"\" # 是否为标题显示打字机动画 typeit = false # 页面底部信息配置 [params.footer] enable = true # 自定义内容 (支持 HTML 格式) custom = '' # 是否显示 Hugo 和主题信息 hugo = true # 是否显示版权信息 copyright = true # 是否显示作者 author = true # 网站创立年份 since = 2019 # ICP 备案信息，仅在中国使用 (支持 HTML 格式) icp = \"\" # 许可协议信息 (支持 HTML 格式) license = '\u003ca rel=\"license external nofollow noopener noreffer\" href=\"https://creativecommons.org/licenses/by-nc/4.0/\" target=\"_blank\"\u003eCC BY-NC 4.0\u003c/a\u003e' # Section (所有文章) 页面配置 [params.section] # section 页面每页显示文章数量 paginate = 20 # 日期格式 (月和日) dateFormat = \"01-02\" # RSS 文章数目 rss = 10 # List (目录或标签) 页面配置 [params.list] # list 页面每页显示文章数量 paginate = 20 # 日期格式 (月和日) dateFormat = \"01-02\" # RSS 文章数目 rss = 10 # 应用图标配置 [params.app] # 当添加到 iOS 主屏幕或者 Android 启动器时的标题, 覆盖默认标题 title = \"我的网站\" # 是否隐藏网站图标资源链接 noFavicon = false # 更现代的 SVG 网站图标, 可替代旧的 .png 和 .ico 文件 svgFavicon = \"\" # Android 浏览器主题色 themeColor = \"#ffffff\" # Safari 图标颜色 iconColor = \"#5bbad5\" # Windows v8-10磁贴颜色 tileColor = \"#da532c\" # 搜索配置 [params.search] enable = true # 搜索引擎的类型 [\"lunr\", \"algolia\"] type = \"lunr\" # 文章内容最长索引长度 contentLength = 4000 # 搜索框的占位提示语 placeholder = \"\" # 最大结果数目 maxResultLength = 10 # 结果内容片段长度 snippetLength = 50 # 搜索结果中高亮部分的 HTML 标签 highlightTag = \"em\" # 是否在搜索索引中使用基于 baseURL 的绝对路径 absoluteURL = false [params.search.algolia] index = \"\" appID = \"\" searchKey = \"\" # 主页配置 [params.home] # RSS 文章数目 rss = 10 # 主页个人信息 [params.home.profile] enable = true # Gravatar 邮箱，用于优先在主页显示的头像 gravatarEmail = \"\" # 主页显示头像的 URL avatarURL = \"/images/avatar.png\" # 主页显示的网站标题 (支持 HTML 格式) title = \"\" # 主页显示的网站副标题 (允许 HTML 格式) subtitle = \"这是我的全新 Hugo 网站\" # 是否为副标题显示打字机动画 typeit = true # 是否显示社交账号 social = true # 免责声明 (支持 HTML 格式) disclaimer = \"\" # 主页文章列表 [params.home.posts] enable = true # 主页每页显示文章数量 paginate = 6 # 被 params.page 中的 hiddenFromHomePage 替代 # 当你没有在文章前置参数中设置 \"hiddenFromHomePage\" 时的默认行为 defaultHiddenFromHomePage = false # 作者的社交信息设置 [params.social] GitHub = \"xxxx\" Linkedin = \"\" Twitter = \"xxxx\" Instagram = \"xxxx\" Facebook = \"xxxx\" Telegram = \"xxxx\" Medium = \"\" Gitlab = \"\" Youtubelegacy = \"\" Youtubecustom = \"\" Youtu","date":"2020-03-06","objectID":"/theme-documentation-basics/:3:1","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"3.2 网站图标, 浏览器配置, 网站清单 强烈建议你把: apple-touch-icon.png (180x180) favicon-32x32.png (32x32) favicon-16x16.png (16x16) mstile-150x150.png (150x150) android-chrome-192x192.png (192x192) android-chrome-512x512.png (512x512) 放在 /static 目录. 利用 https://realfavicongenerator.net/ 可以很容易地生成这些文件. 可以自定义 browserconfig.xml 和 site.webmanifest 文件来设置 theme-color 和 background-color. ","date":"2020-03-06","objectID":"/theme-documentation-basics/:3:2","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"3.3 自定义样式 注意\rHugo extended 版本对于自定义样式是必需的.\r通过定义自定义 .scss 样式文件, LoveIt 主题支持可配置的样式. 包含自定义 .scss 样式文件的目录相对于 你的项目根目录 的路径为 assets/css. 在 assets/css/_override.scss 中, 你可以覆盖 themes/LoveIt/assets/css/_variables.scss 中的变量以自定义样式. 这是一个例子: @import url('https://fonts.googleapis.com/css?family=Fira+Mono:400,700\u0026display=swap\u0026subset=latin-ext'); $code-font-family: Fira Mono, Source Code Pro, Menlo, Consolas, Monaco, monospace; 在 assets/css/_custom.scss 中, 你可以添加一些 CSS 样式代码以自定义样式. ","date":"2020-03-06","objectID":"/theme-documentation-basics/:3:3","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"4 多语言和 i18n LoveIt 主题完全兼容 Hugo 的多语言模式, 并且支持在网页上切换语言. 语言切换\r","date":"2020-03-06","objectID":"/theme-documentation-basics/:4:0","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"4.1 兼容性 语言 Hugo 代码 HTML lang 属性 主题文档 Lunr.js 支持 英语 en en 简体中文 zh-cn zh-CN 繁體中文 zh-tw zh-TW 法语 fr fr 波兰语 pl pl 巴西葡萄牙语 pt-br pt-BR 意大利语 it it 西班牙语 es es 德语 de de 塞尔维亚语 pl pl 俄语 ru ru 罗马尼亚语 ro ro 越南语 vi vi 阿拉伯语 ar ar 加泰罗尼亚语 ca ca 泰语 th th 泰卢固语 te te 印尼语 id id 土耳其语 tr tr 韩语 ko ko 印地语 hi hi ","date":"2020-03-06","objectID":"/theme-documentation-basics/:4:1","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"4.2 基本配置 学习了 Hugo如何处理多语言网站 之后, 请在 站点配置 中定义你的网站语言. 例如, 一个支持英语, 中文和法语的网站配置: # 设置默认的语言 [\"en\", \"zh-cn\", \"fr\", \"pl\", ...] defaultContentLanguage = \"zh-cn\" [languages] [languages.en] weight = 1 title = \"My New Hugo Site\" languageCode = \"en\" languageName = \"English\" [[languages.en.menu.main]] weight = 1 identifier = \"posts\" pre = \"\" post = \"\" name = \"Posts\" url = \"/posts/\" title = \"\" [[languages.en.menu.main]] weight = 2 identifier = \"tags\" pre = \"\" post = \"\" name = \"Tags\" url = \"/tags/\" title = \"\" [[languages.en.menu.main]] weight = 3 identifier = \"categories\" pre = \"\" post = \"\" name = \"Categories\" url = \"/categories/\" title = \"\" [languages.zh-cn] weight = 2 title = \"我的全新 Hugo 网站\" languageCode = \"zh-CN\" languageName = \"简体中文\" hasCJKLanguage = true [[languages.zh-cn.menu.main]] weight = 1 identifier = \"posts\" pre = \"\" post = \"\" name = \"文章\" url = \"/posts/\" title = \"\" [[languages.zh-cn.menu.main]] weight = 2 identifier = \"tags\" pre = \"\" post = \"\" name = \"标签\" url = \"/tags/\" title = \"\" [[languages.zh-cn.menu.main]] weight = 3 identifier = \"categories\" pre = \"\" post = \"\" name = \"分类\" url = \"/categories/\" title = \"\" [languages.fr] weight = 3 title = \"Mon nouveau site Hugo\" languageCode = \"fr\" languageName = \"Français\" [[languages.fr.menu.main]] weight = 1 identifier = \"posts\" pre = \"\" post = \"\" name = \"Postes\" url = \"/posts/\" title = \"\" [[languages.fr.menu.main]] weight = 2 identifier = \"tags\" pre = \"\" post = \"\" name = \"Balises\" url = \"/tags/\" title = \"\" [[languages.fr.menu.main]] weight = 3 identifier = \"categories\" pre = \"\" post = \"\" name = \"Catégories\" url = \"/categories/\" title = \"\" 然后, 对于每个新页面, 将语言代码附加到文件名中. 单个文件 my-page.md 需要分为三个文件: 英语: my-page.en.md 中文: my-page.zh-cn.md 法语: my-page.fr.md 注意\r请注意, 菜单中仅显示翻译的页面. 它不会替换为默认语言内容.\r技巧\r也可以使用 文章前置参数 来翻译网址.\r","date":"2020-03-06","objectID":"/theme-documentation-basics/:4:2","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"4.3 修改默认的翻译字符串 翻译字符串用于在主题中使用的常见默认值. 目前提供一些语言的翻译, 但你可能自定义其他语言或覆盖默认值. 要覆盖默认值, 请在你项目的 i18n 目录 i18n/\u003clanguageCode\u003e.toml 中创建一个新文件，并从 themes/LoveIt/i18n/en.toml 中获得提示. 另外, 由于你的翻译可能会帮助到其他人, 请花点时间通过  创建一个 PR 来贡献主题翻译, 谢谢! ","date":"2020-03-06","objectID":"/theme-documentation-basics/:4:3","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"5 搜索 基于 Lunr.js 或 algolia, LoveIt 主题支持搜索功能. ","date":"2020-03-06","objectID":"/theme-documentation-basics/:5:0","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"5.1 输出配置 为了生成搜索功能所需要的 index.json, 请在你的 网站配置 中添加 JSON 输出文件类型到 outputs 部分的 home 字段中. [outputs] home = [\"HTML\", \"RSS\", \"JSON\"] ","date":"2020-03-06","objectID":"/theme-documentation-basics/:5:1","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"5.2 搜索配置 基于 Hugo 生成的 index.json 文件, 你可以激活搜索功能. 这是你的 网站配置 中的搜索部分: [params.search] enable = true # 搜索引擎的类型 [\"lunr\", \"algolia\"] type = \"lunr\" # 文章内容最长索引长度 contentLength = 4000 # 搜索框的占位提示语 placeholder = \"\" # 最大结果数目 maxResultLength = 10 # 结果内容片段长度 snippetLength = 50 # 搜索结果中高亮部分的 HTML 标签 highlightTag = \"em\" # 是否在搜索索引中使用基于 baseURL 的绝对路径 absoluteURL = false [params.search.algolia] index = \"\" appID = \"\" searchKey = \"\" 怎样选择搜索引擎?\r以下是两种搜索引擎的对比: lunr: 简单, 无需同步 index.json, 没有 contentLength 的限制, 但占用带宽大且性能低 (特别是中文需要一个较大的分词依赖库) algolia: 高性能并且占用带宽低, 但需要同步 index.json 且有 contentLength 的限制 文章内容被 h2 和 h3 HTML 标签切分来提高查询效果并且基本实现全文搜索. contentLength 用来限制 h2 和 h3 HTML 标签开头的内容部分的最大长度. 关于 algolia 的使用技巧\r你需要上传 index.json 到 algolia 来激活搜索功能. 你可以使用浏览器来上传 index.json 文件但是一个自动化的脚本可能效果更好. 官方提供的 Algolia CLI 是一个不错的选择. 为了兼容 Hugo 的多语言模式, 你需要上传不同语言的 index.json 文件到对应的 algolia index, 例如 zh-cn/index.json 或 fr/index.json…\r","date":"2020-03-06","objectID":"/theme-documentation-basics/:5:2","tags":["installation","configuration"],"title":"主题文档 - 基本概念","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"了解如何在 LoveIt 主题中快速, 直观地创建和组织内容.","date":"2020-03-05","objectID":"/theme-documentation-content/","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"了解如何在 LoveIt 主题中快速, 直观地创建和组织内容. ","date":"2020-03-05","objectID":"/theme-documentation-content/:0:0","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"1 内容组织 以下是一些方便你清晰管理和生成文章的目录结构建议: 保持博客文章存放在 content/posts 目录, 例如: content/posts/我的第一篇文章.md 保持简单的静态页面存放在 content 目录, 例如: content/about.md 本地资源组织 本地资源引用\r有三种方法来引用图片和音乐等本地资源: 使用页面包中的页面资源. 你可以使用适用于 Resources.GetMatch 的值或者直接使用相对于当前页面目录的文件路径来引用页面资源. 将本地资源放在 assets 目录中, 默认路径是 /assets. 引用资源的文件路径是相对于 assets 目录的. 将本地资源放在 static 目录中, 默认路径是 /static. 引用资源的文件路径是相对于 static 目录的. 引用的优先级符合以上的顺序. 在这个主题中的很多地方可以使用上面的本地资源引用, 例如 链接, 图片, image shortcode, music shortcode 和前置参数中的部分参数. 页面资源或者 assets 目录中的图片处理会在未来的版本中得到支持. 非常酷的功能! ","date":"2020-03-05","objectID":"/theme-documentation-content/:1:0","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"2 前置参数 Hugo 允许你在文章内容前面添加 yaml, toml 或者 json 格式的前置参数. 注意\r不是所有的以下前置参数都必须在你的每篇文章中设置. 只有在文章的参数和你的 网站设置 中的 page 部分不一致时才有必要这么做.\r这是一个前置参数例子: --- title: \"我的第一篇文章\" subtitle: \"\" date: 2020-03-04T15:58:26+08:00 lastmod: 2020-03-04T15:58:26+08:00 draft: true author: \"\" authorLink: \"\" description: \"\" license: \"\" images: [] tags: [] categories: [] featuredImage: \"\" featuredImagePreview: \"\" hiddenFromHomePage: false hiddenFromSearch: false twemoji: false lightgallery: true ruby: true fraction: true fontawesome: true linkToMarkdown: true rssFullText: false toc: enable: true auto: true code: copy: true maxShownLines: 50 math: enable: false # ... mapbox: # ... share: enable: true # ... comment: enable: true # ... library: css: # someCSS = \"some.css\" # 位于 \"assets/\" # 或者 # someCSS = \"https://cdn.example.com/some.css\" js: # someJS = \"some.js\" # 位于 \"assets/\" # 或者 # someJS = \"https://cdn.example.com/some.js\" seo: images: [] # ... --- title: 文章标题. subtitle: 文章副标题. date: 这篇文章创建的日期时间. 它通常是从文章的前置参数中的 date 字段获取的, 但是也可以在 网站配置 中设置. lastmod: 上次修改内容的日期时间. draft: 如果设为 true, 除非 hugo 命令使用了 --buildDrafts/-D 参数, 这篇文章不会被渲染. author: 文章作者. authorLink: 文章作者的链接. description: 文章内容的描述. license: 这篇文章特殊的许可. images: 页面图片, 用于 Open Graph 和 Twitter Cards. tags: 文章的标签. categories: 文章所属的类别. featuredImage: 文章的特色图片. featuredImagePreview: 用在主页预览的文章特色图片. hiddenFromHomePage: 如果设为 true, 这篇文章将不会显示在主页上. hiddenFromSearch: 如果设为 true, 这篇文章将不会显示在搜索结果中. twemoji: 如果设为 true, 这篇文章会使用 twemoji. lightgallery: 如果设为 true, 文章中的图片将可以按照画廊形式呈现. ruby: 如果设为 true, 这篇文章会使用 上标注释扩展语法. fraction: 如果设为 true, 这篇文章会使用 分数扩展语法. fontawesome: 如果设为 true, 这篇文章会使用 Font Awesome 扩展语法. linkToMarkdown: 如果设为 true, 内容的页脚将显示指向原始 Markdown 文件的链接. rssFullText: 如果设为 true, 在 RSS 中将会显示全文内容. toc: 和 网站配置 中的 params.page.toc 部分相同. code: 和 网站配置 中的 params.page.code 部分相同. math: 和 网站配置 中的 params.page.math 部分相同. mapbox: 和 网站配置 中的 params.page.mapbox 部分相同. share: 和 网站配置 中的 params.page.share 部分相同. comment: 和 网站配置 中的 params.page.comment 部分相同. library: 和 网站配置 中的 params.page.library 部分相同. seo: 和 网站配置 中的 params.page.seo 部分相同. 技巧\rfeaturedImage 和 featuredImagePreview 支持本地资源引用的完整用法. 如果带有在前置参数中设置了 name: featured-image 或 name: featured-image-preview 属性的页面资源, 没有必要在设置 featuredImage 或 featuredImagePreview: resources: - name: featured-image src: featured-image.jpg - name: featured-image-preview src: featured-image-preview.jpg ","date":"2020-03-05","objectID":"/theme-documentation-content/:2:0","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"3 内容摘要 LoveIt 主题使用内容摘要在主页中显示大致文章信息。Hugo 支持生成文章的摘要. 文章摘要预览\r","date":"2020-03-05","objectID":"/theme-documentation-content/:3:0","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"自动摘要拆分 默认情况下, Hugo 自动将内容的前 70 个单词作为摘要. 你可以通过在 网站配置 中设置 summaryLength 来自定义摘要长度. 如果您要使用 CJK中文/日语/韩语 语言创建内容, 并且想使用 Hugo 的自动摘要拆分功能，请在 网站配置 中将 hasCJKLanguage 设置为 true. ","date":"2020-03-05","objectID":"/theme-documentation-content/:3:1","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"手动摘要拆分 另外, 你也可以添加 \u003c!--more--\u003e 摘要分割符来拆分文章生成摘要. 摘要分隔符之前的内容将用作该文章的摘要. 注意\r请小心输入\u003c!--more--\u003e ; 即全部为小写且没有空格.\r","date":"2020-03-05","objectID":"/theme-documentation-content/:3:2","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"前置参数摘要 你可能希望摘要不是文章开头的文字. 在这种情况下, 你可以在文章前置参数的 summary 变量中设置单独的摘要. ","date":"2020-03-05","objectID":"/theme-documentation-content/:3:3","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"使用文章描述作为摘要 你可能希望将文章前置参数中的 description 变量的内容作为摘要. 你仍然需要在文章开头添加 \u003c!--more--\u003e 摘要分割符. 将摘要分隔符之前的内容保留为空. 然后 LoveIt 主题会将你的文章描述作为摘要. ","date":"2020-03-05","objectID":"/theme-documentation-content/:3:4","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"摘要选择的优先级顺序 由于可以通过多种方式指定摘要, 因此了解顺序很有用. 如下: 如果文章中有 \u003c!--more--\u003e 摘要分隔符, 但分隔符之前没有内容, 则使用描述作为摘要. 如果文章中有 \u003c!--more--\u003e 摘要分隔符, 则将按照手动摘要拆分的方法获得摘要. 如果文章前置参数中有摘要变量, 那么将以该值作为摘要. 按照自动摘要拆分方法. 注意\r不建议在摘要内容中包含富文本块元素, 这会导致渲染错误. 例如代码块, 图片, 表格等.\r","date":"2020-03-05","objectID":"/theme-documentation-content/:3:5","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"4 Markdown 基本语法 这部分内容在 Markdown 基本语法页面 中介绍. ","date":"2020-03-05","objectID":"/theme-documentation-content/:4:0","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"5 Markdown 扩展语法 LoveIt 主题提供了一些扩展的语法便于你撰写文章. ","date":"2020-03-05","objectID":"/theme-documentation-content/:5:0","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"Emoji 支持 这部分内容在 Emoji 支持页面 中介绍. ","date":"2020-03-05","objectID":"/theme-documentation-content/:5:1","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"数学公式 LoveIt 基于 $\\KaTeX$ 提供数学公式的支持. 在你的 网站配置 中的 [params.math] 下面设置属性 enable = true, 并在文章的前置参数中设置属性 math: true来启用数学公式的自动渲染. $\\KaTeX$ 根据 特定的分隔符 来自动渲染公式. 技巧\r有一份 $\\KaTeX$ 中支持的 $\\TeX$ 函数 清单.\r注意\r由于 Hugo 在渲染 Markdown 文档时会根据 _/*/\u003e\u003e 之类的语法生成 HTML 文档, 并且有些转义字符形式的文本内容 (如 \\(/\\)/\\[/\\]/\\\\) 会自动进行转义处理, 因此需要对这些地方进行额外的转义字符表达来实现自动渲染: _ -\u003e \\_ * -\u003e \\* \u003e\u003e -\u003e \\\u003e\u003e \\( -\u003e \\\\( \\) -\u003e \\\\) \\[ -\u003e \\\\[ \\] -\u003e \\\\] \\\\ -\u003e \\\\\\\\ LoveIt 主题支持 raw shortcode 以避免这些转义字符, 它可以帮助您编写原始数学公式内容. 一个 raw 示例: 行内公式: 公式块: 呈现的输出效果如下: 行内公式: 公式块: 行内公式 默认的行内公式分割符有: $ ... $ \\( ... \\) (转义的: \\\\( ... \\\\)) 例如: $c = \\pm\\sqrt{a^2 + b^2}$ 和 \\\\(f(x)=\\int_{-\\infty}^{\\infty} \\hat{f}(\\xi) e^{2 \\pi i \\xi x} d \\xi\\\\) 呈现的输出效果如下: $c = \\pm\\sqrt{a^2 + b^2}$ 和 \\(f(x)=\\int_{-\\infty}^{\\infty} \\hat{f}(\\xi) e^{2 \\pi i \\xi x} d \\xi\\) 公式块 默认的公式块分割符有: $$ ... $$ \\[ ... \\] (转义的: \\\\[ ... \\\\]) \\begin{equation} ... \\end{equation} (不编号的: \\begin{equation*} ... \\end{equation*}) \\begin{align} ... \\end{align} (不编号的: \\begin{align*} ... \\end{align*}) \\begin{alignat} ... \\end{alignat} (不编号的: \\begin{alignat*} ... \\end{alignat*}) \\begin{gather} ... \\end{gather} (不编号的: \\begin{gather*} ... \\end{gather*}) \\begin{CD} ... \\end{CD} 例如: $$ c = \\pm\\sqrt{a^2 + b^2} $$ \\\\[ f(x)=\\int_{-\\infty}^{\\infty} \\hat{f}(\\xi) e^{2 \\pi i \\xi x} d \\xi \\\\] \\begin{equation*} \\rho \\frac{\\mathrm{D} \\mathbf{v}}{\\mathrm{D} t}=\\nabla \\cdot \\mathbb{P}+\\rho \\mathbf{f} \\end{equation*} \\begin{equation} \\mathbf{E}=\\sum_{i} \\mathbf{E}\\_{i}=\\mathbf{E}\\_{1}+\\mathbf{E}\\_{2}+\\mathbf{E}_{3}+\\cdots \\end{equation} \\begin{align} a\u0026=b+c \\\\\\\\ d+e\u0026=f \\end{align} \\begin{alignat}{2} 10\u0026x+\u00263\u0026y = 2 \\\\\\\\ 3\u0026x+\u002613\u0026y = 4 \\end{alignat} \\begin{gather} a=b \\\\\\\\ e=b+c \\end{gather} \\begin{CD} A @\u003ea\\\u003e\u003e B \\\\\\\\ @VbVV @AAcA \\\\\\\\ C @= D \\end{CD} 呈现的输出效果如下: $$ c = \\pm\\sqrt{a^2 + b^2} $$ \\[ f(x)=\\int_{-\\infty}^{\\infty} \\hat{f}(\\xi) e^{2 \\pi i \\xi x} d \\xi \\] \\begin{equation*} \\rho \\frac{\\mathrm{D} \\mathbf{v}}{\\mathrm{D} t}=\\nabla \\cdot \\mathbb{P}+\\rho \\mathbf{f} \\end{equation*} \\begin{equation} \\mathbf{E}=\\sum_{i} \\mathbf{E}_{i}=\\mathbf{E}_{1}+\\mathbf{E}_{2}+\\mathbf{E}_{3}+\\cdots \\end{equation} \\begin{align} a\u0026=b+c \\\\ d+e\u0026=f \\end{align} \\begin{alignat}{2} 10\u0026x+\u00263\u0026y = 2 \\\\ 3\u0026x+\u002613\u0026y = 4 \\end{alignat} \\begin{gather} a=b \\\\ e=b+c \\end{gather} \\begin{CD} A @\u003ea\u003e\u003e B \\\\ @VbVV @AAcA \\\\ C @= D \\end{CD} 技巧\r你可以在 网站配置 中自定义行内公式和公式块的分割符.\rCopy-tex Copy-tex 是一个 $\\KaTeX$ 的插件. 通过这个扩展, 在选择并复制 $\\KaTeX$ 渲染的公式时, 会将其 $\\LaTeX$ 源代码复制到剪贴板. 在你的 网站配置 中的 [params.math] 下面设置属性 copyTex = true 来启用 Copy-tex. 选择并复制上一节中渲染的公式, 可以发现复制的内容为 $\\LaTeX$ 源代码. mhchem mhchem 是一个 $\\KaTeX$ 的插件. 通过这个扩展, 你可以在文章中轻松编写漂亮的化学方程式. 在你的 网站配置 中的 [params.math] 下面设置属性 mhchem = true 来启用 mhchem. $$ \\ce{CO2 + C -\u003e 2 CO} $$ $$ \\ce{Hg^2+ -\u003e[I-] HgI2 -\u003e[I-] [Hg^{II}I4]^2-} $$ 呈现的输出效果如下: $$ \\ce{CO2 + C -\u003e 2 CO} $$ $$ \\ce{Hg^2+ -\u003e[I-] HgI2 -\u003e[I-] [Hg^{II}I4]^2-} $$ ","date":"2020-03-05","objectID":"/theme-documentation-content/:5:2","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"字符注音或者注释 LoveIt 主题支持一种 字符注音或者注释 Markdown 扩展语法: [Hugo]^(一个开源的静态网站生成工具) 呈现的输出效果如下: Hugo一个开源的静态网站生成工具 ","date":"2020-03-05","objectID":"/theme-documentation-content/:5:3","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"分数 LoveIt 主题支持一种 分数 Markdown 扩展语法: [浅色]/[深色] [99]/[100] 呈现的输出效果如下: 浅色/深色 90/100 ","date":"2020-03-05","objectID":"/theme-documentation-content/:5:4","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"Font Awesome LoveIt 主题使用 Font Awesome 作为图标库. 你同样可以在文章中轻松使用这些图标. 从 Font Awesome 网站 上获取所需的图标 class. 去露营啦! :(fas fa-campground fa-fw): 很快就回来. 真开心! :(far fa-grin-tears): 呈现的输出效果如下: 去露营啦!  很快就回来. 真开心! ","date":"2020-03-05","objectID":"/theme-documentation-content/:5:5","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"转义字符 在某些特殊情况下 (编写这个主题文档时 ), 你的文章内容会与 Markdown 的基本或者扩展语法冲突, 并且无法避免. 转义字符语法可以帮助你渲染出想要的内容: 呈现的输出效果如下: :joy: 而不是 😂 技巧\r这个方法可以间接解决一个还未解决的 Hugo 的 issue.\r","date":"2020-03-05","objectID":"/theme-documentation-content/:5:6","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"Hugo 提供了多个内置的 Shortcodes, 以方便作者保持 Markdown 内容的整洁.","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"Hugo 提供了多个内置的 Shortcodes, 以方便作者保持 Markdown 内容的整洁. Hugo 使用 Markdown 为其简单的内容格式. 但是, Markdown 在很多方面都无法很好地支持. 你可以使用纯 HTML 来扩展可能性. 但这恰好是一个坏主意. 大家使用 Markdown, 正是因为它即使不经过渲染也可以轻松阅读. 应该尽可能避免使用 HTML 以保持内容简洁. 为了避免这种限制, Hugo 创建了 shortcodes. shortcode 是一个简单代码段, 可以生成合理的 HTML 代码, 并且符合 Markdown 的设计哲学. Hugo 附带了一组预定义的 shortcodes, 它们实现了一些非常常见的用法. 提供这些 shortcodes 是为了方便保持你的 Markdown 内容简洁. ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:0:0","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"1 figure figure 的文档 一个 figure 示例: {{\u003c figure src=\"/images/lighthouse.jpg\" title=\"Lighthouse (figure)\" \u003e}} 呈现的输出效果如下: Lighthouse (figure) 输出的 HTML 看起来像这样: \u003cfigure\u003e \u003cimg src=\"/images/lighthouse.jpg\"/\u003e \u003cfigcaption\u003e \u003ch4\u003eLighthouse (figure)\u003c/h4\u003e \u003c/figcaption\u003e \u003c/figure\u003e ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:1:0","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"2 gist gist 的文档 一个 gist 示例: {{\u003c gist spf13 7896402 \u003e}} 呈现的输出效果如下: 输出的 HTML 看起来像这样: \u003cscript type=\"application/javascript\" src=\"https://gist.github.com/spf13/7896402.js\"\u003e\u003c/script\u003e ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:2:0","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"3 highlight highlight 的文档 一个 highlight 示例: {{\u003c highlight html \u003e}} \u003csection id=\"main\"\u003e \u003cdiv\u003e \u003ch1 id=\"title\"\u003e{{ .Title }}\u003c/h1\u003e {{ range .Pages }} {{ .Render \"summary\"}} {{ end }} \u003c/div\u003e \u003c/section\u003e {{\u003c /highlight \u003e}} 呈现的输出效果如下: \u003csection id=\"main\"\u003e \u003cdiv\u003e \u003ch1 id=\"title\"\u003e{{ .Title }}\u003c/h1\u003e {{ range .Pages }} {{ .Render \"summary\"}} {{ end }} \u003c/div\u003e \u003c/section\u003e ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:3:0","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"4 instagram instagram 的文档 Instagram’s API was deprecated since October 24th, 2020\rThe instagram-shortcode refers an endpoint of Instagram’s API, that’s deprecated since October 24th, 2020. Thus, no images can be fetched from this API endpoint, resulting in an error when the instagram-shortcode is used. For more information please have a look at GitHub issue #7879.\r","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:4:0","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"5 param param 的文档 一个 param 示例: {{\u003c param description \u003e}} 呈现的输出效果如下: Hugo 提供了多个内置的 Shortcodes, 以方便作者保持 Markdown 内容的整洁. ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:5:0","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"6 ref 和 relref ref 和 relref 的文档 ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:6:0","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"7 tweet tweet 的文档 一个 tweet 示例: {{\u003c tweet 917359331535966209 \u003e}} 呈现的输出效果如下: ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:7:0","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"8 vimeo vimeo 的文档 一个 vimeo 示例: {{\u003c vimeo 146022717 \u003e}} 呈现的输出效果如下: ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:8:0","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"9 youtube youtube 的文档 一个 youtube 示例: {{\u003c youtube w7Ft2ymGmfc \u003e}} 呈现的输出效果如下: ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:9:0","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"LoveIt 主题在 Hugo 内置的 shortcode 的基础上提供多个扩展的 shortcode.","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"LoveIt 主题在 Hugo 内置的 shortcode 的基础上提供多个扩展的 shortcode. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:0:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"1 style 注意\rHugo extended 版本对于 style shortcode 是必需的.\rstyle shortcode 用来在你的文章中插入自定义样式. style shortcode 有两个位置参数. 第一个参数是自定义样式的内容. 它支持  SASS 中的嵌套语法, 并且 \u0026 指代这个父元素. 第二个参数是包裹你要更改样式的内容的 HTML 标签, 默认值是 div. 一个 style 示例: ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:1:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"2 link link shortcode 是 Markdown 链接语法 的替代. link shortcode 可以提供一些其它的功能并且可以在代码块中使用. 支持本地资源引用的完整用法. link shortcode 有以下命名参数: href [必需] (第一个位置参数) 链接的目标. content [可选] (第二个位置参数) 链接的内容, 默认值是 href 参数的值. 支持 Markdown 或者 HTML 格式. title [可选] (第三个位置参数) HTML a 标签 的 title 属性, 当悬停在链接上会显示的提示. rel [可选] HTML a 标签 的 rel 补充属性. class [可选] HTML a 标签 的 class 属性. 一个 link 示例: {{\u003c link \"https://assemble.io\" \u003e}} 或者 {{\u003c link href=\"https://assemble.io\" \u003e}} {{\u003c link \"mailto:contact@revolunet.com\" \u003e}} 或者 {{\u003c link href=\"mailto:contact@revolunet.com\" \u003e}} {{\u003c link \"https://assemble.io\" Assemble \u003e}} 或者 {{\u003c link href=\"https://assemble.io\" content=Assemble \u003e}} 呈现的输出效果如下: https://assemble.io mailto:contact@revolunet.com Assemble 一个带有标题的 link 示例: {{\u003c link \"https://github.com/upstage/\" Upstage \"Visit Upstage!\" \u003e}} 或者 {{\u003c link href=\"https://github.com/upstage/\" content=Upstage title=\"Visit Upstage!\" \u003e}} 呈现的输出效果如下 (将鼠标悬停在链接上，会有一行提示): Upstage ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:2:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"3 image image shortcode 是 figure shortcode 的替代. image shortcode 可以充分利用 lazysizes 和 lightGallery 两个依赖库. 支持本地资源引用的完整用法. image shortcode 有以下命名参数: src [必需] (第一个位置参数) 图片的 URL. alt [可选] (第二个位置参数) 图片无法显示时的替代文本, 默认值是 src 参数的值. 支持 Markdown 或者 HTML 格式. caption [可选] (第三个位置参数) 图片标题. 支持 Markdown 或者 HTML 格式. title [可选] 当悬停在图片上会显示的提示. class [可选] HTML figure 标签的 class 属性. src_s [可选] 图片缩略图的 URL, 用在画廊模式中, 默认值是 src 参数的值. src_l [可选] 高清图片的 URL, 用在画廊模式中, 默认值是 src 参数的值. height [可选] 图片的 height 属性. width [可选] 图片的 width 属性. linked [可选] 图片是否需要被链接, 默认值是 true. rel [可选] HTML a 标签 的 rel 补充属性, 仅在 linked 属性设置成 true 时有效. 一个 image 示例: {{\u003c image src=\"/images/lighthouse.jpg\" caption=\"Lighthouse (`image`)\" src_s=\"/images/lighthouse-small.jpg\" src_l=\"/images/lighthouse-large.jpg\" \u003e}} 呈现的输出效果如下: Lighthouse (image)\r","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:3:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"4 admonition admonition shortcode 支持 12 种 帮助你在页面中插入提示的横幅. 支持 Markdown 或者 HTML 格式. 注意\r一个 注意 横幅\r摘要\r一个 摘要 横幅\r信息\r一个 信息 横幅\r技巧\r一个 技巧 横幅\r成功\r一个 成功 横幅\r问题\r一个 问题 横幅\r警告\r一个 警告 横幅\r失败\r一个 失败 横幅\r危险\r一个 危险 横幅\rBug\r一个 Bug 横幅\r示例\r一个 示例 横幅\r引用\r一个 引用 横幅\radmonition shortcode 有以下命名参数: type [可选] (第一个位置参数) admonition 横幅的类型, 默认值是 note. title [可选] (第二个位置参数) admonition 横幅的标题, 默认值是 type 参数的值. open [可选] (第三个位置参数) 横幅内容是否默认展开, 默认值是 true. 一个 admonition 示例: {{\u003c admonition type=tip title=\"This is a tip\" open=false \u003e}} 一个 **技巧** 横幅 {{\u003c /admonition \u003e}} 或者 {{\u003c admonition tip \"This is a tip\" false \u003e}} 一个 **技巧** 横幅 {{\u003c /admonition \u003e}} 呈现的输出效果如下: This is a tip\r一个 技巧 横幅\r","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:4:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"5 mermaid mermaid shortcode 使用 Mermaid 库提供绘制图表和流程图的功能. 完整文档请查看页面 主题文档 - mermaid Shortcode. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:5:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"6 echarts echarts shortcode 使用 ECharts 库提供数据可视化的功能. 完整文档请查看页面 主题文档 - echarts Shortcode. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:6:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"7 mapbox mapbox shortcode 使用 Mapbox GL JS 库提供互动式地图的功能. 完整文档请查看页面 主题文档 - mapbox Shortcode. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:7:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"8 music music shortcode 基于 APlayer 和 MetingJS 库提供了一个内嵌的响应式音乐播放器. 完整文档请查看页面 主题文档 - music Shortcode. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:8:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"9 bilibili bilibili shortcode 提供了一个内嵌的用来播放 bilibili 视频的响应式播放器. 完整文档请查看页面 主题文档 - bilibili Shortcode. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:9:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"10 typeit typeit shortcode 基于 TypeIt 库提供了打字动画. 完整文档请查看页面 主题文档 - typeit Shortcode. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:10:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"11 script script shortcode 用来在你的文章中插入  Javascript 脚本. 注意\r脚本内容可以保证在所有的第三方库加载之后按顺序执行. 所以你可以自由地使用第三方库.\r一个 script 示例: {{\u003c script \u003e}} console.log('Hello LoveIt!'); {{\u003c /script \u003e}} 你可以在开发者工具的控制台中看到输出. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:11:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"12 raw raw shortcode 用来在你的文章中插入原始  HTML 内容. 一个 raw 示例: 行内公式: {{\u003c raw \u003e}}\\(\\mathbf{E}=\\sum_{i} \\mathbf{E}_{i}=\\mathbf{E}_{1}+\\mathbf{E}_{2}+\\mathbf{E}_{3}+\\cdots\\){{\u003c /raw \u003e}} 公式块: {{\u003c raw \u003e}} \\[ a=b+c \\\\ d+e=f \\] {{\u003c /raw \u003e}} 原始的带有 Markdown 语法的内容: {{\u003c raw \u003e}}**Hello**{{\u003c /raw \u003e}} 呈现的输出效果如下: 行内公式: 公式块: 原始的带有 Markdown 语法的内容: ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:12:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"13 person person shortcode 用来在你的文章中以 h-card 的格式插入个人网站链接. person shortcode 有以下命名参数: url [必需] (第一个位置参数) URL of the personal page. name [必需] (第二个位置参数) Name of the person. text [可选] (第三个位置参数) Text to display as hover tooltip of the link. picture [可选] (第四个位置参数) A picture to use as person’s avatar. nick [可选] Nickame of the person. 一个 person 示例: {{\u003c person url=\"https://evgenykuznetsov.org\" name=\"Evgeny Kuznetsov\" nick=\"nekr0z\" text=\"author of this shortcode\" picture=\"https://evgenykuznetsov.org/img/avatar.jpg\" \u003e}} 呈现的输出效果为  Evgeny Kuznetsov (nekr0z). 一个使用通用图标的 person 示例: {{\u003c person \"https://dillonzq.com/\" Dillon \"author of the LoveIt theme\" \u003e}} 呈现的输出效果为  Dillon. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:13:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["Markdown"],"content":"这篇文章展示了基本的 Markdown 语法和格式.","date":"2019-12-01","objectID":"/basic-markdown-syntax/","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"这篇文章提供了可以在 Hugo 的文章中使用的基本 Markdown 语法示例. 注意\r这篇文章借鉴了一篇很棒的来自 Grav 的文章. 如果你想了解 Loveit 主题的扩展 Markdown 语法, 请阅读扩展 Markdown 语法页面. 事实上, 编写 Web 内容很麻烦. WYSIWYG所见即所得 编辑器帮助减轻了这一任务. 但通常会导致代码太糟, 或更糟糕的是, 网页也会很丑. 没有通常伴随的所有复杂和丑陋的问题, Markdown 是一种更好的生成 HTML 内容的方式. 一些主要好处是: Markdown 简单易学, 几乎没有多余的字符, 因此编写内容也更快. 用 Markdown 书写时出错的机会更少. 可以产生有效的 XHTML 输出. 将内容和视觉显示保持分开, 这样就不会打乱网站的外观. 可以在你喜欢的任何文本编辑器或 Markdown 应用程序中编写内容. Markdown 使用起来很有趣! John Gruber, Markdown 的作者如是说: Markdown 格式的首要设计目标是更具可读性. 最初的想法是 Markdown 格式的文档应当以纯文本形式发布, 而不会看起来像被标签或格式说明所标记. 虽然 Markdown 的语法受到几种现有的文本到 HTML 转换工具的影响, 但 Markdown 语法的最大灵感来源是纯文本电子邮件的格式. – John Gruber 话不多说, 我们来回顾一下 Markdown 的主要语法以及生成的 HTML 样式! 技巧\r 将此页保存为书签，以备将来参考!\r","date":"2019-12-01","objectID":"/basic-markdown-syntax/:0:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"1 标题 从 h2 到 h6 的标题在每个级别上都加上一个 ＃: ## h2 标题 ### h3 标题 #### h4 标题 ##### h5 标题 ###### h6 标题 输出的 HTML 看起来像这样: \u003ch2\u003eh2 标题\u003c/h2\u003e \u003ch3\u003eh3 标题\u003c/h3\u003e \u003ch4\u003eh4 标题\u003c/h4\u003e \u003ch5\u003eh5 标题\u003c/h5\u003e \u003ch6\u003eh6 标题\u003c/h6\u003e 标题 ID\r要添加自定义标题 ID, 请在与标题相同的行中将自定义 ID 放在花括号中: ### 一个很棒的标题 {#custom-id} 输出的 HTML 看起来像这样: \u003ch3 id=\"custom-id\"\u003e一个很棒的标题\u003c/h3\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:1:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"2 注释 注释是和 HTML 兼容的： \u003c!-- 这是一段注释 --\u003e 不能看到以下的注释: ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:2:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"3 水平线 HTML 中的 \u003chr\u003e 标签是用来在段落元素之间创建一个 “专题间隔” 的. 使用 Markdown, 你可以用以下方式创建一个 \u003chr\u003e 标签: ___: 三个连续的下划线 ---: 三个连续的破折号 ***: 三个连续的星号 呈现的输出效果如下: ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:3:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"4 段落 按照纯文本的方式书写段落, 纯文本在呈现的 HTML 中将用 \u003cp\u003e/\u003c/p\u003e 标签包裹. 如下段落: Lorem ipsum dolor sit amet, graecis denique ei vel, at duo primis mandamus. Et legere ocurreret pri, animal tacimates complectitur ad cum. Cu eum inermis inimicus efficiendi. Labore officiis his ex, soluta officiis concludaturque ei qui, vide sensibus vim ad. 输出的 HTML 看起来像这样: \u003cp\u003eLorem ipsum dolor sit amet, graecis denique ei vel, at duo primis mandamus. Et legere ocurreret pri, animal tacimates complectitur ad cum. Cu eum inermis inimicus efficiendi. Labore officiis his ex, soluta officiis concludaturque ei qui, vide sensibus vim ad.\u003c/p\u003e 可以使用一个空白行进行换行. ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:4:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"5 内联 HTML 元素 如果你需要某个 HTML 标签 (带有一个类), 则可以简单地像这样使用: Markdown 格式的段落. \u003cdiv class=\"class\"\u003e 这是 \u003cb\u003eHTML\u003c/b\u003e \u003c/div\u003e Markdown 格式的段落. ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:5:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"6 强调 ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:6:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"加粗 用于强调带有较粗字体的文本片段. 以下文本片段会被 渲染为粗体. **渲染为粗体** __渲染为粗体__ 输出的 HTML 看起来像这样: \u003cstrong\u003e渲染为粗体\u003c/strong\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:6:1","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"斜体 用于强调带有斜体的文本片段. 以下文本片段被 渲染为斜体. *渲染为斜体* _渲染为斜体_ 输出的 HTML 看起来像这样: \u003cem\u003e渲染为斜体\u003c/em\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:6:2","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"删除线 按照 GFMGitHub flavored Markdown 你可以使用删除线. ~~这段文本带有删除线.~~ 呈现的输出效果如下: 这段文本带有删除线. 输出的 HTML 看起来像这样: \u003cdel\u003e这段文本带有删除线.\u003c/del\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:6:3","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"组合 加粗, 斜体, 和删除线可以 组合使用. ***加粗和斜体*** ~~**删除线和加粗**~~ ~~*删除线和斜体*~~ ~~***加粗, 斜体和删除线***~~ 呈现的输出效果如下: 加粗和斜体 删除线和加粗 删除线和斜体 加粗, 斜体和删除线 输出的 HTML 看起来像这样: \u003cem\u003e\u003cstrong\u003e加粗和斜体\u003c/strong\u003e\u003c/em\u003e \u003cdel\u003e\u003cstrong\u003e删除线和加粗\u003c/strong\u003e\u003c/del\u003e \u003cdel\u003e\u003cem\u003e删除线和斜体\u003c/em\u003e\u003c/del\u003e \u003cdel\u003e\u003cem\u003e\u003cstrong\u003e加粗, 斜体和删除线\u003c/strong\u003e\u003c/em\u003e\u003c/del\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:6:4","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"7 引用 用于在文档中引用其他来源的内容块. 在要引用的任何文本之前添加 \u003e: \u003e **Fusion Drive** combines a hard drive with a flash storage (solid-state drive) and presents it as a single logical volume with the space of both drives combined. 呈现的输出效果如下: Fusion Drive combines a hard drive with a flash storage (solid-state drive) and presents it as a single logical volume with the space of both drives combined. 输出的 HTML 看起来像这样: \u003cblockquote\u003e \u003cp\u003e \u003cstrong\u003eFusion Drive\u003c/strong\u003e combines a hard drive with a flash storage (solid-state drive) and presents it as a single logical volume with the space of both drives combined. \u003c/p\u003e \u003c/blockquote\u003e 引用也可以嵌套: \u003e Donec massa lacus, ultricies a ullamcorper in, fermentum sed augue. Nunc augue augue, aliquam non hendrerit ac, commodo vel nisi. \u003e\u003e Sed adipiscing elit vitae augue consectetur a gravida nunc vehicula. Donec auctor odio non est accumsan facilisis. Aliquam id turpis in dolor tincidunt mollis ac eu diam. 呈现的输出效果如下: Donec massa lacus, ultricies a ullamcorper in, fermentum sed augue. Nunc augue augue, aliquam non hendrerit ac, commodo vel nisi. Sed adipiscing elit vitae augue consectetur a gravida nunc vehicula. Donec auctor odio non est accumsan facilisis. Aliquam id turpis in dolor tincidunt mollis ac eu diam. ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:7:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"8 列表 ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:8:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"无序列表 一系列项的列表, 其中项的顺序没有明显关系. 你可以使用以下任何符号来表示无序列表中的项: * 一项内容 - 一项内容 + 一项内容 例如: * Lorem ipsum dolor sit amet * Consectetur adipiscing elit * Integer molestie lorem at massa * Facilisis in pretium nisl aliquet * Nulla volutpat aliquam velit * Phasellus iaculis neque * Purus sodales ultricies * Vestibulum laoreet porttitor sem * Ac tristique libero volutpat at * Faucibus porta lacus fringilla vel * Aenean sit amet erat nunc * Eget porttitor lorem 呈现的输出效果如下: Lorem ipsum dolor sit amet Consectetur adipiscing elit Integer molestie lorem at massa Facilisis in pretium nisl aliquet Nulla volutpat aliquam velit Phasellus iaculis neque Purus sodales ultricies Vestibulum laoreet porttitor sem Ac tristique libero volutpat at Faucibus porta lacus fringilla vel Aenean sit amet erat nunc Eget porttitor lorem 输出的 HTML 看起来像这样: \u003cul\u003e \u003cli\u003eLorem ipsum dolor sit amet\u003c/li\u003e \u003cli\u003eConsectetur adipiscing elit\u003c/li\u003e \u003cli\u003eInteger molestie lorem at massa\u003c/li\u003e \u003cli\u003eFacilisis in pretium nisl aliquet\u003c/li\u003e \u003cli\u003eNulla volutpat aliquam velit \u003cul\u003e \u003cli\u003ePhasellus iaculis neque\u003c/li\u003e \u003cli\u003ePurus sodales ultricies\u003c/li\u003e \u003cli\u003eVestibulum laoreet porttitor sem\u003c/li\u003e \u003cli\u003eAc tristique libero volutpat at\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003eFaucibus porta lacus fringilla vel\u003c/li\u003e \u003cli\u003eAenean sit amet erat nunc\u003c/li\u003e \u003cli\u003eEget porttitor lorem\u003c/li\u003e \u003c/ul\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:8:1","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"有序列表 一系列项的列表, 其中项的顺序确实很重要. 1. Lorem ipsum dolor sit amet 2. Consectetur adipiscing elit 3. Integer molestie lorem at massa 4. Facilisis in pretium nisl aliquet 5. Nulla volutpat aliquam velit 6. Faucibus porta lacus fringilla vel 7. Aenean sit amet erat nunc 8. Eget porttitor lorem 呈现的输出效果如下: Lorem ipsum dolor sit amet Consectetur adipiscing elit Integer molestie lorem at massa Facilisis in pretium nisl aliquet Nulla volutpat aliquam velit Faucibus porta lacus fringilla vel Aenean sit amet erat nunc Eget porttitor lorem 输出的 HTML 看起来像这样: \u003col\u003e \u003cli\u003eLorem ipsum dolor sit amet\u003c/li\u003e \u003cli\u003eConsectetur adipiscing elit\u003c/li\u003e \u003cli\u003eInteger molestie lorem at massa\u003c/li\u003e \u003cli\u003eFacilisis in pretium nisl aliquet\u003c/li\u003e \u003cli\u003eNulla volutpat aliquam velit\u003c/li\u003e \u003cli\u003eFaucibus porta lacus fringilla vel\u003c/li\u003e \u003cli\u003eAenean sit amet erat nunc\u003c/li\u003e \u003cli\u003eEget porttitor lorem\u003c/li\u003e \u003c/ol\u003e 技巧\r如果你对每一项使用 1., Markdown 将自动为每一项编号. 例如: 1. Lorem ipsum dolor sit amet 1. Consectetur adipiscing elit 1. Integer molestie lorem at massa 1. Facilisis in pretium nisl aliquet 1. Nulla volutpat aliquam velit 1. Faucibus porta lacus fringilla vel 1. Aenean sit amet erat nunc 1. Eget porttitor lorem 呈现的输出效果如下: Lorem ipsum dolor sit amet Consectetur adipiscing elit Integer molestie lorem at massa Facilisis in pretium nisl aliquet Nulla volutpat aliquam velit Faucibus porta lacus fringilla vel Aenean sit amet erat nunc Eget porttitor lorem ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:8:2","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"任务列表 任务列表使你可以创建带有复选框的列表. 要创建任务列表, 请在任务列表项之前添加破折号 (-) 和带有空格的方括号 ([ ]). 要选择一个复选框，请在方括号之间添加 x ([x]). - [x] Write the press release - [ ] Update the website - [ ] Contact the media 呈现的输出效果如下: Write the press release Update the website Contact the media ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:8:3","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"9 代码 ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:9:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"行内代码 用 ` 包装行内代码段. 在这个例子中, `\u003csection\u003e\u003c/section\u003e` 会被包裹成 **代码**. 呈现的输出效果如下: 在这个例子中, \u003csection\u003e\u003c/section\u003e 会被包裹成 代码. 输出的 HTML 看起来像这样: \u003cp\u003e 在这个例子中, \u003ccode\u003e\u0026lt;section\u0026gt;\u0026lt;/section\u0026gt;\u003c/code\u003e 会被包裹成 \u003cstrong\u003e代码\u003c/strong\u003e. \u003c/p\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:9:1","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"缩进代码 将几行代码缩进至少四个空格，例如: // Some comments line 1 of code line 2 of code line 3 of code 呈现的输出效果如下: // Some comments\rline 1 of code\rline 2 of code\rline 3 of code\r输出的 HTML 看起来像这样: \u003cpre\u003e \u003ccode\u003e // Some comments line 1 of code line 2 of code line 3 of code \u003c/code\u003e \u003c/pre\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:9:2","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"围栏代码块 使用 “围栏” ``` 来生成一段带有语言属性的代码块. ```markdown Sample text here... ``` 输出的 HTML 看起来像这样: \u003cpre language-html\u003e \u003ccode\u003eSample text here...\u003c/code\u003e \u003c/pre\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:9:3","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"语法高亮 GFMGitHub Flavored Markdown 也支持语法高亮. 要激活它，只需在第一个代码 “围栏” 之后直接添加你要使用的语言的文件扩展名, ```js, 语法高亮显示将自动应用于渲染的 HTML 中. 例如, 在以下 JavaScript 代码中应用语法高亮: ```js grunt.initConfig({ assemble: { options: { assets: 'docs/assets', data: 'src/data/*.{json,yml}', helpers: 'src/custom-helpers.js', partials: ['src/partials/**/*.{hbs,md}'] }, pages: { options: { layout: 'default.hbs' }, files: { './': ['src/templates/pages/index.hbs'] } } } }; ``` 呈现的输出效果如下: grunt.initConfig({ assemble: { options: { assets: 'docs/assets', data: 'src/data/*.{json,yml}', helpers: 'src/custom-helpers.js', partials: ['src/partials/**/*.{hbs,md}'] }, pages: { options: { layout: 'default.hbs' }, files: { './': ['src/templates/pages/index.hbs'] } } } }; 注意\rHugo 文档中的 语法高亮页面 介绍了有关语法高亮的更多信息, 包括语法高亮的 shortcode.\r","date":"2019-12-01","objectID":"/basic-markdown-syntax/:9:4","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"10 表格 通过在每个单元格之间添加竖线作为分隔线, 并在标题下添加一行破折号 (也由竖线分隔) 来创建表格. 注意, 竖线不需要垂直对齐. | Option | Description | | ------ | ----------- | | data | path to data files to supply the data that will be passed into templates. | | engine | engine to be used for processing templates. Handlebars is the default. | | ext | extension to be used for dest files. | 呈现的输出效果如下: Option Description data path to data files to supply the data that will be passed into templates. engine engine to be used for processing templates. Handlebars is the default. ext extension to be used for dest files. 输出的 HTML 看起来像这样: \u003ctable\u003e \u003cthead\u003e \u003ctr\u003e \u003cth\u003eOption\u003c/th\u003e \u003cth\u003eDescription\u003c/th\u003e \u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e \u003ctr\u003e \u003ctd\u003edata\u003c/td\u003e \u003ctd\u003epath to data files to supply the data that will be passed into templates.\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003eengine\u003c/td\u003e \u003ctd\u003eengine to be used for processing templates. Handlebars is the default.\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003eext\u003c/td\u003e \u003ctd\u003eextension to be used for dest files.\u003c/td\u003e \u003c/tr\u003e \u003c/tbody\u003e \u003c/table\u003e 文本右对齐或居中对齐\r在任何标题下方的破折号右侧添加冒号将使该列的文本右对齐. 在任何标题下方的破折号两边添加冒号将使该列的对齐文本居中. | Option | Description | |:------:| -----------:| | data | path to data files to supply the data that will be passed into templates. | | engine | engine to be used for processing templates. Handlebars is the default. | | ext | extension to be used for dest files. | 呈现的输出效果如下: Option Description data path to data files to supply the data that will be passed into templates. engine engine to be used for processing templates. Handlebars is the default. ext extension to be used for dest files. ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:10:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"11 链接 ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:11:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"基本链接 \u003chttps://assemble.io\u003e \u003ccontact@revolunet.com\u003e [Assemble](https://assemble.io) 呈现的输出效果如下 (将鼠标悬停在链接上，没有提示): https://assemble.io contact@revolunet.com Assemble 输出的 HTML 看起来像这样: \u003ca href=\"https://assemble.io\"\u003ehttps://assemble.io\u003c/a\u003e \u003ca href=\"mailto:contact@revolunet.com\"\u003econtact@revolunet.com\u003c/a\u003e \u003ca href=\"https://assemble.io\"\u003eAssemble\u003c/a\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:11:1","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"添加一个标题 [Upstage](https://github.com/upstage/ \"Visit Upstage!\") 呈现的输出效果如下 (将鼠标悬停在链接上，会有一行提示): Upstage 输出的 HTML 看起来像这样: \u003ca href=\"https://github.com/upstage/\" title=\"Visit Upstage!\"\u003eUpstage\u003c/a\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:11:2","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"定位标记 定位标记使你可以跳至同一页面上的指定锚点. 例如, 每个章节: ## Table of Contents * [Chapter 1](#chapter-1) * [Chapter 2](#chapter-2) * [Chapter 3](#chapter-3) 将跳转到这些部分: ## Chapter 1 \u003ca id=\"chapter-1\"\u003e\u003c/a\u003e Content for chapter one. ## Chapter 2 \u003ca id=\"chapter-2\"\u003e\u003c/a\u003e Content for chapter one. ## Chapter 3 \u003ca id=\"chapter-3\"\u003e\u003c/a\u003e Content for chapter one. 注意\r定位标记的位置几乎是任意的. 因为它们并不引人注目, 所以它们通常被放在同一行了.\r","date":"2019-12-01","objectID":"/basic-markdown-syntax/:11:3","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"12 脚注 脚注使你可以添加注释和参考, 而不会使文档正文混乱. 当你创建脚注时, 会在添加脚注引用的位置出现带有链接的上标编号. 读者可以单击链接以跳至页面底部的脚注内容. 要创建脚注引用, 请在方括号中添加插入符号和标识符 ([^1]). 标识符可以是数字或单词, 但不能包含空格或制表符. 标识符仅将脚注引用与脚注本身相关联 - 在脚注输出中, 脚注按顺序编号. 在中括号内使用插入符号和数字以及用冒号和文本来添加脚注内容 ([^1]：这是一段脚注). 你不一定要在文档末尾添加脚注. 可以将它们放在除列表, 引用和表格等元素之外的任何位置. 这是一个数字脚注[^1]. 这是一个带标签的脚注[^label] [^1]: 这是一个数字脚注 [^label]: 这是一个带标签的脚注 这是一个数字脚注1. 这是一个带标签的脚注2 ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:12:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"13 图片 图片的语法与链接相似, 但包含一个在前面的感叹号. ![Minion](https://octodex.github.com/images/minion.png) 或者: ![Alt text](https://octodex.github.com/images/stormtroopocat.jpg \"The Stormtroopocat\") The Stormtroopocat\r像链接一样, 图片也具有脚注样式的语法: ![Alt text][id] The Dojocat\r稍后在文档中提供参考内容, 用来定义 URL 的位置: [id]: https://octodex.github.com/images/dojocat.jpg \"The Dojocat\" 技巧\rLoveIt 主题提供了一个包含更多功能的 图片的 shortcode.\r这是一个数字脚注 ↩︎ 这是一个带标签的脚注 ↩︎ ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:13:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["documentation"],"content":"mermaid shortcode 使用 Mermaid 库提供绘制图表和流程图的功能.","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":" mermaid shortcode 使用 Mermaid 库提供绘制图表和流程图的功能. mermaid 是一个可以帮助你在文章中绘制图表和流程图的库, 类似 Markdown 的语法. 只需将你的 mermaid 代码插入 mermaid shortcode 中即可. ","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/:0:0","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":"流程图 一个 流程图 mermaid 示例: {{\u003c mermaid \u003e}} graph LR; A[Hard edge] --\u003e|Link text| B(Round edge) B --\u003e C{Decision} C --\u003e|One| D[Result one] C --\u003e|Two| E[Result two] {{\u003c /mermaid \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/:1:0","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":"时序图 一个 时序图 mermaid 示例: {{\u003c mermaid \u003e}} sequenceDiagram participant Alice participant Bob Alice-\u003e\u003eJohn: Hello John, how are you? loop Healthcheck John-\u003eJohn: Fight against hypochondria end Note right of John: Rational thoughts \u003cbr/\u003eprevail... John--\u003eAlice: Great! John-\u003eBob: How about you? Bob--\u003eJohn: Jolly good! {{\u003c /mermaid \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/:2:0","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":"甘特图 一个 甘特图 mermaid 示例: {{\u003c mermaid \u003e}} gantt dateFormat YYYY-MM-DD title Adding GANTT diagram to mermaid excludes weekdays 2014-01-10 section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d {{\u003c /mermaid \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/:3:0","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":"类图 一个 类图 mermaid 示例: {{\u003c mermaid \u003e}} classDiagram Animal \u003c|-- Duck Animal \u003c|-- Fish Animal \u003c|-- Zebra Animal : +int age Animal : +String gender Animal: +isMammal() Animal: +mate() class Duck{ +String beakColor +swim() +quack() } class Fish{ -int sizeInFeet -canEat() } class Zebra{ +bool is_wild +run() } {{\u003c /mermaid \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/:4:0","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":"状态图 一个 状态图 mermaid 示例: {{\u003c mermaid \u003e}} stateDiagram-v2 [*] --\u003e Still Still --\u003e [*] Still --\u003e Moving Moving --\u003e Still Moving --\u003e Crash Crash --\u003e [*] {{\u003c /mermaid \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/:5:0","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":"Git 图 一个 Git 图 mermaid 示例: {{\u003c mermaid \u003e}} gitGraph commit commit branch develop checkout develop commit commit checkout main merge develop commit commit {{\u003c /mermaid \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/:6:0","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":"实体关系图 一个 实体关系图 mermaid 示例: {{\u003c mermaid \u003e}} erDiagram CUSTOMER ||--o{ ORDER : places ORDER ||--|{ LINE-ITEM : contains CUSTOMER }|..|{ DELIVERY-ADDRESS : uses {{\u003c /mermaid \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/:7:0","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":"用户体验旅程图 一个 用户体验旅程图 mermaid 示例: {{\u003c mermaid \u003e}} journey title My working day section Go to work Make tea: 5: Me Go upstairs: 3: Me Do work: 1: Me, Cat section Go home Go downstairs: 5: Me Sit down: 5: Me {{\u003c /mermaid \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/:8:0","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":"饼图 一个 饼图 mermaid 示例: {{\u003c mermaid \u003e}} pie \"Dogs\" : 386 \"Cats\" : 85 \"Rats\" : 15 {{\u003c /mermaid \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/:9:0","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":"依赖图 一个 依赖图 mermaid 示例: {{\u003c mermaid \u003e}} requirementDiagram requirement test_req { id: 1 text: the test text. risk: high verifymethod: test } element test_entity { type: simulation } test_entity - satisfies -\u003e test_req {{\u003c /mermaid \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/:10:0","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["Markdown"],"content":"Hugo 和 LoveIt 中的 Emoji 的用法指南.","date":"2019-10-01","objectID":"/emoji-support/","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Emoji 可以通过多种方式在 Hugo 项目中启用. emojify 方法可以直接在模板中调用, 或者使用行内 Shortcodes. 要全局使用 emoji, 需要在你的网站配置中设置 enableEmoji 为 true, 然后你就可以直接在文章中输入 emoji 的代码. 它们以冒号开头和结尾，并且包含 emoji 的 代码: 去露营啦! :tent: 很快就回来. 真开心! :joy: 呈现的输出效果如下: 去露营啦! ⛺ 很快就回来. 真开心! 😂 以下符号清单是 emoji 代码的非常有用的参考. ","date":"2019-10-01","objectID":"/emoji-support/:0:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"表情与情感 ","date":"2019-10-01","objectID":"/emoji-support/:1:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"笑脸表情 图标 代码 图标 代码 😀 grinning 😃 smiley 😄 smile 😁 grin 😆 laughing satisfied 😅 sweat_smile 🤣 rofl 😂 joy 🙂 slightly_smiling_face 🙃 upside_down_face 😉 wink 😊 blush 😇 innocent ","date":"2019-10-01","objectID":"/emoji-support/:1:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"爱意表情 图标 代码 图标 代码 😍 heart_eyes 😘 kissing_heart 😗 kissing ☺️ relaxed 😚 kissing_closed_eyes 😙 kissing_smiling_eyes ","date":"2019-10-01","objectID":"/emoji-support/:1:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"吐舌头表情 图标 代码 图标 代码 😋 yum 😛 stuck_out_tongue 😜 stuck_out_tongue_winking_eye 😝 stuck_out_tongue_closed_eyes 🤑 money_mouth_face ","date":"2019-10-01","objectID":"/emoji-support/:1:3","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"带手的表情 图标 代码 图标 代码 🤗 hugs 🤔 thinking ","date":"2019-10-01","objectID":"/emoji-support/:1:4","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"中性表情 图标 代码 图标 代码 🤐 zipper_mouth_face 😐 neutral_face 😑 expressionless 😶 no_mouth 😏 smirk 😒 unamused 🙄 roll_eyes 😬 grimacing 🤥 lying_face ","date":"2019-10-01","objectID":"/emoji-support/:1:5","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"困倦的表情 图标 代码 图标 代码 😌 relieved 😔 pensive 😪 sleepy 🤤 drooling_face 😴 sleeping ","date":"2019-10-01","objectID":"/emoji-support/:1:6","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"不适的表情 图标 代码 图标 代码 😷 mask 🤒 face_with_thermometer 🤕 face_with_head_bandage 🤢 nauseated_face 🤧 sneezing_face 😵 dizzy_face ","date":"2019-10-01","objectID":"/emoji-support/:1:7","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"戴帽子的表情 图标 代码 图标 代码 🤠 cowboy_hat_face ","date":"2019-10-01","objectID":"/emoji-support/:1:8","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"戴眼镜的表情 图标 代码 图标 代码 😎 sunglasses 🤓 nerd_face ","date":"2019-10-01","objectID":"/emoji-support/:1:9","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"担心的表情 图标 代码 图标 代码 😕 confused 😟 worried 🙁 slightly_frowning_face ☹ frowning_face 😮 open_mouth 😯 hushed 😲 astonished 😳 flushed 😦 frowning 😧 anguished 😨 fearful 😰 cold_sweat 😥 disappointed_relieved 😢 cry 😭 sob 😱 scream 😖 confounded 😣 persevere 😞 disappointed 😓 sweat 😩 weary 😫 tired_face ","date":"2019-10-01","objectID":"/emoji-support/:1:10","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"否定的表情 图标 代码 图标 代码 😤 triumph 😡 pout rage 😠 angry 😈 smiling_imp 👿 imp 💀 skull ☠️ skull_and_crossbones ","date":"2019-10-01","objectID":"/emoji-support/:1:11","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"特殊打扮的表情 图标 代码 图标 代码 💩 hankey poop shit 🤡 clown_face 👹 japanese_ogre 👺 japanese_goblin 👻 ghost 👽 alien 👾 space_invader 🤖 robot ","date":"2019-10-01","objectID":"/emoji-support/:1:12","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"猫脸表情 图标 代码 图标 代码 😺 smiley_cat 😸 smile_cat 😹 joy_cat 😻 heart_eyes_cat 😼 smirk_cat 😽 kissing_cat 🙀 scream_cat 😿 crying_cat_face 😾 pouting_cat ","date":"2019-10-01","objectID":"/emoji-support/:1:13","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"猴脸表情 图标 代码 图标 代码 🙈 see_no_evil 🙉 hear_no_evil 🙊 speak_no_evil ","date":"2019-10-01","objectID":"/emoji-support/:1:14","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"情感 图标 代码 图标 代码 💋 kiss 💌 love_letter 💘 cupid 💝 gift_heart 💖 sparkling_heart 💗 heartpulse 💓 heartbeat 💞 revolving_hearts 💕 two_hearts 💟 heart_decoration ❣️ heavy_heart_exclamation 💔 broken_heart ❤️ heart 💛 yellow_heart 💚 green_heart 💙 blue_heart 💜 purple_heart 🖤 black_heart 💯 100 💢 anger 💥 boom collision 💫 dizzy 💦 sweat_drops 💨 dash 🕳️ hole 💣 bomb 💬 speech_balloon 👁️‍🗨️ eye_speech_bubble 🗯️ right_anger_bubble 💭 thought_balloon 💤 zzz ","date":"2019-10-01","objectID":"/emoji-support/:1:15","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"人与身体 ","date":"2019-10-01","objectID":"/emoji-support/:2:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"张开手掌的手势 图标 代码 图标 代码 👋 wave 🤚 raised_back_of_hand 🖐️ raised_hand_with_fingers_splayed ✋ hand raised_hand 🖖 vulcan_salute ","date":"2019-10-01","objectID":"/emoji-support/:2:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"部分手指的手势 图标 代码 图标 代码 👌 ok_hand ✌️ v 🤞 crossed_fingers 🤘 metal 🤙 call_me_hand ","date":"2019-10-01","objectID":"/emoji-support/:2:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"一根手指的手势 图标 代码 图标 代码 👈 point_left 👉 point_right 👆 point_up_2 🖕 fu middle_finger 👇 point_down ☝️ point_up ","date":"2019-10-01","objectID":"/emoji-support/:2:3","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"握紧的手势 图标 代码 图标 代码 👍 +1 thumbsup 👎 -1 thumbsdown ✊ fist fist_raised 👊 facepunch fist_oncoming punch 🤛 fist_left 🤜 fist_right ","date":"2019-10-01","objectID":"/emoji-support/:2:4","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"两只手 图标 代码 图标 代码 👏 clap 🙌 raised_hands 👐 open_hands 🤝 handshake 🙏 pray ","date":"2019-10-01","objectID":"/emoji-support/:2:5","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"握住东西的手势 图标 代码 图标 代码 ✍️ writing_hand 💅 nail_care 🤳 selfie ","date":"2019-10-01","objectID":"/emoji-support/:2:6","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"身体部位 图标 代码 图标 代码 💪 muscle 👂 ear 👃 nose 👀 eyes 👁️ eye 👅 tongue 👄 lips ","date":"2019-10-01","objectID":"/emoji-support/:2:7","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"人 图标 代码 图标 代码 👶 baby 👦 boy 👧 girl :blonde_man: blonde_man person_with_blond_hair 👨 man 👩 woman 👱‍♀️ blonde_woman 👴 older_man 👵 older_woman ","date":"2019-10-01","objectID":"/emoji-support/:2:8","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"身体动作 图标 代码 图标 代码 🙍‍♀️ frowning_woman person_frowning 🙍‍♂️ frowning_man 🙎‍♀️ person_with_pouting_face pouting_woman 🙎‍♂️ pouting_man 🙅‍♀️ ng_woman no_good no_good_woman 🙅‍♂️ ng_man no_good_man 🙆‍♀️ ok_woman 🙆‍♂️ ok_man 💁‍♀️ information_desk_person sassy_woman tipping_hand_woman 💁‍♂️ sassy_man tipping_hand_man 🙋‍♀️ raising_hand raising_hand_woman 🙋‍♂️ raising_hand_man 🙇 bow bowing_man 🙇‍♀️ bowing_woman 🤦‍♂️ man_facepalming 🤦‍♀️ woman_facepalming 🤷‍♂️ man_shrugging 🤷‍♀️ woman_shrugging ","date":"2019-10-01","objectID":"/emoji-support/:2:9","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"人物角色 图标 代码 图标 代码 👨‍⚕️ man_health_worker 👩‍⚕️ woman_health_worker 👨‍🎓 man_student 👩‍🎓 woman_student 👨‍🏫 man_teacher 👩‍🏫 woman_teacher 👨‍⚖️ man_judge 👩‍⚖️ woman_judge 👨‍🌾 man_farmer 👩‍🌾 woman_farmer 👨‍🍳 man_cook 👩‍🍳 woman_cook 👨‍🔧 man_mechanic 👩‍🔧 woman_mechanic 👨‍🏭 man_factory_worker 👩‍🏭 woman_factory_worker 👨‍💼 man_office_worker 👩‍💼 woman_office_worker 👨‍🔬 man_scientist 👩‍🔬 woman_scientist 👨‍💻 man_technologist 👩‍💻 woman_technologist 👨‍🎤 man_singer 👩‍🎤 woman_singer 👨‍🎨 man_artist 👩‍🎨 woman_artist 👨‍✈️ man_pilot 👩‍✈️ woman_pilot 👨‍🚀 man_astronaut 👩‍🚀 woman_astronaut 👨‍🚒 man_firefighter 👩‍🚒 woman_firefighter 👮‍♂️ cop policeman 👮‍♀️ policewoman 🕵 detective male_detective 🕵️‍♀️ female_detective 💂‍♂️ guardsman 💂‍♀️ guardswoman 👷‍♂️ construction_worker construction_worker_man 👷‍♀️ construction_worker_woman 🤴 prince 👸 princess 👳‍♂️ man_with_turban 👳‍♀️ woman_with_turban 👲 man_with_gua_pi_mao 🤵‍♂️ man_in_tuxedo 👰 bride_with_veil 🤰 pregnant_woman ","date":"2019-10-01","objectID":"/emoji-support/:2:10","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"幻想的人物 图标 代码 图标 代码 👼 angel 🎅 santa 🤶 mrs_claus ","date":"2019-10-01","objectID":"/emoji-support/:2:11","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"人物活动 图标 代码 图标 代码 💆‍♀️ massage massage_woman 💆‍♂️ massage_man 💇‍♀️ haircut haircut_woman 💇‍♂️ haircut_man 🚶‍♂️ walking walking_man 🚶‍♀️ walking_woman 🏃‍♂️ runner running running_man 🏃‍♀️ running_woman 💃 dancer 🕺 man_dancing 🕴️ business_suit_levitating 👯‍♀️ dancers dancing_women 👯‍♂️ dancing_men ","date":"2019-10-01","objectID":"/emoji-support/:2:12","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"体育 图标 代码 图标 代码 🤺 person_fencing 🏇 horse_racing ⛷️ skier 🏂 snowboarder 🏌️‍♂️ golfing_man 🏌️‍♀️ golfing_woman 🏄‍♂️ surfer surfing_man 🏄‍♀️ surfing_woman 🚣‍♂️ rowboat rowing_man 🚣‍♀️ rowing_woman 🏊‍♂️ swimmer swimming_man 🏊‍♀️ swimming_woman ⛹️‍♂️ basketball_man ⛹️‍♀️ basketball_woman 🏋️‍♂️ weight_lifting_man 🏋️‍♀️ weight_lifting_woman 🚴‍♂️ bicyclist biking_man 🚴‍♀️ biking_woman 🚵‍♂️ mountain_bicyclist mountain_biking_man 🚵‍♀️ mountain_biking_woman 🤸‍♂️ man_cartwheeling 🤸‍♀️ woman_cartwheeling 🤼‍♂️ men_wrestling 🤼‍♀️ women_wrestling 🤽‍♂️ man_playing_water_polo 🤽‍♀️ woman_playing_water_polo 🤾‍♂️ man_playing_handball 🤾‍♀️ woman_playing_handball 🤹‍♂️ man_juggling 🤹‍♀️ woman_juggling ","date":"2019-10-01","objectID":"/emoji-support/:2:13","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"休息 图标 代码 图标 代码 🛀 bath 🛌 sleeping_bed ","date":"2019-10-01","objectID":"/emoji-support/:2:14","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"家庭 图标 代码 图标 代码 👭 two_women_holding_hands 👫 couple 👬 two_men_holding_hands 👩‍❤️‍💋‍👨 couplekiss_man_woman 👨‍❤️‍💋‍👨 couplekiss_man_man 👩‍❤️‍💋‍👩 couplekiss_woman_woman 💑 couple_with_heart couple_with_heart_woman_man 👨‍❤️‍👨 couple_with_heart_man_man 👩‍❤️‍👩 couple_with_heart_woman_woman 👨‍👩‍👦 family family_man_woman_boy 👨‍👩‍👧 family_man_woman_girl 👨‍👩‍👧‍👦 family_man_woman_girl_boy 👨‍👩‍👦‍👦 family_man_woman_boy_boy 👨‍👩‍👧‍👧 family_man_woman_girl_girl 👨‍👨‍👦 family_man_man_boy 👨‍👨‍👧 family_man_man_girl 👨‍👨‍👧‍👦 family_man_man_girl_boy 👨‍👨‍👦‍👦 family_man_man_boy_boy 👨‍👨‍👧‍👧 family_man_man_girl_girl 👩‍👩‍👦 family_woman_woman_boy 👩‍👩‍👧 family_woman_woman_girl 👩‍👩‍👧‍👦 family_woman_woman_girl_boy 👩‍👩‍👦‍👦 family_woman_woman_boy_boy 👩‍👩‍👧‍👧 family_woman_woman_girl_girl 👨‍👦 family_man_boy 👨‍👦‍👦 family_man_boy_boy 👨‍👧 family_man_girl 👨‍👧‍👦 family_man_girl_boy 👨‍👧‍👧 family_man_girl_girl 👩‍👦 family_woman_boy 👩‍👦‍👦 family_woman_boy_boy 👩‍👧 family_woman_girl 👩‍👧‍👦 family_woman_girl_boy 👩‍👧‍👧 family_woman_girl_girl ","date":"2019-10-01","objectID":"/emoji-support/:2:15","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"人物符号 图标 代码 图标 代码 🗣 speaking_head 👤 bust_in_silhouette 👥 busts_in_silhouette 👣 footprints ","date":"2019-10-01","objectID":"/emoji-support/:2:16","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"动物与自然 ","date":"2019-10-01","objectID":"/emoji-support/:3:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"哺乳动物 图标 代码 图标 代码 🐵 monkey_face 🐒 monkey 🦍 gorilla 🐶 dog 🐕 dog2 🐩 poodle 🐺 wolf 🦊 fox_face 🐱 cat 🐈 cat2 🦁 lion 🐯 tiger 🐅 tiger2 🐆 leopard 🐴 horse 🐎 racehorse 🦄 unicorn 🦌 deer 🐮 cow 🐂 ox 🐃 water_buffalo 🐄 cow2 🐷 pig 🐖 pig2 🐗 boar 🐽 pig_nose 🐏 ram 🐑 sheep 🐐 goat 🐪 dromedary_camel 🐫 camel 🐘 elephant 🦏 rhinoceros 🐭 mouse 🐁 mouse2 🐀 rat 🐹 hamster 🐰 rabbit 🐇 rabbit2 🐿️ chipmunk 🦇 bat 🐻 bear 🐨 koala 🐼 panda_face 🐾 feet paw_prints ","date":"2019-10-01","objectID":"/emoji-support/:3:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"鸟类 图标 代码 图标 代码 🦃 turkey 🐔 chicken 🐓 rooster 🐣 hatching_chick 🐤 baby_chick 🐥 hatched_chick 🐦 bird 🐧 penguin 🕊 dove 🦅 eagle 🦆 duck 🦉 owl ","date":"2019-10-01","objectID":"/emoji-support/:3:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"两栖动物 icon code icon code 🐸 frog ","date":"2019-10-01","objectID":"/emoji-support/:3:3","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"爬虫类 图标 代码 图标 代码 🐊 crocodile 🐢 turtle 🦎 lizard 🐍 snake 🐲 dragon_face 🐉 dragon ","date":"2019-10-01","objectID":"/emoji-support/:3:4","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"海洋动物 图标 代码 图标 代码 🐳 whale 🐋 whale2 🐬 dolphin flipper 🐟 fish 🐠 tropical_fish 🐡 blowfish 🦈 shark 🐙 octopus 🐚 shell ","date":"2019-10-01","objectID":"/emoji-support/:3:5","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"虫类 图标 代码 图标 代码 🐌 snail 🦋 butterfly 🐛 bug 🐜 ant 🐝 bee honeybee 🪲 beetle 🕷️ spider 🕸️ spider_web 🦂 scorpion ","date":"2019-10-01","objectID":"/emoji-support/:3:6","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"花类植物 图标 代码 图标 代码 💐 bouquet 🌸 cherry_blossom 💮 white_flower 🏵️ rosette 🌹 rose 🥀 wilted_flower 🌺 hibiscus 🌻 sunflower 🌼 blossom 🌷 tulip ","date":"2019-10-01","objectID":"/emoji-support/:3:7","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"其它植物 图标 代码 图标 代码 🌱 seedling 🌲 evergreen_tree 🌳 deciduous_tree 🌴 palm_tree 🌵 cactus 🌾 ear_of_rice 🌿 herb ☘️ shamrock 🍀 four_leaf_clover 🍁 maple_leaf 🍂 fallen_leaf 🍃 leaves ","date":"2019-10-01","objectID":"/emoji-support/:3:8","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"食物与饮料 ","date":"2019-10-01","objectID":"/emoji-support/:4:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"水果 图标 代码 图标 代码 🍇 grapes 🍈 melon 🍉 watermelon 🍊 mandarin orange tangerine 🍋 lemon 🍌 banana 🍍 pineapple 🍎 apple 🍏 green_apple 🍐 pear 🍑 peach 🍒 cherries 🍓 strawberry 🥝 kiwi_fruit 🍅 tomato ","date":"2019-10-01","objectID":"/emoji-support/:4:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"蔬菜 图标 代码 图标 代码 🥑 avocado 🍆 eggplant 🥔 potato 🥕 carrot 🌽 corn 🌶️ hot_pepper 🥒 cucumber 🍄 mushroom 🥜 peanuts 🌰 chestnut ","date":"2019-10-01","objectID":"/emoji-support/:4:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"快餐 图标 代码 图标 代码 🍞 bread 🥐 croissant 🥖 baguette_bread 🥞 pancakes 🧀 cheese 🍖 meat_on_bone 🍗 poultry_leg 🥓 bacon 🍔 hamburger 🍟 fries 🍕 pizza 🌭 hotdog 🌮 taco 🌯 burrito 🥙 stuffed_flatbread 🥚 egg 🍳 fried_egg 🥘 shallow_pan_of_food 🍲 stew 🥗 green_salad 🍿 popcorn ","date":"2019-10-01","objectID":"/emoji-support/:4:3","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"亚洲食物 图标 代码 图标 代码 🍱 bento 🍘 rice_cracker 🍙 rice_ball 🍚 rice 🍛 curry 🍜 ramen 🍝 spaghetti 🍠 sweet_potato 🍢 oden 🍣 sushi 🍤 fried_shrimp 🍥 fish_cake 🍡 dango ","date":"2019-10-01","objectID":"/emoji-support/:4:4","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"海鲜 图标 代码 图标 代码 🦀 crab 🦐 shrimp 🦑 squid ","date":"2019-10-01","objectID":"/emoji-support/:4:5","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"甜点 图标 代码 图标 代码 🍦 icecream 🍧 shaved_ice 🍨 ice_cream 🍩 doughnut 🍪 cookie 🎂 birthday 🍰 cake 🍫 chocolate_bar 🍬 candy 🍭 lollipop 🍮 custard 🍯 honey_pot ","date":"2019-10-01","objectID":"/emoji-support/:4:6","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"饮料 图标 代码 图标 代码 🍼 baby_bottle 🥛 milk_glass ☕ coffee 🍵 tea 🍶 sake 🍾 champagne 🍷 wine_glass 🍸 cocktail 🍹 tropical_drink 🍺 beer 🍻 beers 🥂 clinking_glasses 🥃 tumbler_glass ","date":"2019-10-01","objectID":"/emoji-support/:4:7","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"餐具 图标 代码 图标 代码 🍽️ plate_with_cutlery 🍴 fork_and_knife 🥄 spoon 🔪 hocho knife 🏺 amphora ","date":"2019-10-01","objectID":"/emoji-support/:4:8","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"旅游与地理 ","date":"2019-10-01","objectID":"/emoji-support/:5:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"地图 图标 代码 图标 代码 🌍 earth_africa 🌎 earth_americas 🌏 earth_asia 🌐 globe_with_meridians 🗺️ world_map 🗾 japan ","date":"2019-10-01","objectID":"/emoji-support/:5:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"地理现象 图标 代码 图标 代码 🏔 mountain_snow ⛰️ mountain 🌋 volcano 🗻 mount_fuji 🏕️ camping ⛱ beach_umbrella 🏜️ desert 🏝️ desert_island 🏞️ national_park ","date":"2019-10-01","objectID":"/emoji-support/:5:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"建筑物 图标 代码 图标 代码 🏟️ stadium 🏛️ classical_building 🏗️ building_construction 🏘 houses 🏚 derelict_house 🏠 house 🏡 house_with_garden 🏢 office 🏣 post_office 🏤 european_post_office 🏥 hospital 🏦 bank 🏨 hotel 🏩 love_hotel 🏪 convenience_store 🏫 school 🏬 department_store 🏭 factory 🏯 japanese_castle 🏰 european_castle 💒 wedding 🗼 tokyo_tower 🗽 statue_of_liberty ","date":"2019-10-01","objectID":"/emoji-support/:5:3","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"宗教建筑 图标 代码 图标 代码 ⛪ church 🕌 mosque 🕍 synagogue ⛩️ shinto_shrine 🕋 kaaba ","date":"2019-10-01","objectID":"/emoji-support/:5:4","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"其它地点 图标 代码 图标 代码 ⛲ fountain ⛺ tent 🌁 foggy 🌃 night_with_stars 🏙️ cityscape 🌄 sunrise_over_mountains 🌅 sunrise 🌆 city_sunset 🌇 city_sunrise 🌉 bridge_at_night ♨️ hotsprings 🎠 carousel_horse 🎡 ferris_wheel 🎢 roller_coaster 💈 barber 🎪 circus_tent ","date":"2019-10-01","objectID":"/emoji-support/:5:5","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"陆路运输 图标 代码 图标 代码 🚂 steam_locomotive 🚃 railway_car 🚄 bullettrain_side 🚅 bullettrain_front 🚆 train2 🚇 metro 🚈 light_rail 🚉 station 🚊 tram 🚝 monorail 🚞 mountain_railway 🚋 train 🚌 bus 🚍 oncoming_bus 🚎 trolleybus 🚐 minibus 🚑 ambulance 🚒 fire_engine 🚓 police_car 🚔 oncoming_police_car 🚕 taxi 🚖 oncoming_taxi 🚗 car red_car 🚘 oncoming_automobile 🚙 blue_car 🚚 truck 🚛 articulated_lorry 🚜 tractor 🏎️ racing_car 🏍 motorcycle 🛵 motor_scooter 🚲 bike 🛴 kick_scooter 🚏 busstop 🛣️ motorway 🛤️ railway_track 🛢️ oil_drum ⛽ fuelpump 🚨 rotating_light 🚥 traffic_light 🚦 vertical_traffic_light 🛑 stop_sign 🚧 construction ","date":"2019-10-01","objectID":"/emoji-support/:5:6","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"水路运输 图标 代码 图标 代码 ⚓ anchor ⛵ boat sailboat 🛶 canoe 🚤 speedboat 🛳️ passenger_ship ⛴️ ferry 🛥️ motor_boat 🚢 ship ","date":"2019-10-01","objectID":"/emoji-support/:5:7","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"空中运输 图标 代码 图标 代码 ✈️ airplane 🛩️ small_airplane 🛫 flight_departure 🛬 flight_arrival 💺 seat 🚁 helicopter 🚟 suspension_railway 🚠 mountain_cableway 🚡 aerial_tramway 🛰️ artificial_satellite 🚀 rocket ","date":"2019-10-01","objectID":"/emoji-support/:5:8","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"旅馆 icon code icon code 🛎️ bellhop_bell ","date":"2019-10-01","objectID":"/emoji-support/:5:9","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"时间 图标 代码 图标 代码 ⌛ hourglass ⏳ hourglass_flowing_sand ⌚ watch ⏰ alarm_clock ⏱️ stopwatch ⏲️ timer_clock 🕰️ mantelpiece_clock 🕛 clock12 🕧 clock1230 🕐 clock1 🕜 clock130 🕑 clock2 🕝 clock230 🕒 clock3 🕞 clock330 🕓 clock4 🕟 clock430 🕔 clock5 🕠 clock530 🕕 clock6 🕡 clock630 🕖 clock7 🕢 clock730 🕗 clock8 🕣 clock830 🕘 clock9 🕤 clock930 🕙 clock10 🕥 clock1030 🕚 clock11 🕦 clock1130 ","date":"2019-10-01","objectID":"/emoji-support/:5:10","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"天空与天气 图标 代码 图标 代码 🌑 new_moon 🌒 waxing_crescent_moon 🌓 first_quarter_moon 🌔 moon waxing_gibbous_moon 🌕 full_moon 🌖 waning_gibbous_moon 🌗 last_quarter_moon 🌘 waning_crescent_moon 🌙 crescent_moon 🌚 new_moon_with_face 🌛 first_quarter_moon_with_face 🌜 last_quarter_moon_with_face 🌡️ thermometer ☀️ sunny 🌝 full_moon_with_face 🌞 sun_with_face ⭐ star 🌟 star2 🌠 stars 🌌 milky_way ☁️ cloud ⛅ partly_sunny ⛈ cloud_with_lightning_and_rain 🌤 sun_behind_small_cloud 🌥 sun_behind_large_cloud 🌦 sun_behind_rain_cloud 🌧 cloud_with_rain 🌨 cloud_with_snow 🌩 cloud_with_lightning 🌪️ tornado 🌫️ fog 🌬 wind_face 🌀 cyclone 🌈 rainbow 🌂 closed_umbrella ☂️ open_umbrella ☂️ umbrella ⛱️ parasol_on_ground ⚡ zap ❄️ snowflake ☃️ snowman_with_snow ☃️ snowman ☄️ comet 🔥 fire 💧 droplet 🌊 ocean ","date":"2019-10-01","objectID":"/emoji-support/:5:11","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"活动 ","date":"2019-10-01","objectID":"/emoji-support/:6:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"事件 图标 代码 图标 代码 🎃 jack_o_lantern 🎄 christmas_tree 🎆 fireworks 🎇 sparkler ✨ sparkles 🎈 balloon 🎉 tada 🎊 confetti_ball 🎋 tanabata_tree 🎍 bamboo 🎎 dolls 🎏 flags 🎐 wind_chime 🎑 rice_scene 🎀 ribbon 🎁 gift 🎗️ reminder_ribbon 🎟 tickets 🎫 ticket ","date":"2019-10-01","objectID":"/emoji-support/:6:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"奖杯与奖牌 图标 代码 图标 代码 🎖️ medal_military 🏆 trophy 🏅 medal_sports 🥇 1st_place_medal 🥈 2nd_place_medal 🥉 3rd_place_medal ","date":"2019-10-01","objectID":"/emoji-support/:6:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"体育运动 图标 代码 图标 代码 ⚽ soccer ⚾ baseball 🏀 basketball 🏐 volleyball 🏈 football 🏉 rugby_football 🎾 tennis 🎳 bowling 🦗 cricket 🏑 field_hockey 🏒 ice_hockey 🏓 ping_pong 🏸 badminton 🥊 boxing_glove 🥋 martial_arts_uniform 🥅 goal_net ⛳ golf ⛸️ ice_skate 🎣 fishing_pole_and_fish 🎽 running_shirt_with_sash 🎿 ski ","date":"2019-10-01","objectID":"/emoji-support/:6:3","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"游戏 图标 代码 图标 代码 🎯 dart 🎱 8ball 🔮 crystal_ball 🎮 video_game 🕹️ joystick 🎰 slot_machine 🎲 game_die ♠️ spades ♥️ hearts ♦️ diamonds ♣️ clubs 🃏 black_joker 🀄 mahjong 🎴 flower_playing_cards ","date":"2019-10-01","objectID":"/emoji-support/:6:4","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"艺术与工艺 图标 代码 图标 代码 🎭 performing_arts 🖼 framed_picture 🎨 art ","date":"2019-10-01","objectID":"/emoji-support/:6:5","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"物品 ","date":"2019-10-01","objectID":"/emoji-support/:7:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"服装 图标 代码 图标 代码 👓 eyeglasses 🕶️ dark_sunglasses 👔 necktie 👕 shirt tshirt 👖 jeans 👗 dress 👘 kimono 👙 bikini 👚 womans_clothes 👛 purse 👜 handbag 👝 pouch 🛍️ shopping 🎒 school_satchel 👞 mans_shoe shoe 👟 athletic_shoe 👠 high_heel 👡 sandal 👢 boot 👑 crown 👒 womans_hat 🎩 tophat 🎓 mortar_board ⛑️ rescue_worker_helmet 📿 prayer_beads 💄 lipstick 💍 ring 💎 gem ","date":"2019-10-01","objectID":"/emoji-support/:7:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"声音 图标 代码 图标 代码 🔇 mute 🔈 speaker 🔉 sound 🔊 loud_sound 📢 loudspeaker 📣 mega 📯 postal_horn 🔔 bell 🔕 no_bell ","date":"2019-10-01","objectID":"/emoji-support/:7:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"音乐 图标 代码 图标 代码 🎼 musical_score 🎵 musical_note 🎶 notes 🎙️ studio_microphone 🎚️ level_slider 🎛️ control_knobs 🎤 microphone 🎧 headphones 📻 radio ","date":"2019-10-01","objectID":"/emoji-support/:7:3","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"乐器 图标 代码 图标 代码 🎷 saxophone 🎸 guitar 🎹 musical_keyboard 🎺 trumpet 🎻 violin 🥁 drum ","date":"2019-10-01","objectID":"/emoji-support/:7:4","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"电话 图标 代码 图标 代码 📱 iphone 📲 calling ☎️ phone telephone 📞 telephone_receiver 📟 pager 📠 fax ","date":"2019-10-01","objectID":"/emoji-support/:7:5","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"电脑 图标 代码 图标 代码 🔋 battery 🔌 electric_plug 💻 computer 🖥️ desktop_computer 🖨️ printer ⌨️ keyboard 🖱 computer_mouse 🖲️ trackball 💽 minidisc 💾 floppy_disk 💿 cd 📀 dvd ","date":"2019-10-01","objectID":"/emoji-support/:7:6","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"灯光与影像 图标 代码 图标 代码 🎥 movie_camera 🎞️ film_strip 📽️ film_projector 🎬 clapper 📺 tv 📷 camera 📸 camera_flash 📹 video_camera 📼 vhs 🔍 mag 🔎 mag_right 🕯️ candle 💡 bulb 🔦 flashlight 🏮 izakaya_lantern lantern ","date":"2019-10-01","objectID":"/emoji-support/:7:7","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"书与纸张 图标 代码 图标 代码 📔 notebook_with_decorative_cover 📕 closed_book 📖 book open_book 📗 green_book 📘 blue_book 📙 orange_book 📚 books 📓 notebook 📒 ledger 📃 page_with_curl 📜 scroll 📄 page_facing_up 📰 newspaper 🗞️ newspaper_roll 📑 bookmark_tabs 🔖 bookmark 🏷️ label ","date":"2019-10-01","objectID":"/emoji-support/:7:8","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"钱 图标 代码 图标 代码 💰 moneybag 💴 yen 💵 dollar 💶 euro 💷 pound 💸 money_with_wings 💳 credit_card 💹 chart ","date":"2019-10-01","objectID":"/emoji-support/:7:9","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"邮件 图标 代码 图标 代码 ✉️ email envelope 📧 📧 📨 incoming_envelope 📩 envelope_with_arrow 📤 outbox_tray 📥 inbox_tray 📦 package 📫 mailbox 📪 mailbox_closed 📬 mailbox_with_mail 📭 mailbox_with_no_mail 📮 postbox 🗳 ballot_box ","date":"2019-10-01","objectID":"/emoji-support/:7:10","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"书写 图标 代码 图标 代码 ✏️ pencil2 ✒️ black_nib 🖋 fountain_pen 🖊 pen 🖌 paintbrush 🖍 crayon 📝 memo pencil ","date":"2019-10-01","objectID":"/emoji-support/:7:11","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"办公 图标 代码 图标 代码 💼 briefcase 📁 file_folder 📂 open_file_folder 🗂️ card_index_dividers 📅 date 📆 calendar 🗒 spiral_notepad 🗓 spiral_calendar 📇 card_index 📈 chart_with_upwards_trend 📉 chart_with_downwards_trend 📊 bar_chart 📋 clipboard 📌 pushpin 📍 round_pushpin 📎 paperclip 🖇 paperclips 📏 straight_ruler 📐 triangular_ruler ✂️ scissors 🗃️ card_file_box 🗄️ file_cabinet 🗑️ wastebasket ","date":"2019-10-01","objectID":"/emoji-support/:7:12","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"锁 图标 代码 图标 代码 🔒 lock 🔓 unlock 🔏 lock_with_ink_pen 🔐 closed_lock_with_key 🔑 key 🗝️ old_key ","date":"2019-10-01","objectID":"/emoji-support/:7:13","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"工具 图标 代码 图标 代码 🔨 hammer ⛏️ pick ⚒️ hammer_and_pick 🛠️ hammer_and_wrench 🗡 dagger ⚔️ crossed_swords 🔫 gun 🏹 bow_and_arrow 🛡️ shield 🔧 wrench 🔩 nut_and_bolt ⚙️ gear 🗜 clamp ⚖ balance_scale 🔗 link ⛓️ chains ","date":"2019-10-01","objectID":"/emoji-support/:7:14","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"科学 图标 代码 图标 代码 ⚗️ alembic 🔬 microscope 🔭 telescope 🛰️ satellite ","date":"2019-10-01","objectID":"/emoji-support/:7:15","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"医疗 图标 代码 图标 代码 💉 syringe 💊 pill ","date":"2019-10-01","objectID":"/emoji-support/:7:16","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"生活用品 图标 代码 图标 代码 🚪 door 🛏️ bed 🛋️ couch_and_lamp 🚽 toilet 🚿 shower 🛁 bathtub 🛒 shopping_cart ","date":"2019-10-01","objectID":"/emoji-support/:7:17","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"其它物品 图标 代码 图标 代码 🚬 smoking ⚰️ coffin ⚱️ funeral_urn 🗿 moyai ","date":"2019-10-01","objectID":"/emoji-support/:7:18","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"符号 ","date":"2019-10-01","objectID":"/emoji-support/:8:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"交通标识 图标 代码 图标 代码 🏧 atm 🚮 put_litter_in_its_place 🚰 potable_water ♿ wheelchair 🚹 mens 🚺 womens 🚻 restroom 🚼 baby_symbol 🚾 wc 🛂 passport_control 🛃 customs 🛄 baggage_claim 🛅 left_luggage ","date":"2019-10-01","objectID":"/emoji-support/:8:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"警告 图标 代码 图标 代码 ⚠️ warning 🚸 children_crossing ⛔ no_entry 🚫 no_entry_sign 🚳 no_bicycles 🚭 no_smoking 🚯 do_not_litter 🚱 🚱 🚷 no_pedestrians 📵 no_mobile_phones 🔞 underage ☢ radioactive ☣ biohazard ","date":"2019-10-01","objectID":"/emoji-support/:8:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"箭头 图标 代码 图标 代码 ⬆️ arrow_up ↗️ arrow_upper_right ➡️ arrow_right ↘️ arrow_lower_right ⬇️ arrow_down ↙️ arrow_lower_left ⬅️ arrow_left ↖️ arrow_upper_left ↕️ arrow_up_down ↔️ left_right_arrow ↩️ leftwards_arrow_with_hook ↪️ arrow_right_hook ⤴️ arrow_heading_up ⤵️ arrow_heading_down 🔃 arrows_clockwise 🔄 arrows_counterclockwise 🔙 back 🔚 end 🔛 on 🔜 soon 🔝 top ","date":"2019-10-01","objectID":"/emoji-support/:8:3","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"宗教 图标 代码 图标 代码 🛐 place_of_worship ⚛️ atom_symbol 🕉 om ✡️ star_of_david ☸️ wheel_of_dharma ☯️ yin_yang ✝️ latin_cross ☦️ orthodox_cross ☪️ star_and_crescent ☮️ peace_symbol 🕎 menorah 🔯 six_pointed_star ","date":"2019-10-01","objectID":"/emoji-support/:8:4","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"生肖 图标 代码 图标 代码 ♈ aries ♉ taurus ♊ gemini ♋ cancer ♌ leo ♍ virgo ♎ libra ♏ scorpius ♐ sagittarius ♑ capricorn ♒ aquarius ♓ pisces ⛎ ophiuchus ","date":"2019-10-01","objectID":"/emoji-support/:8:5","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"影像符号 图标 代码 图标 代码 🔀 twisted_rightwards_arrows 🔁 repeat 🔂 repeat_one ▶️ arrow_forward ⏩ fast_forward ⏭ next_track_button ⏯ play_or_pause_button ◀️ arrow_backward ⏪ rewind ⏮️ previous_track_button 🔼 arrow_up_small ⏫ arrow_double_up 🔽 arrow_down_small ⏬ arrow_double_down ⏸ pause_button ⏹ stop_button ⏺ record_button 🎦 cinema 🔅 low_brightness 🔆 high_brightness 📶 signal_strength 📳 vibration_mode 📴 mobile_phone_off ","date":"2019-10-01","objectID":"/emoji-support/:8:6","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"数学 图标 代码 图标 代码 ✖️ heavy_multiplication_x ➕ heavy_plus_sign ➖ heavy_minus_sign ➗ heavy_division_sign ","date":"2019-10-01","objectID":"/emoji-support/:8:7","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"标点符号 图标 代码 图标 代码 ‼️ bangbang ⁉️ interrobang ❓ question ❔ grey_question ❕ grey_exclamation ❗ exclamation heavy_exclamation_mark 〰️ wavy_dash ","date":"2019-10-01","objectID":"/emoji-support/:8:8","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"货币 图标 代码 图标 代码 💱 currency_exchange 💲 heavy_dollar_sign ","date":"2019-10-01","objectID":"/emoji-support/:8:9","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"按键符号 图标 代码 图标 代码 #️⃣ hash *️⃣ asterisk 0️⃣ zero 1️⃣ one 2️⃣ two 3️⃣ three 4️⃣ four 5️⃣ five 6️⃣ six 7️⃣ seven 8️⃣ eight 9️⃣ nine 🔟 keycap_ten ","date":"2019-10-01","objectID":"/emoji-support/:8:10","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"字母符号 图标 代码 图标 代码 🔠 capital_abcd 🔡 abcd 🔢 1234 🔣 symbols 🔤 abc 🅰️ a 🆎 ab 🅱️ b 🆑 cl 🆒 cool 🆓 free ℹ️ information_source 🆔 id ⓜ️ m 🆕 new 🆖 ng 🅾️ o2 🆗 ok 🅿️ parking 🆘 sos 🆙 up 🆚 vs 🈁 koko 🈂️ sa 🈷️ u6708 🈶 u6709 🈯 u6307 🉐 ideograph_advantage 🈹 u5272 🈚 u7121 🈲 u7981 🉑 accept 🈸 u7533 🈴 u5408 🈳 u7a7a ㊗️ congratulations ㊙️ secret 🈺 u55b6 🈵 u6e80 ","date":"2019-10-01","objectID":"/emoji-support/:8:11","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"几何符号 图标 代码 图标 代码 🔴 red_circle 🔵 large_blue_circle ⚫ black_circle ⚪ white_circle ⬛ black_large_square ⬜ white_large_square ◼️ black_medium_square ◻️ white_medium_square ◾ black_medium_small_square ◽ white_medium_small_square ▪️ black_small_square ▫️ white_small_square 🔶 large_orange_diamond 🔷 large_blue_diamond 🔸 small_orange_diamond 🔹 small_blue_diamond 🔺 small_red_triangle 🔻 small_red_triangle_down 💠 diamond_shape_with_a_dot_inside 🔘 radio_button 🔳 white_square_button 🔲 black_square_button ","date":"2019-10-01","objectID":"/emoji-support/:8:12","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"其它符合 图标 代码 图标 代码 ♻️ recycle ⚜️ fleur_de_lis 🔱 trident 📛 name_badge 🔰 beginner ⭕ o ✅ white_check_mark ☑️ ballot_box_with_check ✔️ heavy_check_mark ❌ x ❎ negative_squared_cross_mark ➰ curly_loop ➿ loop 〽️ part_alternation_mark ✳️ eight_spoked_asterisk ✴️ eight_pointed_black_star ❇️ sparkle ©️ copyright ®️ registered ™️ tm ","date":"2019-10-01","objectID":"/emoji-support/:8:13","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"旗帜 ","date":"2019-10-01","objectID":"/emoji-support/:9:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"常用旗帜 图标 代码 图标 代码 🏁 checkered_flag 🚩 triangular_flag_on_post 🎌 crossed_flags 🏴 black_flag 🏳 white_flag 🏳️‍🌈 rainbow_flag ","date":"2019-10-01","objectID":"/emoji-support/:9:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"国家和地区旗帜 图标 代码 图标 代码 🇦🇩 andorra 🇦🇪 united_arab_emirates 🇦🇫 afghanistan 🇦🇬 antigua_barbuda 🇦🇮 anguilla 🇦🇱 albania 🇦🇲 armenia 🇦🇴 angola 🇦🇶 antarctica 🇦🇷 argentina 🇦🇸 american_samoa 🇦🇹 austria 🇦🇺 australia 🇦🇼 aruba 🇦🇽 aland_islands 🇦🇿 azerbaijan 🇧🇦 bosnia_herzegovina 🇧🇧 barbados 🇧🇩 bangladesh 🇧🇪 belgium 🇧🇫 burkina_faso 🇧🇬 bulgaria 🇧🇭 bahrain 🇧🇮 burundi 🇧🇯 benin 🇧🇱 st_barthelemy 🇧🇲 bermuda 🇧🇳 brunei 🇧🇴 bolivia 🇧🇶 caribbean_netherlands 🇧🇷 brazil 🇧🇸 bahamas 🇧🇹 bhutan 🇧🇼 botswana 🇧🇾 belarus 🇧🇿 belize 🇨🇦 canada 🇨🇨 cocos_islands 🇨🇩 congo_kinshasa 🇨🇫 central_african_republic 🇨🇬 congo_brazzaville 🇨🇭 switzerland 🇨🇮 cote_divoire 🇨🇰 cook_islands 🇨🇱 chile 🇨🇲 cameroon 🇨🇳 cn 🇨🇴 colombia 🇨🇷 costa_rica 🇨🇺 cuba 🇨🇻 cape_verde 🇨🇼 curacao 🇨🇽 christmas_island 🇨🇾 cyprus 🇨🇿 czech_republic 🇩🇪 de 🇩🇯 djibouti 🇩🇰 denmark 🇩🇲 dominica 🇩🇴 dominican_republic 🇩🇿 algeria 🇪🇨 ecuador 🇪🇪 estonia 🇪🇬 egypt 🇪🇭 western_sahara 🇪🇷 eritrea 🇪🇸 es 🇪🇹 ethiopia 🇪🇺 eu european_union 🇫🇮 finland 🇫🇯 fiji 🇫🇰 falkland_islands 🇫🇲 micronesia 🇫🇴 faroe_islands 🇫🇷 fr 🇬🇦 gabon 🇬🇧 gb uk 🇬🇩 grenada 🇬🇪 georgia 🇬🇫 french_guiana 🇬🇬 guernsey 🇬🇭 ghana 🇬🇮 gibraltar 🇬🇱 greenland 🇬🇲 gambia 🇬🇳 guinea 🇬🇵 guadeloupe 🇬🇶 equatorial_guinea 🇬🇷 greece 🇬🇸 south_georgia_south_sandwich_islands 🇬🇹 guatemala 🇬🇺 guam 🇬🇼 guinea_bissau 🇬🇾 guyana 🇭🇰 hong_kong 🇭🇳 honduras 🇭🇷 croatia 🇭🇹 haiti 🇭🇺 hungary 🇮🇨 canary_islands 🇮🇩 indonesia 🇮🇪 ireland 🇮🇱 israel 🇮🇲 isle_of_man 🇮🇳 india 🇮🇴 british_indian_ocean_territory 🇮🇶 iraq 🇮🇷 iran 🇮🇸 iceland 🇮🇹 it 🇯🇪 jersey 🇯🇲 jamaica 🇯🇴 jordan 🇯🇵 jp 🇰🇪 kenya 🇰🇬 kyrgyzstan 🇰🇭 cambodia 🇰🇮 kiribati 🇰🇲 comoros 🇰🇳 st_kitts_nevis 🇰🇵 north_korea 🇰🇷 kr 🇰🇼 kuwait 🇰🇾 cayman_islands 🇰🇿 kazakhstan 🇱🇦 laos 🇱🇧 lebanon 🇱🇨 st_lucia 🇱🇮 liechtenstein 🇱🇰 sri_lanka 🇱🇷 liberia 🇱🇸 lesotho 🇱🇹 lithuania 🇱🇺 luxembourg 🇱🇻 latvia 🇱🇾 libya 🇲🇦 morocco 🇲🇨 monaco 🇲🇩 moldova 🇲🇪 montenegro 🇲🇬 madagascar 🇲🇭 marshall_islands 🇲🇰 macedonia 🇲🇱 mali 🇲🇲 myanmar 🇲🇳 mongolia 🇲🇴 macau 🇲🇵 northern_mariana_islands 🇲🇶 martinique 🇲🇷 mauritania 🇲🇸 montserrat 🇲🇹 malta 🇲🇺 mauritius 🇲🇻 maldives 🇲🇼 malawi 🇲🇽 mexico 🇲🇾 malaysia 🇲🇿 mozambique 🇳🇦 namibia 🇳🇨 new_caledonia 🇳🇪 niger 🇳🇫 norfolk_island 🇳🇬 nigeria 🇳🇮 nicaragua 🇳🇱 netherlands 🇳🇴 norway 🇳🇵 nepal 🇳🇷 nauru 🇳🇺 niue 🇳🇿 new_zealand 🇴🇲 oman 🇵🇦 panama 🇵🇪 peru 🇵🇫 french_polynesia 🇵🇬 papua_new_guinea 🇵🇭 philippines 🇵🇰 pakistan 🇵🇱 poland 🇵🇲 st_pierre_miquelon 🇵🇳 pitcairn_islands 🇵🇷 puerto_rico 🇵🇸 palestinian_territories 🇵🇹 portugal 🇵🇼 palau 🇵🇾 paraguay 🇶🇦 qatar 🇷🇪 reunion 🇷🇴 romania 🇷🇸 serbia 🇷🇺 ru 🇷🇼 rwanda 🇸🇦 saudi_arabia 🇸🇧 solomon_islands 🇸🇨 seychelles 🇸🇩 sudan 🇸🇪 sweden 🇸🇬 singapore 🇸🇭 st_helena 🇸🇮 slovenia 🇸🇰 slovakia 🇸🇱 sierra_leone 🇸🇲 san_marino 🇸🇳 senegal 🇸🇴 somalia 🇸🇷 suriname 🇸🇸 south_sudan 🇸🇹 sao_tome_principe 🇸🇻 el_salvador 🇸🇽 sint_maarten 🇸🇾 syria 🇸🇿 swaziland 🇹🇨 turks_caicos_islands 🇹🇩 chad 🇹🇫 french_southern_territories 🇹🇬 togo 🇹🇭 thailand 🇹🇯 tajikistan 🇹🇰 tokelau 🇹🇱 timor_leste 🇹🇲 turkmenistan 🇹🇳 tunisia 🇹🇴 tonga 🇹🇷 tr 🇹🇹 trinidad_tobago 🇹🇻 tuvalu 🇹🇼 taiwan 🇹🇿 tanzania 🇺🇦 ukraine 🇺🇬 uganda 🇺🇸 us 🇺🇾 uruguay 🇺🇿 uzbekistan 🇻🇦 vatican_city 🇻🇨 st_vincent_grenadines 🇻🇪 venezuela 🇻🇬 british_virgin_islands 🇻🇮 us_virgin_islands 🇻🇳 vietnam 🇻🇺 vanuatu 🇼🇫 wallis_futuna 🇼🇸 samoa 🇽🇰 kosovo 🇾🇪 yemen 🇾🇹 mayotte 🇿🇦 south_africa 🇿🇲 zambia 🇿🇼 zimbabwe ","date":"2019-10-01","objectID":"/emoji-support/:9:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["documentation"],"content":"echarts shortcode 使用 ECharts 库提供数据可视化的功能.","date":"2020-03-03","objectID":"/theme-documentation-echarts-shortcode/","tags":["shortcodes"],"title":"主题文档 - echarts Shortcode","uri":"/theme-documentation-echarts-shortcode/"},{"categories":["documentation"],"content":"echarts shortcode 使用 ECharts 库提供数据可视化的功能. ECharts 是一个帮助你生成交互式数据可视化的库. ECharts 提供了常规的 折线图, 柱状图, 散点图, 饼图, K线图, 用于统计的 盒形图, 用于地理数据可视化的 地图, 热力图, 线图, 用于关系数据可视化的 关系图, treemap, 旭日图, 多维数据可视化的 平行坐标, 还有用于 BI 的 漏斗图, 仪表盘, 并且支持图与图之间的混搭. 只需在 echarts shortcode 中以 JSON/YAML/TOML格式插入 ECharts 选项即可. 一个 JSON 格式的 echarts 示例: {{\u003c echarts \u003e}} { \"title\": { \"text\": \"折线统计图\", \"top\": \"2%\", \"left\": \"center\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"邮件营销\", \"联盟广告\", \"视频广告\", \"直接访问\", \"搜索引擎\"], \"top\": \"10%\" }, \"grid\": { \"left\": \"5%\", \"right\": \"5%\", \"bottom\": \"5%\", \"top\": \"20%\", \"containLabel\": true }, \"toolbox\": { \"feature\": { \"saveAsImage\": { \"title\": \"保存为图片\" } } }, \"xAxis\": { \"type\": \"category\", \"boundaryGap\": false, \"data\": [\"周一\", \"周二\", \"周三\", \"周四\", \"周五\", \"周六\", \"周日\"] }, \"yAxis\": { \"type\": \"value\" }, \"series\": [ { \"name\": \"邮件营销\", \"type\": \"line\", \"stack\": \"总量\", \"data\": [120, 132, 101, 134, 90, 230, 210] }, { \"name\": \"联盟广告\", \"type\": \"line\", \"stack\": \"总量\", \"data\": [220, 182, 191, 234, 290, 330, 310] }, { \"name\": \"视频广告\", \"type\": \"line\", \"stack\": \"总量\", \"data\": [150, 232, 201, 154, 190, 330, 410] }, { \"name\": \"直接访问\", \"type\": \"line\", \"stack\": \"总量\", \"data\": [320, 332, 301, 334, 390, 330, 320] }, { \"name\": \"搜索引擎\", \"type\": \"line\", \"stack\": \"总量\", \"data\": [820, 932, 901, 934, 1290, 1330, 1320] } ] } {{\u003c /echarts \u003e}} 一个 YAML 格式的 echarts 示例: {{\u003c echarts \u003e}} title: text: 折线统计图 top: 2% left: center tooltip: trigger: axis legend: data: - 邮件营销 - 联盟广告 - 视频广告 - 直接访问 - 搜索引擎 top: 10% grid: left: 5% right: 5% bottom: 5% top: 20% containLabel: true toolbox: feature: saveAsImage: title: 保存为图片 xAxis: type: category boundaryGap: false data: - 周一 - 周二 - 周三 - 周四 - 周五 - 周六 - 周日 yAxis: type: value series: - name: 邮件营销 type: line stack: 总量 data: - 120 - 132 - 101 - 134 - 90 - 230 - 210 - name: 联盟广告 type: line stack: 总量 data: - 220 - 182 - 191 - 234 - 290 - 330 - 310 - name: 视频广告 type: line stack: 总量 data: - 150 - 232 - 201 - 154 - 190 - 330 - 410 - name: 直接访问 type: line stack: 总量 data: - 320 - 332 - 301 - 334 - 390 - 330 - 320 - name: 搜索引擎 type: line stack: 总量 data: - 820 - 932 - 901 - 934 - 1290 - 1330 - 1320 {{\u003c /echarts \u003e}} 一个 TOML 格式的 echarts 示例: {{\u003c echarts \u003e}} [title] text = \"折线统计图\" top = \"2%\" left = \"center\" [tooltip] trigger = \"axis\" [legend] data = [ \"邮件营销\", \"联盟广告\", \"视频广告\", \"直接访问\", \"搜索引擎\" ] top = \"10%\" [grid] left = \"5%\" right = \"5%\" bottom = \"5%\" top = \"20%\" containLabel = true [toolbox] [toolbox.feature] [toolbox.feature.saveAsImage] title = \"保存为图片\" [xAxis] type = \"category\" boundaryGap = false data = [ \"周一\", \"周二\", \"周三\", \"周四\", \"周五\", \"周六\", \"周日\" ] [yAxis] type = \"value\" [[series]] name = \"邮件营销\" type = \"line\" stack = \"总量\" data = [ 120.0, 132.0, 101.0, 134.0, 90.0, 230.0, 210.0 ] [[series]] name = \"联盟广告\" type = \"line\" stack = \"总量\" data = [ 220.0, 182.0, 191.0, 234.0, 290.0, 330.0, 310.0 ] [[series]] name = \"视频广告\" type = \"line\" stack = \"总量\" data = [ 150.0, 232.0, 201.0, 154.0, 190.0, 330.0, 410.0 ] [[series]] name = \"直接访问\" type = \"line\" stack = \"总量\" data = [ 320.0, 332.0, 301.0, 334.0, 390.0, 330.0, 320.0 ] [[series]] name = \"搜索引擎\" type = \"line\" stack = \"总量\" data = [ 820.0, 932.0, 901.0, 934.0, 1290.0, 1330.0, 1320.0 ] {{\u003c /echarts \u003e}} 呈现的输出效果如下: echarts shortcode 还有以下命名参数: width [可选] (第一个位置参数) 数据可视化的宽度, 默认值是 100%. height [可选] (第二个位置参数) 数据可视化的高度, 默认值是 30rem. ","date":"2020-03-03","objectID":"/theme-documentation-echarts-shortcode/:0:0","tags":["shortcodes"],"title":"主题文档 - echarts Shortcode","uri":"/theme-documentation-echarts-shortcode/"},{"categories":["documentation"],"content":"mapbox shortcode 使用 Mapbox GL JS 库提供互动式地图的功能.","date":"2020-03-03","objectID":"/theme-documentation-mapbox-shortcode/","tags":["shortcodes"],"title":"主题文档 - mapbox Shortcode","uri":"/theme-documentation-mapbox-shortcode/"},{"categories":["documentation"],"content":" mapbox shortcode 使用 Mapbox GL JS 库提供互动式地图的功能. Mapbox GL JS 是一个 JavaScript 库，它使用 WebGL, 以 vector tiles 和 Mapbox styles 为来源, 将它们渲染成互动式地图. mapbox shortcode 有以下命名参数来使用 Mapbox GL JS: lng [必需] (第一个位置参数) 地图初始中心点的经度, 以度为单位. lat [必需] (第二个位置参数) 地图初始中心点的纬度, 以度为单位. zoom [可选] (第三个位置参数) 地图的初始缩放级别, 默认值是 10. marked [可选] (第四个位置参数) 是否在地图的初始中心点添加图钉, 默认值是 true. light-style [可选] (第五个位置参数) 浅色主题的地图样式, 默认值是前置参数或者网站配置中设置的值. dark-style [可选] (第六个位置参数) 深色主题的地图样式, 默认值是前置参数或者网站配置中设置的值. navigation [可选] 是否添加 NavigationControl, 默认值是前置参数或者网站配置中设置的值. geolocate [可选] 是否添加 GeolocateControl, 默认值是前置参数或者网站配置中设置的值. scale [可选] 是否添加 ScaleControl, 默认值是前置参数或者网站配置中设置的值. fullscreen [可选] 是否添加 FullscreenControl, 默认值是前置参数或者网站配置中设置的值. width [可选] 地图的宽度, 默认值是 100%. height [可选] 地图的高度, 默认值是 20rem. 一个简单的 mapbox 示例: {{\u003c mapbox 121.485 31.233 12 \u003e}} 或者 {{\u003c mapbox lng=121.485 lat=31.233 zoom=12 \u003e}} 呈现的输出效果如下: 一个带有自定义样式的 mapbox 示例: {{\u003c mapbox -122.252 37.453 10 false \"mapbox://styles/mapbox/streets-zh-v1?optimize=true\" \u003e}} 或者 {{\u003c mapbox lng=-122.252 lat=37.453 zoom=10 marked=false light-style=\"mapbox://styles/mapbox/streets-zh-v1?optimize=true\" \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-mapbox-shortcode/:0:0","tags":["shortcodes"],"title":"主题文档 - mapbox Shortcode","uri":"/theme-documentation-mapbox-shortcode/"},{"categories":["documentation"],"content":"music shortcode 基于 APlayer 和 MetingJS 库提供了一个内嵌的响应式音乐播放器.","date":"2020-03-03","objectID":"/theme-documentation-music-shortcode/","tags":["shortcodes"],"title":"主题文档 - music Shortcode","uri":"/theme-documentation-music-shortcode/"},{"categories":["documentation"],"content":"music shortcode 基于 APlayer 和 MetingJS 库提供了一个内嵌的响应式音乐播放器. 有三种方式使用 music shortcode. ","date":"2020-03-03","objectID":"/theme-documentation-music-shortcode/:0:0","tags":["shortcodes"],"title":"主题文档 - music Shortcode","uri":"/theme-documentation-music-shortcode/"},{"categories":["documentation"],"content":"1 自定义音乐 URL 支持本地资源引用的完整用法. music shortcode 有以下命名参数来使用自定义音乐 URL: server [必需] 音乐的链接. type [可选] 音乐的名称. artist [可选] 音乐的创作者. cover [可选] 音乐的封面链接. 一个使用自定义音乐 URL 的 music 示例: {{\u003c music url=\"/music/Wavelength.mp3\" name=Wavelength artist=oldmanyoung cover=\"/images/Wavelength.jpg\" \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-music-shortcode/:1:0","tags":["shortcodes"],"title":"主题文档 - music Shortcode","uri":"/theme-documentation-music-shortcode/"},{"categories":["documentation"],"content":"2 音乐平台 URL 的自动识别 music shortcode 有一个命名参数来使用音乐平台 URL 的自动识别: auto [必需]] (第一个位置参数) 用来自动识别的音乐平台 URL, 支持 netease, tencent 和 xiami 平台. 一个使用音乐平台 URL 的自动识别的 music 示例: {{\u003c music auto=\"https://music.163.com/#/playlist?id=60198\" \u003e}} 或者 {{\u003c music \"https://music.163.com/#/playlist?id=60198\" \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-music-shortcode/:2:0","tags":["shortcodes"],"title":"主题文档 - music Shortcode","uri":"/theme-documentation-music-shortcode/"},{"categories":["documentation"],"content":"3 自定义音乐平台, 类型和 ID music shortcode 有以下命名参数来使用自定义音乐平台: server [必需] (第一个位置参数) [netease, tencent, kugou, xiami, baidu] 音乐平台. type [必需] (第二个位置参数) [song, playlist, album, search, artist] 音乐类型. id [必需] (第三个位置参数) 歌曲 ID, 或者播放列表 ID, 或者专辑 ID, 或者搜索关键词, 或者创作者 ID. 一个使用自定义音乐平台的 music 示例: {{\u003c music server=\"netease\" type=\"song\" id=\"1868553\" \u003e}} 或者 {{\u003c music netease song 1868553 \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-music-shortcode/:3:0","tags":["shortcodes"],"title":"主题文档 - music Shortcode","uri":"/theme-documentation-music-shortcode/"},{"categories":["documentation"],"content":"4 其它参数 music shortcode 有一些可以应用于以上三种方式的其它命名参数: theme [可选] 音乐播放器的主题色, 默认值是 #448aff. fixed [可选] 是否开启固定模式, 默认值是 false. mini [可选] 是否开启迷你模式, 默认值是 false. autoplay [可选] 是否自动播放音乐, 默认值是 false. volume [可选] 第一次打开播放器时的默认音量, 会被保存在浏览器缓存中, 默认值是 0.7. mutex [可选] 是否自动暂停其它播放器, 默认值是 true. music shortcode 还有一些只适用于音乐列表方式的其它命名参数: loop [可选] [all, one, none] 音乐列表的循环模式, 默认值是 none. order [可选] [list, random] 音乐列表的播放顺序, 默认值是 list. list-folded [可选] 初次打开的时候音乐列表是否折叠, 默认值是 false. list-max-height [可选] 音乐列表的最大高度, 默认值是 340px. ","date":"2020-03-03","objectID":"/theme-documentation-music-shortcode/:4:0","tags":["shortcodes"],"title":"主题文档 - music Shortcode","uri":"/theme-documentation-music-shortcode/"},{"categories":["documentation"],"content":"bilibili shortcode 提供了一个内嵌的用来播放 bilibili 视频的响应式播放器.","date":"2020-03-03","objectID":"/theme-documentation-bilibili-shortcode/","tags":["shortcodes"],"title":"主题文档 - bilibili Shortcode","uri":"/theme-documentation-bilibili-shortcode/"},{"categories":["documentation"],"content":" bilibili shortcode 提供了一个内嵌的用来播放 bilibili 视频的响应式播放器. 如果视频只有一个部分, 则仅需要视频的 BV id, 例如: https://www.bilibili.com/video/BV1Sx411T7QQ 一个 bilibili 示例: {{\u003c bilibili BV1Sx411T7QQ \u003e}} 或者 {{\u003c bilibili id=BV1Sx411T7QQ \u003e}} 呈现的输出效果如下: 如果视频包含多个部分, 则除了视频的 BV id 之外, 还需要 p, 默认值为 1, 例如: https://www.bilibili.com/video/BV1TJ411C7An?p=3 一个带有 p 参数的 bilibili 示例: {{\u003c bilibili BV1TJ411C7An 3 \u003e}} 或者 {{\u003c bilibili id=BV1TJ411C7An p=3 \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-bilibili-shortcode/:0:0","tags":["shortcodes"],"title":"主题文档 - bilibili Shortcode","uri":"/theme-documentation-bilibili-shortcode/"},{"categories":["documentation"],"content":"typeit shortcode 基于 TypeIt 库提供了打字动画.","date":"2020-03-03","objectID":"/theme-documentation-typeit-shortcode/","tags":["shortcodes"],"title":"主题文档 - typeit Shortcode","uri":"/theme-documentation-typeit-shortcode/"},{"categories":["documentation"],"content":"typeit shortcode 基于 TypeIt 库提供了打字动画. 只需将你需要打字动画的内容插入 typeit shortcode 中即可. ","date":"2020-03-03","objectID":"/theme-documentation-typeit-shortcode/:0:0","tags":["shortcodes"],"title":"主题文档 - typeit Shortcode","uri":"/theme-documentation-typeit-shortcode/"},{"categories":["documentation"],"content":"1 简单内容 允许使用 Markdown 格式的简单内容, 并且 不包含 富文本的块内容, 例如图像等等… 一个 typeit 示例: {{\u003c typeit \u003e}} 这一个带有基于 [TypeIt](https://typeitjs.com/) 的 **打字动画** 的 *段落*... {{\u003c /typeit \u003e}} 呈现的输出效果如下: 另外, 你也可以自定义 HTML 标签. 一个带有 h4 标签的 typeit 示例: {{\u003c typeit tag=h4 \u003e}} 这一个带有基于 [TypeIt](https://typeitjs.com/) 的 **打字动画** 的 *段落*... {{\u003c /typeit \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-typeit-shortcode/:1:0","tags":["shortcodes"],"title":"主题文档 - typeit Shortcode","uri":"/theme-documentation-typeit-shortcode/"},{"categories":["documentation"],"content":"2 代码内容 代码内容也是允许的, 并且通过使用参数 code 指定语言类型可以实习语法高亮. 一个带有 code 参数的 typeit 示例: {{\u003c typeit code=java \u003e}} public class HelloWorld { public static void main(String []args) { System.out.println(\"Hello World\"); } } {{\u003c /typeit \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-typeit-shortcode/:2:0","tags":["shortcodes"],"title":"主题文档 - typeit Shortcode","uri":"/theme-documentation-typeit-shortcode/"},{"categories":["documentation"],"content":"3 分组内容 默认情况下, 所有打字动画都是同时开始的. 但是有时你可能需要按顺序开始一组 typeit 内容的打字动画. 一组具有相同 group 参数值的 typeit 内容将按顺序开始打字动画. 一个带有 group 参数的 typeit 示例: {{\u003c typeit group=paragraph \u003e}} **首先**, 这个段落开始 {{\u003c /typeit \u003e}} {{\u003c typeit group=paragraph \u003e}} **然后**, 这个段落开始 {{\u003c /typeit \u003e}} 呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-typeit-shortcode/:3:0","tags":["shortcodes"],"title":"主题文档 - typeit Shortcode","uri":"/theme-documentation-typeit-shortcode/"},{"categories":null,"content":"关于 LoveIt","date":"2019-08-02","objectID":"/about/","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"  LoveIt 是一个由  Dillon 开发的简洁、优雅且高效的 Hugo 博客主题。 它的原型基于 LeaveIt 主题 和 KeepIt 主题。 Hugo 主题 LoveIt\r","date":"2019-08-02","objectID":"/about/:0:0","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"特性 ","date":"2019-08-02","objectID":"/about/:1:0","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"性能和 SEO  性能优化：在 Google PageSpeed Insights 中， 99/100 的移动设备得分和 100/100 的桌面设备得分  使用基于 JSON-LD 格式 的 SEO SCHEMA 文件进行 SEO 优化  支持 Google Analytics  支持 Fathom Analytics  支持 Plausible Analytics  支持 Yandex Metrica  支持搜索引擎的网站验证 (Google, Bind, Yandex and Baidu)  支持所有第三方库的 CDN  基于 lazysizes 自动转换图片为懒加载 ","date":"2019-08-02","objectID":"/about/:1:1","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"外观和布局  桌面端/移动端 响应式布局  浅色/深色 主题模式  全局一致的设计语言  支持分页  易用和自动展开的文章目录  支持多语言和国际化  美观的 CSS 动画 社交和评论系统  支持 Gravatar 头像  支持本地头像  支持多达 73 种社交链接  支持多达 24 种网站分享  支持 Disqus 评论系统  支持 Gitalk 评论系统  支持 Valine 评论系统  支持 Facebook comments 评论系统  支持 Telegram comments 评论系统  支持 Commento 评论系统  支持 utterances 评论系统  支持 giscus 评论系统 ","date":"2019-08-02","objectID":"/about/:1:2","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"扩展功能  支持基于 Lunr.js 或 algolia 的搜索  支持 Twemoji  支持代码高亮  一键复制代码到剪贴板  支持基于 lightGallery 的图片画廊  支持 Font Awesome 图标的扩展 Markdown 语法  支持上标注释的扩展 Markdown 语法  支持分数的扩展 Markdown 语法  支持基于 $\\KaTeX$ 的数学公式  支持基于 mermaid 的图表 shortcode  支持基于 ECharts 的交互式数据可视化 shortcode  支持基于 Mapbox GL JS 的 Mapbox shortcode  支持基于 APlayer 和 MetingJS 的音乐播放器 shortcode  支持 Bilibili 视频 shortcode  支持多种注释的 shortcode  支持自定义样式的 shortcode  支持自定义脚本的 shortcode  支持基于 TypeIt 的打字动画 shortcode  支持基于 cookieconsent 的 Cookie 许可横幅  支持人物标签的 shortcode … ","date":"2019-08-02","objectID":"/about/:1:3","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"许可协议 LoveIt 根据 MIT 许可协议授权。 更多信息请查看 LICENSE 文件。 ","date":"2019-08-02","objectID":"/about/:2:0","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"特别感谢 LoveIt 主题中用到了以下项目，感谢它们的作者： normalize.css Font Awesome Simple Icons Animate.css autocomplete Lunr.js algoliasearch lazysizes object-fit-images Twemoji emoji-data lightGallery clipboard.js Sharer.js TypeIt $\\KaTeX$ mermaid ECharts Mapbox GL JS APlayer MetingJS Gitalk Valine cookieconsent ","date":"2019-08-02","objectID":"/about/:3:0","tags":null,"title":"关于 LoveIt","uri":"/about/"}]